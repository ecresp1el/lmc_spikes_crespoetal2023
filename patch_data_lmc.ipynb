{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spiketurnpike_postanalysis/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyabf\n",
      "Version: 2.3.8\n",
      "Summary: Python library for reading files in Axon Binary Format (ABF)\n",
      "Home-page: http://swharden.com/pyabf\n",
      "Author: Scott W Harden\n",
      "Author-email: SWHarden@gmail.com\n",
      "License: MIT License\n",
      "Location: /opt/anaconda3/envs/spiketurnpike_postanalysis/lib/python3.9/site-packages\n",
      "Requires: matplotlib, numpy, pytest\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyabf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiketurnpike_postanalysis.Extract_patch_data_from_abf import PatchDataExtractor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re  # Import the re module for regular expressions\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import pyabf\n",
    "import pyabf.plot\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "    \n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BladePatchDataProcessor:\n",
    "    def __init__(self, base_path, time_units=\"sec\", voltage_units=\"mV\"):\n",
    "        \"\"\"\n",
    "        Initialize the BladePatchDataProcessor with the base path.\n",
    "\n",
    "        Args:\n",
    "            base_path (str): Path to the BLADe_patch_data folder.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path  # Base directory path\n",
    "        self.dataframe = None  # DataFrame to store metadata\n",
    "        self.unique_groups = None  # List of unique group names\n",
    "        self.time_units = time_units\n",
    "        self.voltage_units = voltage_units\n",
    "        self._abf_cache = {}  # Cache for ABF objects to avoid repeated loading, Cache for ABF objects: {(group, recording_id, label): abf_obj}\n",
    "\n",
    "    def process_data(self):\n",
    "        \"\"\"\n",
    "        Process the BLADe_patch_data folder to extract metadata about .abf files.\n",
    "\n",
    "        This method populates the `dataframe` and `unique_groups` attributes.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        # Walk through each group in the base path\n",
    "        for group_dir in os.listdir(self.base_path):\n",
    "            group_path = os.path.join(self.base_path, group_dir)\n",
    "            if not os.path.isdir(group_path):\n",
    "                continue  # Skip non-directory files\n",
    "\n",
    "            # Determine linking pattern based on the group\n",
    "            if group_dir == \"L + CS-Veh\":\n",
    "                identifier_pattern = r\"Veh-\\d+\"\n",
    "            else:\n",
    "                identifier_pattern = r\"CTZ-\\d+\"\n",
    "\n",
    "            # Look for .abf files in the group directory\n",
    "            for file_name in os.listdir(group_path):\n",
    "                if file_name.endswith(\".abf\"):\n",
    "                    # Parse the label (\"Before\" or \"After\")\n",
    "                    if \"Before\" in file_name:\n",
    "                        label = \"Before\"\n",
    "                    elif \"After\" in file_name:\n",
    "                        label = \"After\"\n",
    "                    else:\n",
    "                        continue  # Skip files without \"Before\" or \"After\"\n",
    "\n",
    "                    # Extract linking identifier (e.g., \"CTZ-1\", \"Veh-2\")\n",
    "                    match = re.search(identifier_pattern, file_name)\n",
    "                    if not match:\n",
    "                        print(f\"Warning: No linking identifier found in file {file_name}. Skipping.\")\n",
    "                        continue\n",
    "                    recording_id = match.group(0)\n",
    "\n",
    "                    # Append metadata to the list\n",
    "                    data.append({\n",
    "                        \"Group\": group_dir,\n",
    "                        \"Recording_ID\": recording_id,\n",
    "                        \"Label\": label,\n",
    "                        \"File_Path\": os.path.join(group_path, file_name)\n",
    "                    })\n",
    "\n",
    "        # Convert the list of metadata to a DataFrame\n",
    "        self.dataframe = pd.DataFrame(data)\n",
    "\n",
    "        # Sort the DataFrame by Group and Recording ID for clarity\n",
    "        if not self.dataframe.empty:\n",
    "            self.dataframe = self.dataframe.sort_values(by=[\"Group\", \"Recording_ID\", \"Label\"]).reset_index(drop=True)\n",
    "            # Extract unique group names\n",
    "            self.unique_groups = self.dataframe[\"Group\"].unique().tolist()\n",
    "        else:\n",
    "            self.unique_groups = []\n",
    "\n",
    "    def get_recording_ids(self, group):\n",
    "        \"\"\"\n",
    "        Given a group name, return all unique recording IDs associated with that group.\n",
    "        \"\"\"\n",
    "        group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "        return group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "    def get_abf_file(self, group, recording_id, label):\n",
    "        \"\"\"\n",
    "        Return a pyabf.ABF object for the given group, recording_id, and label.\n",
    "        Uses caching to avoid reloading the file multiple times.\n",
    "        \"\"\"\n",
    "        key = (group, recording_id, label)\n",
    "        if key in self._abf_cache:\n",
    "            return self._abf_cache[key]\n",
    "\n",
    "        entry = self.dataframe[(self.dataframe[\"Group\"] == group) &\n",
    "                               (self.dataframe[\"Recording_ID\"] == recording_id) & \n",
    "                               (self.dataframe[\"Label\"] == label)]\n",
    "        if entry.empty:\n",
    "            raise ValueError(f\"No entry found for Group: {group}, Recording ID: {recording_id} with label: {label}\")\n",
    "\n",
    "        file_path = entry[\"File_Path\"].iloc[0]\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"ABF file not found at: {file_path}\")\n",
    "\n",
    "        abf = pyabf.ABF(file_path)\n",
    "        self._abf_cache[key] = abf\n",
    "        return abf\n",
    "\n",
    "    def get_sweep_data(self, group, recording_id, label, sweep_number, channel=0):\n",
    "        \"\"\"\n",
    "        Retrieve time and voltage arrays for a given sweep from the specified group and recording.\n",
    "        \"\"\"\n",
    "        abf = self.get_abf_file(group, recording_id, label)\n",
    "        abf.setSweep(sweepNumber=sweep_number, channel=channel)\n",
    "        return abf.sweepX, abf.sweepY\n",
    "\n",
    "    def colors_binned(self, count, colormap=\"viridis\", reverse=False):\n",
    "        cmap = plt.get_cmap(colormap)\n",
    "        colors = [cmap(i / count) for i in range(count)]\n",
    "        if reverse:\n",
    "            colors.reverse()\n",
    "        return colors\n",
    "\n",
    "    def plot_sweeps(self, ax, group, recording_id, label, sweep_numbers=None,\n",
    "                    offsetXsec=0.3, offsetYunits=40, startAtSec=0, endAtSec=None,\n",
    "                    color=None, alpha=0.5, linewidth=1, hideAxis=True):\n",
    "        \"\"\"\n",
    "        Plot multiple sweeps from a given group, recording, and label on the given axis.\n",
    "        This version includes parameters for offsetting and time-limiting sweeps,\n",
    "        and the option to hide axes.\n",
    "        \"\"\"\n",
    "        abf = self.get_abf_file(group, recording_id, label)\n",
    "        if sweep_numbers is None:\n",
    "            sweep_numbers = abf.sweepList\n",
    "\n",
    "        data_rate = abf.dataRate\n",
    "        i1 = int(data_rate * startAtSec)\n",
    "        i2 = int(data_rate * endAtSec) if endAtSec else None\n",
    "\n",
    "        # Handle colors\n",
    "        if color is None and len(sweep_numbers) > 1:\n",
    "            colors = self.colors_binned(len(sweep_numbers))\n",
    "        else:\n",
    "            colors = [color] * len(sweep_numbers)\n",
    "\n",
    "        for i, sweep_num in enumerate(sweep_numbers):\n",
    "            time, voltage = self.get_sweep_data(group, recording_id, label, sweep_num)\n",
    "            ax.plot(\n",
    "                time[i1:i2] + offsetXsec * sweep_num,\n",
    "                voltage[i1:i2] + offsetYunits * sweep_num,\n",
    "                color=colors[i] if colors[i] else 'C0',\n",
    "                alpha=alpha,\n",
    "                linewidth=linewidth\n",
    "            )\n",
    "\n",
    "        # Remove all axis lines, ticks, and labels if requested\n",
    "        if hideAxis:\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            # If you ever want axes, you can customize here:\n",
    "            ax.set_xlabel(self.time_units)\n",
    "            ax.set_ylabel(self.voltage_units)\n",
    "\n",
    "    def plot_scalebar(self, ax, scaleXms=200, scaleYmV=50, fontSize=8, lineWidth=2,\n",
    "                    hideTicks=True, hideFrame=True):\n",
    "        \"\"\"\n",
    "        Add a scale bar to the given axis dynamically.\n",
    "        By default, shows a scale bar for 200 ms (0.2 s) and 50 mV.\n",
    "\n",
    "        Args:\n",
    "            ax (matplotlib.axes.Axes): The axis to draw the scale bar on.\n",
    "            scaleXms (float): The horizontal scale length in milliseconds (default 200 ms).\n",
    "            scaleYmV (float): The vertical scale length in millivolts (default 50 mV).\n",
    "            fontSize (int): Font size of the scale bar labels.\n",
    "            lineWidth (int): Line width of the scale bar lines.\n",
    "            hideTicks (bool): If True, hides the axis ticks.\n",
    "            hideFrame (bool): If True, hides the axis frame/spines.\n",
    "\n",
    "        The scale bar will be placed in the lower-right corner of the axis.\n",
    "        \"\"\"\n",
    "        # Convert ms to seconds for data coordinates\n",
    "        scaleXsize_data = scaleXms / 1000.0  # e.g., 200 ms = 0.2 s\n",
    "        scaleYsize_data = scaleYmV  # mV stays mV\n",
    "\n",
    "        x1, x2 = ax.get_xlim()\n",
    "        y1, y2 = ax.get_ylim()\n",
    "        xs, ys = abs(x2 - x1), abs(y2 - y1)\n",
    "\n",
    "        # Position the scale bar in the lower-right corner\n",
    "        scaleBarPadX = 0.10\n",
    "        scaleBarPadY = 0.10\n",
    "        scaleBarX = x2 - scaleBarPadX * xs\n",
    "        scaleBarX2 = scaleBarX - scaleXsize_data\n",
    "        scaleBarY = y1 + scaleBarPadY * ys\n",
    "        scaleBarY2 = scaleBarY + scaleYsize_data\n",
    "\n",
    "        scaleBarXs = [scaleBarX2, scaleBarX, scaleBarX]\n",
    "        scaleBarYs = [scaleBarY, scaleBarY, scaleBarY2]\n",
    "\n",
    "        # Hide ticks/frames if requested\n",
    "        if hideTicks:\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "        if hideFrame:\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "        # Draw the scale bar\n",
    "        ax.plot(scaleBarXs, scaleBarYs, 'k-', lw=lineWidth)\n",
    "\n",
    "        # Padding for labels\n",
    "        lblPadMult = 0.005 + 0.002 * lineWidth\n",
    "        lblPadX = xs * lblPadMult\n",
    "        lblPadY = ys * lblPadMult\n",
    "\n",
    "        # Create labels with units\n",
    "        # For time, we label in ms; for voltage, in mV\n",
    "        time_label = f\"{scaleXms} ms\"\n",
    "        voltage_label = f\"{scaleYmV} mV\"\n",
    "\n",
    "        # Add text labels\n",
    "        ax.text((scaleBarX + scaleBarX2) / 2, scaleBarY - lblPadY, time_label,\n",
    "                ha='center', va='top', fontsize=fontSize)\n",
    "        ax.text(scaleBarX + lblPadX, (scaleBarY + scaleBarY2) / 2, voltage_label,\n",
    "                ha='left', va='center', fontsize=fontSize)\n",
    "\n",
    "    def plot_before_after_comparison(self, group, recording_id,\n",
    "                                    before_label=\"Before\", after_label=\"After\",\n",
    "                                    sweep_numbers=None, startAtSec=0, endAtSec=1.5,\n",
    "                                    offsetXsec=0.3, offsetYunits=40,\n",
    "                                    color_before=None, color_after=\"red\",\n",
    "                                    alpha=0.5, linewidth=1,\n",
    "                                    add_suptitle=True, suptitle_fontsize=14):\n",
    "        \"\"\"\n",
    "        Create a figure comparing Before and After sweeps for a single recording in a group.\n",
    "        This version allows control over offsets, time, and whether to show titles or axes.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "        # Plot Before sweeps\n",
    "        self.plot_sweeps(axes[0], group, recording_id, before_label,\n",
    "                        sweep_numbers=sweep_numbers,\n",
    "                        offsetXsec=offsetXsec, offsetYunits=offsetYunits,\n",
    "                        startAtSec=startAtSec, endAtSec=endAtSec,\n",
    "                        color=color_before, alpha=alpha, linewidth=linewidth, hideAxis=True)\n",
    "\n",
    "        # Plot After sweeps\n",
    "        self.plot_sweeps(axes[1], group, recording_id, after_label,\n",
    "                        sweep_numbers=sweep_numbers,\n",
    "                        offsetXsec=offsetXsec, offsetYunits=offsetYunits,\n",
    "                        startAtSec=startAtSec, endAtSec=endAtSec,\n",
    "                        color=color_after, alpha=alpha, linewidth=linewidth, hideAxis=True)\n",
    "\n",
    "        # Optionally add a figure-level title\n",
    "        if add_suptitle:\n",
    "            fig.suptitle(f\"Group: {group}, Recording: {recording_id}\",\n",
    "                        fontsize=suptitle_fontsize)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig, axes\n",
    "\n",
    "    def export_group_to_pdf(self, group, output_pdf_path,\n",
    "                            before_label=\"Before\", after_label=\"After\",\n",
    "                            sweep_numbers=None, startAtSec=0, endAtSec=1.5,\n",
    "                            offsetXsec=0.3, offsetYunits=40,\n",
    "                            color_before=None, color_after=\"red\",\n",
    "                            alpha=0.5, linewidth=1):\n",
    "        \"\"\"\n",
    "        Create a multipage PDF for all recordings in a given group.\n",
    "        Each page shows Before/After comparison for a single recording in that group.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(output_pdf_path), exist_ok=True)\n",
    "        recording_ids = self.get_recording_ids(group)\n",
    "\n",
    "        with PdfPages(output_pdf_path) as pdf:\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = self.dataframe[(self.dataframe[\"Group\"] == group) &\n",
    "                                            (self.dataframe[\"Recording_ID\"] == recording_id) &\n",
    "                                            (self.dataframe[\"Label\"] == before_label)]\n",
    "                after_entry = self.dataframe[(self.dataframe[\"Group\"] == group) &\n",
    "                                            (self.dataframe[\"Recording_ID\"] == recording_id) &\n",
    "                                            (self.dataframe[\"Label\"] == after_label)]\n",
    "\n",
    "                # Only create the page if both Before and After exist for this (Group, Recording_ID)\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    continue\n",
    "\n",
    "                fig, axes = self.plot_before_after_comparison(\n",
    "                    group, recording_id,\n",
    "                    before_label=before_label, after_label=after_label,\n",
    "                    sweep_numbers=sweep_numbers,\n",
    "                    startAtSec=startAtSec, endAtSec=endAtSec,\n",
    "                    offsetXsec=offsetXsec, offsetYunits=offsetYunits,\n",
    "                    color_before=color_before, color_after=color_after,\n",
    "                    alpha=alpha, linewidth=linewidth,\n",
    "                    add_suptitle=True\n",
    "                )\n",
    "\n",
    "                # Add a scale bar to the bottom axis if desired\n",
    "                # Adjust hideFrame and hideTicks to maintain a clean look\n",
    "                self.plot_scalebar(axes[1], hideTicks=True, hideFrame=True)\n",
    "\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "            \n",
    "    def export_all_groups_to_pdfs(self, output_dir,\n",
    "                                before_label=\"Before\", after_label=\"After\",\n",
    "                                sweep_numbers=None,\n",
    "                                startAtSec=0, endAtSec=1.5,\n",
    "                                offsetXsec=0.3, offsetYunits=40,\n",
    "                                color_before=None, color_after=\"red\",\n",
    "                                alpha=0.5, linewidth=1):\n",
    "        \"\"\"\n",
    "        Export one PDF per group, with each PDF containing all recordings \n",
    "        (Before/After) for that group.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save all the PDF files.\n",
    "            before_label (str): Label for the \"Before\" condition.\n",
    "            after_label (str): Label for the \"After\" condition.\n",
    "            sweep_numbers (list or None): Specific sweeps to plot. \n",
    "                                        If None, all sweeps are plotted.\n",
    "            startAtSec (float): Start time (seconds) of the data to plot.\n",
    "            endAtSec (float): End time (seconds) of the data to plot.\n",
    "            offsetXsec (float): Horizontal offset per sweep.\n",
    "            offsetYunits (float): Vertical offset per sweep.\n",
    "            color_before (str or None): Color for \"Before\" sweeps. \n",
    "                                        None means use default or colormap.\n",
    "            color_after (str): Color for \"After\" sweeps.\n",
    "            alpha (float): Transparency of the sweep lines.\n",
    "            linewidth (float): Width of the sweep lines.\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for grp in self.unique_groups:\n",
    "            pdf_path = os.path.join(output_dir, f\"{grp}_plots.pdf\")\n",
    "            self.export_group_to_pdf(\n",
    "                group=grp,\n",
    "                output_pdf_path=pdf_path,\n",
    "                before_label=before_label,\n",
    "                after_label=after_label,\n",
    "                sweep_numbers=sweep_numbers,\n",
    "                startAtSec=startAtSec,\n",
    "                endAtSec=endAtSec,\n",
    "                offsetXsec=offsetXsec,\n",
    "                offsetYunits=offsetYunits,\n",
    "                color_before=color_before,\n",
    "                color_after=color_after,\n",
    "                alpha=alpha,\n",
    "                linewidth=linewidth\n",
    "            )\n",
    "            print(f\"PDF saved for group {grp} at: {pdf_path}\")\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        Get a summary of the processed data.\n",
    "\n",
    "        Returns:\n",
    "            str: A summary string including number of groups and recordings.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None:\n",
    "            return \"No data processed yet.\"\n",
    "\n",
    "        summary = (\n",
    "            f\"Total Groups: {len(self.unique_groups)}\\n\"\n",
    "            f\"Unique Groups: {self.unique_groups}\\n\"\n",
    "            f\"Total Recordings: {len(self.dataframe)}\"\n",
    "        )\n",
    "        return summary\n",
    "    \n",
    "    def plot_sweeps_pdf(self, output_dir):\n",
    "        \"\"\"\n",
    "        Generate a multipage PDF for each group, with each page containing Before and After sweeps.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Path to save the generated PDF files.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Group by unique groups\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}.pdf\")\n",
    "            saved_pages = 0  # Track the number of pages added to the PDF\n",
    "\n",
    "            # Determine linking pattern based on the group\n",
    "            if group == \"L + CS-Veh\":\n",
    "                identifier_pattern = \"Veh\"\n",
    "            else:\n",
    "                identifier_pattern = \"CTZ\"\n",
    "\n",
    "            # Extract unique linking pairs\n",
    "            linking_ids = group_data[\"Recording_ID\"].str.extract(f\"({identifier_pattern}-\\d+)\")[0].dropna().unique()\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                for link_id in linking_ids:\n",
    "                    # Filter \"Before\" and \"After\" files for the current linking pair\n",
    "                    before_file = group_data[(group_data[\"Recording_ID\"].str.contains(link_id)) & (group_data[\"Label\"] == \"Before\")][\"File_Path\"]\n",
    "                    after_file = group_data[(group_data[\"Recording_ID\"].str.contains(link_id)) & (group_data[\"Label\"] == \"After\")][\"File_Path\"]\n",
    "\n",
    "                    if before_file.empty or after_file.empty:\n",
    "                        print(f\"Skipping incomplete pair for Link ID: {link_id} in group {group}\")\n",
    "                        continue  # Skip if \"Before\" or \"After\" is missing\n",
    "\n",
    "                    before_file = before_file.iloc[0]\n",
    "                    after_file = after_file.iloc[0]\n",
    "\n",
    "                    # Create the figure with 1x2 layout\n",
    "                    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "                    fig.suptitle(f\"Group: {group}, Link ID: {link_id}\", fontsize=14)\n",
    "\n",
    "                    # Plot Before\n",
    "                    abf_before = pyabf.ABF(before_file)\n",
    "                    for sweepNumber in abf_before.sweepList:\n",
    "                        abf_before.setSweep(sweepNumber)\n",
    "                        offset = 140 * sweepNumber\n",
    "                        axes[0].plot(abf_before.sweepX, abf_before.sweepY + offset, color='C0')\n",
    "                    axes[0].set_title(\"Before\")\n",
    "                    axes[0].get_yaxis().set_visible(False)\n",
    "                    axes[0].set_xlabel(abf_before.sweepLabelX)\n",
    "\n",
    "                    # Plot After\n",
    "                    abf_after = pyabf.ABF(after_file)\n",
    "                    for sweepNumber in abf_after.sweepList:\n",
    "                        abf_after.setSweep(sweepNumber)\n",
    "                        offset = 140 * sweepNumber\n",
    "                        axes[1].plot(abf_after.sweepX, abf_after.sweepY + offset, color='C1')\n",
    "                    axes[1].set_title(\"After\")\n",
    "                    axes[1].get_yaxis().set_visible(False)\n",
    "                    axes[1].set_xlabel(abf_after.sweepLabelX)\n",
    "\n",
    "                    # Save the current figure to the PDF\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "                    saved_pages += 1\n",
    "\n",
    "                if saved_pages == 0:\n",
    "                    print(f\"No valid data to plot for group '{group}'. Deleting empty PDF.\")\n",
    "                    os.remove(pdf_path)\n",
    "                else:\n",
    "                    print(f\"Saved PDF for group '{group}' to: {pdf_path}\")              \n",
    "\n",
    "    def detect_action_potentials(self, group, recording_id, label, sweep_number, height=None, prominence=None, distance=None, width=None):\n",
    "            \"\"\"\n",
    "            Detect and plot action potentials (APs) for a specific recording from the DataFrame.\n",
    "\n",
    "            Args:\n",
    "                group (str): The group name.\n",
    "                recording_id (str): The recording ID (e.g., \"CTZ-1\").\n",
    "                label (str): The label (\"Before\" or \"After\").\n",
    "                sweep_number (int): Sweep number to analyze.\n",
    "                height (float, optional): Minimum height of peaks (APs) to detect.\n",
    "                prominence (float, optional): Minimum prominence of peaks (APs) to detect.\n",
    "                distance (float, optional): Minimum distance between consecutive peaks.\n",
    "                width (float, optional): Minimum width of peaks.\n",
    "\n",
    "            Returns:\n",
    "                dict: A dictionary with sweep number, detected AP count, and peak indices.\n",
    "            \"\"\"\n",
    "            # Locate the file path from the DataFrame\n",
    "            entry = self.dataframe[\n",
    "                (self.dataframe[\"Group\"] == group) &\n",
    "                (self.dataframe[\"Recording_ID\"] == recording_id) &\n",
    "                (self.dataframe[\"Label\"] == label)\n",
    "            ]\n",
    "\n",
    "            if entry.empty:\n",
    "                raise ValueError(f\"No entry found for Group: {group}, Recording ID: {recording_id}, Label: {label}\")\n",
    "\n",
    "            file_path = entry[\"File_Path\"].iloc[0]\n",
    "\n",
    "            # Load the ABF file\n",
    "            abf = pyabf.ABF(file_path)\n",
    "            abf.setSweep(sweepNumber=sweep_number)\n",
    "\n",
    "            # Extract time (X-axis) and voltage (Y-axis) data\n",
    "            time = abf.sweepX\n",
    "            voltage = abf.sweepY\n",
    "\n",
    "            # Detect peaks using scipy's find_peaks\n",
    "            peaks, properties = find_peaks(\n",
    "                voltage,\n",
    "                height=height,\n",
    "                prominence=prominence,\n",
    "                distance=distance,\n",
    "                width=width\n",
    "            )\n",
    "\n",
    "            # Count the number of action potentials (peaks)\n",
    "            ap_count = len(peaks)\n",
    "\n",
    "            # Plot the sweep with peaks overlaid\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(time, voltage, label=\"Voltage Trace\", color=\"C0\")\n",
    "            plt.plot(time[peaks], voltage[peaks], \"x\", label=\"Detected Peaks\", color=\"C3\")\n",
    "            plt.title(f\"Group: {group}, Recording ID: {recording_id}, Sweep {sweep_number}: Detected {ap_count} Action Potentials\")\n",
    "            plt.xlabel(\"Time (s)\")\n",
    "            plt.ylabel(\"Voltage (mV)\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Return a summary of results\n",
    "            return {\n",
    "                \"group\": group,\n",
    "                \"recording_id\": recording_id,\n",
    "                \"label\": label,\n",
    "                \"sweep_number\": sweep_number,\n",
    "                \"action_potential_count\": ap_count,\n",
    "                \"peak_indices\": peaks\n",
    "            }\n",
    "            \n",
    "    def create_group_pdf_with_peaks(self, output_dir, height=None, prominence=None, distance=None, width=None):\n",
    "        \"\"\"\n",
    "        Generate a 2x1 multipage PDF for each group. Each page contains \"Before\" and \"After\"\n",
    "        sweeps of a single recording with action potential peaks annotated.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Path to save the generated PDF files.\n",
    "            height (float, optional): Minimum height of peaks (APs) to detect.\n",
    "            prominence (float, optional): Minimum prominence of peaks (APs) to detect.\n",
    "            distance (float, optional): Minimum distance between consecutive peaks.\n",
    "            width (float, optional): Minimum width of peaks.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}_wide.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                # Get unique recording IDs for the group\n",
    "                recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "                for recording_id in recording_ids:\n",
    "                    # Locate Before and After entries\n",
    "                    before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                    after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                    if before_entry.empty or after_entry.empty:\n",
    "                        print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                        continue\n",
    "\n",
    "                    # Load Before and After ABF files\n",
    "                    before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                    after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                    before_abf = pyabf.ABF(before_file)\n",
    "                    after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                    # Create a 2x1 figure for the recording\n",
    "                    fig, axes = plt.subplots(2, 1, figsize=(16, 12), sharex=True)\n",
    "                    fig.suptitle(f\"Group: {group}, Recording ID: {recording_id}\", fontsize=16)\n",
    "\n",
    "                    # Plot Before sweeps (Top Panel)\n",
    "                    before_peak_counts = []\n",
    "                    for sweep_number in before_abf.sweepList:\n",
    "                        before_abf.setSweep(sweepNumber=sweep_number)\n",
    "                        time = before_abf.sweepX\n",
    "                        voltage = before_abf.sweepY\n",
    "\n",
    "                        # Detect peaks\n",
    "                        peaks, properties = find_peaks(\n",
    "                            voltage,\n",
    "                            height=height,\n",
    "                            prominence=prominence,\n",
    "                            distance=distance,\n",
    "                            width=width\n",
    "                        )\n",
    "\n",
    "                        # Count peaks and store for annotation\n",
    "                        before_peak_counts.append(len(peaks))\n",
    "\n",
    "                        # Plot the sweep with peaks\n",
    "                        offset = 140 * sweep_number  # Offset to stack sweeps visually\n",
    "                        axes[0].plot(time, voltage + offset, label=f\"Sweep {sweep_number}\", color=\"C0\")\n",
    "                        axes[0].plot(time[peaks], voltage[peaks] + offset, \"x\", color=\"C3\")\n",
    "\n",
    "                        # Annotate number of peaks and sweep number for Before sweeps\n",
    "                        for sweep_number, peak_count in enumerate(before_peak_counts):\n",
    "                            axes[0].text(\n",
    "                                -0.05,  # Slightly to the left of the x-axis start\n",
    "                                140 * sweep_number,  # Same vertical position as the trace\n",
    "                                f\"Sweep {sweep_number}: {peak_count} APs\",  # Add sweep number and AP count\n",
    "                                fontsize=10,\n",
    "                                color=\"C0\",\n",
    "                                ha=\"right\"  # Align text to the right\n",
    "                            )\n",
    "\n",
    "                    axes[0].set_title(\"Before\")\n",
    "                    axes[0].get_yaxis().set_visible(False)\n",
    "\n",
    "                    # Plot After sweeps (Bottom Panel)\n",
    "                    after_peak_counts = []\n",
    "                    for sweep_number in after_abf.sweepList:\n",
    "                        after_abf.setSweep(sweepNumber=sweep_number)\n",
    "                        time = after_abf.sweepX\n",
    "                        voltage = after_abf.sweepY\n",
    "\n",
    "                        # Detect peaks\n",
    "                        peaks, properties = find_peaks(\n",
    "                            voltage,\n",
    "                            height=height,\n",
    "                            prominence=prominence,\n",
    "                            distance=distance,\n",
    "                            width=width\n",
    "                        )\n",
    "\n",
    "                        # Count peaks and store for annotation\n",
    "                        after_peak_counts.append(len(peaks))\n",
    "\n",
    "                        # Plot the sweep with peaks\n",
    "                        offset = 140 * sweep_number  # Offset to stack sweeps visually\n",
    "                        axes[1].plot(time, voltage + offset, label=f\"Sweep {sweep_number}\", color=\"C1\")\n",
    "                        axes[1].plot(time[peaks], voltage[peaks] + offset, \"x\", color=\"C4\")\n",
    "                        \n",
    "                        # Annotate number of peaks and sweep number for After sweeps\n",
    "                        for sweep_number, peak_count in enumerate(after_peak_counts):\n",
    "                            axes[1].text(\n",
    "                                -0.05,  # Slightly to the left of the x-axis start\n",
    "                                140 * sweep_number,  # Same vertical position as the trace\n",
    "                                f\"Sweep {sweep_number}: {peak_count} APs\",  # Add sweep number and AP count\n",
    "                                fontsize=10,\n",
    "                                color=\"C1\",\n",
    "                                ha=\"right\"  # Align text to the right\n",
    "                            )\n",
    "                    axes[1].set_title(\"After\")\n",
    "                    axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "                    # Decorate the figure\n",
    "                    axes[1].set_xlabel(\"Time (s)\")\n",
    "                    for ax in axes:\n",
    "                        ax.set_ylabel(\"Voltage (mV)\")\n",
    "                        ax.legend(loc=\"upper right\")\n",
    "\n",
    "                    # Save the current page to the PDF\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "            print(f\"Saved wide PDF for group '{group}' to: {pdf_path}\")\n",
    "            \n",
    "    def process_peaks(self, height=None, prominence=None, distance=None, width=None, save_csv_path=None):\n",
    "        \"\"\"\n",
    "        Process all recordings to detect action potentials (peaks) for each sweep\n",
    "        and optionally save the results to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            height (float, optional): Minimum height of peaks (APs) to detect.\n",
    "            prominence (float, optional): Minimum prominence of peaks (APs) to detect.\n",
    "            distance (float, optional): Minimum distance between consecutive peaks.\n",
    "            width (float, optional): Minimum width of peaks.\n",
    "            save_csv_path (str, optional): Path to save the processed peaks data as a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the results in self.peak_dataframe and optionally saves it as a CSV.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to process. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        peak_data = []\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "\n",
    "            for _, row in group_data.iterrows():\n",
    "                recording_id = row[\"Recording_ID\"]\n",
    "                label = row[\"Label\"]\n",
    "                file_path = row[\"File_Path\"]\n",
    "\n",
    "                # Load the ABF file\n",
    "                abf = pyabf.ABF(file_path)\n",
    "\n",
    "                for sweep_number in abf.sweepList:\n",
    "                    abf.setSweep(sweepNumber=sweep_number)\n",
    "                    voltage = abf.sweepY\n",
    "\n",
    "                    # Detect peaks\n",
    "                    peaks, _ = find_peaks(\n",
    "                        voltage,\n",
    "                        height=height,\n",
    "                        prominence=prominence,\n",
    "                        distance=distance,\n",
    "                        width=width\n",
    "                    )\n",
    "\n",
    "                    # Append results to the list\n",
    "                    peak_data.append({\n",
    "                        \"Group\": group,\n",
    "                        \"Recording_ID\": recording_id,\n",
    "                        \"Label\": label,\n",
    "                        \"Sweep_Number\": sweep_number,\n",
    "                        \"AP_Count\": len(peaks)\n",
    "                    })\n",
    "\n",
    "        # Create a DataFrame from the results\n",
    "        self.peak_dataframe = pd.DataFrame(peak_data)\n",
    "        print(f\"Processed peaks for {len(self.peak_dataframe)} sweeps.\")\n",
    "\n",
    "        # Save to CSV if path is provided\n",
    "        if save_csv_path:\n",
    "            self.peak_dataframe.to_csv(save_csv_path, index=False)\n",
    "            print(f\"Saved peak data to {save_csv_path}.\")\n",
    "                     \n",
    "    def import_csv_and_plot_mean_peaks(self, csv_path, output_pdf_path):\n",
    "        \"\"\"\n",
    "        Import a CSV file containing peak data and plot the mean AP counts\n",
    "        for \"Before\" and \"After\" sweeps for each group.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file containing peak data.\n",
    "            output_pdf_path (str): Path to save the output PDF.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Load the CSV into a DataFrame\n",
    "        try:\n",
    "            peak_data = pd.read_csv(csv_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            return\n",
    "\n",
    "        # Validate the required columns\n",
    "        required_columns = [\"Group\", \"Recording_ID\", \"Label\", \"Sweep_Number\", \"AP_Count\"]\n",
    "        if not all(col in peak_data.columns for col in required_columns):\n",
    "            print(\"The CSV file is missing one or more required columns.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_pdf_path), exist_ok=True)\n",
    "\n",
    "        # Open a PDF for plotting\n",
    "        with PdfPages(output_pdf_path) as pdf:\n",
    "            # Group by \"Group\" and compute means\n",
    "            groups = peak_data[\"Group\"].unique()\n",
    "            for group in groups:\n",
    "                group_data = peak_data[peak_data[\"Group\"] == group]\n",
    "\n",
    "                # Compute mean and SEM for Before and After\n",
    "                before_data = group_data[group_data[\"Label\"] == \"Before\"]\n",
    "                after_data = group_data[group_data[\"Label\"] == \"After\"]\n",
    "\n",
    "                mean_before = before_data.groupby(\"Recording_ID\")[\"AP_Count\"].mean()\n",
    "                mean_after = after_data.groupby(\"Recording_ID\")[\"AP_Count\"].mean()\n",
    "\n",
    "                sem_before = before_data.groupby(\"Recording_ID\")[\"AP_Count\"].sem()\n",
    "                sem_after = after_data.groupby(\"Recording_ID\")[\"AP_Count\"].sem()\n",
    "\n",
    "                # Plot the data\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                ax.bar(\n",
    "                    x=[\"Before\", \"After\"],\n",
    "                    height=[mean_before.mean(), mean_after.mean()],\n",
    "                    yerr=[sem_before.mean(), sem_after.mean()],\n",
    "                    capsize=5,\n",
    "                    color=[\"C0\", \"C1\"],\n",
    "                    alpha=0.7,\n",
    "                    label=[\"Before\", \"After\"]\n",
    "                )\n",
    "                ax.set_title(f\"Group: {group}\", fontsize=14)\n",
    "                ax.set_ylabel(\"Mean AP Count (± SEM)\")\n",
    "                ax.set_xlabel(\"Condition\")\n",
    "                ax.legend()\n",
    "\n",
    "                # Save the page to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "        print(f\"Saved mean AP count plots to: {output_pdf_path}\")\n",
    "   \n",
    "    def import_csv_and_plot_mean_peaks_lineplot(self, csv_path, output_pdf_path):\n",
    "        \"\"\"\n",
    "        Import a CSV file containing peak data and plot the mean AP counts\n",
    "        as a function of sweep number for each group.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file containing peak data.\n",
    "            output_pdf_path (str): Path to save the output PDF.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Load the CSV into a DataFrame\n",
    "        try:\n",
    "            peak_data = pd.read_csv(csv_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            return\n",
    "\n",
    "        # Validate the required columns\n",
    "        required_columns = [\"Group\", \"Recording_ID\", \"Label\", \"Sweep_Number\", \"AP_Count\"]\n",
    "        if not all(col in peak_data.columns for col in required_columns):\n",
    "            print(\"The CSV file is missing one or more required columns.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_pdf_path), exist_ok=True)\n",
    "\n",
    "        # Open a PDF for plotting\n",
    "        with PdfPages(output_pdf_path) as pdf:\n",
    "            # Get unique groups\n",
    "            groups = peak_data[\"Group\"].unique()\n",
    "            for group in groups:\n",
    "                group_data = peak_data[peak_data[\"Group\"] == group]\n",
    "\n",
    "                # Compute mean and SEM for \"Before\" and \"After\" by Sweep_Number\n",
    "                before_data = group_data[group_data[\"Label\"] == \"Before\"]\n",
    "                after_data = group_data[group_data[\"Label\"] == \"After\"]\n",
    "\n",
    "                mean_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "                sem_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "                mean_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "                sem_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "                # Plot the data\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                sweep_numbers = mean_before.index\n",
    "\n",
    "                # Plot \"Before\" with SEM\n",
    "                ax.plot(\n",
    "                    sweep_numbers, mean_before,\n",
    "                    label=\"Before\",\n",
    "                    color=\"C0\",\n",
    "                    linewidth=2\n",
    "                )\n",
    "                ax.fill_between(\n",
    "                    sweep_numbers,\n",
    "                    mean_before - sem_before,\n",
    "                    mean_before + sem_before,\n",
    "                    color=\"C0\",\n",
    "                    alpha=0.3\n",
    "                )\n",
    "\n",
    "                # Plot \"After\" with SEM\n",
    "                ax.plot(\n",
    "                    sweep_numbers, mean_after,\n",
    "                    label=\"After\",\n",
    "                    color=\"C1\",\n",
    "                    linewidth=2\n",
    "                )\n",
    "                ax.fill_between(\n",
    "                    sweep_numbers,\n",
    "                    mean_after - sem_after,\n",
    "                    mean_after + sem_after,\n",
    "                    color=\"C1\",\n",
    "                    alpha=0.3\n",
    "                )\n",
    "\n",
    "                # Decorate the plot\n",
    "                ax.set_title(f\"Group: {group}\", fontsize=14)\n",
    "                ax.set_xlabel(\"Sweep Number\")\n",
    "                ax.set_ylabel(\"Mean AP Count (± SEM)\")\n",
    "                ax.legend()\n",
    "                ax.grid(True)\n",
    "\n",
    "                # Save the page to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "        print(f\"Saved line plots for mean AP counts to: {output_pdf_path}\")   \n",
    "\n",
    "    def import_csv_and_plot_mean_peaks_with_error_bars(self, csv_path, output_pdf_path):\n",
    "        \"\"\"\n",
    "        Import a CSV file containing peak data and plot the mean AP counts\n",
    "        with SEM for \"Before\" and \"After\" at each sweep number as error bars.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file containing peak data.\n",
    "            output_pdf_path (str): Path to save the output PDF.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Load the CSV into a DataFrame\n",
    "        try:\n",
    "            peak_data = pd.read_csv(csv_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            return\n",
    "\n",
    "        # Validate the required columns\n",
    "        required_columns = [\"Group\", \"Recording_ID\", \"Label\", \"Sweep_Number\", \"AP_Count\"]\n",
    "        if not all(col in peak_data.columns for col in required_columns):\n",
    "            print(\"The CSV file is missing one or more required columns.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(os.path.dirname(output_pdf_path), exist_ok=True)\n",
    "\n",
    "        # Open a PDF for plotting\n",
    "        with PdfPages(output_pdf_path) as pdf:\n",
    "            # Get unique groups\n",
    "            groups = peak_data[\"Group\"].unique()\n",
    "            for group in groups:\n",
    "                group_data = peak_data[peak_data[\"Group\"] == group]\n",
    "\n",
    "                # Compute mean and SEM for \"Before\" and \"After\" by Sweep_Number\n",
    "                before_data = group_data[group_data[\"Label\"] == \"Before\"]\n",
    "                after_data = group_data[group_data[\"Label\"] == \"After\"]\n",
    "\n",
    "                mean_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "                sem_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "                mean_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "                sem_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "                # Plot the data\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                sweep_numbers = mean_before.index\n",
    "\n",
    "                # Plot \"Before\" means and SEM as error bars\n",
    "                ax.errorbar(\n",
    "                    sweep_numbers - 0.2,  # Offset \"Before\" slightly to the left\n",
    "                    mean_before,\n",
    "                    yerr=sem_before,\n",
    "                    fmt=\"o\",  # Circle markers for \"Before\"\n",
    "                    label=\"Before\",\n",
    "                    color=\"C0\",\n",
    "                    capsize=5,\n",
    "                    markersize=8,\n",
    "                )\n",
    "\n",
    "                # Plot \"After\" means and SEM as error bars\n",
    "                ax.errorbar(\n",
    "                    sweep_numbers + 0.2,  # Offset \"After\" slightly to the right\n",
    "                    mean_after,\n",
    "                    yerr=sem_after,\n",
    "                    fmt=\"o\",  # Circle markers for \"After\"\n",
    "                    label=\"After\",\n",
    "                    color=\"C1\",\n",
    "                    capsize=5,\n",
    "                    markersize=8,\n",
    "                )\n",
    "\n",
    "                # Decorate the plot\n",
    "                ax.set_title(f\"Group: {group}\", fontsize=14)\n",
    "                ax.set_xlabel(\"Sweep Number\")\n",
    "                ax.set_ylabel(\"Mean AP Count (± SEM)\")\n",
    "                ax.legend()\n",
    "                ax.grid(True)\n",
    "\n",
    "                # Save the page to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "        print(f\"Saved error bar plots for mean AP counts to: {output_pdf_path}\")\n",
    "        \n",
    "    def process_peaks_in_window(self, height=None, prominence=None, distance=None, width=None, \n",
    "                                save_csv_path=None, start_time=None, end_time=None):\n",
    "        \"\"\"\n",
    "        Process all recordings to detect action potentials (peaks) for each sweep within a specified time window,\n",
    "        and optionally save the results to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            height (float, optional): Minimum height of peaks (APs) to detect.\n",
    "            prominence (float, optional): Minimum prominence of peaks (APs) to detect.\n",
    "            distance (float, optional): Minimum distance between consecutive peaks.\n",
    "            width (float, optional): Minimum width of peaks.\n",
    "            save_csv_path (str, optional): Path to save the processed peaks data as a CSV file.\n",
    "            start_time (float, optional): Start time (in seconds) of the window to analyze. Defaults to the beginning.\n",
    "            end_time (float, optional): End time (in seconds) of the window to analyze. Defaults to the end.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the results in self.peak_window_dataframe and optionally saves it as a CSV.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to process. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        peak_data = []\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "\n",
    "            for _, row in group_data.iterrows():\n",
    "                recording_id = row[\"Recording_ID\"]\n",
    "                label = row[\"Label\"]\n",
    "                file_path = row[\"File_Path\"]\n",
    "\n",
    "                # Load the ABF file\n",
    "                abf = pyabf.ABF(file_path)\n",
    "\n",
    "                for sweep_number in abf.sweepList:\n",
    "                    abf.setSweep(sweepNumber=sweep_number)\n",
    "                    voltage = abf.sweepY\n",
    "                    time = abf.sweepX  # Time vector in seconds\n",
    "\n",
    "                    # If time window is specified, extract the relevant portion\n",
    "                    if start_time is not None and end_time is not None:\n",
    "                        mask = (time >= start_time) & (time <= end_time)\n",
    "                        voltage = voltage[mask]\n",
    "                        time = time[mask]\n",
    "\n",
    "                    # Detect peaks within the specified window\n",
    "                    peaks, _ = find_peaks(\n",
    "                        voltage,\n",
    "                        height=height,\n",
    "                        prominence=prominence,\n",
    "                        distance=distance,\n",
    "                        width=width\n",
    "                    )\n",
    "\n",
    "                    # Append results to the list\n",
    "                    peak_data.append({\n",
    "                        \"Group\": group,\n",
    "                        \"Recording_ID\": recording_id,\n",
    "                        \"Label\": label,\n",
    "                        \"Sweep_Number\": sweep_number,\n",
    "                        \"Start_Time\": start_time if start_time is not None else 0,\n",
    "                        \"End_Time\": end_time if end_time is not None else time[-1],\n",
    "                        \"AP_Count\": len(peaks)\n",
    "                    })\n",
    "\n",
    "        # Create a DataFrame from the results\n",
    "        self.peak_window_dataframe = pd.DataFrame(peak_data)\n",
    "        print(f\"Processed peaks for {len(self.peak_window_dataframe)} sweeps.\")\n",
    "\n",
    "        # Save to CSV if path is provided\n",
    "        if save_csv_path:\n",
    "            self.peak_window_dataframe.to_csv(save_csv_path, index=False)\n",
    "            print(f\"Saved peak data to {save_csv_path}.\")\n",
    "               \n",
    "    def process_peaks_by_phase(self, height=None, prominence=None, distance=None, width=None, \n",
    "                            early_start=None, early_end=None, late_start=None, late_end=None, \n",
    "                            save_csv_path=None):\n",
    "        \"\"\"\n",
    "        Process recordings to detect action potentials (peaks) within early and late phases of each sweep,\n",
    "        save indices of detected spikes, and optionally save the results to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            height (float, optional): Minimum height of peaks (APs) to detect.\n",
    "            prominence (float, optional): Minimum prominence of peaks (APs) to detect.\n",
    "            distance (float, optional): Minimum distance between consecutive peaks.\n",
    "            width (float, optional): Minimum width of peaks.\n",
    "            early_start (float, optional): Start time (in seconds) for the early phase window.\n",
    "            early_end (float, optional): End time (in seconds) for the early phase window.\n",
    "            late_start (float, optional): Start time (in seconds) for the late phase window.\n",
    "            late_end (float, optional): End time (in seconds) for the late phase window.\n",
    "            save_csv_path (str, optional): Path to save the processed peaks data as a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            None: Stores the results in self.phase_peak_dataframe and optionally saves it as a CSV.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to process. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        phase_peak_data = []\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "\n",
    "            for _, row in group_data.iterrows():\n",
    "                recording_id = row[\"Recording_ID\"]\n",
    "                label = row[\"Label\"]\n",
    "                file_path = row[\"File_Path\"]\n",
    "\n",
    "                # Load the ABF file\n",
    "                abf = pyabf.ABF(file_path)\n",
    "\n",
    "                for sweep_number in abf.sweepList:\n",
    "                    abf.setSweep(sweepNumber=sweep_number)\n",
    "                    voltage = abf.sweepY\n",
    "                    time = abf.sweepX  # Time vector in seconds\n",
    "\n",
    "                    # Initialize variables for indices\n",
    "                    early_indices = []\n",
    "                    late_indices = []\n",
    "\n",
    "                    # Process Early Phase\n",
    "                    if early_start is not None and early_end is not None:\n",
    "                        early_mask = (time >= early_start) & (time <= early_end)\n",
    "                        early_voltage = voltage[early_mask]\n",
    "                        early_peaks, _ = find_peaks(\n",
    "                            early_voltage,\n",
    "                            height=height,\n",
    "                            prominence=prominence,\n",
    "                            distance=distance,\n",
    "                            width=width\n",
    "                        )\n",
    "                        early_ap_count = len(early_peaks)\n",
    "                        early_indices = np.where(early_mask)[0][early_peaks]  # Map local to global indices\n",
    "                    else:\n",
    "                        early_ap_count = 0\n",
    "\n",
    "                    # Process Late Phase\n",
    "                    if late_start is not None and late_end is not None:\n",
    "                        late_mask = (time >= late_start) & (time <= late_end)\n",
    "                        late_voltage = voltage[late_mask]\n",
    "                        late_peaks, _ = find_peaks(\n",
    "                            late_voltage,\n",
    "                            height=height,\n",
    "                            prominence=prominence,\n",
    "                            distance=distance,\n",
    "                            width=width\n",
    "                        )\n",
    "                        late_ap_count = len(late_peaks)\n",
    "                        late_indices = np.where(late_mask)[0][late_peaks]  # Map local to global indices\n",
    "                    else:\n",
    "                        late_ap_count = 0\n",
    "\n",
    "                    # Append results to the list\n",
    "                    phase_peak_data.append({\n",
    "                        \"Group\": group,\n",
    "                        \"Recording_ID\": recording_id,\n",
    "                        \"Label\": label,\n",
    "                        \"Sweep_Number\": sweep_number,\n",
    "                        \"Early_AP_Count\": early_ap_count,\n",
    "                        \"Late_AP_Count\": late_ap_count,\n",
    "                        \"Early_Indices\": early_indices.tolist(),  # Save indices as a list\n",
    "                        \"Late_Indices\": late_indices.tolist(),    # Save indices as a list\n",
    "                        \"Early_Start_Time\": early_start if early_start is not None else 0,\n",
    "                        \"Early_End_Time\": early_end if early_end is not None else 0,\n",
    "                        \"Late_Start_Time\": late_start if late_start is not None else 0,\n",
    "                        \"Late_End_Time\": late_end if late_end is not None else 0\n",
    "                    })\n",
    "\n",
    "        # Create a DataFrame from the results\n",
    "        self.phase_peak_dataframe = pd.DataFrame(phase_peak_data)\n",
    "        print(f\"Processed early and late peaks for {len(self.phase_peak_dataframe)} sweeps.\")\n",
    "\n",
    "        # Save to CSV if path is provided\n",
    "        if save_csv_path:\n",
    "            self.phase_peak_dataframe.to_csv(save_csv_path, index=False)\n",
    "            print(f\"Saved phase peak data to {save_csv_path}.\")\n",
    "            \n",
    "    def create_group_pdf_with_deltas_from_dataframe(self, output_dir):\n",
    "        \"\"\"\n",
    "        Generate PDFs for each group, with each page showing ΔAP (Late-Early) across sweeps\n",
    "        for a single recording, using data from phase_peak_dataframe.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save the generated PDF files.\n",
    "        \"\"\"\n",
    "        if self.phase_peak_dataframe is None or self.phase_peak_dataframe.empty:\n",
    "            print(\"No data in `phase_peak_dataframe`. Run `process_peaks_by_phase` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Group the data by Group\n",
    "        group_data = self.phase_peak_dataframe.groupby(\"Group\")\n",
    "\n",
    "        for group, group_df in group_data:\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}_delta_ap.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                # Iterate over each recording within the group\n",
    "                recordings = group_df.groupby(\"Recording_ID\")\n",
    "\n",
    "                for recording_id, recording_df in recordings:\n",
    "                    # Separate Before and After conditions\n",
    "                    before_data = recording_df[recording_df[\"Label\"] == \"Before\"]\n",
    "                    after_data = recording_df[recording_df[\"Label\"] == \"After\"]\n",
    "\n",
    "                    # Calculate ΔAP (Late - Early)\n",
    "                    before_data[\"Delta_AP\"] = before_data[\"Late_AP_Count\"] - before_data[\"Early_AP_Count\"]\n",
    "                    after_data[\"Delta_AP\"] = after_data[\"Late_AP_Count\"] - after_data[\"Early_AP_Count\"]\n",
    "\n",
    "                    # Debugging Info\n",
    "                    print(f\"\\nGroup: {group}, Recording ID: {recording_id}\")\n",
    "                    print(\"Before ΔAP:\")\n",
    "                    print(before_data[[\"Sweep_Number\", \"Early_AP_Count\", \"Late_AP_Count\", \"Delta_AP\"]])\n",
    "                    print(\"After ΔAP:\")\n",
    "                    print(after_data[[\"Sweep_Number\", \"Early_AP_Count\", \"Late_AP_Count\", \"Delta_AP\"]])\n",
    "\n",
    "                    # Create the plot\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    ax.plot(\n",
    "                        before_data[\"Sweep_Number\"],\n",
    "                        before_data[\"Delta_AP\"],\n",
    "                        label=\"Before Luciferin\",\n",
    "                        marker=\"o\",\n",
    "                        color=\"blue\"\n",
    "                    )\n",
    "                    ax.plot(\n",
    "                        after_data[\"Sweep_Number\"],\n",
    "                        after_data[\"Delta_AP\"],\n",
    "                        label=\"After Luciferin\",\n",
    "                        marker=\"o\",\n",
    "                        color=\"orange\"\n",
    "                    )\n",
    "                    ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\n",
    "                    ax.set_xlabel(\"Sweep Number\")\n",
    "                    ax.set_ylabel(\"ΔAP (Late - Early)\")\n",
    "                    ax.set_title(f\"ΔAP (Late - Early) for Recording ID: {recording_id}\")\n",
    "                    ax.legend()\n",
    "                    ax.grid()\n",
    "\n",
    "                    # Ensure y-axis scales dynamically\n",
    "                    ax.autoscale(enable=True, axis='y', tight=True)\n",
    "\n",
    "                    # Save the current page to the PDF\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "            print(f\"Saved ΔAP PDF for group '{group}' to: {pdf_path}\")\n",
    "            \n",
    "    def create_group_pdf_with_early_vs_late_counts(self, output_dir):\n",
    "        \"\"\"\n",
    "        Generate PDFs for each group, with each page showing Early vs. Late phase spike counts\n",
    "        for a single recording, using data from phase_peak_dataframe.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save the generated PDF files.\n",
    "        \"\"\"\n",
    "        if self.phase_peak_dataframe is None or self.phase_peak_dataframe.empty:\n",
    "            print(\"No data in `phase_peak_dataframe`. Run `process_peaks_by_phase` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Group the data by Group\n",
    "        group_data = self.phase_peak_dataframe.groupby(\"Group\")\n",
    "\n",
    "        for group, group_df in group_data:\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}_early_vs_late_counts.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                # Iterate over each recording within the group\n",
    "                recordings = group_df.groupby(\"Recording_ID\")\n",
    "\n",
    "                for recording_id, recording_df in recordings:\n",
    "                    # Separate Before and After data\n",
    "                    before_data = recording_df[recording_df[\"Label\"] == \"Before\"]\n",
    "                    after_data = recording_df[recording_df[\"Label\"] == \"After\"]\n",
    "\n",
    "                    # Skip if no data for Before or After conditions\n",
    "                    if before_data.empty or after_data.empty:\n",
    "                        print(f\"Skipping recording {recording_id} in group {group}: Missing 'Before' or 'After' data.\")\n",
    "                        continue\n",
    "\n",
    "                    # Create the plot\n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "                    bar_width = 0.35\n",
    "                    indices = range(len(before_data))\n",
    "\n",
    "                    # Ensure indices match the size of data\n",
    "                    if len(before_data) != len(after_data):\n",
    "                        print(f\"Skipping recording {recording_id} in group {group}: Mismatched sweep counts between 'Before' and 'After'.\")\n",
    "                        continue\n",
    "\n",
    "                    # Early phase counts\n",
    "                    ax.bar(\n",
    "                        [i - bar_width / 2 for i in indices],\n",
    "                        before_data[\"Early_AP_Count\"],\n",
    "                        width=bar_width,\n",
    "                        label=\"Before Early\",\n",
    "                        color=\"lightgrey\",\n",
    "                        alpha=0.8\n",
    "                    )\n",
    "                    ax.bar(\n",
    "                        [i + bar_width / 2 for i in indices],\n",
    "                        after_data[\"Early_AP_Count\"],\n",
    "                        width=bar_width,\n",
    "                        label=\"After Early\",\n",
    "                        color=\"lightblue\",\n",
    "                        alpha=0.8\n",
    "                    )\n",
    "\n",
    "                    # Late phase counts\n",
    "                    ax.bar(\n",
    "                        [i - bar_width / 2 for i in indices],\n",
    "                        before_data[\"Late_AP_Count\"],\n",
    "                        width=bar_width,\n",
    "                        label=\"Before Late\",\n",
    "                        color=\"dimgrey\",\n",
    "                        alpha=0.8,\n",
    "                        bottom=before_data[\"Early_AP_Count\"]\n",
    "                    )\n",
    "                    ax.bar(\n",
    "                        [i + bar_width / 2 for i in indices],\n",
    "                        after_data[\"Late_AP_Count\"],\n",
    "                        width=bar_width,\n",
    "                        label=\"After Late\",\n",
    "                        color=\"royalblue\",\n",
    "                        alpha=0.8,\n",
    "                        bottom=after_data[\"Early_AP_Count\"]\n",
    "                    )\n",
    "\n",
    "                    # Formatting\n",
    "                    ax.set_xticks(indices)\n",
    "                    ax.set_xticklabels(before_data[\"Sweep_Number\"])\n",
    "                    ax.set_xlabel(\"Sweep Number\")\n",
    "                    ax.set_ylabel(\"Spike Counts\")\n",
    "                    ax.set_title(f\"Early vs. Late Phase Spike Counts\\nGroup: {group}, Recording ID: {recording_id}\")\n",
    "                    ax.legend()\n",
    "                    ax.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "                    # Save the current page to the PDF\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "            print(f\"Saved Early vs. Late Phase Spike Counts PDF for group '{group}' to: {pdf_path}\")\n",
    "            \n",
    "    def create_group_pdf_with_early_to_late_ratios(self, output_dir):\n",
    "        \"\"\"\n",
    "        Generate PDFs for each group, with each page showing Early-to-Late ratios\n",
    "        for a single recording, using data from phase_peak_dataframe.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save the generated PDF files.\n",
    "        \"\"\"\n",
    "        if self.phase_peak_dataframe is None or self.phase_peak_dataframe.empty:\n",
    "            print(\"No data in `phase_peak_dataframe`. Run `process_peaks_by_phase` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Group the data by Group\n",
    "        group_data = self.phase_peak_dataframe.groupby(\"Group\")\n",
    "\n",
    "        for group, group_df in group_data:\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}_early_to_late_ratios.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                # Iterate over each recording within the group\n",
    "                recordings = group_df.groupby(\"Recording_ID\")\n",
    "\n",
    "                for recording_id, recording_df in recordings:\n",
    "                    # Separate Before and After data\n",
    "                    before_data = recording_df[recording_df[\"Label\"] == \"Before\"].copy()\n",
    "                    after_data = recording_df[recording_df[\"Label\"] == \"After\"].copy()\n",
    "\n",
    "                    # Calculate Early-to-Late ratios\n",
    "                    before_data[\"Ratio\"] = before_data[\"Late_AP_Count\"] / before_data[\"Early_AP_Count\"].replace(0, np.nan)\n",
    "                    after_data[\"Ratio\"] = after_data[\"Late_AP_Count\"] / after_data[\"Early_AP_Count\"].replace(0, np.nan)\n",
    "\n",
    "                    # Skip if no valid data for either condition\n",
    "                    if before_data[\"Ratio\"].isna().all() and after_data[\"Ratio\"].isna().all():\n",
    "                        print(f\"Skipping recording {recording_id} in group {group}: No valid ratios to plot.\")\n",
    "                        continue\n",
    "\n",
    "                    # Create the plot\n",
    "                    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "                    # Plot Before and After ratios\n",
    "                    ax.plot(\n",
    "                        before_data[\"Sweep_Number\"],\n",
    "                        before_data[\"Ratio\"],\n",
    "                        label=\"Before\",\n",
    "                        marker=\"o\",\n",
    "                        color=\"grey\",\n",
    "                        alpha=0.8\n",
    "                    )\n",
    "                    ax.plot(\n",
    "                        after_data[\"Sweep_Number\"],\n",
    "                        after_data[\"Ratio\"],\n",
    "                        label=\"After\",\n",
    "                        marker=\"o\",\n",
    "                        color=\"blue\",\n",
    "                        alpha=0.8\n",
    "                    )\n",
    "\n",
    "                    # Formatting\n",
    "                    ax.axhline(1, color=\"black\", linestyle=\"--\", linewidth=0.8, label=\"Ratio = 1\")\n",
    "                    ax.set_xticks(before_data[\"Sweep_Number\"])\n",
    "                    ax.set_xlabel(\"Sweep Number\")\n",
    "                    ax.set_ylabel(\"Early-to-Late Ratio\")\n",
    "                    ax.set_title(f\"Early-to-Late Ratio\\nGroup: {group}, Recording ID: {recording_id}\")\n",
    "                    ax.legend()\n",
    "                    ax.grid(axis='y', linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "                    # Save the current page to the PDF\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "            print(f\"Saved Early-to-Late Ratio PDF for group '{group}' to: {pdf_path}\")\n",
    "            \n",
    "    def create_group_pdf_with_mean_and_sem(self, output_dir):\n",
    "        \"\"\"\n",
    "        Generate a PDF for each group, with a single page showing the mean and SEM\n",
    "        of ΔAP (Late - Early) across sweeps for the group.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save the generated PDF files.\n",
    "        \"\"\"\n",
    "        if self.phase_peak_dataframe is None or self.phase_peak_dataframe.empty:\n",
    "            print(\"No data in `phase_peak_dataframe`. Run `process_peaks_by_phase` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        pdf_path = os.path.join(output_dir, \"group_mean_and_sem_delta_ap.pdf\")\n",
    "\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            # Group the data by Group\n",
    "            group_data = self.phase_peak_dataframe.groupby(\"Group\")\n",
    "\n",
    "            for group, group_df in group_data:\n",
    "                # Calculate ΔAP (Late - Early)\n",
    "                group_df[\"Delta_AP\"] = group_df[\"Late_AP_Count\"] - group_df[\"Early_AP_Count\"]\n",
    "\n",
    "                # Separate Before and After conditions\n",
    "                before_data = group_df[group_df[\"Label\"] == \"Before\"]\n",
    "                after_data = group_df[group_df[\"Label\"] == \"After\"]\n",
    "\n",
    "                # Calculate mean and SEM for Before and After\n",
    "                mean_before = before_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].mean()\n",
    "                sem_before = before_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].sem()\n",
    "\n",
    "                mean_after = after_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].mean()\n",
    "                sem_after = after_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].sem()\n",
    "\n",
    "                # Create the plot\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "                # Plot mean and SEM for Before\n",
    "                ax.errorbar(\n",
    "                    mean_before.index,\n",
    "                    mean_before,\n",
    "                    yerr=sem_before,\n",
    "                    label=\"Before Luciferin\",\n",
    "                    fmt=\"o-\",\n",
    "                    color=\"blue\",\n",
    "                    capsize=3\n",
    "                )\n",
    "\n",
    "                # Plot mean and SEM for After\n",
    "                ax.errorbar(\n",
    "                    mean_after.index,\n",
    "                    mean_after,\n",
    "                    yerr=sem_after,\n",
    "                    label=\"After Luciferin\",\n",
    "                    fmt=\"o-\",\n",
    "                    color=\"orange\",\n",
    "                    capsize=3\n",
    "                )\n",
    "\n",
    "                # Formatting\n",
    "                ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\n",
    "                ax.set_xlabel(\"Sweep Number\")\n",
    "                ax.set_ylabel(\"Mean ΔAP (Late - Early) ± SEM\")\n",
    "                ax.set_title(f\"Mean ΔAP (Late - Early) ± SEM for Group: {group}\")\n",
    "                ax.legend()\n",
    "                ax.grid()\n",
    "\n",
    "                # Save the page to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved group mean and SEM ΔAP PDF to: {pdf_path}\")\n",
    "            \n",
    "    def create_group_pdf_with_mean_and_sem_and_store_data(self, output_dir):\n",
    "        \"\"\"\n",
    "        Generate a PDF for each group, with a single page showing the mean and SEM\n",
    "        of ΔAP (Late-Early) across sweeps for the group, and store data as an attribute.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Directory to save the generated PDF files.\n",
    "        \"\"\"\n",
    "        if self.phase_peak_dataframe is None or self.phase_peak_dataframe.empty:\n",
    "            print(\"No data in `phase_peak_dataframe`. Run `process_peaks_by_phase` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        pdf_path = os.path.join(output_dir, \"group_mean_and_sem_delta_ap.pdf\")\n",
    "\n",
    "        # Initialize a dictionary to store data\n",
    "        self.group_delta_ap_stats = {}\n",
    "\n",
    "        with PdfPages(pdf_path) as pdf:\n",
    "            # Group the data by Group\n",
    "            group_data = self.phase_peak_dataframe.groupby(\"Group\")\n",
    "\n",
    "            for group, group_df in group_data:\n",
    "                # Calculate ΔAP (Late - Early)\n",
    "                group_df[\"Delta_AP\"] = group_df[\"Late_AP_Count\"] - group_df[\"Early_AP_Count\"]\n",
    "\n",
    "                # Separate Before and After conditions\n",
    "                before_data = group_df[group_df[\"Label\"] == \"Before\"]\n",
    "                after_data = group_df[group_df[\"Label\"] == \"After\"]\n",
    "\n",
    "                # Calculate mean and SEM for Before and After\n",
    "                mean_before = before_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].mean()\n",
    "                sem_before = before_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].sem()\n",
    "                mean_after = after_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].mean()\n",
    "                sem_after = after_data.groupby(\"Sweep_Number\")[\"Delta_AP\"].sem()\n",
    "\n",
    "                # Save raw data for stats\n",
    "                self.group_delta_ap_stats[group] = {\n",
    "                    \"before\": before_data[[\"Sweep_Number\", \"Delta_AP\"]],\n",
    "                    \"after\": after_data[[\"Sweep_Number\", \"Delta_AP\"]],\n",
    "                    \"mean_before\": mean_before,\n",
    "                    \"sem_before\": sem_before,\n",
    "                    \"mean_after\": mean_after,\n",
    "                    \"sem_after\": sem_after,\n",
    "                }\n",
    "\n",
    "                # Create the plot\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "                # Plot mean and SEM for Before\n",
    "                ax.errorbar(\n",
    "                    mean_before.index,\n",
    "                    mean_before,\n",
    "                    yerr=sem_before,\n",
    "                    label=\"Before Luciferin\",\n",
    "                    fmt=\"o-\",\n",
    "                    color=\"blue\",\n",
    "                    capsize=3\n",
    "                )\n",
    "\n",
    "                # Plot mean and SEM for After\n",
    "                ax.errorbar(\n",
    "                    mean_after.index,\n",
    "                    mean_after,\n",
    "                    yerr=sem_after,\n",
    "                    label=\"After Luciferin\",\n",
    "                    fmt=\"o-\",\n",
    "                    color=\"orange\",\n",
    "                    capsize=3\n",
    "                )\n",
    "\n",
    "                # Formatting\n",
    "                ax.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.8)\n",
    "                ax.set_xlabel(\"Sweep Number\")\n",
    "                ax.set_ylabel(\"Mean ΔAP (Late - Early) ± SEM\")\n",
    "                ax.set_title(f\"Mean ΔAP (Late - Early) ± SEM for Group: {group}\")\n",
    "                ax.legend()\n",
    "                ax.grid()\n",
    "\n",
    "                # Save the page to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved group mean and SEM ΔAP PDF to: {pdf_path}\")\n",
    "\n",
    "    def run_two_way_anova_with_correction(self):\n",
    "        \"\"\"\n",
    "        Perform a two-way repeated-measures ANOVA for each group comparing Before and After\n",
    "        conditions at each stimulus sweep, with Bonferroni correction for multiple comparisons.\n",
    "\n",
    "        Returns:\n",
    "            results (dict): Dictionary of ANOVA results with corrected p-values for each group.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"group_delta_ap_stats\"):\n",
    "            print(\"No stored data. Run `create_group_pdf_with_mean_and_sem_and_store_data` first.\")\n",
    "            return\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for group, data in self.group_delta_ap_stats.items():\n",
    "            # Combine before and after data into one DataFrame\n",
    "            combined_data = pd.concat(\n",
    "                [\n",
    "                    data[\"before\"].assign(Condition=\"Before\"),\n",
    "                    data[\"after\"].assign(Condition=\"After\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Aggregate data to ensure one observation per Sweep_Number and Condition\n",
    "            combined_data = combined_data.groupby([\"Sweep_Number\", \"Condition\"])[\"Delta_AP\"].mean().reset_index()\n",
    "\n",
    "            # Perform repeated-measures ANOVA\n",
    "            aovrm = AnovaRM(combined_data, depvar=\"Delta_AP\", subject=\"Sweep_Number\", within=[\"Condition\"])\n",
    "            anova_results = aovrm.fit()\n",
    "\n",
    "            # Extract p-values and apply Bonferroni correction\n",
    "            p_values = [anova_results.anova_table[\"Pr > F\"].iloc[0]]  # Single condition comparison\n",
    "            corrected_p_values = multipletests(p_values, method=\"bonferroni\")[1]\n",
    "\n",
    "            # Store results\n",
    "            results[group] = {\n",
    "                \"anova_summary\": anova_results.summary(),\n",
    "                \"corrected_p_values\": corrected_p_values\n",
    "            }\n",
    "\n",
    "            # Print results for debugging\n",
    "            print(f\"\\nGroup: {group}\")\n",
    "            print(anova_results)\n",
    "            print(f\"Corrected P-Values: {corrected_p_values}\")\n",
    "\n",
    "        return results, combined_data\n",
    "        \n",
    "    def create_group_pdf_with_peaks_complex(\n",
    "            self,\n",
    "            output_dir,\n",
    "            height=None,\n",
    "            prominence=None,\n",
    "            distance=None,\n",
    "            width=None,\n",
    "            time_range=None,\n",
    "            early_phase=(0, 0.5),\n",
    "            late_phase=(0.5, 1.0)):\n",
    "        \"\"\"\n",
    "        Generate a multi-row, multi-column PDF for each group with modular design.\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}_complex.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "                for recording_id in recording_ids:\n",
    "                    before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                    after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                    if before_entry.empty or after_entry.empty:\n",
    "                        print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                        continue\n",
    "\n",
    "                    before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                    after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                    before_abf = pyabf.ABF(before_file)\n",
    "                    after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                    fig = plt.figure(figsize=(20, 24))\n",
    "                    fig.suptitle(f\"Group: {group}, Recording ID: {recording_id}\", fontsize=16)\n",
    "                    gs = gridspec.GridSpec(9, 4, figure=fig)\n",
    "                    plt.subplots_adjust(wspace=0.25, hspace=0.35)\n",
    "\n",
    "                    bar_data = {\"sweep\": [], \"early_before\": [], \"late_before\": [], \"early_after\": [], \"late_after\": []}\n",
    "                    before_isi = []\n",
    "                    after_isi = []\n",
    "\n",
    "                    for sweep_number in range(min(9, len(before_abf.sweepList))):  # Limit to 9 rows\n",
    "                        time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                            before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                        )\n",
    "                        time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                            after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                        )\n",
    "\n",
    "                        bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                        bar_data[\"early_before\"].append(early_before)\n",
    "                        bar_data[\"late_before\"].append(late_before)\n",
    "                        bar_data[\"early_after\"].append(early_after)\n",
    "                        bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                        before_isi.extend(calculate_isi_distribution(peaks_before, time_before))\n",
    "                        after_isi.extend(calculate_isi_distribution(peaks_after, time_after))\n",
    "\n",
    "                        ax_before = fig.add_subplot(gs[sweep_number, 0])\n",
    "                        plot_peaks(ax_before, time_before, voltage_before, peaks_before, \"Before\", \"C0\", f\"Sweep {sweep_number + 1} (Before)\", ylabel=\"Voltage (mV)\")\n",
    "\n",
    "                        ax_after = fig.add_subplot(gs[sweep_number, 1])\n",
    "                        plot_peaks(ax_after, time_after, voltage_after, peaks_after, \"After\", \"C1\", f\"Sweep {sweep_number + 1} (After)\")\n",
    "\n",
    "                    ax_bar = fig.add_subplot(gs[0:2, 2:])\n",
    "                    generate_wide_plot_1(ax_bar, bar_data)\n",
    "                    \n",
    "                    ax_plot_2 = fig.add_subplot(gs[2:4, 2:])\n",
    "                    generate_wide_plot_2(ax_plot_2, bar_data)\n",
    "\n",
    "                    ax_plot_3 = fig.add_subplot(gs[4:6, 2:])\n",
    "                    generate_wide_plot_3(ax_plot_3, before_isi, after_isi)\n",
    "                    \n",
    "                    ## Add more plots here to fill the remaining space\n",
    "                    # Calculate and plot fractions of early spikes\n",
    "                    fractions = calculate_fractions(bar_data)\n",
    "                    ax_fraction = fig.add_subplot(gs[6:8, 2:])\n",
    "                    generate_wide_plot_fraction(ax_fraction, fractions)\n",
    "                    \n",
    "                    ### add the ECDF plot \n",
    "                    ax_ecdf = fig.add_subplot(gs[8:9, 2:])\n",
    "                    generate_ecdf_plot(ax_ecdf, fractions)\n",
    "\n",
    "                    pdf.savefig(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "                print(f\"Saved complex PDF for group '{group}' to: {pdf_path}\")\n",
    "\n",
    "    def create_group_pooled_ecdf(\n",
    "        self,\n",
    "        output_dir,\n",
    "        height=None,\n",
    "        prominence=None,\n",
    "        distance=None,\n",
    "        width=None,\n",
    "        time_range=None,\n",
    "        early_phase=(0.1, 0.40),\n",
    "        late_phase=(0.5, 1.12),\n",
    "        x_min=-1.0,\n",
    "        x_max=1.0,\n",
    "        step_mode=\"post\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a pooled ECDF plot for each group by combining fraction differences (early_fraction - late_fraction)\n",
    "        from all cells and sweeps. A single PDF per group is saved containing just the ECDF plot.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save the resulting PDFs.\n",
    "        height : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        prominence : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        distance : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        width : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        time_range : tuple or None\n",
    "            Time range for analyzing data. If None, use full sweep.\n",
    "        early_phase : tuple (start, end)\n",
    "            Time window defining the early phase fraction of the sweep.\n",
    "        late_phase : tuple (start, end)\n",
    "            Time window defining the late phase fraction of the sweep.\n",
    "        x_min : float\n",
    "            Minimum x-value for plotting ECDF. Default is -1.\n",
    "        x_max : float\n",
    "            Maximum x-value for plotting ECDF. Default is 1.\n",
    "        step_mode : str\n",
    "            How the step plot is drawn. Default \"post\".\n",
    "            Options: \"pre\", \"post\", \"mid\".\n",
    "        \"\"\"\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            # Collect all fraction differences across all recordings of this group\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                # Process each sweep and accumulate early/late spike counts\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                # Compute fractions for this recording\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                # Filter out NaNs\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                # Append to pooled lists\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "            # Now plot the pooled ECDF for this group\n",
    "            pdf_path = os.path.join(output_dir, f\"{group}_pooled_ecdf.pdf\")\n",
    "\n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "                # Sort the data\n",
    "                diff_before_sorted = np.sort(pooled_diff_before) if len(pooled_diff_before) > 0 else []\n",
    "                diff_after_sorted = np.sort(pooled_diff_after) if len(pooled_diff_after) > 0 else []\n",
    "\n",
    "                def ecdf(data):\n",
    "                    if len(data) == 0:\n",
    "                        # No data: just plot a flat line at zero\n",
    "                        x = [x_min, x_max]\n",
    "                        y = [0.0, 0.0]\n",
    "                    else:\n",
    "                        # Compute normal ECDF\n",
    "                        y = np.arange(1, len(data) + 1) / len(data)\n",
    "                        # Add starting point (x_min,0.0) and ending point (x_max,1.0)\n",
    "                        x = np.concatenate([[x_min], data, [x_max]])\n",
    "                        y = np.concatenate([[0.0], y, [1.0]])\n",
    "                    return x, y\n",
    "\n",
    "                x_before, y_before = ecdf(diff_before_sorted)\n",
    "                x_after, y_after = ecdf(diff_after_sorted)\n",
    "\n",
    "                # Plot ECDFs without axis lines\n",
    "                ax.step(x_before, y_before, color=\"gray\", label=\"Before\", where=step_mode)\n",
    "                ax.step(x_after, y_after, color=\"blue\", label=\"After\", where=step_mode)\n",
    "\n",
    "                # Set axis limits\n",
    "                ax.set_xlim(x_min, x_max)\n",
    "                ax.set_ylim(0, 1.0)\n",
    "\n",
    "                # Remove axis lines (spines)\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "                ax.set_xlabel(\"Fraction Difference (Early - Late)\", fontsize=12)\n",
    "                ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "                ax.set_title(\"Pooled ECDF of Fraction Differences (Before vs. After)\", fontsize=14)\n",
    "                ax.legend()\n",
    "\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved pooled ECDF PDF for group '{group}' to: {pdf_path}\")\n",
    "\n",
    "    def create_group_pooled_ecdf_svg(\n",
    "        self,\n",
    "        output_dir,\n",
    "        height=None,\n",
    "        prominence=None,\n",
    "        distance=None,\n",
    "        width=None,\n",
    "        time_range=None,\n",
    "        early_phase=(0.1, 0.40),\n",
    "        late_phase=(0.5, 1.12),\n",
    "        x_min=-1.0,\n",
    "        x_max=1.0,\n",
    "        step_mode=\"post\",\n",
    "        fig_width=8,\n",
    "        fig_height=6,\n",
    "        remove_spines=True,\n",
    "        transparent=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a pooled ECDF plot for each group by combining fraction differences (early_fraction - late_fraction)\n",
    "        from all cells and sweeps. A single .svg per group is saved containing the ECDF plot.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save the resulting SVGs.\n",
    "        height : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        prominence : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        distance : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        width : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        time_range : tuple or None\n",
    "            Time range for analyzing data. If None, use full sweep.\n",
    "        early_phase : tuple (start, end)\n",
    "            Time window defining the early phase fraction of the sweep.\n",
    "        late_phase : tuple (start, end)\n",
    "            Time window defining the late phase fraction of the sweep.\n",
    "        x_min : float\n",
    "            Minimum x-value for plotting ECDF.\n",
    "        x_max : float\n",
    "            Maximum x-value for plotting ECDF.\n",
    "        step_mode : str\n",
    "            How the step plot is drawn. Default \"post\".\n",
    "            Options: \"pre\", \"post\", \"mid\".\n",
    "        fig_width : float\n",
    "            Width of the figure in inches.\n",
    "        fig_height : float\n",
    "            Height of the figure in inches.\n",
    "        remove_spines : bool\n",
    "            If True, removes axis spines for a cleaner look.\n",
    "        transparent : bool\n",
    "            If True, sets the plot background to transparent.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        # Ensure text remains editable in SVG\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            # Collect all fraction differences across all recordings of this group\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                # Process each sweep and accumulate early/late spike counts\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                # Compute fractions for this recording\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                # Filter out NaNs\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                # Append to pooled lists\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "            # Now plot the pooled ECDF for this group\n",
    "            svg_path = os.path.join(output_dir, f\"{group}_pooled_ecdf.svg\")\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "            # Set transparent background if requested\n",
    "            if transparent:\n",
    "                fig.patch.set_facecolor('none')\n",
    "                fig.patch.set_alpha(0)\n",
    "                ax.set_facecolor('none')\n",
    "\n",
    "            # Sort the data\n",
    "            diff_before_sorted = np.sort(pooled_diff_before) if len(pooled_diff_before) > 0 else []\n",
    "            diff_after_sorted = np.sort(pooled_diff_after) if len(pooled_diff_after) > 0 else []\n",
    "\n",
    "            def ecdf(data):\n",
    "                if len(data) == 0:\n",
    "                    # No data: just plot a flat line at zero\n",
    "                    x = [x_min, x_max]\n",
    "                    y = [0.0, 0.0]\n",
    "                else:\n",
    "                    # Compute normal ECDF\n",
    "                    y = np.arange(1, len(data) + 1) / len(data)\n",
    "                    # Add starting point (x_min,0.0) and ending point (x_max,1.0)\n",
    "                    x = np.concatenate([[x_min], data, [x_max]])\n",
    "                    y = np.concatenate([[0.0], y, [1.0]])\n",
    "                return x, y\n",
    "\n",
    "            x_before, y_before = ecdf(diff_before_sorted)\n",
    "            x_after, y_after = ecdf(diff_after_sorted)\n",
    "\n",
    "            # Plot ECDFs\n",
    "            ax.step(x_before, y_before, color=\"gray\", label=\"Before\", where=step_mode)\n",
    "            ax.step(x_after, y_after, color=\"blue\", label=\"After\", where=step_mode)\n",
    "\n",
    "            # Set axis limits\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "\n",
    "            # Remove spines if requested\n",
    "            if remove_spines:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "            ax.set_xlabel(\"Fraction Difference (Early - Late)\", fontsize=12)\n",
    "            ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "            ax.set_title(\"Pooled ECDF of Fraction Differences (Before vs. After)\", fontsize=14)\n",
    "            ax.legend()\n",
    "\n",
    "            fig.savefig(svg_path, format=\"svg\", bbox_inches='tight', transparent=transparent)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Saved pooled ECDF SVG for group '{group}' to: {svg_path}\")\n",
    "\n",
    "\n",
    "    def export_all_groups_to_svgs(self, output_dir,\n",
    "                                before_label=\"Before\", after_label=\"After\",\n",
    "                                sweep_numbers=None, startAtSec=0, endAtSec=1.5,\n",
    "                                offsetXsec=0.3, offsetYunits=40,\n",
    "                                color_before=None, color_after=\"red\",\n",
    "                                alpha=0.5, linewidth=1,\n",
    "                                dpi=300, add_suptitle=True,\n",
    "                                scaleXms=200, scaleYmV=50):\n",
    "        \"\"\"\n",
    "        Export .svg files for each recording in each group.\n",
    "        Each group will have its own folder, and inside it each recording\n",
    "        will have its own folder. One .svg file per recording.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): Base directory to save all the .svg files.\n",
    "            before_label (str): Label for the \"Before\" condition.\n",
    "            after_label (str): Label for the \"After\" condition.\n",
    "            sweep_numbers (list or None): Sweeps to plot. None = all sweeps.\n",
    "            startAtSec (float): Start time for plotting.\n",
    "            endAtSec (float): End time for plotting.\n",
    "            offsetXsec (float): Horizontal offset per sweep.\n",
    "            offsetYunits (float): Vertical offset per sweep.\n",
    "            color_before (str or None): Color for \"Before\" sweeps.\n",
    "            color_after (str): Color for \"After\" sweeps.\n",
    "            alpha (float): Transparency of lines.\n",
    "            linewidth (float): Width of the plotted lines.\n",
    "            dpi (int): DPI for potential rasterization (mainly for embedded images).\n",
    "            add_suptitle (bool): Whether to add a suptitle with Group and Recording ID.\n",
    "            scaleXms (float): Horizontal scale bar length in ms.\n",
    "            scaleYmV (float): Vertical scale bar height in mV.\n",
    "        \"\"\"\n",
    "        # Ensure text remains editable (not converted to paths)\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for grp in self.unique_groups:\n",
    "            group_dir = os.path.join(output_dir, grp)\n",
    "            os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "            recording_ids = self.get_recording_ids(grp)\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = self.dataframe[(self.dataframe[\"Group\"] == grp) &\n",
    "                                            (self.dataframe[\"Recording_ID\"] == recording_id) &\n",
    "                                            (self.dataframe[\"Label\"] == before_label)]\n",
    "                after_entry = self.dataframe[(self.dataframe[\"Group\"] == grp) &\n",
    "                                            (self.dataframe[\"Recording_ID\"] == recording_id) &\n",
    "                                            (self.dataframe[\"Label\"] == after_label)]\n",
    "\n",
    "                # Skip if we don't have both Before and After\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    continue\n",
    "\n",
    "                # Create a folder for each recording\n",
    "                recording_dir = os.path.join(group_dir, recording_id)\n",
    "                os.makedirs(recording_dir, exist_ok=True)\n",
    "\n",
    "                # Create the figure\n",
    "                fig, axes = plt.subplots(2, 1, figsize=(4, 10), sharex=True)\n",
    "\n",
    "                # Plot Before\n",
    "                self.plot_sweeps(axes[0], grp, recording_id, before_label,\n",
    "                                sweep_numbers=sweep_numbers,\n",
    "                                offsetXsec=offsetXsec, offsetYunits=offsetYunits,\n",
    "                                startAtSec=startAtSec, endAtSec=endAtSec,\n",
    "                                color=color_before, alpha=alpha, linewidth=linewidth,\n",
    "                                hideAxis=True)\n",
    "\n",
    "                # Plot After\n",
    "                self.plot_sweeps(axes[1], grp, recording_id, after_label,\n",
    "                                sweep_numbers=sweep_numbers,\n",
    "                                offsetXsec=offsetXsec, offsetYunits=offsetYunits,\n",
    "                                startAtSec=startAtSec, endAtSec=endAtSec,\n",
    "                                color=color_after, alpha=alpha, linewidth=linewidth,\n",
    "                                hideAxis=True)\n",
    "\n",
    "                if add_suptitle:\n",
    "                    fig.suptitle(f\"Group: {grp}, Recording: {recording_id}\", fontsize=14)\n",
    "\n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Add a fixed scalebar to the bottom axis\n",
    "                self.plot_scalebar(axes[1], scaleXms=scaleXms, scaleYmV=scaleYmV,\n",
    "                                hideTicks=True, hideFrame=True)\n",
    "\n",
    "                # Save as SVG\n",
    "                svg_path = os.path.join(recording_dir, f\"{recording_id}.svg\")\n",
    "                fig.savefig(svg_path, format=\"svg\", dpi=dpi, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "\n",
    "    def import_csv_and_plot_mean_peaks_with_error_bars_svg(self, csv_path, output_dir,\n",
    "                                                        fig_width=10, fig_height=6,\n",
    "                                                        ymin=None, ymax=None):\n",
    "        \"\"\"\n",
    "        Import a CSV file containing peak data and plot the mean AP counts\n",
    "        with SEM for \"Before\" (grey) and \"After\" (blue) at each sweep number as error bars.\n",
    "        Save as .svg files, one per group, with a transparent background and customizable figure size.\n",
    "        Allows specifying y-axis limits to keep the same scale across plots.\n",
    "\n",
    "        Args:\n",
    "            csv_path (str): Path to the CSV file containing peak data.\n",
    "            output_dir (str): Directory to save the output .svg files.\n",
    "            fig_width (float): Width of the figure in inches.\n",
    "            fig_height (float): Height of the figure in inches.\n",
    "            ymin (float or None): Minimum y-limit. If None, it will auto-scale.\n",
    "            ymax (float or None): Maximum y-limit. If None, it will auto-scale.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Ensure text remains editable in SVG\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "\n",
    "        # Load the CSV into a DataFrame\n",
    "        try:\n",
    "            peak_data = pd.read_csv(csv_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {csv_path}\")\n",
    "            return\n",
    "\n",
    "        # Validate the required columns\n",
    "        required_columns = [\"Group\", \"Recording_ID\", \"Label\", \"Sweep_Number\", \"AP_Count\"]\n",
    "        if not all(col in peak_data.columns for col in required_columns):\n",
    "            print(\"The CSV file is missing one or more required columns.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Get unique groups\n",
    "        groups = peak_data[\"Group\"].unique()\n",
    "        for group in groups:\n",
    "            group_data = peak_data[peak_data[\"Group\"] == group]\n",
    "\n",
    "            # Compute mean and SEM for \"Before\" and \"After\" by Sweep_Number\n",
    "            before_data = group_data[group_data[\"Label\"] == \"Before\"]\n",
    "            after_data = group_data[group_data[\"Label\"] == \"After\"]\n",
    "\n",
    "            mean_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "            sem_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "            mean_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "            sem_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "            # Plot the data with the specified figure size\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "            \n",
    "            # Set figure and axis backgrounds to transparent\n",
    "            fig.patch.set_facecolor('none')\n",
    "            fig.patch.set_alpha(0)\n",
    "            ax.set_facecolor('none')\n",
    "\n",
    "            sweep_numbers = mean_before.index\n",
    "\n",
    "            # Plot \"Before\" means and SEM as error bars (grey)\n",
    "            ax.errorbar(\n",
    "                sweep_numbers - 0.2,  # Offset \"Before\" slightly to the left\n",
    "                mean_before,\n",
    "                yerr=sem_before,\n",
    "                fmt=\"o\",  # Circle markers\n",
    "                label=\"Before\",\n",
    "                color=\"gray\",\n",
    "                capsize=5,\n",
    "                markersize=8,\n",
    "            )\n",
    "\n",
    "            # Plot \"After\" means and SEM as error bars (blue)\n",
    "            ax.errorbar(\n",
    "                sweep_numbers + 0.2,  # Offset \"After\" slightly to the right\n",
    "                mean_after,\n",
    "                yerr=sem_after,\n",
    "                fmt=\"o\",  # Circle markers\n",
    "                label=\"After\",\n",
    "                color=\"blue\",\n",
    "                capsize=5,\n",
    "                markersize=8,\n",
    "            )\n",
    "\n",
    "            # Remove grid lines, keep axis lines\n",
    "            ax.grid(False)\n",
    "\n",
    "            # Set y-limits if provided\n",
    "            if ymin is not None or ymax is not None:\n",
    "                ax.set_ylim(ymin, ymax)\n",
    "\n",
    "            # Add labels and legend\n",
    "            ax.set_xlabel(\"Sweep Number\")\n",
    "            ax.set_ylabel(\"Mean AP Count (± SEM)\")\n",
    "            ax.legend()\n",
    "\n",
    "            # Create a subdirectory for the group\n",
    "            group_dir = os.path.join(output_dir, group)\n",
    "            os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "            svg_path = os.path.join(group_dir, f\"{group}_mean_peaks.svg\")\n",
    "            fig.savefig(svg_path, format=\"svg\", bbox_inches='tight', transparent=True)\n",
    "            plt.close(fig)\n",
    "\n",
    "        print(f\"Saved error bar plots as transparent SVG files in: {output_dir}\")\n",
    "  \n",
    "    def create_group_pooled_mean_and_individual_traces_ecdf(\n",
    "        self,\n",
    "        output_dir,\n",
    "        height=None,\n",
    "        prominence=None,\n",
    "        distance=None,\n",
    "        width=None,\n",
    "        time_range=None,\n",
    "        early_phase=(0.1, 0.40),\n",
    "        late_phase=(0.5, 1.12),\n",
    "        x_min=-1.0,\n",
    "        x_max=1.0,\n",
    "        step_mode=\"post\",\n",
    "        fig_width=8,\n",
    "        fig_height=6,\n",
    "        remove_spines=True,\n",
    "        transparent=True,\n",
    "        individual_line_alpha=0.3,\n",
    "        individual_line_style=\":\",\n",
    "        individual_line_width=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Similar to create_group_pooled_ecdf, but also plots individual ECDF traces for each recording \n",
    "        that contributed to the pooled ECDF. The pooled ECDF lines (\"Before\" and \"After\") are plotted \n",
    "        on top, and individual recordings' ECDF lines are shown underneath with thinner, dotted lines \n",
    "        and higher transparency.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save the resulting SVGs.\n",
    "        height : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        prominence : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        distance : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        width : float or None\n",
    "            Peak detection parameter passed to process_sweep_data.\n",
    "        time_range : tuple or None\n",
    "            Time range for analyzing data. If None, use full sweep.\n",
    "        early_phase : tuple (start, end)\n",
    "            Time window defining the early phase fraction of the sweep.\n",
    "        late_phase : tuple (start, end)\n",
    "            Time window defining the late phase fraction of the sweep.\n",
    "        x_min : float\n",
    "            Minimum x-value for plotting ECDF.\n",
    "        x_max : float\n",
    "            Maximum x-value for plotting ECDF.\n",
    "        step_mode : str\n",
    "            How the step plot is drawn. Default \"post\".\n",
    "            Options: \"pre\", \"post\", \"mid\".\n",
    "        fig_width : float\n",
    "            Width of the figure in inches.\n",
    "        fig_height : float\n",
    "            Height of the figure in inches.\n",
    "        remove_spines : bool\n",
    "            If True, removes axis spines for a cleaner look.\n",
    "        transparent : bool\n",
    "            If True, sets the plot background to transparent.\n",
    "        individual_line_alpha : float\n",
    "            Transparency for the individual ECDF lines.\n",
    "        individual_line_style : str\n",
    "            Line style for individual ECDF lines (e.g., \":\", \"--\", \"-.\").\n",
    "        individual_line_width : float\n",
    "            Line width for individual ECDF lines.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        # Ensure text remains editable in SVG\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            # Lists to hold individual recordings' differences for separate plotting\n",
    "            individual_diffs_before = []\n",
    "            individual_diffs_after = []\n",
    "\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                # Process each sweep and accumulate early/late spike counts\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                # Compute fractions for this recording\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                # Filter out NaNs\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                # Append to pooled lists\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "                # Store individual differences for later plotting\n",
    "                individual_diffs_before.append(diff_before)\n",
    "                individual_diffs_after.append(diff_after)\n",
    "\n",
    "            # Now plot the pooled ECDF for this group along with individual traces\n",
    "            svg_path = os.path.join(output_dir, f\"{group}_pooled_ecdf_individual.svg\")\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "            if transparent:\n",
    "                fig.patch.set_facecolor('none')\n",
    "                fig.patch.set_alpha(0)\n",
    "                ax.set_facecolor('none')\n",
    "\n",
    "            def ecdf(data):\n",
    "                if len(data) == 0:\n",
    "                    # No data: flat line at zero\n",
    "                    x = [x_min, x_max]\n",
    "                    y = [0.0, 0.0]\n",
    "                else:\n",
    "                    # Compute ECDF\n",
    "                    data_sorted = np.sort(data)\n",
    "                    y = np.arange(1, len(data_sorted) + 1) / len(data_sorted)\n",
    "                    x = np.concatenate([[x_min], data_sorted, [x_max]])\n",
    "                    y = np.concatenate([[0.0], y, [1.0]])\n",
    "                return x, y\n",
    "\n",
    "            # Plot individual traces first (Before)\n",
    "            for diffs in individual_diffs_before:\n",
    "                x_i, y_i = ecdf(diffs)\n",
    "                ax.step(x_i, y_i, where=step_mode,\n",
    "                        color=\"gray\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Plot individual traces for After\n",
    "            for diffs in individual_diffs_after:\n",
    "                x_i, y_i = ecdf(diffs)\n",
    "                ax.step(x_i, y_i, where=step_mode,\n",
    "                        color=\"blue\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Now plot pooled ECDF lines on top\n",
    "            x_before, y_before = ecdf(pooled_diff_before)\n",
    "            x_after, y_after = ecdf(pooled_diff_after)\n",
    "\n",
    "            ax.step(x_before, y_before, color=\"gray\", label=\"Before (Pooled)\", where=step_mode, linewidth=2)\n",
    "            ax.step(x_after, y_after, color=\"blue\", label=\"After (Pooled)\", where=step_mode, linewidth=2)\n",
    "\n",
    "            # Set axis limits\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "\n",
    "            if remove_spines:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "            ax.set_xlabel(\"Fraction Difference (Early - Late)\", fontsize=12)\n",
    "            ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "            ax.set_title(\"Pooled ECDF with Individual Traces (Before vs. After)\", fontsize=14)\n",
    "            ax.legend()\n",
    "\n",
    "            fig.savefig(svg_path, format=\"svg\", bbox_inches='tight', transparent=transparent)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Saved pooled ECDF with individual traces for group '{group}' to: {svg_path}\")\n",
    "  \n",
    "    def create_group_pooled_mean_and_individual_sigmoid_ecdf(\n",
    "        self,\n",
    "        output_dir,\n",
    "        height=None,\n",
    "        prominence=None,\n",
    "        distance=None,\n",
    "        width=None,\n",
    "        time_range=None,\n",
    "        early_phase=(0.1, 0.40),\n",
    "        late_phase=(0.5, 1.12),\n",
    "        x_min=-1.0,\n",
    "        x_max=1.0,\n",
    "        fig_width=8,\n",
    "        fig_height=6,\n",
    "        remove_spines=True,\n",
    "        transparent=True,\n",
    "        individual_line_alpha=0.3,\n",
    "        individual_line_style=\":\",\n",
    "        individual_line_width=1,\n",
    "        main_line_width=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Similar to create_group_pooled_mean_and_individual_traces_ecdf, but instead of plotting a step ECDF, \n",
    "        we fit and plot a sigmoid (logistic) curve to both the individual and pooled distributions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save the resulting SVGs.\n",
    "        height, prominence, distance, width : Peak detection parameters.\n",
    "        time_range : tuple or None\n",
    "            Time range for analyzing data. If None, use full sweep.\n",
    "        early_phase : tuple (start, end)\n",
    "            Time window defining the early phase fraction.\n",
    "        late_phase : tuple (start, end)\n",
    "            Time window defining the late phase fraction.\n",
    "        x_min, x_max : float\n",
    "            Limits for the x-axis.\n",
    "        fig_width, fig_height : float\n",
    "            Dimensions of the figure.\n",
    "        remove_spines : bool\n",
    "            If True, remove axis spines.\n",
    "        transparent : bool\n",
    "            If True, make background transparent.\n",
    "        individual_line_alpha : float\n",
    "            Transparency for individual ECDF lines.\n",
    "        individual_line_style : str\n",
    "            Line style for individual ECDF lines.\n",
    "        individual_line_width : float\n",
    "            Line width for individual ECDF lines.\n",
    "        main_line_width : float\n",
    "            Line width for the pooled mean sigmoid lines.\n",
    "\n",
    "        The function calculates fraction differences for Before and After for each recording, \n",
    "        fits a logistic curve to their cumulative distribution, and plots them along with the pooled distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        # Ensure text remains editable in SVG\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Logistic (sigmoid) function\n",
    "        def logistic(x, x0, k):\n",
    "            return 1.0 / (1.0 + np.exp(-(x - x0) / k))\n",
    "\n",
    "        def fit_sigmoid(x_data, y_data):\n",
    "            # Initial guesses for x0 and k\n",
    "            # x0 ~ median of data, k based on rough guess\n",
    "            x0_guess = np.median(x_data)\n",
    "            k_guess = (x_max - x_min) / 10\n",
    "            try:\n",
    "                popt, _ = curve_fit(logistic, x_data, y_data, p0=[x0_guess, k_guess])\n",
    "                return popt\n",
    "            except:\n",
    "                # If fitting fails, return default parameters\n",
    "                return (x0_guess, k_guess)\n",
    "\n",
    "        # Function to compute ECDF points (x,y)\n",
    "        # Then we fit a logistic curve to these points\n",
    "        def compute_and_fit(data):\n",
    "            if len(data) == 0:\n",
    "                # No data, return a flat line\n",
    "                xs = np.linspace(x_min, x_max, 200)\n",
    "                ys = np.zeros_like(xs)\n",
    "                return xs, ys\n",
    "\n",
    "            # Sort data\n",
    "            data_sorted = np.sort(data)\n",
    "            y = np.arange(1, len(data_sorted) + 1) / len(data_sorted)\n",
    "            x_full = np.concatenate([[x_min], data_sorted, [x_max]])\n",
    "            y_full = np.concatenate([[0.0], y, [1.0]])\n",
    "\n",
    "            # Fit logistic curve to these ECDF points\n",
    "            # Use the actual data points (excluding artificial endpoints) for better fitting\n",
    "            popt = fit_sigmoid(data_sorted, y)\n",
    "\n",
    "            # Generate a smooth x array and compute the fitted logistic\n",
    "            xs_smooth = np.linspace(x_min, x_max, 200)\n",
    "            ys_smooth = logistic(xs_smooth, *popt)\n",
    "            return xs_smooth, ys_smooth\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            # Lists to hold individual recordings' differences\n",
    "            individual_diffs_before = []\n",
    "            individual_diffs_after = []\n",
    "\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                # Process each sweep\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                # Filter out NaNs\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                # Append to pooled lists\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "                # Store individual differences\n",
    "                individual_diffs_before.append(diff_before)\n",
    "                individual_diffs_after.append(diff_after)\n",
    "\n",
    "            # Now plot the pooled ECDF for this group along with individual traces as sigmoid\n",
    "            svg_path = os.path.join(output_dir, f\"{group}_pooled_sigmoid_ecdf_individual.svg\")\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "            if transparent:\n",
    "                fig.patch.set_facecolor('none')\n",
    "                fig.patch.set_alpha(0)\n",
    "                ax.set_facecolor('none')\n",
    "\n",
    "            # Plot individual traces (Before)\n",
    "            for diffs in individual_diffs_before:\n",
    "                xs, ys = compute_and_fit(diffs)\n",
    "                ax.plot(xs, ys,\n",
    "                        color=\"gray\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Plot individual traces (After)\n",
    "            for diffs in individual_diffs_after:\n",
    "                xs, ys = compute_and_fit(diffs)\n",
    "                ax.plot(xs, ys,\n",
    "                        color=\"blue\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Pooled data\n",
    "            xs_before, ys_before = compute_and_fit(pooled_diff_before)\n",
    "            xs_after, ys_after = compute_and_fit(pooled_diff_after)\n",
    "\n",
    "            # Plot pooled lines on top\n",
    "            ax.plot(xs_before, ys_before, color=\"gray\", label=\"Before (Pooled)\", linewidth=main_line_width)\n",
    "            ax.plot(xs_after, ys_after, color=\"blue\", label=\"After (Pooled)\", linewidth=main_line_width)\n",
    "\n",
    "            # Set axis limits\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "\n",
    "            if remove_spines:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "            ax.set_xlabel(\"Fraction Difference (Early - Late)\", fontsize=12)\n",
    "            ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "            ax.set_title(\"Pooled Sigmoid ECDF with Individual Traces (Before vs. After)\", fontsize=14)\n",
    "            ax.legend()\n",
    "\n",
    "            fig.savefig(svg_path, format=\"svg\", bbox_inches='tight', transparent=transparent)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Saved pooled sigmoid ECDF with individual traces for group '{group}' to: {svg_path}\")\n",
    " \n",
    "    def create_group_pooled_mean_and_individual_sigmoid_ecdf(\n",
    "        self,\n",
    "        output_dir,\n",
    "        height=None,\n",
    "        prominence=None,\n",
    "        distance=None,\n",
    "        width=None,\n",
    "        time_range=None,\n",
    "        early_phase=(0.1, 0.40),\n",
    "        late_phase=(0.5, 1.12),\n",
    "        x_min=-1.0,\n",
    "        x_max=1.0,\n",
    "        fig_width=8,\n",
    "        fig_height=6,\n",
    "        remove_spines=True,\n",
    "        transparent=True,\n",
    "        individual_line_alpha=0.3,\n",
    "        individual_line_style=\":\",\n",
    "        individual_line_width=1,\n",
    "        main_line_width=2,\n",
    "        smoothing_factor=1.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a pooled ECDF plot with individual traces as sigmoid (logistic) curves.\n",
    "        The smoothing_factor parameter scales the fitted 'k' parameter of the logistic,\n",
    "        allowing control over the steepness/gradualness of the sigmoid curves.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save the resulting SVGs.\n",
    "        height, prominence, distance, width : Peak detection parameters.\n",
    "        time_range : tuple or None\n",
    "            Time range for analyzing data. If None, use full sweep.\n",
    "        early_phase : tuple (start, end)\n",
    "            Time window defining the early phase fraction.\n",
    "        late_phase : tuple (start, end)\n",
    "            Time window defining the late phase fraction.\n",
    "        x_min, x_max : float\n",
    "            Limits for the x-axis.\n",
    "        fig_width, fig_height : float\n",
    "            Dimensions of the figure.\n",
    "        remove_spines : bool\n",
    "            If True, remove axis spines.\n",
    "        transparent : bool\n",
    "            If True, make background transparent.\n",
    "        individual_line_alpha : float\n",
    "            Transparency for individual ECDF lines.\n",
    "        individual_line_style : str\n",
    "            Line style for individual ECDF lines.\n",
    "        individual_line_width : float\n",
    "            Line width for individual ECDF lines.\n",
    "        main_line_width : float\n",
    "            Line width for the pooled mean sigmoid lines.\n",
    "        smoothing_factor : float\n",
    "            Factor by which to multiply the fitted 'k' parameter to adjust smoothing.\n",
    "            >1 makes the curve more gradual (smoother), <1 makes it steeper.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        # Ensure text remains editable in SVG\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Logistic (sigmoid) function\n",
    "        def logistic(x, x0, k):\n",
    "            return 1.0 / (1.0 + np.exp(-(x - x0) / k))\n",
    "\n",
    "        def fit_sigmoid(x_data, y_data):\n",
    "            x0_guess = np.median(x_data)\n",
    "            k_guess = (x_max - x_min) / 10\n",
    "            try:\n",
    "                popt, _ = curve_fit(logistic, x_data, y_data, p0=[x0_guess, k_guess])\n",
    "                # Adjust k by smoothing_factor\n",
    "                popt = (popt[0], popt[1] * smoothing_factor)\n",
    "                return popt\n",
    "            except:\n",
    "                # If fitting fails, return defaults adjusted by smoothing_factor\n",
    "                return (x0_guess, k_guess * smoothing_factor)\n",
    "\n",
    "        def compute_and_fit(data):\n",
    "            if len(data) == 0:\n",
    "                # No data, return a flat line\n",
    "                xs = np.linspace(x_min, x_max, 200)\n",
    "                ys = np.zeros_like(xs)\n",
    "                return xs, ys\n",
    "\n",
    "            # Sort data\n",
    "            data_sorted = np.sort(data)\n",
    "            y = np.arange(1, len(data_sorted) + 1) / len(data_sorted)\n",
    "\n",
    "            # Fit logistic curve to actual data points (no artificial endpoints for fitting)\n",
    "            popt = fit_sigmoid(data_sorted, y)\n",
    "\n",
    "            # Generate a smooth x array and compute the fitted logistic\n",
    "            xs_smooth = np.linspace(x_min, x_max, 200)\n",
    "            ys_smooth = logistic(xs_smooth, *popt)\n",
    "            return xs_smooth, ys_smooth\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            individual_diffs_before = []\n",
    "            individual_diffs_after = []\n",
    "\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "                individual_diffs_before.append(diff_before)\n",
    "                individual_diffs_after.append(diff_after)\n",
    "\n",
    "            svg_path = os.path.join(output_dir, f\"{group}_pooled_sigmoid_ecdf_individual.svg\")\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "            if transparent:\n",
    "                fig.patch.set_facecolor('none')\n",
    "                fig.patch.set_alpha(0)\n",
    "                ax.set_facecolor('none')\n",
    "\n",
    "            # Plot individual traces (Before)\n",
    "            for diffs in individual_diffs_before:\n",
    "                xs, ys = compute_and_fit(diffs)\n",
    "                ax.plot(xs, ys, color=\"gray\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Plot individual traces (After)\n",
    "            for diffs in individual_diffs_after:\n",
    "                xs, ys = compute_and_fit(diffs)\n",
    "                ax.plot(xs, ys, color=\"blue\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Pooled data\n",
    "            xs_before, ys_before = compute_and_fit(pooled_diff_before)\n",
    "            xs_after, ys_after = compute_and_fit(pooled_diff_after)\n",
    "\n",
    "            # Plot pooled lines on top\n",
    "            ax.plot(xs_before, ys_before, color=\"gray\", label=\"Before (Pooled)\", linewidth=main_line_width)\n",
    "            ax.plot(xs_after, ys_after, color=\"blue\", label=\"After (Pooled)\", linewidth=main_line_width)\n",
    "\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "\n",
    "            if remove_spines:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "            ax.set_xlabel(\"Fraction Difference (Early - Late)\", fontsize=12)\n",
    "            ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "            ax.set_title(\"Pooled Sigmoid ECDF with Individual Traces (Before vs. After)\", fontsize=14)\n",
    "            ax.legend()\n",
    "\n",
    "            fig.savefig(svg_path, format=\"svg\", bbox_inches='tight', transparent=transparent)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Saved pooled sigmoid ECDF with individual traces for group '{group}' to: {svg_path}\") \n",
    "                     \n",
    "    def create_group_pooled_mean_and_individual_smoothed_ecdf(\n",
    "        self,\n",
    "        output_dir,\n",
    "        height=None,\n",
    "        prominence=None,\n",
    "        distance=None,\n",
    "        width=None,\n",
    "        time_range=None,\n",
    "        early_phase=(0.1, 0.40),\n",
    "        late_phase=(0.5, 1.12),\n",
    "        x_min=-1.0,\n",
    "        x_max=1.0,\n",
    "        fig_width=8,\n",
    "        fig_height=6,\n",
    "        remove_spines=True,\n",
    "        transparent=True,\n",
    "        individual_line_alpha=0.3,\n",
    "        individual_line_style=\":\",\n",
    "        individual_line_width=1,\n",
    "        main_line_width=2,\n",
    "        smoothing_window=5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a pooled ECDF plot with individual traces, applying a simple moving average \n",
    "        smoothing directly onto the ECDF (no sigmoid, no external filters).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_dir : str\n",
    "            Directory to save the resulting SVGs.\n",
    "        height, prominence, distance, width : Peak detection parameters.\n",
    "        time_range : tuple or None\n",
    "            Time range for analyzing data. If None, use full sweep.\n",
    "        early_phase : tuple (start, end)\n",
    "            Time window defining the early phase fraction.\n",
    "        late_phase : tuple (start, end)\n",
    "            Time window defining the late phase fraction.\n",
    "        x_min : float\n",
    "            Minimum x-value for plotting ECDF.\n",
    "        x_max : float\n",
    "            Maximum x-value for plotting ECDF.\n",
    "        fig_width, fig_height : float\n",
    "            Dimensions of the figure in inches.\n",
    "        remove_spines : bool\n",
    "            If True, remove axis spines.\n",
    "        transparent : bool\n",
    "            If True, make background transparent.\n",
    "        individual_line_alpha : float\n",
    "            Transparency for individual ECDF lines.\n",
    "        individual_line_style : str\n",
    "            Line style for individual ECDF lines.\n",
    "        individual_line_width : float\n",
    "            Line width for individual ECDF lines.\n",
    "        main_line_width : float\n",
    "            Line width for the pooled mean lines.\n",
    "        smoothing_window : int\n",
    "            Size of the moving average window for smoothing the ECDF.\n",
    "            Must be an odd number greater than 1 for best results.\n",
    "\n",
    "        The method calculates ECDF points for each dataset and then applies a \n",
    "        simple moving average to the ECDF y-values to produce a smoother curve.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to plot. Run `process_data` first.\")\n",
    "            return\n",
    "\n",
    "        # Ensure text remains editable in SVG\n",
    "        plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        def compute_ecdf(data):\n",
    "            if len(data) == 0:\n",
    "                # No data: flat line at zero\n",
    "                x = np.linspace(x_min, x_max, 200)\n",
    "                y = np.zeros_like(x)\n",
    "                return x, y\n",
    "            data_sorted = np.sort(data)\n",
    "            y = np.arange(1, len(data_sorted) + 1) / len(data_sorted)\n",
    "            # Add boundaries at x_min and x_max\n",
    "            x_full = np.concatenate([[x_min], data_sorted, [x_max]])\n",
    "            y_full = np.concatenate([[0.0], y, [1.0]])\n",
    "            return x_full, y_full\n",
    "\n",
    "        def moving_average_smooth(y, window_size):\n",
    "            # Simple moving average:\n",
    "            # For each point, take the mean of points around it defined by the window.\n",
    "            # We'll pad the ends so we can average uniformly.\n",
    "            half_win = window_size // 2\n",
    "            padded_y = np.pad(y, (half_win, half_win), mode='edge')\n",
    "            y_smooth = np.empty_like(y)\n",
    "            for i in range(len(y)):\n",
    "                y_smooth[i] = np.mean(padded_y[i:i+window_size])\n",
    "            return y_smooth\n",
    "\n",
    "        def smooth_ecdf(x, y, window_size):\n",
    "            # Ensure window_size is valid\n",
    "            if window_size < 3 or window_size > len(x) or window_size % 2 == 0:\n",
    "                # If invalid, return original\n",
    "                return x, y\n",
    "            y_smooth = moving_average_smooth(y, window_size)\n",
    "            return x, y_smooth\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            individual_diffs_before = []\n",
    "            individual_diffs_after = []\n",
    "\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    print(f\"Skipping incomplete pair for Recording ID: {recording_id} in group {group}\")\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                # Process each sweep\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number + 1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "                individual_diffs_before.append(diff_before)\n",
    "                individual_diffs_after.append(diff_after)\n",
    "\n",
    "            svg_path = os.path.join(output_dir, f\"{group}_pooled_smoothed_ecdf_individual.svg\")\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "            if transparent:\n",
    "                fig.patch.set_facecolor('none')\n",
    "                fig.patch.set_alpha(0)\n",
    "                ax.set_facecolor('none')\n",
    "\n",
    "            # Plot individual traces (Before)\n",
    "            for diffs in individual_diffs_before:\n",
    "                x_i, y_i = compute_ecdf(diffs)\n",
    "                x_i, y_i = smooth_ecdf(x_i, y_i, smoothing_window)\n",
    "                ax.plot(x_i, y_i, color=\"gray\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Plot individual traces (After)\n",
    "            for diffs in individual_diffs_after:\n",
    "                x_i, y_i = compute_ecdf(diffs)\n",
    "                x_i, y_i = smooth_ecdf(x_i, y_i, smoothing_window)\n",
    "                ax.plot(x_i, y_i, color=\"blue\", alpha=individual_line_alpha,\n",
    "                        linestyle=individual_line_style, linewidth=individual_line_width)\n",
    "\n",
    "            # Pooled data\n",
    "            x_before, y_before = compute_ecdf(pooled_diff_before)\n",
    "            x_before, y_before = smooth_ecdf(x_before, y_before, smoothing_window)\n",
    "            x_after, y_after = compute_ecdf(pooled_diff_after)\n",
    "            x_after, y_after = smooth_ecdf(x_after, y_after, smoothing_window)\n",
    "\n",
    "            # Plot pooled lines on top\n",
    "            ax.plot(x_before, y_before, color=\"gray\", label=\"Before (Pooled)\", linewidth=main_line_width)\n",
    "            ax.plot(x_after, y_after, color=\"blue\", label=\"After (Pooled)\", linewidth=main_line_width)\n",
    "\n",
    "            ax.set_xlim(x_min, x_max)\n",
    "            ax.set_ylim(0, 1.0)\n",
    "\n",
    "            if remove_spines:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "            ax.set_xlabel(\"Fraction Difference (Early - Late)\", fontsize=12)\n",
    "            ax.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
    "            ax.set_title(\"Pooled Smoothed ECDF with Individual Traces (Before vs. After)\", fontsize=14)\n",
    "            ax.legend()\n",
    "\n",
    "            fig.savefig(svg_path, format=\"svg\", bbox_inches='tight', transparent=transparent)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Saved pooled smoothed ECDF with individual traces for group '{group}' to: {svg_path}\")\n",
    "            \n",
    "    def compare_group_distributions(self):\n",
    "        \"\"\"\n",
    "        Compute pooled group-level fraction difference distributions for Before and After conditions,\n",
    "        store them in a DataFrame, and run a Kolmogorov-Smirnov test for each group.\n",
    "\n",
    "        Returns:\n",
    "            group_distribution_df (pd.DataFrame): \n",
    "                A DataFrame in long format with columns:\n",
    "                [\"Group\", \"Condition\", \"FractionDifference\"].\n",
    "                Each row is a single sweep's fraction difference value, pooled at the group level.\n",
    "            \n",
    "            summary_df (pd.DataFrame): \n",
    "                A DataFrame summarizing the KS test results for each group.\n",
    "                Columns: [\"Group\", \"N_Before_Values\", \"N_After_Values\", \"KS_Statistic\", \"KS_pValue\"].\n",
    "        \"\"\"\n",
    "\n",
    "        if self.dataframe is None or self.dataframe.empty:\n",
    "            print(\"No data to analyze. Run `process_data` first.\")\n",
    "            return None, None\n",
    "\n",
    "        # Lists to accumulate group-level fraction differences and results\n",
    "        distribution_records = []\n",
    "        summary_records = []\n",
    "\n",
    "        for group in self.unique_groups:\n",
    "            group_data = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            recording_ids = group_data[\"Recording_ID\"].unique()\n",
    "\n",
    "            pooled_diff_before = []\n",
    "            pooled_diff_after = []\n",
    "\n",
    "            # Process each recording within the group\n",
    "            for recording_id in recording_ids:\n",
    "                before_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"Before\")]\n",
    "                after_entry = group_data[(group_data[\"Recording_ID\"] == recording_id) & (group_data[\"Label\"] == \"After\")]\n",
    "\n",
    "                if before_entry.empty or after_entry.empty:\n",
    "                    # Skip if we don't have both conditions for this recording\n",
    "                    continue\n",
    "\n",
    "                before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "                after_file = after_entry[\"File_Path\"].iloc[0]\n",
    "\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "\n",
    "                bar_data = {\n",
    "                    \"sweep\": [],\n",
    "                    \"early_before\": [],\n",
    "                    \"late_before\": [],\n",
    "                    \"early_after\": [],\n",
    "                    \"late_after\": []\n",
    "                }\n",
    "\n",
    "                # Process each sweep for this recording\n",
    "                for sweep_number in range(len(before_abf.sweepList)):\n",
    "                    # Use your desired parameters for process_sweep_data:\n",
    "                    time_before, voltage_before, peaks_before, early_before, late_before = process_sweep_data(\n",
    "                        before_abf, sweep_number, height=None, prominence=None, distance=None, width=None,\n",
    "                        time_range=None, early_phase=(0.1,0.40), late_phase=(0.5,1.12)\n",
    "                    )\n",
    "                    time_after, voltage_after, peaks_after, early_after, late_after = process_sweep_data(\n",
    "                        after_abf, sweep_number, height=None, prominence=None, distance=None, width=None,\n",
    "                        time_range=None, early_phase=(0.1,0.40), late_phase=(0.5,1.12)\n",
    "                    )\n",
    "\n",
    "                    bar_data[\"sweep\"].append(sweep_number+1)\n",
    "                    bar_data[\"early_before\"].append(early_before)\n",
    "                    bar_data[\"late_before\"].append(late_before)\n",
    "                    bar_data[\"early_after\"].append(early_after)\n",
    "                    bar_data[\"late_after\"].append(late_after)\n",
    "\n",
    "                # Calculate fractions and differences for this recording\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "                diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "                # Filter out NaNs\n",
    "                diff_before = diff_before[~np.isnan(diff_before)]\n",
    "                diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "                # Append to pooled group-level distributions\n",
    "                pooled_diff_before.extend(diff_before)\n",
    "                pooled_diff_after.extend(diff_after)\n",
    "\n",
    "            # Now we have pooled distributions for this group\n",
    "            # Store them in the distribution_records\n",
    "            for val in pooled_diff_before:\n",
    "                distribution_records.append({\n",
    "                    \"Group\": group,\n",
    "                    \"Condition\": \"Before\",\n",
    "                    \"FractionDifference\": val\n",
    "                })\n",
    "            for val in pooled_diff_after:\n",
    "                distribution_records.append({\n",
    "                    \"Group\": group,\n",
    "                    \"Condition\": \"After\",\n",
    "                    \"FractionDifference\": val\n",
    "                })\n",
    "\n",
    "            # Run KS test if both distributions have data\n",
    "            if len(pooled_diff_before) > 0 and len(pooled_diff_after) > 0:\n",
    "                ks_stat, ks_pvalue = ks_2samp(pooled_diff_before, pooled_diff_after)\n",
    "            else:\n",
    "                ks_stat, ks_pvalue = np.nan, np.nan\n",
    "\n",
    "            summary_records.append({\n",
    "                \"Group\": group,\n",
    "                \"N_Before_Values\": len(pooled_diff_before),\n",
    "                \"N_After_Values\": len(pooled_diff_after),\n",
    "                \"KS_Statistic\": ks_stat,\n",
    "                \"KS_pValue\": ks_pvalue\n",
    "            })\n",
    "\n",
    "        # Convert lists to DataFrames\n",
    "        group_distribution_df = pd.DataFrame(distribution_records, \n",
    "                                            columns=[\"Group\", \"Condition\", \"FractionDifference\"])\n",
    "        summary_df = pd.DataFrame(summary_records, \n",
    "                                columns=[\"Group\", \"N_Before_Values\", \"N_After_Values\", \"KS_Statistic\", \"KS_pValue\"])\n",
    "\n",
    "        # Return the DataFrames\n",
    "        return group_distribution_df, summary_df\n",
    "        \n",
    "def plot_peaks(ax, time, voltage, peaks, label, color, title, xlabel=None, ylabel=None):\n",
    "    \"\"\"\n",
    "    Plot peaks on a given axis.\n",
    "    \"\"\"\n",
    "    ax.plot(time, voltage, label=label, color=color)\n",
    "    ax.plot(time[peaks], voltage[peaks], \"x\", color=\"C3\")\n",
    "    ax.set_title(title)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "\n",
    "def process_sweep_data(abf_file, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase):\n",
    "    \"\"\"\n",
    "    Process a single sweep from an ABF file and extract AP counts for early and late phases.\n",
    "    \"\"\"\n",
    "    abf_file.setSweep(sweepNumber=sweep_number)\n",
    "    time = abf_file.sweepX\n",
    "    voltage = abf_file.sweepY\n",
    "\n",
    "    if time_range:\n",
    "        mask = (time >= time_range[0]) & (time <= time_range[1])\n",
    "        time = time[mask]\n",
    "        voltage = voltage[mask]\n",
    "\n",
    "    peaks, _ = find_peaks(\n",
    "        voltage,\n",
    "        height=height,\n",
    "        prominence=prominence,\n",
    "        distance=distance,\n",
    "        width=width\n",
    "    )\n",
    "\n",
    "    early_mask = (time[peaks] >= early_phase[0]) & (time[peaks] <= early_phase[1])\n",
    "    late_mask = (time[peaks] >= late_phase[0]) & (time[peaks] <= late_phase[1])\n",
    "\n",
    "    return time, voltage, peaks, early_mask.sum(), late_mask.sum()\n",
    "\n",
    "\n",
    "def generate_wide_plot_1(ax, bar_data):\n",
    "    \"\"\"\n",
    "    Generate the first wide plot showing AP counts in early and late phases.\n",
    "    \"\"\"\n",
    "    width = 0.2  # Bar width\n",
    "    x = range(len(bar_data[\"sweep\"]))\n",
    "    ax.bar([p - 1.5 * width for p in x], bar_data[\"early_before\"], width, label=\"Early Before\", color=\"lightgray\")\n",
    "    ax.bar([p - 0.5 * width for p in x], bar_data[\"late_before\"], width, label=\"Late Before\", color=\"gray\")\n",
    "    ax.bar([p + 0.5 * width for p in x], bar_data[\"early_after\"], width, label=\"Early After\", color=\"lightblue\")\n",
    "    ax.bar([p + 1.5 * width for p in x], bar_data[\"late_after\"], width, label=\"Late After\", color=\"blue\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(bar_data[\"sweep\"])\n",
    "    ax.set_xlabel(\"Sweep Number\")\n",
    "    ax.set_ylabel(\"AP Count\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"AP Counts in Early and Late Phases\")\n",
    "\n",
    "def calculate_ap_change(bar_data):\n",
    "    \"\"\"\n",
    "    Calculate the change in AP counts between late and early phases for before and after periods.\n",
    "    \"\"\"\n",
    "    ap_change = {\n",
    "        \"stimulus\": [],  # Use the sweep number or any defined stimulus input as x-axis\n",
    "        \"before_change\": [],  # Late - Early (Before)\n",
    "        \"after_change\": []    # Late - Early (After)\n",
    "    }\n",
    "\n",
    "    for i, sweep in enumerate(bar_data[\"sweep\"]):\n",
    "        before_change = bar_data[\"late_before\"][i] - bar_data[\"early_before\"][i]\n",
    "        after_change = bar_data[\"late_after\"][i] - bar_data[\"early_after\"][i]\n",
    "\n",
    "        ap_change[\"stimulus\"].append(sweep)  # Use the sweep number as a proxy for stimulus\n",
    "        ap_change[\"before_change\"].append(before_change)\n",
    "        ap_change[\"after_change\"].append(after_change)\n",
    "\n",
    "    return ap_change\n",
    "\n",
    "\n",
    "def generate_wide_plot_2(ax, bar_data):\n",
    "    \"\"\"\n",
    "    Generate the second wide plot showing AP change between late and early phases for before and after periods.\n",
    "    \"\"\"\n",
    "    ap_change = calculate_ap_change(bar_data)\n",
    "\n",
    "    # Plot before and after changes as a dot plot\n",
    "    ax.scatter(ap_change[\"stimulus\"], ap_change[\"before_change\"], color=\"gray\", label=\"Before\")\n",
    "    ax.scatter(ap_change[\"stimulus\"], ap_change[\"after_change\"], color=\"blue\", label=\"After\")\n",
    "\n",
    "    # Connect the points with lines for visualization\n",
    "    for i in range(len(ap_change[\"stimulus\"])):\n",
    "        ax.plot(\n",
    "            [ap_change[\"stimulus\"][i], ap_change[\"stimulus\"][i]],\n",
    "            [ap_change[\"before_change\"][i], ap_change[\"after_change\"][i]],\n",
    "            color=\"black\",\n",
    "            alpha=0.5,\n",
    "            linestyle=\"--\"\n",
    "        )\n",
    "\n",
    "    # Set labels, legend, and title\n",
    "    ax.set_xlabel(\"Stimulus Input (Sweep Number)\")\n",
    "    ax.set_ylabel(\"Change in AP Count (Late - Early)\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Change in AP Count (Late - Early) Before and After\")\n",
    "\n",
    "\n",
    "def calculate_isi_distribution(peaks, time):\n",
    "    \"\"\"\n",
    "    Calculate interspike intervals (ISIs) from detected peaks.\n",
    "    \"\"\"\n",
    "    if len(peaks) < 2:  # Need at least two peaks to calculate ISIs\n",
    "        return []\n",
    "    return np.diff(time[peaks])\n",
    "\n",
    "\n",
    "def generate_wide_plot_3(ax, before_isi, after_isi):\n",
    "    \"\"\"\n",
    "    Generate the third wide plot showing the ISI distribution for before and after periods using histograms.\n",
    "    \"\"\"\n",
    "    # Define a consistent range and bin size for histograms\n",
    "    all_isi = before_isi + after_isi\n",
    "    if not all_isi:  # Handle the case where no ISIs are present\n",
    "        ax.text(0.5, 0.5, \"No ISIs detected\", ha=\"center\", va=\"center\", fontsize=12)\n",
    "        ax.set_title(\"ISI Distribution Before and After\")\n",
    "        return\n",
    "\n",
    "    bins = np.histogram_bin_edges(all_isi, bins=30)  # Determine bin edges based on combined ISI data\n",
    "\n",
    "    # Plot histograms for before and after ISIs\n",
    "    ax.hist(before_isi, bins=bins, alpha=0.5, color=\"gray\", label=\"Before\", density=True)\n",
    "    ax.hist(after_isi, bins=bins, alpha=0.5, color=\"blue\", label=\"After\", density=True)\n",
    "\n",
    "    # Set axis labels, legend, and title\n",
    "    ax.set_xlabel(\"Interspike Interval (s)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"ISI Distribution Before and After\")\n",
    "\n",
    "def calculate_fractions(bar_data):\n",
    "    \"\"\"\n",
    "    Calculate the fractions of early and late spikes for before and after.\n",
    "    \"\"\"\n",
    "    fractions = {\n",
    "        \"sweep\": bar_data[\"sweep\"],\n",
    "        \"early_fraction_before\": [],\n",
    "        \"late_fraction_before\": [],\n",
    "        \"early_fraction_after\": [],\n",
    "        \"late_fraction_after\": []\n",
    "    }\n",
    "\n",
    "    for i, sweep in enumerate(bar_data[\"sweep\"]):\n",
    "        total_before = bar_data[\"early_before\"][i] + bar_data[\"late_before\"][i]\n",
    "        total_after = bar_data[\"early_after\"][i] + bar_data[\"late_after\"][i]\n",
    "\n",
    "        if total_before > 0:\n",
    "            fractions[\"early_fraction_before\"].append(bar_data[\"early_before\"][i] / total_before)\n",
    "            fractions[\"late_fraction_before\"].append(bar_data[\"late_before\"][i] / total_before)\n",
    "        else:\n",
    "            fractions[\"early_fraction_before\"].append(np.nan)\n",
    "            fractions[\"late_fraction_before\"].append(np.nan)\n",
    "\n",
    "        if total_after > 0:\n",
    "            fractions[\"early_fraction_after\"].append(bar_data[\"early_after\"][i] / total_after)\n",
    "            fractions[\"late_fraction_after\"].append(bar_data[\"late_after\"][i] / total_after)\n",
    "        else:\n",
    "            fractions[\"early_fraction_after\"].append(np.nan)\n",
    "            fractions[\"late_fraction_after\"].append(np.nan)\n",
    "\n",
    "    return fractions\n",
    "\n",
    "def generate_wide_plot_fraction(ax, fractions):\n",
    "    \"\"\"\n",
    "    Generate a bar plot showing the fraction of early and late spikes before and after,\n",
    "    mirroring the style and color scheme of generate_wide_plot_1.\n",
    "    \"\"\"\n",
    "    width = 0.2  # Same width as generate_wide_plot_1\n",
    "    x = range(len(fractions[\"sweep\"]))\n",
    "\n",
    "    # Plot four bars per sweep, just like generate_wide_plot_1\n",
    "    ax.bar([p - 1.5 * width for p in x], fractions[\"early_fraction_before\"], width, label=\"Early Fraction Before\", color=\"lightgray\")\n",
    "    ax.bar([p - 0.5 * width for p in x], fractions[\"late_fraction_before\"], width, label=\"Late Fraction Before\", color=\"gray\")\n",
    "    ax.bar([p + 0.5 * width for p in x], fractions[\"early_fraction_after\"], width, label=\"Early Fraction After\", color=\"lightblue\")\n",
    "    ax.bar([p + 1.5 * width for p in x], fractions[\"late_fraction_after\"], width, label=\"Late Fraction After\", color=\"blue\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(fractions[\"sweep\"])\n",
    "    ax.set_xlabel(\"Sweep Number\")\n",
    "    ax.set_ylabel(\"Fraction of Spikes\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Fraction of Early and Late Spikes Before and After\")\n",
    "\n",
    "def generate_ecdf_plot(ax, fractions):\n",
    "    \"\"\"\n",
    "    Generate an ECDF plot comparing the distribution of fraction differences\n",
    "    (early_fraction - late_fraction) before and after.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes on which to draw the plot.\n",
    "    fractions : dict\n",
    "        A dictionary containing:\n",
    "            \"early_fraction_before\"\n",
    "            \"late_fraction_before\"\n",
    "            \"early_fraction_after\"\n",
    "            \"late_fraction_after\"\n",
    "    \"\"\"\n",
    "    # Compute the differences for before and after conditions\n",
    "    diff_before = np.array(fractions[\"early_fraction_before\"]) - np.array(fractions[\"late_fraction_before\"])\n",
    "    diff_after = np.array(fractions[\"early_fraction_after\"]) - np.array(fractions[\"late_fraction_after\"])\n",
    "\n",
    "    # Remove NaN values if any exist (in cases where no spikes were detected)\n",
    "    diff_before = diff_before[~np.isnan(diff_before)]\n",
    "    diff_after = diff_after[~np.isnan(diff_after)]\n",
    "\n",
    "    # Sort the data\n",
    "    diff_before_sorted = np.sort(diff_before)\n",
    "    diff_after_sorted = np.sort(diff_after)\n",
    "\n",
    "    # Compute ECDF y-values\n",
    "    y_before = np.arange(1, len(diff_before_sorted) + 1) / len(diff_before_sorted) if len(diff_before_sorted) > 0 else []\n",
    "    y_after = np.arange(1, len(diff_after_sorted) + 1) / len(diff_after_sorted) if len(diff_after_sorted) > 0 else []\n",
    "\n",
    "    # Plot the ECDFs\n",
    "    ax.plot(diff_before_sorted, y_before, color=\"gray\", label=\"Before\")\n",
    "    ax.plot(diff_after_sorted, y_after, color=\"blue\", label=\"After\")\n",
    "\n",
    "    # Set axis limits and labels\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_xlabel(\"Fraction Difference (Early - Late)\")\n",
    "    ax.set_ylabel(\"Cumulative Probability\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"ECDF of Fraction Differences Before and After\")\n",
    "\n",
    "### compare ecdf acrross groups \n",
    "\n",
    "### FOR LINE PLOT \n",
    "def plot_traces(ax_before, ax_after, time_before, voltage_before, time_after, voltage_after):\n",
    "    \"\"\"\n",
    "    Plot Before and After traces on separate axes.\n",
    "\n",
    "    Args:\n",
    "        ax_before (matplotlib axis): Axis to plot Before trace.\n",
    "        ax_after (matplotlib axis): Axis to plot After trace.\n",
    "        time_before, time_after: Time arrays for Before and After traces.\n",
    "        voltage_before, voltage_after: Voltage arrays for Before and After traces.\n",
    "    \"\"\"\n",
    "    # Plot Before traces\n",
    "    ax_before.plot(time_before, voltage_before, color=\"gray\")\n",
    "    ax_before.set_title(\"Before\", fontsize=8)\n",
    "    ax_before.set_xlabel(\"Time (s)\", fontsize=6)\n",
    "    ax_before.set_ylabel(\"Voltage (mV)\", fontsize=6)\n",
    "\n",
    "    # Plot After traces\n",
    "    ax_after.plot(time_after, voltage_after, color=\"blue\")\n",
    "    ax_after.set_title(\"After\", fontsize=8)\n",
    "    ax_after.set_xlabel(\"Time (s)\", fontsize=6)\n",
    "\n",
    "def plot_io_curve(ax, group_data, label, color, offset):\n",
    "    \"\"\"\n",
    "    Plot input-output curve with mean and SEM for a single condition.\n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib axis): Axis to plot on.\n",
    "        group_data (DataFrame): Filtered data for the condition.\n",
    "        label (str): Legend label.\n",
    "        color (str): Color for the markers and lines.\n",
    "        offset (float): Offset for the sweep numbers.\n",
    "    \"\"\"\n",
    "    mean = group_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "    sem = group_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "    sweep_numbers = mean.index\n",
    "\n",
    "    ax.errorbar(\n",
    "        sweep_numbers + offset,\n",
    "        mean,\n",
    "        yerr=sem,\n",
    "        fmt=\"o\",\n",
    "        color=color,\n",
    "        label=label,\n",
    "        capsize=3,\n",
    "        markersize=4,\n",
    "    )\n",
    "    ax.set_xlabel(\"Sweep Number\", fontsize=6)\n",
    "    ax.set_ylabel(\"Mean AP Count (± SEM)\", fontsize=6)\n",
    "    ax.legend(fontsize=6)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame:\n",
      "            Group Recording_ID   Label  \\\n",
      "0    L + ACR2-CTZ        CTZ-1   After   \n",
      "1    L + ACR2-CTZ        CTZ-1  Before   \n",
      "2    L + ACR2-CTZ       CTZ-11   After   \n",
      "3    L + ACR2-CTZ       CTZ-11  Before   \n",
      "4    L + ACR2-CTZ       CTZ-12   After   \n",
      "..            ...          ...     ...   \n",
      "114         Plain        CTZ-7  Before   \n",
      "115         Plain        CTZ-8   After   \n",
      "116         Plain        CTZ-8  Before   \n",
      "117         Plain        CTZ-9   After   \n",
      "118         Plain        CTZ-9  Before   \n",
      "\n",
      "                                             File_Path  \n",
      "0    /Users/ecrespo/Desktop/BLADe_patch_data/L + AC...  \n",
      "1    /Users/ecrespo/Desktop/BLADe_patch_data/L + AC...  \n",
      "2    /Users/ecrespo/Desktop/BLADe_patch_data/L + AC...  \n",
      "3    /Users/ecrespo/Desktop/BLADe_patch_data/L + AC...  \n",
      "4    /Users/ecrespo/Desktop/BLADe_patch_data/L + AC...  \n",
      "..                                                 ...  \n",
      "114  /Users/ecrespo/Desktop/BLADe_patch_data/Plain/...  \n",
      "115  /Users/ecrespo/Desktop/BLADe_patch_data/Plain/...  \n",
      "116  /Users/ecrespo/Desktop/BLADe_patch_data/Plain/...  \n",
      "117  /Users/ecrespo/Desktop/BLADe_patch_data/Plain/...  \n",
      "118  /Users/ecrespo/Desktop/BLADe_patch_data/Plain/...  \n",
      "\n",
      "[119 rows x 4 columns]\n",
      "\n",
      "Unique Groups:\n",
      "['L + ACR2-CTZ', 'L + CS-CTZ', 'L + CS-Veh', 'L + DUD-CTZ', 'L Only', 'Plain']\n",
      "\n",
      "Summary:\n",
      "Total Groups: 6\n",
      "Unique Groups: ['L + ACR2-CTZ', 'L + CS-CTZ', 'L + CS-Veh', 'L + DUD-CTZ', 'L Only', 'Plain']\n",
      "Total Recordings: 119\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "base_path = \"/Users/ecrespo/Desktop/BLADe_patch_data\"\n",
    "processor = BladePatchDataProcessor(base_path)\n",
    "\n",
    "# Process the data\n",
    "processor.process_data()\n",
    "\n",
    "# Access the attributes\n",
    "print(\"DataFrame:\")\n",
    "print(processor.dataframe)\n",
    "print(\"\\nUnique Groups:\")\n",
    "print(processor.unique_groups)\n",
    "\n",
    "# Print a summary\n",
    "print(\"\\nSummary:\")\n",
    "print(processor.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Available Groups and Recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups: ['L + ACR2-CTZ', 'L + CS-CTZ', 'L + CS-Veh', 'L + DUD-CTZ', 'L Only', 'Plain']\n",
      "Recordings in this group: ['CTZ-1' 'CTZ-11' 'CTZ-12' ... 'CTZ-7' 'CTZ-8' 'CTZ-9']\n"
     ]
    }
   ],
   "source": [
    "print(\"Groups:\", processor.unique_groups)\n",
    "# For a given group:\n",
    "group = processor.unique_groups[0]  # for example, pick the first group\n",
    "print(\"Recordings in this group:\", processor.get_recording_ids(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a Single Recording’s Before/After Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_id = processor.get_recording_ids(group)[0]  # take the first recording in the group\n",
    "fig, axes = processor.plot_before_after_comparison(group, recording_id,\n",
    "                                                   before_label=\"Before\",\n",
    "                                                   after_label=\"After\",\n",
    "                                                   sweep_numbers=None,  # or specify sweeps like [0, 1, 2]\n",
    "                                                   startAtSec=0,\n",
    "                                                   endAtSec=1.5)\n",
    "plt.show()\n",
    "processor.plot_scalebar(axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pdf_path = \"/Users/ecrespo/Desktop/BLADe_patch_data_output\"\n",
    "\n",
    "processor.export_all_groups_to_pdfs(\n",
    "    output_dir=output_pdf_path,\n",
    "    before_label=\"Before\",\n",
    "    after_label=\"After\",\n",
    "    sweep_numbers=None,\n",
    "    startAtSec=0.08,\n",
    "    endAtSec=1.2,\n",
    "    offsetXsec=0.3,\n",
    "    offsetYunits=40,\n",
    "    color_before=None,\n",
    "    color_after=\"red\",\n",
    "    alpha=0.5,\n",
    "    linewidth=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pdf_path = \"/Users/ecrespo/Desktop/BLADe_patch_data_output\"\n",
    "\n",
    "processor.export_all_groups_to_svgs(output_dir=output_pdf_path,\n",
    "                                    before_label=\"Before\",\n",
    "                                    after_label=\"After\",\n",
    "                                    sweep_numbers=None,\n",
    "                                    startAtSec=0.08,\n",
    "                                    endAtSec=1.2,\n",
    "                                    offsetXsec=0.1,\n",
    "                                    offsetYunits=90,\n",
    "                                    color_before=\"grey\",\n",
    "                                    color_after=\"blue\",\n",
    "                                    alpha=0.5,\n",
    "                                    linewidth=0.5,\n",
    "                                    dpi=300,    # For any raster elements\n",
    "                                    add_suptitle=True,\n",
    "                                    scaleXms=200,\n",
    "                                    scaleYmV=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abf = pyabf.ABF('/Users/ecrespo/Desktop/BLADe_patch_data/L + ACR2-CTZ/L + ACR2 After CTZ-1 21224028.abf')\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(211 )\n",
    "ax1.set_title(\"ABF Recording\")\n",
    "ax1.set_ylabel(abf.sweepLabelY)\n",
    "ax1.plot(abf.sweepX, abf.sweepY, 'b', lw=.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/Users/ecrespo/Desktop/BLADe_patch_data_output\"\n",
    "processor.plot_sweeps_pdf(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = processor.detect_action_potentials(\n",
    "    group=\"Plain\",\n",
    "    recording_id=\"CTZ-1\",\n",
    "    label=\"Before\",\n",
    "    sweep_number=5,\n",
    "    height=0.1,  # Adjust based on signal properties\n",
    "    prominence=0.05,\n",
    "    distance=50\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_standard\"\n",
    "processor.create_group_pdf_with_peaks(\n",
    "    output_dir=output_dir,\n",
    "    height=0.1,  # Adjust based on signal properties\n",
    "    prominence=0.05,\n",
    "    distance=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pdf_with_peaks_complex(\n",
    "    output_dir=\"/Users/ecrespo/Desktop/BLADe_patch_data_output_complex\",\n",
    "    height=0.1,  # Adjust based on signal properties\n",
    "    prominence=0.05,\n",
    "    distance=50, \n",
    "    time_range=None,\n",
    "    early_phase=(0.1,0.49),\n",
    "    late_phase=(0.5,1.12)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pooled ECDF PDF for group 'L + ACR2-CTZ' to: /Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf/L + ACR2-CTZ_pooled_ecdf.pdf\n",
      "Saved pooled ECDF PDF for group 'L + CS-CTZ' to: /Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf/L + CS-CTZ_pooled_ecdf.pdf\n",
      "Saved pooled ECDF PDF for group 'L + CS-Veh' to: /Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf/L + CS-Veh_pooled_ecdf.pdf\n",
      "Skipping incomplete pair for Recording ID: CTZ-3 in group L + DUD-CTZ\n",
      "Saved pooled ECDF PDF for group 'L + DUD-CTZ' to: /Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf/L + DUD-CTZ_pooled_ecdf.pdf\n",
      "Saved pooled ECDF PDF for group 'L Only' to: /Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf/L Only_pooled_ecdf.pdf\n",
      "Saved pooled ECDF PDF for group 'Plain' to: /Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf/Plain_pooled_ecdf.pdf\n"
     ]
    }
   ],
   "source": [
    "# groups ecdf\n",
    "processor.create_group_pooled_ecdf(\n",
    "    output_dir=\"/Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf\",\n",
    "    height=0.1,  \n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    time_range=None,\n",
    "    early_phase=(0.1, 0.49),\n",
    "    late_phase=(0.5, 1.12), \n",
    "    x_min=-1.0,\n",
    "    x_max=1.0,\n",
    "    step_mode=\"mid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pooled_ecdf_svg(\n",
    "    output_dir=\"/Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf_svgs\",\n",
    "    height=0.1,  \n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    time_range=None,\n",
    "    early_phase=(0.1, 0.49),\n",
    "    late_phase=(0.5, 1.12), \n",
    "    x_min=-1.0,\n",
    "    x_max=1.0,\n",
    "    step_mode=\"mid\", \n",
    "    fig_width=5,\n",
    "    fig_height=3,\n",
    "    remove_spines=True,\n",
    "    transparent=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pooled_mean_and_individual_traces_ecdf(\n",
    "    output_dir=\"/Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf_svgs_individual\",\n",
    "    height=0.1,\n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    time_range=None,\n",
    "    early_phase=(0.1, 0.49),\n",
    "    late_phase=(0.5, 1.12),\n",
    "    x_min=-1.0,\n",
    "    x_max=1.0,\n",
    "    step_mode=\"mid\",\n",
    "    fig_width=5,\n",
    "    fig_height=3,\n",
    "    remove_spines=True,\n",
    "    transparent=True,\n",
    "    individual_line_alpha=0.3,\n",
    "    individual_line_style=\":\",\n",
    "    individual_line_width=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pooled_mean_and_individual_sigmoid_ecdf(\n",
    "    output_dir=\"/Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf_svgs_sigmoid\",\n",
    "    height=0.1,\n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    time_range=None,\n",
    "    early_phase=(0.1, 0.49),\n",
    "    late_phase=(0.5, 1.12),\n",
    "    x_min=-1.0,\n",
    "    x_max=1.0,\n",
    "    fig_width=5,\n",
    "    fig_height=3,\n",
    "    remove_spines=True,\n",
    "    transparent=True,\n",
    "    individual_line_alpha=0.3,\n",
    "    individual_line_style=\":\",\n",
    "    individual_line_width=0.5,\n",
    "    smoothing_factor=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_data_df, summary_df = processor.compare_group_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cell_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pooled_mean_and_individual_smoothed_ecdf(\n",
    "    output_dir=\"/Users/ecrespo/Desktop/BLADe_patch_data_output_ecdf_svgs_smoothed\",\n",
    "    height=0.1,\n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    time_range=None,\n",
    "    early_phase=(0.1, 0.49),\n",
    "    late_phase=(0.5, 1.12),\n",
    "    x_min=-1.0,\n",
    "    x_max=1.0,\n",
    "    fig_width=5,\n",
    "    fig_height=3,\n",
    "    remove_spines=True,\n",
    "    transparent=True,\n",
    "    individual_line_alpha=0.3,\n",
    "    individual_line_style=\":\",\n",
    "    individual_line_width=0.5,\n",
    "    main_line_width=1.5,\n",
    "    smoothing_window=-.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized\"\n",
    "processor.create_group_pdf_with_peaks(\n",
    "    output_dir=output_dir,\n",
    "    height=0.08,  # Adjust based on signal properties\n",
    "    prominence=0.1,\n",
    "    distance=30, \n",
    "    width=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.process_peaks(\n",
    "    height=0.08,  # Adjust based on signal properties\n",
    "    prominence=0.1,\n",
    "    distance=30, \n",
    "    width=0.01,\n",
    "    save_csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.import_csv_and_plot_mean_peaks(\n",
    "    csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified.csv',\n",
    "    output_pdf_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified_plot.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.import_csv_and_plot_mean_peaks_lineplot(csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified.csv',\n",
    "    output_pdf_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified_lineplot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.import_csv_and_plot_mean_peaks_with_error_bars(csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified.csv',\n",
    "    output_pdf_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified_lineplotwitherrorbars.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.import_csv_and_plot_mean_peaks_with_error_bars_svg(\n",
    "    csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified.csv',\n",
    "    output_dir='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/svg_lineplots',\n",
    "    fig_width=5,\n",
    "    fig_height=3, \n",
    "    ymin=-10, \n",
    "    ymax=70,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.process_peaks_in_window(height=0.8,\n",
    "    prominence=0.2,\n",
    "    distance=20,\n",
    "    start_time=0.1,\n",
    "    end_time=1.2,\n",
    "    save_csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_specific_window.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.import_csv_and_plot_mean_peaks_with_error_bars(csv_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_specific_window.csv',\n",
    "    output_pdf_path='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified_lineplotwitherrorbars_specific_window.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.peak_window_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.process_peaks_by_phase(\n",
    "    height=0.8,\n",
    "    prominence=0.2,\n",
    "    distance=20,\n",
    "    early_start=0.1,\n",
    "    early_end=0.2,\n",
    "    late_start=0.21,\n",
    "    late_end=0.41,\n",
    "    save_csv_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.phase_peak_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `processor` is your class instance\n",
    "processor.create_group_pdf_with_deltas_from_dataframe(output_dir='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pdf_with_early_vs_late_counts(output_dir='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pdf_with_early_to_late_ratios(output_dir='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pdf_with_mean_and_sem(output_dir='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_group_pdf_with_mean_and_sem_and_store_data(output_dir='/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_results = processor.run_two_way_anova_with_correction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 'L + CS-CTZ' will use recording(s): ['CTZ-6']\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + CS-CTZ/L + CS Before CTZ-6 21201009.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + CS-CTZ/L + CTZ After CTZ-6 21201011.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "Group 'L + CS-Veh' will use recording(s): ['Veh-5']\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + CS-Veh/L + CS Before Veh-5 21212015.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + CS-Veh/L + CS After Veh-5 21212016.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "Group 'L + ACR2-CTZ' will use recording(s): ['CTZ-5']\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + ACR2-CTZ/L + ACR2 Before CTZ-5 21309004.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + ACR2-CTZ/L + ACR2 After CTZ-5 21309005.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "Group 'L + DUD-CTZ' will use recording(s): ['CTZ-9']\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + DUD-CTZ/L + DUD Before CTZ-9 21211000.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "ABF File: /Users/ecrespo/Desktop/BLADe_patch_data/L + DUD-CTZ/L + DUD After CTZ-9 21211001.abf\n",
      "Sweep count: 10\n",
      "------------------------------------------------------------\n",
      "Sweep 0:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 1:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=-100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 2:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=0.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 3:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=100.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 4:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=200.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 5:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=300.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 6:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=400.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 7:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=500.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 8:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=600.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Sweep 9:\n",
      "  Epoch 0: starts at point 0, type=Step, level=0.0\n",
      "  Epoch 1: starts at point 234, type=Step, level=0.0\n",
      "  Epoch 2: starts at point 1234, type=Step, level=700.0\n",
      "  Epoch 3: starts at point 11234, type=Step, level=0.0\n",
      "  Epoch 4: starts at point 14234, type=Step, level=0.0\n",
      "\n",
      "Done.\n",
      "\n",
      "Final figure saved to: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/final_attempt.svg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def load_trace_data(file_path, time_window):\n",
    "    \"\"\"\n",
    "    Load and slice trace data for a given file and time window.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the trace data file (e.g., ABF file).\n",
    "        time_window (tuple): (start, end) time range to extract.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (time, voltage), arrays of time and voltage data within the window.\n",
    "    \"\"\"\n",
    "    abf = pyabf.ABF(file_path)\n",
    "    abf.setSweep(0)  # Use the first sweep for simplicity\n",
    "    time = abf.sweepX\n",
    "    voltage = abf.sweepY\n",
    "\n",
    "    # Apply time window\n",
    "    mask = (time >= time_window[0]) & (time <= time_window[1])\n",
    "    return time[mask], voltage[mask]\n",
    "\n",
    "\n",
    "def plot_traces(ax_before, ax_after, time_before, voltage_before, time_after, voltage_after):\n",
    "    \"\"\"\n",
    "    Plot Before and After traces on separate axes.\n",
    "\n",
    "    Args:\n",
    "        ax_before (matplotlib axis): Axis to plot Before trace.\n",
    "        ax_after (matplotlib axis): Axis to plot After trace.\n",
    "        time_before, time_after: Time arrays for Before and After traces.\n",
    "        voltage_before, voltage_after: Voltage arrays for Before and After traces.\n",
    "    \"\"\"\n",
    "    # Plot Before traces\n",
    "    ax_before.plot(time_before, voltage_before, color=\"gray\")\n",
    "    ax_before.set_title(\"Before\", fontsize=8)\n",
    "    ax_before.set_xlabel(\"Time (s)\", fontsize=6)\n",
    "    ax_before.set_ylabel(\"Voltage (mV)\", fontsize=6)\n",
    "\n",
    "    # Plot After traces\n",
    "    ax_after.plot(time_after, voltage_after, color=\"blue\")\n",
    "    ax_after.set_title(\"After\", fontsize=8)\n",
    "    ax_after.set_xlabel(\"Time (s)\", fontsize=6)\n",
    "\n",
    "\n",
    "def plot_combined_io_curve(ax, before_data, after_data):\n",
    "    \"\"\"\n",
    "    Overlay Before and After Input-Output curves on a single axis.\n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib axis): Axis to plot on.\n",
    "        before_data (DataFrame): Filtered data for the Before condition.\n",
    "        after_data (DataFrame): Filtered data for the After condition.\n",
    "    \"\"\"\n",
    "    # Compute mean and SEM for Before\n",
    "    mean_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "    sem_before = before_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "    # Compute mean and SEM for After\n",
    "    mean_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].mean()\n",
    "    sem_after = after_data.groupby(\"Sweep_Number\")[\"AP_Count\"].sem()\n",
    "\n",
    "    # Sweep numbers\n",
    "    sweep_numbers = mean_before.index\n",
    "\n",
    "    # Plot Before data\n",
    "    ax.errorbar(\n",
    "        sweep_numbers,\n",
    "        mean_before,\n",
    "        yerr=sem_before,\n",
    "        fmt=\"o\",\n",
    "        color=\"gray\",\n",
    "        label=\"Before\",\n",
    "        capsize=3,\n",
    "        markersize=4,\n",
    "    )\n",
    "\n",
    "    # Plot After data\n",
    "    ax.errorbar(\n",
    "        sweep_numbers,\n",
    "        mean_after,\n",
    "        yerr=sem_after,\n",
    "        fmt=\"o\",\n",
    "        color=\"blue\",\n",
    "        label=\"After\",\n",
    "        capsize=3,\n",
    "        markersize=4,\n",
    "    )\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    ax.set_xlabel(\"Sweep Number\", fontsize=6)\n",
    "    ax.set_ylabel(\"Mean AP Count (± SEM)\", fontsize=6)\n",
    "    ax.legend(fontsize=6)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "def print_sweep_epoch_info(abf_file_path):\n",
    "    \"\"\"\n",
    "    Open an ABF file and print the epoch table for each sweep.\n",
    "    For each epoch, we display:\n",
    "      - The epoch index\n",
    "      - The point index where the epoch begins\n",
    "      - The epoch type (e.g., \"Step\", \"Ramp\")\n",
    "      - The epoch level (usually in pA for current-clamp or mV for voltage-clamp)\n",
    "    \"\"\"\n",
    "    abf = pyabf.ABF(abf_file_path)\n",
    "    print(f\"ABF File: {abf_file_path}\")\n",
    "    print(f\"Sweep count: {abf.sweepCount}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for sweep_num in abf.sweepList:\n",
    "        abf.setSweep(sweep_num)\n",
    "        print(f\"Sweep {sweep_num}:\")\n",
    "\n",
    "        # abf.sweepEpochs.p1s    -> list of epoch start points (in data-point indices)\n",
    "        # abf.sweepEpochs.types  -> list of epoch types (e.g. \"Step\", \"Ramp\")\n",
    "        # abf.sweepEpochs.levels -> list of epoch levels (e.g., -70.0 for mV or 50.0 for pA)\n",
    "        for i, p1 in enumerate(abf.sweepEpochs.p1s):\n",
    "            epoch_type = abf.sweepEpochs.types[i]\n",
    "            epoch_level = abf.sweepEpochs.levels[i]\n",
    "            print(f\"  Epoch {i}: starts at point {p1}, type={epoch_type}, level={epoch_level}\")\n",
    "        print()\n",
    "\n",
    "    print(\"Done.\\n\")\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "def create_final_figure_by_group(\n",
    "    group_recording_map,  # Ordered dictionary mapping group name -> desired recording_id\n",
    "    processor,\n",
    "    time_window,\n",
    "    csv_path,\n",
    "    output_svg_path,  # Path where the final SVG will be saved\n",
    "    voltage_y_range=(-100, 500),  # y-axis limits for voltage traces\n",
    "    io_y_range=(-10, 70)          # y-axis limits for IO curves\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a multi-row figure where each row corresponds to a specific group.\n",
    "    Only groups specified in group_recording_map are plotted, in the order provided.\n",
    "    For each group, the left two columns show the 'Before' and 'After' voltage traces for a specified recording,\n",
    "    and the right columns show the group summary Input–Output (I–O) curve.\n",
    "    \n",
    "    The time axis is converted from seconds to milliseconds (ms) and the corresponding labels are updated.\n",
    "    The final figure is saved as an SVG file with editable text.\n",
    "    \"\"\"\n",
    "    # Load the Input–Output (I–O) data from the CSV file.\n",
    "    try:\n",
    "        peak_data = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # Validate that processor has data.\n",
    "    if processor.dataframe is None or processor.dataframe.empty:\n",
    "        print(\"No data available in processor.dataframe.\")\n",
    "        return\n",
    "\n",
    "    # Use only the groups provided in group_recording_map, in the order given.\n",
    "    groups_to_plot = list(group_recording_map.keys())\n",
    "    n_groups = len(groups_to_plot)\n",
    "    if n_groups == 0:\n",
    "        print(\"No groups specified in group_recording_map. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    plt.rcParams.update({\"font.family\": \"Arial\", \"font.size\": 8})\n",
    "    # Adjust figure height based on the number of groups.\n",
    "    fig = plt.figure(figsize=(6.5, n_groups * 2.5))\n",
    "    gs = gridspec.GridSpec(n_groups, 4, figure=fig)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "\n",
    "    for i, group in enumerate(groups_to_plot):\n",
    "        # Get the recording_id for this group.\n",
    "        recording_id = group_recording_map.get(group)\n",
    "        if not recording_id:\n",
    "            print(f\"No recording specified for group '{group}'. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Find rows matching the desired recording_id and group.\n",
    "        rep_data = processor.dataframe[\n",
    "            (processor.dataframe[\"Recording_ID\"] == recording_id) &\n",
    "            (processor.dataframe[\"Group\"] == group)\n",
    "        ]\n",
    "        if rep_data.empty:\n",
    "            print(f\"Recording '{recording_id}' for group '{group}' not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # For debugging: print which recording(s) are being used.\n",
    "        used_recordings = rep_data[\"Recording_ID\"].unique()\n",
    "        print(f\"Group '{group}' will use recording(s): {used_recordings}\")\n",
    "\n",
    "        # Get file paths for 'Before' and 'After' conditions.\n",
    "        try:\n",
    "            before_file = rep_data[rep_data[\"Label\"] == \"Before\"][\"File_Path\"].iloc[0]\n",
    "            after_file  = rep_data[rep_data[\"Label\"] == \"After\"][\"File_Path\"].iloc[0]\n",
    "        except IndexError:\n",
    "            print(f\"Missing file path information for group '{group}'. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Load the traces using pyABF.\n",
    "        before_abf = pyabf.ABF(before_file)\n",
    "        after_abf  = pyabf.ABF(after_file)\n",
    "        \n",
    "        print_sweep_epoch_info(before_file)\n",
    "        print_sweep_epoch_info(after_file)\n",
    "        \n",
    "        # --- Plot the voltage traces ---\n",
    "        # Plot the 'Before' trace.\n",
    "        ax_before = fig.add_subplot(gs[i, 0])\n",
    "        for sweep_number in before_abf.sweepList[::2]:  # Plot every other sweep\n",
    "            before_abf.setSweep(sweep_number)\n",
    "            time_before = before_abf.sweepX\n",
    "            voltage_before = before_abf.sweepY\n",
    "            mask = (time_before >= time_window[0]) & (time_before <= time_window[1])\n",
    "            # Convert time from seconds to milliseconds.\n",
    "            ax_before.plot(time_before[mask] * 1000, voltage_before[mask],\n",
    "                           color=\"gray\", alpha=0.7, linewidth=0.8)\n",
    "        ax_before.set_title(f\"{group} - Before\", fontsize=8)\n",
    "        ax_before.set_xlabel(\"Time (ms)\", fontsize=6)\n",
    "        ax_before.set_ylabel(\"Voltage (mV)\", fontsize=6)\n",
    "        ax_before.set_ylim(voltage_y_range)\n",
    "        # Set x-axis ticks every 200 ms\n",
    "        ax_before.xaxis.set_major_locator(MultipleLocator(200))\n",
    "\n",
    "        # Plot the 'After' trace.\n",
    "        ax_after = fig.add_subplot(gs[i, 1])\n",
    "        for sweep_number in after_abf.sweepList[::2]:\n",
    "            after_abf.setSweep(sweep_number)\n",
    "            time_after = after_abf.sweepX\n",
    "            voltage_after = after_abf.sweepY\n",
    "            mask = (time_after >= time_window[0]) & (time_after <= time_window[1])\n",
    "            ax_after.plot(time_after[mask] * 1000, voltage_after[mask],\n",
    "                          color=\"blue\", alpha=0.7, linewidth=0.8)\n",
    "        ax_after.set_title(f\"{group} - After\", fontsize=8)\n",
    "        ax_after.set_xlabel(\"Time (ms)\", fontsize=6)\n",
    "        ax_after.set_ylim(voltage_y_range)\n",
    "        # Set x-axis ticks every 200 ms\n",
    "        ax_after.xaxis.set_major_locator(MultipleLocator(200))\n",
    "\n",
    "        # --- Plot the group summary I–O curve ---\n",
    "        ax_io = fig.add_subplot(gs[i, 2:4])\n",
    "        group_before_data = peak_data[(peak_data[\"Group\"] == group) & (peak_data[\"Label\"] == \"Before\")]\n",
    "        group_after_data  = peak_data[(peak_data[\"Group\"] == group) & (peak_data[\"Label\"] == \"After\")]\n",
    "        plot_combined_io_curve(ax_io, group_before_data, group_after_data)\n",
    "        ax_io.set_ylim(io_y_range)\n",
    "        ax_io.set_title(f\"I–O Curve: {group}\", fontsize=8)\n",
    "\n",
    "    # Save the final figure as an SVG file.\n",
    "    os.makedirs(os.path.dirname(output_svg_path), exist_ok=True)\n",
    "    fig.savefig(output_svg_path, format=\"svg\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Final figure saved to: {output_svg_path}\")\n",
    "\n",
    "\n",
    "\n",
    "group_recording_map = {\n",
    "    'L + CS-CTZ':   'CTZ-6',\n",
    "    'L + CS-Veh':   'Veh-5',\n",
    "    'L + ACR2-CTZ': 'CTZ-5',\n",
    "    'L + DUD-CTZ':  'CTZ-9'\n",
    "}\n",
    "time_window = (0, 1.2)  # Specify your desired time window (start, end in seconds)\n",
    "csv_path = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified.csv\"\n",
    "output_svg_path = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/final_attempt.svg\"\n",
    "\n",
    "create_final_figure_by_group(\n",
    "    group_recording_map,\n",
    "    processor,\n",
    "    time_window=(0, 1.2),\n",
    "    csv_path=csv_path,\n",
    "    output_svg_path=output_svg_path,\n",
    "    voltage_y_range=(-90, 50),\n",
    "    io_y_range=(-10, 80)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final figure saved to: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/final_attempt.svg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "def save_io_ecdf_data(\n",
    "    group_recording_map,\n",
    "    processor,\n",
    "    csv_path,\n",
    "    output_io_csv,\n",
    "    output_ecdf_csv,\n",
    "    height=0.1,\n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    width=None,\n",
    "    time_range=(0, 1.2),\n",
    "    early_phase=(0.1, 0.40),\n",
    "    late_phase=(0.5, 1.12)\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts and saves I-O curve data and ECDF distributions for later statistical analysis.\n",
    "    This version **removes trials with no spikes** from the ECDF analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_recording_map : dict\n",
    "        Ordered dict mapping group_name -> desired recording_id.\n",
    "    processor : object\n",
    "        Your data processor with the main dataframe.\n",
    "    csv_path : str\n",
    "        Path to the CSV with peak data for I-O curves.\n",
    "    output_io_csv : str\n",
    "        Path where the input-output data will be saved.\n",
    "    output_ecdf_csv : str\n",
    "        Path where the ECDF data will be saved.\n",
    "    height, prominence, distance, width : float\n",
    "        Parameters for peak detection.\n",
    "    time_range : tuple\n",
    "        Time window for extracting data.\n",
    "    early_phase, late_phase : tuple\n",
    "        Time windows for computing early and late fractions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load I-O peak data\n",
    "    try:\n",
    "        peak_data = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        return\n",
    "    \n",
    "    # Store I-O data in tidy format\n",
    "    io_data_list = []\n",
    "    ecdf_data_list = []\n",
    "\n",
    "    for group in group_recording_map.keys():\n",
    "        # Extract I-O Data\n",
    "        group_before = peak_data[(peak_data[\"Group\"] == group) & (peak_data[\"Label\"] == \"Before\")]\n",
    "        group_after  = peak_data[(peak_data[\"Group\"] == group) & (peak_data[\"Label\"] == \"After\")]\n",
    "\n",
    "        # Add to list\n",
    "        io_data_list.append(group_before.assign(Condition=\"Before\"))\n",
    "        io_data_list.append(group_after.assign(Condition=\"After\"))\n",
    "\n",
    "        # Extract ECDF Data (Fraction Differences)\n",
    "        group_data_all = processor.dataframe[processor.dataframe[\"Group\"] == group]\n",
    "        all_rec_ids = group_data_all[\"Recording_ID\"].unique()\n",
    "\n",
    "        for rec_id in all_rec_ids:\n",
    "            sub_df = group_data_all[group_data_all[\"Recording_ID\"] == rec_id]\n",
    "            before_row = sub_df[sub_df[\"Label\"] == \"Before\"]\n",
    "            after_row  = sub_df[sub_df[\"Label\"] == \"After\"]\n",
    "\n",
    "            if before_row.empty or after_row.empty:\n",
    "                continue  # Skip if missing\n",
    "\n",
    "            before_path = before_row[\"File_Path\"].iloc[0]\n",
    "            after_path  = after_row[\"File_Path\"].iloc[0]\n",
    "\n",
    "            # Load ABFs\n",
    "            b_abf = pyabf.ABF(before_path)\n",
    "            a_abf = pyabf.ABF(after_path)\n",
    "\n",
    "            for sweep_number in b_abf.sweepList:\n",
    "                # Process \"Before\" sweep\n",
    "                _, _, _, early_b, late_b = process_sweep_data(\n",
    "                    b_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                )\n",
    "                # Process \"After\" sweep\n",
    "                _, _, _, early_a, late_a = process_sweep_data(\n",
    "                    a_abf, sweep_number, height, prominence, distance, width, time_range, early_phase, late_phase\n",
    "                )\n",
    "\n",
    "                # Compute fraction difference\n",
    "                diff_b = early_b - late_b\n",
    "                diff_a = early_a - late_a\n",
    "\n",
    "                # **Filter out trials where both early and late have zero spikes**\n",
    "                if early_b == 0 and late_b == 0:\n",
    "                    continue  # Skip this trial for Before\n",
    "                if early_a == 0 and late_a == 0:\n",
    "                    continue  # Skip this trial for After\n",
    "\n",
    "                # Store only relevant trials\n",
    "                ecdf_data_list.append({\n",
    "                    \"Group\": group,\n",
    "                    \"Recording_ID\": rec_id,\n",
    "                    \"Sweep\": sweep_number,\n",
    "                    \"Condition\": \"Before\",\n",
    "                    \"Fraction_Diff\": diff_b\n",
    "                })\n",
    "                ecdf_data_list.append({\n",
    "                    \"Group\": group,\n",
    "                    \"Recording_ID\": rec_id,\n",
    "                    \"Sweep\": sweep_number,\n",
    "                    \"Condition\": \"After\",\n",
    "                    \"Fraction_Diff\": diff_a\n",
    "                })\n",
    "\n",
    "    # Convert to DataFrame and save\n",
    "    io_data_df = pd.concat(io_data_list, ignore_index=True)\n",
    "    ecdf_data_df = pd.DataFrame(ecdf_data_list)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_io_csv), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(output_ecdf_csv), exist_ok=True)\n",
    "\n",
    "    io_data_df.to_csv(output_io_csv, index=False)\n",
    "    ecdf_data_df.to_csv(output_ecdf_csv, index=False)\n",
    "\n",
    "    print(f\"I-O data saved to: {output_io_csv}\")\n",
    "    print(f\"ECDF data saved to: {output_ecdf_csv}\")\n",
    "\n",
    "def perform_statistical_tests(io_csv, ecdf_csv, output_stats_csv):\n",
    "    \"\"\"\n",
    "    Performs Wilcoxon signed-rank test on I-O data (AP_Count) and KS test on ECDF distributions.\n",
    "    The KS test now directly compares the two ECDF distributions instead of looking at differences.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    io_csv : str\n",
    "        Path to the input-output data CSV.\n",
    "    ecdf_csv : str\n",
    "        Path to the ECDF fraction difference data CSV.\n",
    "    output_stats_csv : str\n",
    "        Path where statistical results will be saved.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    try:\n",
    "        io_data = pd.read_csv(io_csv)\n",
    "        ecdf_data = pd.read_csv(ecdf_csv)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    stats_results = []\n",
    "\n",
    "    # Validate required columns\n",
    "    if \"AP_Count\" not in io_data.columns:\n",
    "        print(\"Error: 'AP_Count' column missing in I-O data. Check input data.\")\n",
    "        return\n",
    "    if \"Fraction_Diff\" not in ecdf_data.columns:\n",
    "        print(\"Error: 'Fraction_Diff' column missing in ECDF data. Check input data.\")\n",
    "        return\n",
    "\n",
    "    # --- Wilcoxon Signed-Rank Test (I-O) ---\n",
    "    for group in io_data[\"Group\"].unique():\n",
    "        group_io = io_data[io_data[\"Group\"] == group]\n",
    "\n",
    "        before_io = group_io[group_io[\"Condition\"] == \"Before\"][\"AP_Count\"]\n",
    "        after_io = group_io[group_io[\"Condition\"] == \"After\"][\"AP_Count\"]\n",
    "\n",
    "        if len(before_io) == len(after_io) and len(before_io) > 0:\n",
    "            stat, p = stats.wilcoxon(before_io, after_io)\n",
    "            mean_diff = np.mean(after_io - before_io)\n",
    "            median_diff = np.median(after_io - before_io)\n",
    "            std_diff = np.std(after_io - before_io, ddof=1)\n",
    "            N = len(before_io)\n",
    "            stats_results.append([\"Wilcoxon\", group, N, mean_diff, median_diff, std_diff, p])\n",
    "        else:\n",
    "            print(f\"Skipping Wilcoxon for {group}: Unequal or insufficient samples.\")\n",
    "\n",
    "    # --- Kolmogorov-Smirnov Test (ECDF) ---\n",
    "    for group in ecdf_data[\"Group\"].unique():\n",
    "        group_ecdf = ecdf_data[ecdf_data[\"Group\"] == group]\n",
    "\n",
    "        before_ecdf = group_ecdf[group_ecdf[\"Condition\"] == \"Before\"][\"Fraction_Diff\"]\n",
    "        after_ecdf = group_ecdf[group_ecdf[\"Condition\"] == \"After\"][\"Fraction_Diff\"]\n",
    "\n",
    "        if len(before_ecdf) > 0 and len(after_ecdf) > 0:\n",
    "            # Perform KS test directly on the distributions\n",
    "            stat, p = stats.ks_2samp(before_ecdf, after_ecdf)\n",
    "            N = len(before_ecdf)\n",
    "            stats_results.append([\"KS Test\", group, N, stat, np.nan, np.nan, p])\n",
    "        else:\n",
    "            print(f\"Skipping KS test for {group}: Insufficient ECDF samples.\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    stats_df = pd.DataFrame(stats_results, columns=[\"Test\", \"Group\", \"N\", \"Stat\", \"Median_Diff\", \"SD\", \"P_Value\"])\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_stats_csv), exist_ok=True)\n",
    "    stats_df.to_csv(output_stats_csv, index=False)\n",
    "\n",
    "    print(\"Statistical Test Results:\")\n",
    "    print(stats_df)\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def create_final_figure_by_group_appendededf(\n",
    "    group_recording_map,\n",
    "    processor,\n",
    "    time_window,\n",
    "    csv_path,\n",
    "    output_svg_path,\n",
    "    voltage_y_range=(-100, 500),\n",
    "    io_y_range=(-10, 70),\n",
    "    early_phase=(0.1, 0.40),\n",
    "    late_phase=(0.5, 1.12),\n",
    "    step_mode=\"mid\",\n",
    "    fig_width=8,\n",
    "    fig_height_per_group=3,\n",
    "    transparent=True,\n",
    "    # New parameters (optional) for your peak detection\n",
    "    height=0.1,\n",
    "    prominence=0.05,\n",
    "    distance=50,\n",
    "    width=None\n",
    "):\n",
    "    \"\"\"\n",
    "    This function merges the logic of create_final_figure_by_group (plotting Before/After\n",
    "    traces and I–O curves) with the ECDF logic from create_group_pooled_ecdf_svg.\n",
    "    Each group gets a row of subplots:\n",
    "      - col 0: Before traces (representative recording)\n",
    "      - col 1: After traces  (representative recording)\n",
    "      - col 2: I–O curve     (entire group, from CSV)\n",
    "      - col 3: ECDF of (EarlyFraction - LateFraction) pooled across *all* recordings in that group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_recording_map : dict\n",
    "        Ordered dict mapping group_name -> desired recording_id (the \"representative\" for voltage).\n",
    "    processor : object\n",
    "        Your data processor with a dataframe of columns [\"Recording_ID\", \"Group\", \"Label\", \"File_Path\", ...].\n",
    "    time_window : tuple\n",
    "        (start_sec, end_sec) for extracting a slice of the sweep for plotting (columns 0/1).\n",
    "    csv_path : str\n",
    "        Path to your CSV with peak_data for plotting I–O curves in column 2.\n",
    "    output_svg_path : str\n",
    "        Where to save the final SVG file.\n",
    "    voltage_y_range : tuple\n",
    "        Y-axis limits for the voltage traces (Before/After).\n",
    "    io_y_range : tuple\n",
    "        Y-axis limits for the I–O curve.\n",
    "    early_phase : tuple\n",
    "        (start_sec, end_sec) for the \"early\" portion of each sweep (for fraction).\n",
    "    late_phase : tuple\n",
    "        (start_sec, end_sec) for the \"late\" portion of each sweep (for fraction).\n",
    "    step_mode : str\n",
    "        How the ECDF step lines are drawn: \"pre\", \"post\", or \"mid\".\n",
    "    fig_width : float\n",
    "        The overall figure width in inches.\n",
    "    fig_height_per_group : float\n",
    "        How tall each group’s row should be (in inches).\n",
    "    transparent : bool\n",
    "        If True, set figure background to transparent.\n",
    "    height, prominence, distance, width : float or None\n",
    "        Peak detection parameters to pass into `process_sweep_data(...).`\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib as mpl\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    from matplotlib.ticker import MultipleLocator\n",
    "    import pyabf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # Ensure SVG text remains editable\n",
    "    mpl.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "\n",
    "    # 1) Load the Input–Output data from CSV (for column 2)\n",
    "    try:\n",
    "        peak_data = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # 2) Validate that processor has data\n",
    "    if processor.dataframe is None or processor.dataframe.empty:\n",
    "        print(\"No data available in processor.dataframe.\")\n",
    "        return\n",
    "\n",
    "    # 3) Determine which groups to plot\n",
    "    groups_to_plot = list(group_recording_map.keys())\n",
    "    n_groups = len(groups_to_plot)\n",
    "    if n_groups == 0:\n",
    "        print(\"No groups specified in group_recording_map. Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    # 4) Create the figure with 4 columns: (Before, After, I–O, ECDF)\n",
    "    fig_height = n_groups * fig_height_per_group\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "    gs = gridspec.GridSpec(n_groups, 4, figure=fig)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "    if transparent:\n",
    "        fig.patch.set_facecolor('none')\n",
    "\n",
    "    # Go group-by-group\n",
    "    for i, group in enumerate(groups_to_plot):\n",
    "        recording_id = group_recording_map.get(group)\n",
    "        if not recording_id:\n",
    "            print(f\"No 'representative' recording specified for group '{group}'. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # For the entire group, gather all rows\n",
    "        group_data_all = processor.dataframe[processor.dataframe[\"Group\"] == group]\n",
    "        if group_data_all.empty:\n",
    "            print(f\"No data found at all for group '{group}'. Skipping row.\")\n",
    "            continue\n",
    "\n",
    "        # --- Columns 0 & 1: Plot \"representative\" Before/After from group_recording_map ---\n",
    "        rep_data = group_data_all[group_data_all[\"Recording_ID\"] == recording_id]\n",
    "        if rep_data.empty:\n",
    "            print(f\"Representative rec_id '{recording_id}' not found for group '{group}'.\")\n",
    "            print(\"We'll skip the 'Before/After' voltage subplots but still do the pooled ECDF.\")\n",
    "        else:\n",
    "            try:\n",
    "                before_file = rep_data[rep_data[\"Label\"] == \"Before\"][\"File_Path\"].iloc[0]\n",
    "                after_file  = rep_data[rep_data[\"Label\"] == \"After\"][\"File_Path\"].iloc[0]\n",
    "            except IndexError:\n",
    "                print(f\"Incomplete 'Before'/'After' for group '{group}', rec_id='{recording_id}'.\")\n",
    "                before_file, after_file = None, None\n",
    "\n",
    "            # Column 0: \"Before\" trace(s)\n",
    "            ax_before = fig.add_subplot(gs[i, 0])\n",
    "            if before_file:\n",
    "                before_abf = pyabf.ABF(before_file)\n",
    "                for sweep_number in before_abf.sweepList[::2]:\n",
    "                    before_abf.setSweep(sweep_number)\n",
    "                    t = before_abf.sweepX\n",
    "                    v = before_abf.sweepY\n",
    "                    mask = (t >= time_window[0]) & (t <= time_window[1])\n",
    "                    ax_before.plot(t[mask] * 1000, v[mask],\n",
    "                                   color=\"gray\", alpha=0.7, linewidth=0.8)\n",
    "                ax_before.set_title(f\"{group}\\nRep: {recording_id}\\nBefore\", fontsize=8)\n",
    "            else:\n",
    "                ax_before.text(0.5, 0.5, \"No 'Before' file\", transform=ax_before.transAxes,\n",
    "                               ha=\"center\", va=\"center\")\n",
    "            ax_before.set_xlabel(\"Time (ms)\", fontsize=6)\n",
    "            ax_before.set_ylabel(\"Voltage (mV)\", fontsize=6)\n",
    "            ax_before.set_ylim(voltage_y_range)\n",
    "            ax_before.xaxis.set_major_locator(MultipleLocator(200))\n",
    "\n",
    "            # Column 1: \"After\" trace(s)\n",
    "            ax_after = fig.add_subplot(gs[i, 1])\n",
    "            if after_file:\n",
    "                after_abf = pyabf.ABF(after_file)\n",
    "                for sweep_number in after_abf.sweepList[::2]:\n",
    "                    after_abf.setSweep(sweep_number)\n",
    "                    t = after_abf.sweepX\n",
    "                    v = after_abf.sweepY\n",
    "                    mask = (t >= time_window[0]) & (t <= time_window[1])\n",
    "                    ax_after.plot(t[mask] * 1000, v[mask],\n",
    "                                  color=\"blue\", alpha=0.7, linewidth=0.8)\n",
    "                ax_after.set_title(f\"{group}\\nRep: {recording_id}\\nAfter\", fontsize=8)\n",
    "            else:\n",
    "                ax_after.text(0.5, 0.5, \"No 'After' file\", transform=ax_after.transAxes,\n",
    "                              ha=\"center\", va=\"center\")\n",
    "            ax_after.set_xlabel(\"Time (ms)\", fontsize=6)\n",
    "            ax_after.set_ylim(voltage_y_range)\n",
    "            ax_after.xaxis.set_major_locator(MultipleLocator(200))\n",
    "\n",
    "        # --- Column 2: The group-level I–O curve from the entire CSV for that group ---\n",
    "        ax_io = fig.add_subplot(gs[i, 2])\n",
    "        group_before_data = peak_data[(peak_data[\"Group\"] == group) & (peak_data[\"Label\"] == \"Before\")]\n",
    "        group_after_data  = peak_data[(peak_data[\"Group\"] == group) & (peak_data[\"Label\"] == \"After\")]\n",
    "        plot_combined_io_curve(ax_io, group_before_data, group_after_data)\n",
    "        ax_io.set_ylim(io_y_range)\n",
    "        ax_io.set_title(f\"I–O Curve: {group}\", fontsize=8)\n",
    "\n",
    "        # --- Column 3: Pooled ECDF across all recordings in THIS group ---\n",
    "        ax_ecdf = fig.add_subplot(gs[i, 3])\n",
    "        pooled_diff_before = []\n",
    "        pooled_diff_after  = []\n",
    "\n",
    "        # For each recording in THIS group, find \"Before\" / \"After\" ABFs and\n",
    "        # gather fraction differences from all sweeps\n",
    "        all_rec_ids = group_data_all[\"Recording_ID\"].unique()\n",
    "        for rec_id in all_rec_ids:\n",
    "            sub_df = group_data_all[group_data_all[\"Recording_ID\"] == rec_id]\n",
    "            # We expect 1 \"Before\" row, 1 \"After\" row\n",
    "            before_row = sub_df[sub_df[\"Label\"] == \"Before\"]\n",
    "            after_row  = sub_df[sub_df[\"Label\"] == \"After\"]\n",
    "            if before_row.empty or after_row.empty:\n",
    "                # skip incomplete\n",
    "                continue\n",
    "\n",
    "            before_path = before_row[\"File_Path\"].iloc[0]\n",
    "            after_path  = after_row[\"File_Path\"].iloc[0]\n",
    "\n",
    "            # Load ABFs\n",
    "            b_abf = pyabf.ABF(before_path)\n",
    "            a_abf = pyabf.ABF(after_path)\n",
    "\n",
    "            # Loop all sweeps for each ABF, measure fraction difference\n",
    "            for sweep_number in b_abf.sweepList:\n",
    "                # \"Before\" sweep\n",
    "                time_b, volt_b, peaks_b, early_b, late_b = process_sweep_data(\n",
    "                    b_abf,\n",
    "                    sweep_number=sweep_number,\n",
    "                    height=height,\n",
    "                    prominence=prominence,\n",
    "                    distance=distance,\n",
    "                    width=width,\n",
    "                    time_range=time_window,\n",
    "                    early_phase=early_phase,\n",
    "                    late_phase=late_phase\n",
    "                )\n",
    "                # \"After\" sweep (same sweep_number, typically)\n",
    "                time_a, volt_a, peaks_a, early_a, late_a = process_sweep_data(\n",
    "                    a_abf,\n",
    "                    sweep_number=sweep_number,\n",
    "                    height=height,\n",
    "                    prominence=prominence,\n",
    "                    distance=distance,\n",
    "                    width=width,\n",
    "                    time_range=time_window,\n",
    "                    early_phase=early_phase,\n",
    "                    late_phase=late_phase\n",
    "                )\n",
    "\n",
    "                # Build dict for fraction function\n",
    "                bar_data = {\n",
    "                    \"sweep\": [sweep_number],\n",
    "                    \"early_before\": [early_b],\n",
    "                    \"late_before\":  [late_b],\n",
    "                    \"early_after\":  [early_a],\n",
    "                    \"late_after\":   [late_a],\n",
    "                }\n",
    "                fractions = calculate_fractions(bar_data)\n",
    "                diff_b = fractions[\"early_fraction_before\"][0] - fractions[\"late_fraction_before\"][0]\n",
    "                diff_a = fractions[\"early_fraction_after\"][0]  - fractions[\"late_fraction_after\"][0]\n",
    "\n",
    "                if not np.isnan(diff_b):\n",
    "                    pooled_diff_before.append(diff_b)\n",
    "                if not np.isnan(diff_a):\n",
    "                    pooled_diff_after.append(diff_a)\n",
    "\n",
    "        # Now we have big lists of fraction differences from ALL recordings in this group\n",
    "        diff_before_sorted = np.sort(pooled_diff_before)\n",
    "        diff_after_sorted  = np.sort(pooled_diff_after)\n",
    "\n",
    "        # A local ECDF helper\n",
    "        def ecdf(data, x_min=-1.0, x_max=1.0):\n",
    "            if len(data) == 0:\n",
    "                x = [x_min, x_max]\n",
    "                y = [0.0, 0.0]\n",
    "            else:\n",
    "                y = np.arange(1, len(data) + 1) / len(data)\n",
    "                x = np.concatenate([[x_min], data, [x_max]])\n",
    "                y = np.concatenate([[0.0], y, [1.0]])\n",
    "            return x, y\n",
    "\n",
    "        x_b, y_b = ecdf(diff_before_sorted)\n",
    "        x_a, y_a = ecdf(diff_after_sorted)\n",
    "\n",
    "        ax_ecdf.step(x_b, y_b, color=\"gray\", label=\"Before\", where=step_mode)\n",
    "        ax_ecdf.step(x_a, y_a, color=\"blue\",  label=\"After\",  where=step_mode)\n",
    "        ax_ecdf.set_xlim(-1.0, 1.0)\n",
    "        ax_ecdf.set_ylim(0, 1.0)\n",
    "        ax_ecdf.set_xlabel(\"Frac. Diff (Early - Late)\", fontsize=6)\n",
    "        ax_ecdf.set_ylabel(\"Cumulative Probability\",    fontsize=6)\n",
    "        ax_ecdf.set_title(\"Pooled ECDF\", fontsize=8)\n",
    "        ax_ecdf.legend(fontsize=6)\n",
    "        ax_ecdf.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "    # -- End for each group --\n",
    "\n",
    "    # Save the final figure as SVG\n",
    "    os.makedirs(os.path.dirname(output_svg_path), exist_ok=True)\n",
    "    fig.savefig(output_svg_path, format=\"svg\", transparent=transparent, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Final figure saved to: {output_svg_path}\")\n",
    "    \n",
    "    # Paths for data collection and stats\n",
    "\n",
    "output_io_csv = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/io_curve_data.csv\"\n",
    "output_ecdf_csv = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/ecdf_fraction_data.csv\"\n",
    "output_stats_csv = \"/Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/statistical_results.csv\"\n",
    "\n",
    "create_final_figure_by_group_appendededf(\n",
    "    group_recording_map,\n",
    "    processor,\n",
    "    time_window=(0, 1.2),\n",
    "    csv_path=csv_path,\n",
    "    output_svg_path=output_svg_path,\n",
    "    voltage_y_range=(-90, 50),\n",
    "    io_y_range=(-10, 80), \n",
    "    early_phase=(0.1, 0.40),\n",
    "    late_phase=(0.5, 1.12),\n",
    "    step_mode=\"post\",\n",
    "    fig_width=8,\n",
    "    fig_height_per_group=3,\n",
    "    transparent=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-O data saved to: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/io_curve_data.csv\n",
      "ECDF data saved to: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/ecdf_fraction_data.csv\n"
     ]
    }
   ],
   "source": [
    "save_io_ecdf_data(\n",
    "    group_recording_map,\n",
    "    processor,\n",
    "    csv_path=csv_path,\n",
    "    output_io_csv=output_io_csv,\n",
    "    output_ecdf_csv=output_ecdf_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Wilcoxon for L + DUD-CTZ: Unequal or insufficient samples.\n",
      "Statistical Test Results:\n",
      "       Test         Group    N      Stat  Median_Diff  SD       P_Value\n",
      "0  Wilcoxon    L + CS-CTZ  110       NaN          NaN NaN  4.954578e-09\n",
      "1  Wilcoxon    L + CS-Veh   90       NaN          NaN NaN  3.117688e-01\n",
      "2  Wilcoxon  L + ACR2-CTZ  110       NaN          NaN NaN  4.309037e-10\n",
      "3   KS Test    L + CS-CTZ   48  0.375000          NaN NaN  2.134584e-03\n",
      "4   KS Test    L + CS-Veh   48  0.145833          NaN NaN  6.926601e-01\n",
      "5   KS Test  L + ACR2-CTZ   23  0.565217          NaN NaN  9.901949e-04\n",
      "6   KS Test   L + DUD-CTZ   29  0.068966          NaN NaN  1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Run statistical analysis on the exact data used for plotting\n",
    "stats_results = perform_statistical_tests(\n",
    "    io_csv=output_io_csv,\n",
    "    ecdf_csv=output_ecdf_csv,\n",
    "    output_stats_csv=output_stats_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_path: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/peak_data_modified.csv\n",
      "output_svg_path: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/final_attempt.svg\n",
      "output_io_csv: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/io_curve_data.csv\n",
      "output_ecdf_csv: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/ecdf_fraction_data.csv\n",
      "output_stats_csv: /Users/ecrespo/Desktop/BLADe_patch_data_output_optimized/statistical_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Debug: Print path variables\n",
    "print(f\"csv_path: {csv_path}\")\n",
    "print(f\"output_svg_path: {output_svg_path}\")\n",
    "print(f\"output_io_csv: {output_io_csv}\")\n",
    "print(f\"output_ecdf_csv: {output_ecdf_csv}\")\n",
    "print(f\"output_stats_csv: {output_stats_csv}\")\n",
    "\n",
    "# Ensure none of the paths are empty\n",
    "assert csv_path.strip() != \"\", \"Error: csv_path is empty!\"\n",
    "assert output_svg_path.strip() != \"\", \"Error: output_svg_path is empty!\"\n",
    "assert output_io_csv.strip() != \"\", \"Error: output_io_csv is empty!\"\n",
    "assert output_ecdf_csv.strip() != \"\", \"Error: output_ecdf_csv is empty!\"\n",
    "assert output_stats_csv.strip() != \"\", \"Error: output_stats_csv is empty!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a GUI for counting spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.figure import Figure\n",
    "import pyabf\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "class DraggableThresholdLine:\n",
    "    \"\"\"\n",
    "    A simpler line-drag tool (no concurrency management). \n",
    "    Uses direct \"button_press_event\" / \"button_release_event\" / \"motion_notify_event\" \n",
    "    and a small pixel-distance check around the line for user-friendly picking.\n",
    "    \"\"\"\n",
    "    def __init__(self, ax, init_y, on_release_callback, on_drag_start=None, pick_tolerance_pixels=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ax (Axes): The subplot on which to place the line.\n",
    "            init_y (float): Initial Y position for the line.\n",
    "            on_release_callback(callable): Called when user releases mouse, \n",
    "                with final y-value as argument.\n",
    "            on_drag_start(callable): Called when user first picks the line.\n",
    "            pick_tolerance_pixels(int): Pixel radius to consider a line \"picked\".\n",
    "        \"\"\"\n",
    "        self.ax = ax\n",
    "        self.canvas = ax.figure.canvas\n",
    "        self.on_release_callback = on_release_callback\n",
    "        self.on_drag_start = on_drag_start\n",
    "        self.pick_tolerance = pick_tolerance_pixels\n",
    "\n",
    "        # Create the line\n",
    "        self.line = ax.axhline(y=init_y, color='r', linestyle='--', lw=1)\n",
    "\n",
    "        # Drag state\n",
    "        self.is_dragging = False\n",
    "\n",
    "        # Event IDs\n",
    "        self.cid_press   = self.canvas.mpl_connect(\"button_press_event\",   self.on_press)\n",
    "        self.cid_release = self.canvas.mpl_connect(\"button_release_event\", self.on_release)\n",
    "        self.cid_motion  = self.canvas.mpl_connect(\"motion_notify_event\",  self.on_motion)\n",
    "\n",
    "    def on_press(self, event):\n",
    "        \"\"\"Check if the user clicked near the line.\"\"\"\n",
    "        # Only consider left-click in the same axes\n",
    "        if event.inaxes != self.ax:\n",
    "            return\n",
    "        if event.button != 1:  # left mouse button\n",
    "            return\n",
    "\n",
    "        # Convert the line's y-value to pixel coords\n",
    "        y_line = self.line.get_ydata()[0]\n",
    "        x_min, x_max = self.ax.get_xlim()\n",
    "        y_min, y_max = self.ax.get_ylim()\n",
    "\n",
    "        # Transform to display (pixel) space\n",
    "        line_disp = self.ax.transData.transform((x_min, y_line))\n",
    "        click_disp = self.ax.transData.transform((event.xdata, event.ydata))\n",
    "        # We only compare the Y difference in display space \n",
    "        # (since it doesn't matter where horizontally you click)\n",
    "        dist_pixels = abs(line_disp[1] - click_disp[1])\n",
    "\n",
    "        if dist_pixels <= self.pick_tolerance:\n",
    "            self.is_dragging = True\n",
    "            if self.on_drag_start:\n",
    "                self.on_drag_start()\n",
    "\n",
    "    def on_motion(self, event):\n",
    "        \"\"\"While dragging, move the line's y.\"\"\"\n",
    "        if not self.is_dragging:\n",
    "            return\n",
    "        if event.inaxes != self.ax:\n",
    "            return\n",
    "        if event.button != 1:  # must keep left mouse pressed\n",
    "            return\n",
    "\n",
    "        new_y = event.ydata\n",
    "        self.line.set_ydata([new_y, new_y])\n",
    "        self.canvas.draw()\n",
    "\n",
    "    def on_release(self, event):\n",
    "        \"\"\"Finalize the drag on mouse release.\"\"\"\n",
    "        if not self.is_dragging:\n",
    "            return\n",
    "        if event.button != 1:  # left mouse\n",
    "            return\n",
    "\n",
    "        self.is_dragging = False\n",
    "        final_y = self.line.get_ydata()[0]\n",
    "        if self.on_release_callback:\n",
    "            self.on_release_callback(final_y)\n",
    "\n",
    "class ManualPeakCounterGUI:\n",
    "    \"\"\"\n",
    "    A simpler user-friendly GUI:\n",
    "      - Each threshold line uses DraggableThresholdLine (no concurrency manager).\n",
    "      - We do a confirm/unconfirm approach for each sweep.\n",
    "      - We store peak counts & indices in the final CSV.\n",
    "      - We always run detection on both subplots at load-time \n",
    "        so you see spikes right away.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master, dataframe, output_csv=\"manual_peak_counts.csv\"):\n",
    "        self.master = master\n",
    "        self.master.title(\"Simple Manual Peak Counter (User-Friendly)\")\n",
    "\n",
    "        self.dataframe = dataframe.copy()\n",
    "        self.output_csv = output_csv\n",
    "\n",
    "        # Sort data\n",
    "        self.dataframe.sort_values(by=[\"Group\", \"Recording_ID\"], inplace=True)\n",
    "\n",
    "        # Build list of (group, rec_id)\n",
    "        self.group_rec_pairs = []\n",
    "        for group in self.dataframe[\"Group\"].unique():\n",
    "            sub = self.dataframe[self.dataframe[\"Group\"] == group]\n",
    "            for rec_id in sub[\"Recording_ID\"].unique():\n",
    "                self.group_rec_pairs.append((group, rec_id))\n",
    "\n",
    "        self.current_cell_index  = 0\n",
    "        self.current_sweep_index = 0\n",
    "\n",
    "        self.before_abf = None\n",
    "        self.after_abf  = None\n",
    "        self.before_sweep_count = 0\n",
    "        self.after_sweep_count  = 0\n",
    "\n",
    "        # results dict -> store threshold + counts + indices\n",
    "        # results[(group, rec_id)] = {\n",
    "        #   \"before_counts\": [...],\n",
    "        #   \"before_thresholds\": [...],\n",
    "        #   \"before_indices\": [...],\n",
    "        #   \"after_counts\": [...],\n",
    "        #   \"after_thresholds\": [...],\n",
    "        #   \"after_indices\": [...]\n",
    "        # }\n",
    "        self.results = {}\n",
    "\n",
    "        # Confirmation dictionary\n",
    "        # confirm_dict[(group, rec_id, sweep_idx)] = bool\n",
    "        self.confirm_dict = {}\n",
    "\n",
    "        self.create_widgets()\n",
    "        self.load_current_cell()\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Top bar: nav + zero\n",
    "        top_frame = ttk.Frame(self.master)\n",
    "        top_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)\n",
    "\n",
    "        btn_prev_cell = ttk.Button(top_frame, text=\"Prev Cell\", command=self.go_to_prev_cell)\n",
    "        btn_prev_cell.pack(side=tk.LEFT, padx=3)\n",
    "        btn_next_cell = ttk.Button(top_frame, text=\"Next Cell\", command=self.go_to_next_cell)\n",
    "        btn_next_cell.pack(side=tk.LEFT, padx=3)\n",
    "\n",
    "        btn_prev_sweep = ttk.Button(top_frame, text=\"Prev Sweep\", command=self.go_to_prev_sweep)\n",
    "        btn_prev_sweep.pack(side=tk.LEFT, padx=3)\n",
    "        btn_next_sweep = ttk.Button(top_frame, text=\"Next Sweep\", command=self.go_to_next_sweep)\n",
    "        btn_next_sweep.pack(side=tk.LEFT, padx=3)\n",
    "\n",
    "        btn_b_zero = ttk.Button(top_frame, text=\"Set Before=0\", command=self.set_before_zero)\n",
    "        btn_b_zero.pack(side=tk.LEFT, padx=3)\n",
    "        btn_a_zero = ttk.Button(top_frame, text=\"Set After=0\", command=self.set_after_zero)\n",
    "        btn_a_zero.pack(side=tk.LEFT, padx=3)\n",
    "\n",
    "        mid_frame = ttk.Frame(self.master)\n",
    "        mid_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)\n",
    "        self.confirm_var = tk.BooleanVar(value=False)\n",
    "        self.chk_confirm = ttk.Checkbutton(\n",
    "            mid_frame, text=\"Confirm Current Sweep\",\n",
    "            variable=self.confirm_var,\n",
    "            command=self.on_confirm_toggle\n",
    "        )\n",
    "        self.chk_confirm.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        # Figure area\n",
    "        self.fig_frame = ttk.Frame(self.master)\n",
    "        self.fig_frame.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Status label\n",
    "        self.status_label = ttk.Label(self.master, text=\"No data yet.\")\n",
    "        self.status_label.pack(side=tk.TOP, padx=5, pady=5)\n",
    "\n",
    "        # Save & Quit\n",
    "        bottom_frame = ttk.Frame(self.master)\n",
    "        bottom_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)\n",
    "        btn_save_quit = ttk.Button(bottom_frame, text=\"Save & Quit\", command=self.save_and_quit)\n",
    "        btn_save_quit.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "        self.fig = None\n",
    "        self.canvas = None\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Navigation\n",
    "    # ---------------------------------------------------------------\n",
    "    def go_to_prev_cell(self):\n",
    "        if not self.allow_navigation():\n",
    "            return\n",
    "        self.current_cell_index -= 1\n",
    "        if self.current_cell_index < 0:\n",
    "            self.current_cell_index = 0\n",
    "        self.load_current_cell()\n",
    "\n",
    "    def go_to_next_cell(self):\n",
    "        if not self.allow_navigation():\n",
    "            return\n",
    "        self.current_cell_index += 1\n",
    "        if self.current_cell_index >= len(self.group_rec_pairs):\n",
    "            self.current_cell_index = len(self.group_rec_pairs) - 1\n",
    "        self.load_current_cell()\n",
    "\n",
    "    def go_to_prev_sweep(self):\n",
    "        if not self.allow_navigation():\n",
    "            return\n",
    "        self.current_sweep_index -= 1\n",
    "        if self.current_sweep_index < 0:\n",
    "            self.current_sweep_index = 0\n",
    "        self.show_current_sweep()\n",
    "\n",
    "    def go_to_next_sweep(self):\n",
    "        if not self.allow_navigation():\n",
    "            return\n",
    "        self.current_sweep_index += 1\n",
    "        max_sw = max(self.before_sweep_count, self.after_sweep_count)\n",
    "        if self.current_sweep_index >= max_sw:\n",
    "            self.current_sweep_index = max_sw - 1\n",
    "        self.show_current_sweep()\n",
    "\n",
    "    def allow_navigation(self):\n",
    "        \"\"\"Block nav unless user has confirmed this sweep.\"\"\"\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        cval = self.confirm_dict.get((group, rec_id, self.current_sweep_index), False)\n",
    "        if not cval:\n",
    "            messagebox.showwarning(\"Sweep Not Confirmed\",\n",
    "                \"Please confirm the current sweep before navigating away.\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Confirm / Unconfirm\n",
    "    # ---------------------------------------------------------------\n",
    "    def on_confirm_toggle(self):\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        idx = self.current_sweep_index\n",
    "        val = self.confirm_var.get()\n",
    "        self.confirm_dict[(group, rec_id, idx)] = val\n",
    "\n",
    "    def unconfirm_current_sweep(self):\n",
    "        \"\"\"If user drags or sets zero, unconfirm.\"\"\"\n",
    "        self.confirm_var.set(False)\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        idx = self.current_sweep_index\n",
    "        self.confirm_dict[(group, rec_id, idx)] = False\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Loading a cell & sweeps\n",
    "    # ---------------------------------------------------------------\n",
    "    def load_current_cell(self):\n",
    "        if self.current_cell_index < 0:\n",
    "            self.current_cell_index = 0\n",
    "        if self.current_cell_index >= len(self.group_rec_pairs):\n",
    "            self.current_cell_index = len(self.group_rec_pairs)-1\n",
    "\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        sdf = self.dataframe[\n",
    "            (self.dataframe[\"Group\"] == group) &\n",
    "            (self.dataframe[\"Recording_ID\"] == rec_id)\n",
    "        ]\n",
    "        before_entry = sdf[sdf[\"Label\"] == \"Before\"]\n",
    "        after_entry  = sdf[sdf[\"Label\"] == \"After\"]\n",
    "        if before_entry.empty or after_entry.empty:\n",
    "            print(f\"Incomplete pair for {group}, {rec_id}\")\n",
    "            return\n",
    "\n",
    "        before_file = before_entry[\"File_Path\"].iloc[0]\n",
    "        after_file  = after_entry[\"File_Path\"].iloc[0]\n",
    "\n",
    "        self.before_abf = pyabf.ABF(before_file)\n",
    "        self.after_abf  = pyabf.ABF(after_file)\n",
    "\n",
    "        self.before_sweep_count = len(self.before_abf.sweepList)\n",
    "        self.after_sweep_count  = len(self.after_abf.sweepList)\n",
    "\n",
    "        if (group, rec_id) not in self.results:\n",
    "            self.results[(group, rec_id)] = {\n",
    "                \"before_counts\":    [0]*self.before_sweep_count,\n",
    "                \"before_thresholds\":[None]*self.before_sweep_count,\n",
    "                \"before_indices\":   [None]*self.before_sweep_count,\n",
    "                \"after_counts\":     [0]*self.after_sweep_count,\n",
    "                \"after_thresholds\": [None]*self.after_sweep_count,\n",
    "                \"after_indices\":    [None]*self.after_sweep_count,\n",
    "            }\n",
    "\n",
    "        self.current_sweep_index = 0\n",
    "        self.show_current_sweep()\n",
    "\n",
    "    def show_current_sweep(self):\n",
    "        # Destroy old figure\n",
    "        if self.fig and self.canvas:\n",
    "            self.canvas.get_tk_widget().destroy()\n",
    "            plt.close(self.fig)\n",
    "\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        data = self.results[(group, rec_id)]\n",
    "        b_idx = min(self.current_sweep_index, self.before_sweep_count-1)\n",
    "        a_idx = min(self.current_sweep_index, self.after_sweep_count-1)\n",
    "\n",
    "        self.fig = Figure(figsize=(8,6))\n",
    "        axB = self.fig.add_subplot(2,1,1)\n",
    "        axA = self.fig.add_subplot(2,1,2)\n",
    "        self.fig.suptitle(f\"{group} - {rec_id} | Sweep {self.current_sweep_index}\")\n",
    "\n",
    "        # Plot BEFORE\n",
    "        self.before_abf.setSweep(b_idx)\n",
    "        tB, vB = self.before_abf.sweepX, self.before_abf.sweepY\n",
    "        axB.plot(tB, vB, \"C0\")\n",
    "        axB.set_title(f\"Before (sweep {b_idx})\")\n",
    "\n",
    "        # Plot AFTER\n",
    "        self.after_abf.setSweep(a_idx)\n",
    "        tA, vA = self.after_abf.sweepX, self.after_abf.sweepY\n",
    "        axA.plot(tA, vA, \"C1\")\n",
    "        axA.set_title(f\"After (sweep {a_idx})\")\n",
    "        axA.set_xlabel(\"Time (s)\")\n",
    "\n",
    "        b_thresh = data[\"before_thresholds\"][b_idx]\n",
    "        if b_thresh is None:\n",
    "            b_thresh = np.mean(vB) + 0.5*np.std(vB)\n",
    "        a_thresh = data[\"after_thresholds\"][a_idx]\n",
    "        if a_thresh is None:\n",
    "            a_thresh = np.mean(vA) + 0.5*np.std(vA)\n",
    "\n",
    "        # Make draggable lines\n",
    "        # If user picks line => unconfirm\n",
    "        self.line_before = DraggableThresholdLine(\n",
    "            axB, b_thresh,\n",
    "            on_release_callback=lambda val: self.update_peak_counts(\"before\", val),\n",
    "            on_drag_start=self.unconfirm_current_sweep,\n",
    "            pick_tolerance_pixels=10\n",
    "        )\n",
    "        self.line_after = DraggableThresholdLine(\n",
    "            axA, a_thresh,\n",
    "            on_release_callback=lambda val: self.update_peak_counts(\"after\", val),\n",
    "            on_drag_start=self.unconfirm_current_sweep,\n",
    "            pick_tolerance_pixels=10\n",
    "        )\n",
    "\n",
    "        self.canvas = FigureCanvasTkAgg(self.fig, master=self.fig_frame)\n",
    "        self.canvas.draw()\n",
    "        self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Always do detection on load\n",
    "        self.update_peak_counts(\"before\", b_thresh, replot=False)\n",
    "        self.update_peak_counts(\"after\",  a_thresh, replot=False)\n",
    "        self.draw_peaks()\n",
    "\n",
    "        # Reflect confirm\n",
    "        is_conf = self.confirm_dict.get((group, rec_id, self.current_sweep_index), False)\n",
    "        self.confirm_var.set(is_conf)\n",
    "        self.update_status_label()\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Peak counting\n",
    "    # ---------------------------------------------------------------\n",
    "    def update_peak_counts(self, subplot, threshold, replot=True):\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        data = self.results[(group, rec_id)]\n",
    "        b_idx = min(self.current_sweep_index, self.before_sweep_count-1)\n",
    "        a_idx = min(self.current_sweep_index, self.after_sweep_count-1)\n",
    "\n",
    "        if subplot == \"before\":\n",
    "            self.before_abf.setSweep(b_idx)\n",
    "            volt = self.before_abf.sweepY\n",
    "            pk, _ = find_peaks(volt, height=threshold)\n",
    "            data[\"before_counts\"][b_idx]     = len(pk)\n",
    "            data[\"before_thresholds\"][b_idx] = threshold\n",
    "            data[\"before_indices\"][b_idx]    = pk\n",
    "        else:\n",
    "            self.after_abf.setSweep(a_idx)\n",
    "            volt = self.after_abf.sweepY\n",
    "            pk, _ = find_peaks(volt, height=threshold)\n",
    "            data[\"after_counts\"][a_idx]     = len(pk)\n",
    "            data[\"after_thresholds\"][a_idx] = threshold\n",
    "            data[\"after_indices\"][a_idx]    = pk\n",
    "\n",
    "        if replot:\n",
    "            self.draw_peaks()\n",
    "        self.update_status_label()\n",
    "\n",
    "    def draw_peaks(self):\n",
    "        \"\"\"Re-plot both subplots with the final thresholds & peaks.\"\"\"\n",
    "        if not self.fig or not self.canvas:\n",
    "            return\n",
    "\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        data = self.results[(group, rec_id)]\n",
    "        b_idx = min(self.current_sweep_index, self.before_sweep_count-1)\n",
    "        a_idx = min(self.current_sweep_index, self.after_sweep_count-1)\n",
    "\n",
    "        axB, axA = self.fig.axes\n",
    "        axB.clear()\n",
    "        axA.clear()\n",
    "\n",
    "        # BEFORE\n",
    "        self.before_abf.setSweep(b_idx)\n",
    "        tB, vB = self.before_abf.sweepX, self.before_abf.sweepY\n",
    "        axB.plot(tB, vB, 'C0')\n",
    "        b_thr = data[\"before_thresholds\"][b_idx]\n",
    "        b_pks = data[\"before_indices\"][b_idx]\n",
    "        if b_thr is not None:\n",
    "            axB.axhline(b_thr, color='r', linestyle='--', lw=1)\n",
    "        if b_pks is not None and len(b_pks) > 0:\n",
    "            axB.plot(tB[b_pks], vB[b_pks], 'rx')\n",
    "        axB.set_title(f\"Before (sweep {b_idx})\")\n",
    "\n",
    "        # AFTER\n",
    "        self.after_abf.setSweep(a_idx)\n",
    "        tA, vA = self.after_abf.sweepX, self.after_abf.sweepY\n",
    "        axA.plot(tA, vA, 'C1')\n",
    "        a_thr = data[\"after_thresholds\"][a_idx]\n",
    "        a_pks = data[\"after_indices\"][a_idx]\n",
    "        if a_thr is not None:\n",
    "            axA.axhline(a_thr, color='r', linestyle='--', lw=1)\n",
    "        if a_pks is not None and len(a_pks) > 0:\n",
    "            axA.plot(tA[a_pks], vA[a_pks], 'rx')\n",
    "        axA.set_title(f\"After (sweep {a_idx})\")\n",
    "        axA.set_xlabel(\"Time (s)\")\n",
    "\n",
    "        self.canvas.draw()\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Zeroing\n",
    "    # ---------------------------------------------------------------\n",
    "    def set_before_zero(self):\n",
    "        self.unconfirm_current_sweep()\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        data = self.results[(group, rec_id)]\n",
    "        b_idx = min(self.current_sweep_index, self.before_sweep_count-1)\n",
    "        data[\"before_counts\"][b_idx] = 0\n",
    "        data[\"before_thresholds\"][b_idx] = None\n",
    "        data[\"before_indices\"][b_idx] = []\n",
    "        self.draw_peaks()\n",
    "        self.update_status_label()\n",
    "\n",
    "    def set_after_zero(self):\n",
    "        self.unconfirm_current_sweep()\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        data = self.results[(group, rec_id)]\n",
    "        a_idx = min(self.current_sweep_index, self.after_sweep_count-1)\n",
    "        data[\"after_counts\"][a_idx] = 0\n",
    "        data[\"after_thresholds\"][a_idx] = None\n",
    "        data[\"after_indices\"][a_idx] = []\n",
    "        self.draw_peaks()\n",
    "        self.update_status_label()\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Status & Saving\n",
    "    # ---------------------------------------------------------------\n",
    "    def update_status_label(self):\n",
    "        group, rec_id = self.group_rec_pairs[self.current_cell_index]\n",
    "        data = self.results[(group, rec_id)]\n",
    "        bc = data[\"before_counts\"]\n",
    "        ac = data[\"after_counts\"]\n",
    "        b_str = ','.join(str(x) for x in bc)\n",
    "        a_str = ','.join(str(x) for x in ac)\n",
    "        txt = (f\"Cell: {group}-{rec_id}\\n\"\n",
    "               f\"Before counts: [{b_str}]\\n\"\n",
    "               f\"After counts : [{a_str}]\")\n",
    "        self.status_label.config(text=txt)\n",
    "\n",
    "    def save_and_quit(self):\n",
    "        rows = []\n",
    "        for (group, rec_id), val in self.results.items():\n",
    "            bc = val[\"before_counts\"]\n",
    "            bt = val[\"before_thresholds\"]\n",
    "            bi = val[\"before_indices\"]\n",
    "            ac = val[\"after_counts\"]\n",
    "            at = val[\"after_thresholds\"]\n",
    "            ai = val[\"after_indices\"]\n",
    "\n",
    "            max_len = max(len(bc), len(ac))\n",
    "            for i in range(max_len):\n",
    "                # Convert arrays to strings, e.g. \"[10, 22, 55]\"\n",
    "                bi_str = str(list(bi[i])) if bi[i] is not None else \"[]\"\n",
    "                ai_str = str(list(ai[i])) if ai[i] is not None else \"[]\"\n",
    "\n",
    "                row = {\n",
    "                    \"Group\": group,\n",
    "                    \"Recording_ID\": rec_id,\n",
    "                    \"Sweep_Index\": i,\n",
    "                    \"Manual_Before_Count\": bc[i],\n",
    "                    \"Before_Threshold\":    bt[i],\n",
    "                    \"Before_Peak_Indices\": bi_str,\n",
    "                    \"Manual_After_Count\":  ac[i],\n",
    "                    \"After_Threshold\":     at[i],\n",
    "                    \"After_Peak_Indices\":  ai_str\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        out_df = pd.DataFrame(rows)\n",
    "        out_df.to_csv(self.output_csv, index=False)\n",
    "        print(f\"Saved {len(rows)} rows to {self.output_csv}\")\n",
    "        self.master.quit()\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Example usage\n",
    "# -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example mock DataFrame\n",
    "\n",
    "    df = pd.DataFrame(processor.dataframe)\n",
    "\n",
    "    root = tk.Tk()\n",
    "    app = ManualPeakCounterGUI(root, df, output_csv=\"manual_peak_counts.csv\")\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiketurnpike_postanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
