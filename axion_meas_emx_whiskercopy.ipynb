{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiketurnpike_postanalysis.Extract_ephys_from_struct import ExtractEphysData\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu, kruskal, shapiro, ttest_ind\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, permutation_test\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class and methods set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataFrameManager:\n",
    "    def __init__(self, eed_object):\n",
    "        self.eed = eed_object\n",
    "        self.dataframes = {}  # Dictionary to hold multiple DataFrames\n",
    "        self.detailed_dataframes = {}\n",
    "        \n",
    "    def create_dataframe(self, columns, df_name):\n",
    "        data = []\n",
    "        for groupname, recordings in self.eed.all_data.items():\n",
    "            for recordingname, cells in recordings.items():\n",
    "                for cid, metrics in cells.items():\n",
    "                    \n",
    "                    row = {'groupname': groupname, 'recordingname': recordingname, 'cid': cid}\n",
    "                    for column in columns:\n",
    "                        value = metrics.get(column, None)\n",
    "                        \n",
    "                        # Reassign 'IsSingleUnit' based on 'ISI_violations_percent' if necessary\n",
    "                        if column == 'ISI_violations_percent' and value is not None and value < 0.01:\n",
    "                            print(f\"Changing IsSingleUnit to 1.0 for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                            row['IsSingleUnit'] = 1.0\n",
    "                            continue  # Skip to the next column\n",
    "                        \n",
    "                        # Reassign 'Cell_Type' based on 'TroughToPeak_duration' if necessary\n",
    "                        if column == 'TroughToPeak_duration' and value is not None and value <= 0.5:\n",
    "                            print(f\"Changing Cell_Type to FS for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                            row['Cell_Type'] = 'FS'\n",
    "                            # Now add the 'TroughToPeak_duration' value to the row\n",
    "                            row['TroughToPeak_duration'] = value\n",
    "                            continue  # Skip to the next column\n",
    "                        \n",
    "                        # Reassign 'ModulationIndex' based on its value if necessary\n",
    "                        if column == 'ModulationIndex' and value is not None:\n",
    "                            if value >= 0.3:\n",
    "                                print(f\"Changing ModulationIndex {value} to positive for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                                row['ModulationIndex'] = 'positive'\n",
    "                            elif value <= -0.3:\n",
    "                                print(f\"Changing ModulationIndex {value} to negative for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                                row['ModulationIndex'] = 'negative'\n",
    "                            else:\n",
    "                                print(f\"Changing ModulationIndex {value} to none for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                                row['ModulationIndex'] = 'none'\n",
    "                            continue  # Skip to the next column\n",
    "\n",
    "                        # Add the original value if no reassignment happened\n",
    "                        row[column] = value\n",
    "                    \n",
    "                    data.append(row)\n",
    "        \n",
    "        new_df = pd.DataFrame(data)\n",
    "        if df_name in self.dataframes:\n",
    "            self.dataframes[df_name] = pd.concat([self.dataframes[df_name], new_df], ignore_index=True)\n",
    "        else:\n",
    "            self.dataframes[df_name] = new_df\n",
    "\n",
    "    def revert_modulation_index_to_numeric(self):\n",
    "        \"\"\"\n",
    "        Revert the 'ModulationIndex' labels ('positive', 'negative', 'none') \n",
    "        back to the original numeric values and store them in a new column \n",
    "        named 'ModulationIndex_Numeric'.\n",
    "        \"\"\"\n",
    "        for df_name, df in self.dataframes.items():\n",
    "            if 'ModulationIndex' in df.columns:\n",
    "                for index, row in df.iterrows():\n",
    "                    groupname = row['groupname']\n",
    "                    recordingname = row['recordingname']\n",
    "                    cid = row['cid']\n",
    "                    \n",
    "                    # Retrieve the original ModulationIndex value from the self.eed.all_data structure\n",
    "                    original_value = self.eed.all_data[groupname][recordingname][cid].get('ModulationIndex', None)\n",
    "                    \n",
    "                    if original_value is not None:\n",
    "                        print(f\"Reverting ModulationIndex from {row['ModulationIndex']} to {original_value} for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                        df.at[index, 'ModulationIndex_Numeric'] = original_value  # Store the numeric value in a new column\n",
    "\n",
    "                # Ensure the DataFrame is updated in the self.dataframes dictionary\n",
    "                self.dataframes[df_name] = df\n",
    "\n",
    "\n",
    "    def append_data(self, df_name, new_data):\n",
    "        if df_name in self.dataframes:\n",
    "            self.dataframes[df_name] = pd.concat([self.dataframes[df_name], new_data], ignore_index=True)\n",
    "        else:\n",
    "            self.dataframes[df_name] = new_data    \n",
    "    \n",
    "                \n",
    "    def get_filtered_data(self, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, groupname=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Retrieve filtered data from the specified DataFrame based on IsSingleUnit, Cell_Type, \n",
    "        StimResponsivity, groupname, and ModulationIndex with the option to not filter on any of these by passing None.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        modulation_label (str or None): Filter for 'positive', 'negative', or 'none' in the ModulationIndex column.\n",
    "                                        If None, no filtering by ModulationIndex.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The filtered DataFrame.\n",
    "        \"\"\"\n",
    "        \n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        # Start with the full DataFrame\n",
    "        filtered_df = self.dataframes[df_name]\n",
    "\n",
    "        # Filter by cell type if provided\n",
    "        if cell_type:\n",
    "            filtered_df = filtered_df[filtered_df['Cell_Type'] == cell_type]\n",
    "\n",
    "        # Filter by IsSingleUnit if not None\n",
    "        if is_single_unit is not None:\n",
    "            filtered_df = filtered_df[filtered_df['IsSingleUnit'] == is_single_unit]\n",
    "\n",
    "        # Filter by StimResponsivity if not None\n",
    "        if stim_responsivity is not None:\n",
    "            filtered_df = filtered_df[filtered_df['StimResponsivity'] == stim_responsivity]\n",
    "\n",
    "        # Filter by groupname if provided\n",
    "        if groupname is not None:\n",
    "            filtered_df = filtered_df[filtered_df['groupname'] == groupname]\n",
    "\n",
    "        # Filter by ModulationIndex if provided\n",
    "        if modulation_label is not None:\n",
    "            filtered_df = filtered_df[filtered_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        return filtered_df\n",
    "   \n",
    "   \n",
    "    def create_psth_dataframe(self):\n",
    "        \"\"\"\n",
    "        Creates and stores a DataFrame for each stimulation type using the 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns from the base PSTH DataFrame. \n",
    "        Each DataFrame is stored as an attribute of the DataFrameManager under a name that corresponds to the stimulation type.\n",
    "\n",
    "        Details:\n",
    "            'SpikeTrains_for_PSTHs' is expected to be a pandas Series where each entry is a list of arrays.\n",
    "            Each array in the list corresponds to spike train data for one of the four distinct stimulations, with dimensions (n_trials, n_time_points), \n",
    "            where 'n_trials' varies per stimulation and 'n_time_points' is consistent (usually the length of the trial in ms).\n",
    "\n",
    "            'PSTHs_conv' is expected to be a pandas Series where each entry is a numpy ndarray with dimensions (4, n_time_points), \n",
    "            where the first dimension corresponds to the four stimulation types and 'n_time_points' matches the second dimension of the arrays in 'SpikeTrains_for_PSTHs'.\n",
    "\n",
    "        Processes:\n",
    "            - A base DataFrame is created with necessary columns.\n",
    "            - For each stimulation label (e.g., 'Zero', 'Low', 'Mid', 'Max'), a new DataFrame is created.\n",
    "            - Each new DataFrame includes adjusted 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns to isolate the data corresponding to the respective stimulation type.\n",
    "            - Each DataFrame is stored in the class dictionary, keyed by the name 'psth_dataframe_' followed by the stimulation label.\n",
    "        \"\"\"\n",
    "        # Create the base dataframe for PSTH analysis\n",
    "        self.create_dataframe(['Cell_Type', 'LaminarLabel','IsSingleUnit', 'StimResponsivity', 'SpikeTrains_for_PSTHs', 'PSTHs_conv', 'ModulationIndex'], 'psth_dataframe')\n",
    "\n",
    "        # Extracting trial tags\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        # Process each label\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            # Retrieve spike trains and PSTHs for each label and create a new DataFrame\n",
    "            df_name = f'psth_dataframe_{label}'\n",
    "            self.dataframes[df_name] = self.dataframes['psth_dataframe'].copy()\n",
    "            self.dataframes[df_name]['SpikeTrains_for_PSTHs'] = self.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'].apply(lambda x: x[index])\n",
    "            self.dataframes[df_name]['PSTHs_conv'] = self.dataframes['psth_dataframe']['PSTHs_conv'].apply(lambda x: x[index])\n",
    "\n",
    "    def create_psth_dataframe_opto(self, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Creates and stores a DataFrame for optogenetic stimulation using the 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns from the base PSTH DataFrame. \n",
    "        The DataFrame is stored as an attribute of the DataFrameManager under a name that corresponds to the optogenetic stimulation type.\n",
    "\n",
    "        Details:\n",
    "            'SpikeTrains_for_PSTHs' is expected to be a pandas Series where each entry is an array of spike train data for the optogenetic stimulation,\n",
    "            with dimensions (n_trials, n_time_points), where 'n_trials' is the number of trials and 'n_time_points' is consistent (usually the length of the trial in ms).\n",
    "\n",
    "            'PSTHs_conv' is expected to be a pandas Series where each entry is a numpy ndarray with dimensions (n_time_points),\n",
    "            where 'n_time_points' is consistent and should be 1500, corresponding to the time points of the optogenetic stimulation.\n",
    "\n",
    "        Processes:\n",
    "            - A base DataFrame is created with necessary columns.\n",
    "            - Retrieve trial tags specific for optogenetic stimulation.\n",
    "            - For each label (expected to be one for optogenetic stimulation), a new DataFrame is created.\n",
    "            - Adjusted 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns are created to isolate the data corresponding to the optogenetic stimulation.\n",
    "            - The DataFrame is stored in the class dictionary, keyed by the name 'psth_dataframe_' followed by the stimulation label.\n",
    "        \"\"\"\n",
    "        # Create the base dataframe for PSTH analysis\n",
    "        \n",
    "          # Create the base dataframe for PSTH analysis with modulation filtering\n",
    "        self.create_dataframe(['Cell_Type', \n",
    "            'LaminarLabel', \n",
    "            'IsSingleUnit', \n",
    "            'StimResponsivity', \n",
    "            'SpikeTrains_for_PSTHs', \n",
    "            'PSTHs_conv', \n",
    "            'ModulationIndex',\n",
    "            'FirstSpikeLatency', \n",
    "            'FirstSpikeLatency_Reliability', \n",
    "            'TroughToPeak_duration', \n",
    "            'SpikeHalfWidth', \n",
    "            'PeakToPeak_ratio', \n",
    "            'peak1_normalized_amplitude', \n",
    "            'Peak1ToTrough_ratio', \n",
    "            'Peak2ToTrough_ratio', \n",
    "            'Template_Channel', \n",
    "            'MeanFR_baseline', \n",
    "            'Normalized_Template_Waveform', \n",
    "            'UnNormalized_Template_Waveform',\n",
    "            'SpikeTimes_all', \n",
    "            'ISI_baseline_CV', \n",
    "            'ISI_baseline_vec', \n",
    "            'ISI_pdf_peak_xy', \n",
    "            'ISI_pdf_x', \n",
    "            'ISI_pdf_y',\n",
    "            'StimProb', \n",
    "            'MeanFR_stim', \n",
    "            'PeakEvokedFR', \n",
    "            'PeakEvokedFR_Latency', \n",
    "            'FanoFactor_baseline', \n",
    "            'FanoFactor_stim', \n",
    "            'MeanFR_inst_stim', \n",
    "            'ISI_violations_percent', \n",
    "            'Recording_Duration', \n",
    "            'Sampling_Frequency'], 'psth_dataframe')\n",
    "        \n",
    "       \n",
    "        # Extracting trial tags\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # This should only contain the optogenetic stimulation label\n",
    "\n",
    "        # Process each label (typically only one for optogenetic stimulation)\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            # Create a new DataFrame for optogenetic stimulation\n",
    "            df_name = f'psth_dataframe_{label}'\n",
    "            self.dataframes[df_name] = self.dataframes['psth_dataframe'].copy()\n",
    "            self.dataframes[df_name]['SpikeTrains_for_PSTHs'] = self.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'].apply(lambda x: x[index])\n",
    "            # Keep the same number of time points (1500) for the PSTH data\n",
    "            self.dataframes[df_name]['PSTHs_conv'] = self.dataframes['psth_dataframe']['PSTHs_conv'].apply(lambda x: x[:, index] if x.ndim > 1 else x)\n",
    "            \n",
    "    def plot_psth(self, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Plots the PSTH for a given stimulation type, with optional filtering on cell type, single unit status, and stimulus responsivity.\n",
    "        Uses the 'relative_time_ms' from the ExtractEphysData object to correctly label the time axis.\n",
    "\n",
    "        Args:\n",
    "            stim_label (str): The label of the stimulation type to plot (e.g., 'Zero', 'Low', 'Mid', 'Max').\n",
    "            cell_type (str, optional): Filter for specific cell types (e.g., 'FS', 'RS'). Default is None, which means no filtering by cell type.\n",
    "            is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0). None means no filtering.\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity (1.0, 0.0, -1.0). None means no filtering.\n",
    "\n",
    "        Processes:\n",
    "            - Retrieves the corresponding DataFrame for the specified stimulation.\n",
    "            - Applies additional filtering based on the provided arguments.\n",
    "            - Averages the PSTH data across all remaining units and plots the result using the relative time axis.\n",
    "        \"\"\"\n",
    "        # Retrieve the DataFrame for the specified stimulation\n",
    "        df_name = f'psth_dataframe_{stim_label}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the stimulation: {stim_label}\")\n",
    "            return\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "\n",
    "        # Apply filtering based on the optional parameters\n",
    "        if cell_type is not None:\n",
    "            df = df[df['Cell_Type'] == cell_type]\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "        if groupname is not None:\n",
    "            df = df[df['groupname'] == groupname]\n",
    "\n",
    "        # Check if there is any data left after filtering\n",
    "        if df.empty:\n",
    "            print(\"No data matches the specified filters.\")\n",
    "            return\n",
    "\n",
    "        # Get relative time array for x-axis\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Aggregate the PSTH data\n",
    "        aggregated_psth = df['PSTHs_conv'].apply(pd.Series).mean(axis=0)\n",
    "\n",
    "        # Plotting the aggregated PSTH\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_array, aggregated_psth, label=f'PSTH for {stim_label}')\n",
    "        plt.title(f'PSTH for {stim_label} - {cell_type or \"All Types\"}, SingleUnit: {is_single_unit}, Responsivity: {stim_responsivity}')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Average Spike Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_time_mask(self, time_array, time_range):\n",
    "        \"\"\"\n",
    "        Creates a mask for the time array based on the specified time range.\n",
    "        \n",
    "        Args:\n",
    "            time_array (np.array): The array of time points.\n",
    "            time_range (tuple): The start and end time for the mask (in ms).\n",
    "            \n",
    "        Returns:\n",
    "            time_mask (np.array): Boolean array where True indicates the time points within the specified range.\n",
    "        \"\"\"\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "        return time_mask\n",
    "\n",
    "    def filter_data(self, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, groupname=None, modulation_label=None, recordingname=None):\n",
    "        df_name = f'psth_dataframe_{stim_label}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the stimulation: {stim_label}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "        \n",
    "        # Apply filtering based on the optional parameters\n",
    "        if cell_type is not None:\n",
    "            df = df[df['Cell_Type'] == cell_type]\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "        if groupname is not None:\n",
    "            df = df[df['groupname'] == groupname]\n",
    "        if recordingname is not None:\n",
    "            df = df[df['recordingname'] == recordingname]\n",
    "        \n",
    "        if modulation_label is not None:\n",
    "            # Validate the modulation label input\n",
    "            if modulation_label not in ['positive', 'negative', 'none']:\n",
    "                raise ValueError(\"Modulation label must be one of 'positive', 'negative', or 'none'.\")\n",
    "            \n",
    "            # Apply modulation label filtering\n",
    "            if modulation_label == 'positive' or modulation_label == 'negative':\n",
    "                df = df[df['ModulationIndex'] == modulation_label]\n",
    "            elif modulation_label == 'none':\n",
    "                df = df[df['ModulationIndex'].isnull()]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def plot_individual_psths(self, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, groupname=None, modulation_label=None, time_range=None):\n",
    "        df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label)\n",
    "        \n",
    "        # Print the number of units that match the filter\n",
    "        print(f\"Number of units that match the filter: {df.shape[0]}\")\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"No data matches the specified filters.\")\n",
    "            return\n",
    "\n",
    "        # Get relative time array for x-axis\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        \n",
    "        # Create time mask and adjust time array\n",
    "        time_mask = self.create_time_mask(time_array, time_range)\n",
    "        time_array = time_array[time_mask]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            individual_psth = row['PSTHs_conv']\n",
    "            # Apply time mask to individual PSTH\n",
    "            individual_psth = np.array(individual_psth)[time_mask]\n",
    "            # Apply a smoothing window\n",
    "            window = np.ones(3) / 3  # 3ms window of smoothing\n",
    "\n",
    "\n",
    "            \n",
    "            individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "            plt.plot(time_array, individual_psth, label=f'Unit {index}')\n",
    "\n",
    "        plt.title(f'Individual PSTHs for {stim_label} - {cell_type or \"All Types\"}, SingleUnit: {is_single_unit}, Responsivity: {stim_responsivity}, Modulation: {modulation_label}')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Spike Rate')\n",
    "        plt.legend().set_visible(False)\n",
    "        plt.show()\n",
    "        \n",
    "    def compare_groups(self, group1, group2, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Compares PSTH data between two specified groups for a given stimulation.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): The first group name for comparison.\n",
    "            group2 (str): The second group name for comparison.\n",
    "            stim_label (str): The label of the stimulation type (e.g., 'Zero', 'Low', 'Mid', 'Max').\n",
    "            cell_type (str, optional): Specific cell type to filter by.\n",
    "            is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0).\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "            modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two pandas DataFrames representing the filtered data for each group.\n",
    "        \"\"\"\n",
    "        df_name = f'psth_dataframe_{stim_label}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the stimulation: {stim_label}\")\n",
    "            return None, None\n",
    "\n",
    "        base_df = self.dataframes[df_name]\n",
    "        \n",
    "        # Print the shape of the pre-filtered DataFrame\n",
    "        print(f\"Pre-filtered DataFrame shape: {base_df.shape}\")\n",
    "\n",
    "        # Define a helper function to create a mask for a given group\n",
    "        def create_mask(group):\n",
    "            mask = (base_df['groupname'] == group)\n",
    "            if cell_type is not None:\n",
    "                mask &= (base_df['Cell_Type'] == cell_type)\n",
    "            if is_single_unit is not None:\n",
    "                mask &= (base_df['IsSingleUnit'] == is_single_unit)\n",
    "            if stim_responsivity is not None:\n",
    "                mask &= (base_df['StimResponsivity'] == stim_responsivity)\n",
    "            if modulation_label is not None:\n",
    "                if modulation_label not in ['positive', 'negative', 'none']:\n",
    "                    raise ValueError(\"Modulation label must be one of 'positive', 'negative', or 'none'.\")\n",
    "                mask &= (base_df['ModulationIndex'] == modulation_label)\n",
    "            return mask\n",
    "\n",
    "        # Create masks for each group\n",
    "        mask1 = create_mask(group1)\n",
    "        mask2 = create_mask(group2)\n",
    "\n",
    "        # Filter the DataFrame for each group\n",
    "        channel1 = base_df[mask1]\n",
    "        channel2 = base_df[mask2]\n",
    "\n",
    "        # Print the shape of the filtered DataFrames\n",
    "        print(f\"Filtered df1 (group {group1}) shape: {channel1.shape}\")\n",
    "        print(f\"Filtered df2 (group {group2}) shape: {channel2.shape}\")\n",
    "\n",
    "        return channel1, channel2\n",
    "\n",
    "        \n",
    "    def plot_all_stimulations(self, group1, group2, cell_type=None, is_single_unit=None, stim_responsivity=None, time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a 1x4 subplot of PSTH comparisons for all stimulation types.\n",
    "        1. Grabs the trial tags labels from the ExtractEphysData object to determine the stimulation types\n",
    "        and create labels for the subplots.\n",
    "        2. Calls the plot_psth_comparison method for each stimulation type and plots them in a single figure.\n",
    "        \n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            cell_type (str): 'FS' or 'RS'\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "\n",
    "        for i, stim_label in enumerate(stim_labels):\n",
    "            self.plot_psth_comparison(group1, group2, stim_label, axs[i], cell_type=cell_type, is_single_unit=is_single_unit, stim_responsivity=stim_responsivity, time_range=time_range, plot_mode=plot_mode, smoothing_window=smoothing_window)\n",
    "            axs[i].set_title(stim_label)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_psth_comparison(self, group1, group2, stim_label, \n",
    "                             ax=None, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_label=None,\n",
    "                             time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        called within plot_all_stimulations which is called within the main function plot_all_stimulations\n",
    "        \n",
    "        uses the compare_groups method to get the data for the two groups and the specified stimulation type \n",
    "        and fetches the two DataFrames for the groups\n",
    "        \n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided.\n",
    "        Optionally overlays the stimulator signal as a thin black line.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            stim_label (str): Stimulation label.\n",
    "            ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "            groups_to_plot (str, optional): Specifies which group(s) to plot ('both', 'group1', 'group2').\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'}\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_groups(group1, group2, stim_label, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        \n",
    "        # DataFrame to store all traces ###new addition to the code\n",
    "        all_traces_df = pd.DataFrame()\n",
    "        \n",
    "        # Process and plot data\n",
    "        for df, group in zip([df1, df2], [group1, group2]):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "                \n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "            \n",
    "            # Store each trace in the DataFrame ## new addition to the code \n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = stim_label\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            \n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            \n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "        # Set plot attributes\n",
    "        ax.set_title(f'PSTH Comparison of {stim_label} between {group1} and {group2}')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Average Spike Rate')\n",
    "        ax.legend()\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "    \n",
    "    def plot_psth_comparison_grouprecordings(self, group1, group2, stim_label, \n",
    "                                            ax=None, cell_type=None, is_single_unit=None, stim_responsivity=None, \n",
    "                                            time_range=None, plot_mode='mean', smoothing_window=None, group_by_recordings=False):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided.\n",
    "        Optionally overlays the stimulator signal as a thin black line.\n",
    "        Optionally groups data by recordings before calculating mean PSTHs.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            stim_label (str): Stimulation label.\n",
    "            ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "            group_by_recordings (bool, optional): If True, calculates the mean PSTHs at the recording level.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "        \n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_groups(group1, group2, stim_label, cell_type, is_single_unit, stim_responsivity)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # Helper function to process data\n",
    "        def process_data(df, group):\n",
    "            if group_by_recordings:\n",
    "                grouped = df.groupby('recordingname')\n",
    "                grouped_data = grouped['PSTHs_conv'].apply(lambda x: np.mean([np.array(i)[time_mask] for i in x], axis=0))\n",
    "            else:\n",
    "                grouped_data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                grouped_data = grouped_data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "            \n",
    "            return grouped_data\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group in zip([df1, df2], [group1, group2]):\n",
    "            data = process_data(df, group)\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "            \n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            \n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "        # Set plot attributes\n",
    "        ax.set_title(f'PSTH Comparison of {stim_label} between {group1} and {group2}')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Average Spike Rate')\n",
    "        ax.legend()\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        return data \n",
    "\n",
    "    def extract_stim_signals(self):\n",
    "        \"\"\"\n",
    "        Extracts and formats the stimulation signals for each relevant stimulation type and synthesizes a flat line for the 'Zero' stimulation.\n",
    "        \"\"\"\n",
    "        stim_voltages = self.eed.StimVoltageTraces_ms['StimVoltageTraces_ms']\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        aligned_signals = {}\n",
    "        full_length = len(self.eed.relative_time_ms['relative_time_ms'])\n",
    "        stim_start_index = 500\n",
    "        stim_end_index = 1000\n",
    "\n",
    "        # Determine the global maximum voltage to set a unified y-axis for the stimulation signals\n",
    "        max_voltage = np.max(stim_voltages)\n",
    "\n",
    "        # Extract and pad signals\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            if label in ['Low', 'Mid', 'Max']:\n",
    "                signal_column_index = ['Low', 'Mid', 'Max'].index(label)\n",
    "                signal = stim_voltages[:, signal_column_index]\n",
    "\n",
    "                pre_padding = np.zeros(stim_start_index)\n",
    "                post_padding = np.zeros(full_length - stim_end_index)\n",
    "                padded_signal = np.concatenate((pre_padding, signal, post_padding))\n",
    "\n",
    "                aligned_signals[label] = padded_signal[:1500]\n",
    "\n",
    "            elif label == 'Zero':\n",
    "                # Create a flat line using the minimum voltage from 'Low'\n",
    "                min_voltage = np.min(stim_voltages[:, 0])\n",
    "                flat_signal = np.full(full_length, min_voltage)\n",
    "                aligned_signals[label] = flat_signal[:1500]\n",
    "\n",
    "        return aligned_signals, max_voltage\n",
    "    \n",
    "    def extract_stim_signals_opto(self):\n",
    "        \"\"\"\n",
    "        Extracts and formats the stimulation signals for the optogenetic stimulation type. \n",
    "        Since there is only one stimulation type and the array is 1D, the method simplifies handling of the array.\n",
    "        \"\"\"\n",
    "        stim_voltages = self.eed.StimVoltageTraces_ms['StimVoltageTrace_ms'] #Trace not Traces for opto\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # This should contain only the optogenetic stimulation label.\n",
    "\n",
    "        aligned_signals = {}\n",
    "        full_length = len(self.eed.relative_time_ms['relative_time_ms'])\n",
    "        stim_start_index = 500\n",
    "        stim_end_index = 1000\n",
    "\n",
    "        # Determine the global maximum voltage to set a unified y-axis for the stimulation signals\n",
    "        max_voltage = np.max(stim_voltages)\n",
    "\n",
    "        # Extract and pad the signal for the optogenetic stimulation\n",
    "        for label in stim_labels:\n",
    "            signal = stim_voltages  # Directly use the 1D array as there's only one type of stimulation\n",
    "\n",
    "            pre_padding = np.zeros(stim_start_index)\n",
    "            post_padding = np.zeros(full_length - stim_end_index)\n",
    "            padded_signal = np.concatenate((pre_padding, signal, post_padding))\n",
    "\n",
    "            aligned_signals[label] = padded_signal[:1500]\n",
    "\n",
    "        return aligned_signals, max_voltage\n",
    "\n",
    "    def plot_psth_with_stim(self, group1, group2, stim_label, ax=None, max_voltage=None, cell_type=None, is_single_unit=None, stim_responsivity=None, \n",
    "                            time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided,\n",
    "        and overlays the stimulation signal on a secondary y-axis.\n",
    "\n",
    "        The other parameters function as in the original plot_psth_comparison method.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add debug statement to check input parameters and data\n",
    "        print(f\"Plotting PSTH for groups: {group1}, {group2} with stimulation: {stim_label}\")\n",
    "        \n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            created_fig = True\n",
    "        else:\n",
    "            created_fig = False\n",
    "\n",
    "        # Plot PSTH data as before\n",
    "        traces_df = self.plot_psth_comparison(group1, group2, stim_label, ax=ax, cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                                              stim_responsivity=stim_responsivity, time_range=time_range, plot_mode=plot_mode, \n",
    "                                              smoothing_window=smoothing_window)\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        stim_signals, _ = self.extract_stim_signals()\n",
    "        if stim_label in stim_signals:\n",
    "            stim_signal = stim_signals[stim_label]  # Assuming full signal is handled correctly\n",
    "            stim_signal = stim_signal[:1500]\n",
    "            if time_range:\n",
    "                stim_signal = stim_signal[time_mask]  # Apply time mask if time range is specified\n",
    "            ax2.plot(time_array, stim_signal, 'r-', label='Stim Signal', alpha=0.5)\n",
    "            ax2.set_ylabel('Stimulation Voltage (uV)', color='r')\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylim(0, max_voltage)  # Set consistent y-axis scale\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if created_fig:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return traces_df\n",
    "    \n",
    "    def plot_psth_with_stim_opto(self, group1, group2, stim_label, ax=None, max_voltage=None, cell_type=None, is_single_unit=None, stim_responsivity=None, time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided,\n",
    "        and overlays the stimulation signal on a secondary y-axis.\n",
    "\n",
    "        The other parameters function as in the original plot_psth_comparison method.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            created_fig = True\n",
    "        else:\n",
    "            created_fig = False\n",
    "\n",
    "        # Plot PSTH data as before\n",
    "        self.plot_psth_comparison(group1, group2, stim_label, ax=ax, \n",
    "                                  cell_type=cell_type, is_single_unit=is_single_unit, stim_responsivity=stim_responsivity, \n",
    "                                  time_range=time_range, plot_mode=plot_mode, smoothing_window=smoothing_window)\n",
    "        ax2 = ax.twinx()\n",
    "        stim_signals, _ = self.extract_stim_signals_opto()\n",
    "        if stim_label in stim_signals:\n",
    "            stim_signal = stim_signals[stim_label]  # Assuming full signal is handled correctly\n",
    "            stim_signal = stim_signal[:1500]\n",
    "            if time_range:\n",
    "                stim_signal = stim_signal[time_mask]  # Apply time mask if time range is specified\n",
    "            ax2.plot(time_array, stim_signal, 'r-', label='Stim Signal', alpha=0.5)\n",
    "            ax2.set_ylabel('LED Stimulation Voltage (uV)', color='r')\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylim(0, max_voltage)  # Set consistent y-axis scale\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if created_fig:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def plot_all_stimulations_with_stim(self, group1, group2, cell_type=None, is_single_unit=None, stim_responsivity=None, time_range=None, plot_mode='mean', smoothing_window=None, directory=None, file_name=None, firstspike_latency=None):\n",
    "        \n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        fig, axs = plt.subplots(1, len(stim_labels), figsize=(20, 5), sharey=True)\n",
    "\n",
    "        # Retrieve stimulation signals to determine the maximum voltage\n",
    "        stim_signals, max_voltage = self.extract_stim_signals()\n",
    "\n",
    "        all_traces_df = pd.DataFrame()\n",
    "        \n",
    "        for i, stim_label in enumerate(stim_labels):\n",
    "            traces_df = self.plot_psth_with_stim(group1, group2, stim_label, axs[i], max_voltage=max_voltage, \n",
    "                                                 cell_type=cell_type, is_single_unit=is_single_unit, stim_responsivity=stim_responsivity, \n",
    "                                                 time_range=time_range, plot_mode=plot_mode, \n",
    "                                                 smoothing_window=smoothing_window, firstspike_latency=firstspike_latency)\n",
    "            \n",
    "            traces_df['Stimulation'] = stim_label  # Ensure the correct stim label is assigned\n",
    "            \n",
    "            all_traces_df = pd.concat([all_traces_df, traces_df], ignore_index=True)\n",
    "            axs[i].set_title(stim_label)\n",
    "            \n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        fig.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return all_traces_df\n",
    "    \n",
    "    def calculate_basic_stats(self, group1, group2, stim_label=None, baseline_range=(-100, 1), stim_range=(0, 50), cell_type=None, is_single_unit=None, stim_responsivity=None, smoothing_window=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Calculates basic statistics for baseline and stimulation windows for two groups across all or a specific stimulation type and stores the detailed data used for these calculations.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            stim_label (str or None): Specific stimulation label to analyze, or None to analyze all.\n",
    "            baseline_range (tuple): Time range for baseline window.\n",
    "            stim_range (tuple): Time range for stimulation window.\n",
    "            cell_type (str, optional): Filter for specific cell types.\n",
    "            is_single_unit (float, optional): Filter for single units.\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "            smoothing_window (int, optional): Size of the smoothing window.\n",
    "            modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing means and standard deviations for the specified windows.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        if stim_label:\n",
    "            stim_labels = [stim_label]  # If specific stim_label is provided, use that\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for stim in stim_labels:\n",
    "            df1, df2 = self.compare_groups(group1, group2, stim, cell_type, is_single_unit, \n",
    "                                           stim_responsivity, modulation_label)\n",
    "\n",
    "            # Function to calculate stats and add to DataFrame\n",
    "            def calculate_and_store_stats(df, time_range, window_label):\n",
    "                if df is None or df.empty:\n",
    "                    return df\n",
    "\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "                \n",
    "                # Apply the time mask and store the result in a new column safely\n",
    "                df.loc[:, 'masked_data'] = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "\n",
    "                if smoothing_window:\n",
    "                    window = np.ones(smoothing_window) / smoothing_window\n",
    "                    df.loc[:, 'smoothed_data'] = df['masked_data'].apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "                else:\n",
    "                    df.loc[:, 'smoothed_data'] = df['masked_data']\n",
    "\n",
    "                # Calculate mean and standard deviation and store them in new columns\n",
    "                df.loc[:, f'mean_{window_label}'] = df['smoothed_data'].apply(np.max)\n",
    "                df.loc[:, f'std_{window_label}'] = df['smoothed_data'].apply(np.std)\n",
    "\n",
    "                return df\n",
    "\n",
    "\n",
    "            # Calculate and store stats in the DataFrames\n",
    "            df1 = calculate_and_store_stats(df1, baseline_range, 'baseline')\n",
    "            df1 = calculate_and_store_stats(df1, stim_range, 'stimulation')\n",
    "            df2 = calculate_and_store_stats(df2, baseline_range, 'baseline')\n",
    "            df2 = calculate_and_store_stats(df2, stim_range, 'stimulation')\n",
    "\n",
    "            self.detailed_dataframes[(group1, stim)] = df1\n",
    "            self.detailed_dataframes[(group2, stim)] = df2\n",
    "\n",
    "            # Extract group-level results to return\n",
    "            baseline_stats_group1 = {'mean': df1['mean_baseline'].max(), 'std': df1['std_baseline'].max()}\n",
    "            stim_stats_group1 = {'mean': df1['mean_stimulation'].max(), 'std': df1['std_stimulation'].max()}\n",
    "            baseline_stats_group2 = {'mean': df2['mean_baseline'].max(), 'std': df2['std_baseline'].max()}\n",
    "            stim_stats_group2 = {'mean': df2['mean_stimulation'].max(), 'std': df2['std_stimulation'].max()}\n",
    "\n",
    "            results.extend([\n",
    "                {'Group': group1, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group1['mean'], 'Std': baseline_stats_group1['std']},\n",
    "                {'Group': group1, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group1['mean'], 'Std': stim_stats_group1['std']},\n",
    "                {'Group': group2, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group2['mean'], 'Std': baseline_stats_group2['std']},\n",
    "                {'Group': group2, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group2['mean'], 'Std': stim_stats_group2['std']}\n",
    "            ])\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def calculate_basic_stats_laminar(self, group1, group2, stim_label=None, baseline_range=(-100, 1), stim_range=(0, 50), cell_type=None, is_single_unit=None, stim_responsivity=None, smoothing_window=None, modulation_label=None):\n",
    "            \"\"\"\n",
    "            Calculates basic statistics for baseline and stimulation windows for two groups across all or a specific stimulation type and stores the detailed data used for these calculations.\n",
    "\n",
    "            Args:\n",
    "                group1 (str): First group name.\n",
    "                group2 (str): Second group name.\n",
    "                stim_label (str or None): Specific stimulation label to analyze, or None to analyze all.\n",
    "                baseline_range (tuple): Time range for baseline window.\n",
    "                stim_range (tuple): Time range for stimulation window.\n",
    "                cell_type (str, optional): Filter for specific cell types.\n",
    "                is_single_unit (float, optional): Filter for single units.\n",
    "                stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "                smoothing_window (int, optional): Size of the smoothing window.\n",
    "                modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "            Returns:\n",
    "                pandas.DataFrame: A DataFrame containing means and standard deviations for the specified windows.\n",
    "            \"\"\"\n",
    "            stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "            if stim_label:\n",
    "                stim_labels = [stim_label]  # If specific stim_label is provided, use that\n",
    "\n",
    "            results = []\n",
    "\n",
    "            for stim in stim_labels:\n",
    "                df1, df2 = self.compare_groups(group1, group2, stim, cell_type, is_single_unit, \n",
    "                                            stim_responsivity, modulation_label)\n",
    "\n",
    "                # Function to calculate stats and add to DataFrame\n",
    "                def calculate_and_store_stats(df, time_range, window_label):\n",
    "                    if df is None or df.empty:\n",
    "                        return df\n",
    "\n",
    "                    time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                    time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "                    \n",
    "                    # Apply the time mask and store the result in a new column safely\n",
    "                    df.loc[:, 'masked_data'] = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "\n",
    "                    if smoothing_window:\n",
    "                        window = np.ones(smoothing_window) / smoothing_window\n",
    "                        df.loc[:, 'smoothed_data'] = df['masked_data'].apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "                    else:\n",
    "                        df.loc[:, 'smoothed_data'] = df['masked_data']\n",
    "\n",
    "                    # Calculate mean and standard deviation and store them in new columns\n",
    "                    df.loc[:, f'mean_{window_label}'] = df['smoothed_data'].apply(np.max)\n",
    "                    df.loc[:, f'std_{window_label}'] = df['smoothed_data'].apply(np.std)\n",
    "\n",
    "                    return df\n",
    "\n",
    "\n",
    "                # Calculate and store stats in the DataFrames\n",
    "                df1 = calculate_and_store_stats(df1, baseline_range, 'baseline')\n",
    "                df1 = calculate_and_store_stats(df1, stim_range, 'stimulation')\n",
    "                df2 = calculate_and_store_stats(df2, baseline_range, 'baseline')\n",
    "                df2 = calculate_and_store_stats(df2, stim_range, 'stimulation')\n",
    "\n",
    "                self.detailed_dataframes[(group1, stim)] = df1\n",
    "                self.detailed_dataframes[(group2, stim)] = df2\n",
    "\n",
    "                # Extract group-level results to return\n",
    "                baseline_stats_group1 = {'mean': df1['mean_baseline'].max(), 'std': df1['std_baseline'].max()}\n",
    "                stim_stats_group1 = {'mean': df1['mean_stimulation'].max(), 'std': df1['std_stimulation'].max()}\n",
    "                baseline_stats_group2 = {'mean': df2['mean_baseline'].max(), 'std': df2['std_baseline'].max()}\n",
    "                stim_stats_group2 = {'mean': df2['mean_stimulation'].max(), 'std': df2['std_stimulation'].max()}\n",
    "\n",
    "                results.extend([\n",
    "                    {'Group': group1, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group1['mean'], 'Std': baseline_stats_group1['std']},\n",
    "                    {'Group': group1, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group1['mean'], 'Std': stim_stats_group1['std']},\n",
    "                    {'Group': group2, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group2['mean'], 'Std': baseline_stats_group2['std']},\n",
    "                    {'Group': group2, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group2['mean'], 'Std': stim_stats_group2['std']}\n",
    "                ])\n",
    "\n",
    "            return pd.DataFrame(results)    \n",
    "    \n",
    "    \n",
    "    def prepare_for_boxplot(self):\n",
    "        \"\"\"\n",
    "        Organizes data into a DataFrame suitable for plotting boxplots. It extracts the 'mean_stimulation'\n",
    "        values from detailed DataFrames, including labels for stimulation type and group.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with columns for 'mean_stimulation', 'Stimulation', and 'Group'.\n",
    "        \"\"\"\n",
    "        boxplot_data = []\n",
    "\n",
    "        # Iterate over each stored DataFrame key (group and stimulation)\n",
    "        for (group, stim), df in self.detailed_dataframes.items():\n",
    "            if not df.empty:  # Corrected check for an empty DataFrame\n",
    "                # Extract 'mean_stimulation' and corresponding labels\n",
    "                for index, row in df.iterrows():\n",
    "                    boxplot_data.append({\n",
    "                        'mean_stimulation': row['mean_stimulation'],\n",
    "                        'Stimulation': stim,\n",
    "                        'Group': group, \n",
    "                        'recordingname': row['recordingname'],\n",
    "                        'ModulationIndex': row['ModulationIndex'], \n",
    "                        'cid': row['cid'], \n",
    "                        'Cell_Type': row['Cell_Type'], \n",
    "                        'Template_Channel': row['Template_Channel'],\n",
    "                        'LaminarLabel': row['LaminarLabel'],\n",
    "                        'IsSingleUnit': row['IsSingleUnit'],\n",
    "                        'TroughToPeak_duration': row['TroughToPeak_duration'],\n",
    "                        'PeakToPeak_ratio': row['PeakToPeak_ratio'],\n",
    "                        'peak1_normalized_amplitude': row['peak1_normalized_amplitude'],\n",
    "                        'Peak1ToTrough_ratio': row['Peak1ToTrough_ratio'],\n",
    "                        'Peak2ToTrough_ratio': row['Peak2ToTrough_ratio'],\n",
    "                        'SpikeHalfWidth': row['SpikeHalfWidth'],\n",
    "                        'MeanFR_baseline': row['MeanFR_baseline'],\n",
    "                        'StimResponsivity': row['StimResponsivity'],\n",
    "                        'UnNormalized_Template_Waveform': row['UnNormalized_Template_Waveform'],\n",
    "                        'Normalized_Template_Waveform': row['Normalized_Template_Waveform'],\n",
    "                        'FirstSpikeLatency': row['FirstSpikeLatency'],\n",
    "                        'FirstSpikeLatency_Reliability': row['FirstSpikeLatency_Reliability'],\n",
    "                        'FanoFactor_baseline': row['FanoFactor_baseline'],\n",
    "                        'FanoFactor_stim': row['FanoFactor_stim']\n",
    "                    })\n",
    "\n",
    "        # Convert list of data to DataFrame\n",
    "        boxplot_df = pd.DataFrame(boxplot_data)\n",
    "\n",
    "        return boxplot_df\n",
    "    \n",
    "    def plot_box_and_strip(self, df, groups=None, stimulations=None, show_outliers=True, hue_order=None, directory=None, file_name=None, ylim=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations, with color adjustments made directly in the plotting calls.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame containing the data to plot made with prepare_for_boxplot.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "     \n",
    "        # Filter by specified groups and stimulations\n",
    "        boxplot_df = df\n",
    "        # Filter by specified groups and stimulations\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "        if modulation_label:\n",
    "            boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='Stimulation', y='mean_stimulation', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=boxplot_df, x='Stimulation', y='mean_stimulation', hue='Group', **stripplot_kwargs)\n",
    "        \n",
    "        #control the upper and lower limits of the y-axis\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        return boxplot_df # Return the DataFrame for further analysis\n",
    "\n",
    "    def plot_box_and_strip_with_controls(self, df, groups=None, stimulations=None, show_outliers=True, show_scatter=True, hue_order=None, directory=None, file_name=None, ylim=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and optional stripplots for specified groups and stimulations, with control over outlier and scatter display.\n",
    "        \n",
    "        Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the data to plot made with prepare_for_boxplot.\n",
    "        groups (list of str, optional): List of groups to include in the plot.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "        show_outliers (bool, optional): Whether to show outliers in both boxplot and stripplot. Default is True.\n",
    "        show_scatter (bool, optional): Whether to show the scatter plot (stripplot). Default is True.\n",
    "        hue_order (list, optional): Order of the hue levels.\n",
    "        directory (str, optional): Directory to save the plot.\n",
    "        file_name (str, optional): File name to save the plot.\n",
    "        ylim (tuple, optional): Y-axis limits.\n",
    "        modulation_label (str, optional): Label for modulation index filtering.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Filter by specified groups and stimulations\n",
    "        boxplot_df = df\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "        if modulation_label:\n",
    "            boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        # Calculate outliers\n",
    "        Q1 = boxplot_df.groupby(['Stimulation', 'Group'])['mean_stimulation'].transform('quantile', 0.25)\n",
    "        Q3 = boxplot_df.groupby(['Stimulation', 'Group'])['mean_stimulation'].transform('quantile', 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (boxplot_df['mean_stimulation'] < Q1 - 1.5 * IQR) | (boxplot_df['mean_stimulation'] > Q3 + 1.5 * IQR)\n",
    "        \n",
    "        # Separate outliers and non-outliers\n",
    "        outliers = boxplot_df[outlier_mask]\n",
    "        non_outliers = boxplot_df[~outlier_mask]\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='Stimulation', y='mean_stimulation', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot if show_scatter is True\n",
    "        if show_scatter:\n",
    "            # Add stripplot for non-outliers\n",
    "            sns.stripplot(data=non_outliers, x='Stimulation', y='mean_stimulation', hue='Group', **stripplot_kwargs)\n",
    "\n",
    "            # Add stripplot for outliers if show_outliers is True\n",
    "            if show_outliers:\n",
    "                sns.stripplot(data=outliers, x='Stimulation', y='mean_stimulation', hue='Group', \n",
    "                            **{**stripplot_kwargs, 'marker': 'D', 'size': 8})\n",
    "\n",
    "        # Control the upper and lower limits of the y-axis\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "        plt.show()\n",
    "\n",
    "        return boxplot_df  # Return the DataFrame for further analysis\n",
    "\n",
    "    def plot_mean_and_sem_lineplot(self, df, groups=None, stimulations=None, show_outliers=True, hue_order=None, directory=None, file_name=None, ylim=None, modulation_label=None, file_format='svg'):\n",
    "        \"\"\"\n",
    "        Plots mean and SEM bars for specified groups and stimulations, with color adjustments.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame containing the data to plot made with prepare_for_boxplot \n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "            directory (str, optional): Directory to save the plot.\n",
    "            file_name (str, optional): File name to save the plot.\n",
    "            ylim (tuple, optional): Y-axis limits for the plot.\n",
    "            modulation_label (str, optional): Label to filter modulation.\n",
    "            firstspike_latency (bool, optional): Whether to filter by first spike latency.\n",
    "            file_format (str, optional): Format to save the plot ('svg' or 'png'). Default is 'svg'.\n",
    "        \"\"\"\n",
    "        # Validate the file format\n",
    "        if file_format not in ['svg', 'png']:\n",
    "            raise ValueError(\"file_format must be either 'svg' or 'png'\")\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Define the order of stimulations\n",
    "        stim_order = ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        boxplot_df = df\n",
    "\n",
    "        # Filter by specified groups and stimulations\n",
    "        #if groups:\n",
    "        #    boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        #if stimulations:\n",
    "        #    boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "        #if modulation_label:\n",
    "        #    boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        # Calculate mean and SEM\n",
    "        summary_df = boxplot_df.groupby(['Stimulation', 'Group']).agg(\n",
    "            mean_stimulation=('mean_stimulation', 'mean'),\n",
    "            sem_stimulation=('mean_stimulation', 'sem')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Ensure the stimulations are ordered correctly\n",
    "        summary_df['Stimulation'] = pd.Categorical(summary_df['Stimulation'], categories=stim_order, ordered=True)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = sns.lineplot(data=summary_df, x='Stimulation', y='mean_stimulation', hue='Group', palette=group_colors, hue_order=hue_order, markers=True, style='Group', markersize=10, linewidth=2.5)\n",
    "\n",
    "        # Add error bars\n",
    "        for group in summary_df['Group'].unique():\n",
    "            group_data = summary_df[summary_df['Group'] == group]\n",
    "            plt.errorbar(group_data['Stimulation'], group_data['mean_stimulation'], yerr=group_data['sem_stimulation'], fmt='o', c=group_colors[group], capsize=5, elinewidth=2, markersize=10)\n",
    "\n",
    "        # Control the upper and lower limits of the y-axis\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG or PNG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.{file_format}')\n",
    "        plt.savefig(file_path, format=file_format, transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def plot_rasters_for_cid(self, groupname, recordingname, cid, time_window=None):\n",
    "        \"\"\"\n",
    "        Plots raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                           Default is None, which uses the full range.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']  # Full time array from -500 to 999 ms\n",
    "\n",
    "        # Determine the indices for slicing time_array based on the specified or default time window\n",
    "        if time_window is not None:\n",
    "            start_index = np.searchsorted(time_array, time_window[0])\n",
    "            end_index = np.searchsorted(time_array, time_window[1], side='right')\n",
    "        else:\n",
    "            start_index, end_index = 0, len(time_array)  # Use full range if no window is specified\n",
    "        \n",
    "        # Prepare the figure\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "        fig.suptitle(f'Raster plots for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        # Iterate through each stimulation type\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            ax = axes[i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                # Filter data for specific cid\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values #a numpy.ndarry of shape(1,) and size 1 which contains a list of arrays\n",
    "                \n",
    "                # Check if there is any data to plot\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # extract the binary spike trains with numpy.ndarray of shape (n_trials, n_time_points)\n",
    "                    # Plot each trial's spikes within the time window\n",
    "                    for trial_index, trial in enumerate(spike_trains): #enumerate over the trials which are the rows of the spike_trains. trial is a numpy.ndarray of shape (n_time_points,) and trial_index is the index of the trial\n",
    "                        spikes = np.where(trial == 1)[0]  # Get indices where spikes occur\n",
    "                        spikes = spikes[(spikes >= start_index) & (spikes < end_index)]  # Filter spikes by time window\n",
    "                        spike_times = time_array[spikes]  # Convert indices to times\n",
    "                        ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, colors='black')\n",
    "\n",
    "                    ax.set_xlim(time_window[0], time_window[1]) if time_window else ax.set_xlim(time_array[start_index], time_array[end_index-1])\n",
    "                    ax.set_title(f'Stimulation: {stim}')\n",
    "                    ax.set_xlabel('Time (ms)')\n",
    "                    if i == 0:\n",
    "                        ax.set_ylabel('Trial')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rasters_for_cid_2msbins(self, groupname, recordingname, cid, time_window=None):\n",
    "        \"\"\"\n",
    "        Plots raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']  # Full time array from -500 to 999 ms\n",
    "\n",
    "        # Create a new time array with 2ms bins\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, 2)\n",
    "\n",
    "        # Determine the indices for slicing time_array based on the specified or default time window\n",
    "        if time_window is not None:\n",
    "            start_index = np.searchsorted(new_time_array, time_window[0])\n",
    "            end_index = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "        else:\n",
    "            start_index, end_index = 0, len(new_time_array)  # Use full range if no window is specified\n",
    "        \n",
    "        # Prepare the figure\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "        fig.suptitle(f'Raster plots for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        # Iterate through each stimulation type\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            ax = axes[i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                # Filter data for specific cid\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values #a numpy.ndarry of shape(1,) and size 1 which contains a list of arrays\n",
    "                \n",
    "                # Check if there is any data to plot\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # extract the binary spike trains with numpy.ndarray of shape (n_trials, n_time_points)\n",
    "                    \n",
    "                    # Bin the spike trains into 2ms intervals\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], 2), axis=1)\n",
    "                    \n",
    "                    # Plot each trial's spikes within the time window\n",
    "                    for trial_index, trial in enumerate(binned_spike_trains): #enumerate over the trials which are the rows of the spike_trains. trial is a numpy.ndarray of shape (n_time_points,) and trial_index is the index of the trial\n",
    "                        spikes = np.where(trial > 0)[0]  # Get indices where spikes occur (considering 2ms bins)\n",
    "                        spikes = spikes[(spikes >= start_index) & (spikes < end_index)]  # Filter spikes by time window\n",
    "                        spike_times = new_time_array[spikes]  # Convert indices to times\n",
    "                        ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, colors='black')\n",
    "\n",
    "                    ax.set_xlim(time_window[0], time_window[1]) if time_window else ax.set_xlim(new_time_array[start_index], new_time_array[end_index-1])\n",
    "                    ax.set_title(f'Stimulation: {stim}')\n",
    "                    ax.set_xlabel('Time (ms)')\n",
    "                    if i == 0:\n",
    "                        ax.set_ylabel('Trial')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_rasters_for_cid_changebinsize(self, groupname, recordingname, cid, time_window=None, bin_size=1, filter_empty_trials=False, recording_dir=None):\n",
    "        \"\"\"\n",
    "        Plots raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds. Default is 1ms.\n",
    "            filter_empty_trials (bool, optional): If True, only plots trials with at least one spike. Default is False.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']  # Full time array from -500 to 999 ms\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Determine the indices for slicing time_array based on the specified or default time window\n",
    "        if time_window is not None:\n",
    "            start_index = np.searchsorted(new_time_array, time_window[0])\n",
    "            end_index = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "        else:\n",
    "            start_index, end_index = 0, len(new_time_array)  # Use full range if no window is specified\n",
    "        \n",
    "        # Prepare the figure\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "        fig.suptitle(f'Raster plots for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        # Iterate through each stimulation type\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            ax = axes[i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                # Filter data for specific cid\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "                \n",
    "                # Check if there is any data to plot\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # extract the binary spike trains\n",
    "                    \n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "                    \n",
    "                    # Optionally filter out trials without any spikes\n",
    "                    if filter_empty_trials:\n",
    "                        trial_spike_counts = binned_spike_trains.sum(axis=1)\n",
    "                        binned_spike_trains = binned_spike_trains[trial_spike_counts > 0]\n",
    "                    \n",
    "                    # Plot each trial's spikes within the time window\n",
    "                    for trial_index, trial in enumerate(binned_spike_trains):\n",
    "                        spikes = np.where(trial > 0)[0]  # Get indices where spikes occur (considering bin size)\n",
    "                        spikes = spikes[(spikes >= start_index) & (spikes < end_index)]  # Filter spikes by time window\n",
    "                        spike_times = new_time_array[spikes]  # Convert indices to times\n",
    "                        ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, colors='black')\n",
    "\n",
    "                    ax.set_xlim(time_window[0], time_window[1]) if time_window else ax.set_xlim(new_time_array[start_index], new_time_array[end_index-1])\n",
    "                    ax.set_title(f'Stimulation: {stim}')\n",
    "                    ax.set_xlabel('Time (ms)')\n",
    "                    if i == 0:\n",
    "                        ax.set_ylabel('Trial')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        save_path = os.path.join(recording_dir, f'raster_{groupname}_{recordingname}_{cid}.svg')\n",
    "        fig.savefig(save_path, format='svg', transparent=True)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Raster plots have been saved successfully.\")\n",
    "\n",
    "\n",
    "    def smooth_data(self, data, window_size=5):\n",
    "        \"\"\"Smooths data using a moving average filter with a specified window size.\"\"\"\n",
    "        window = np.ones(int(window_size)) / float(window_size)\n",
    "        return np.convolve(data, window, 'same')\n",
    "    \n",
    "    def plot_combined_psth_and_raster(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, show=False):\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial == 1)[0] for trial in spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(time_array), range=(0, len(time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    # Plot Smoothed PSTH\n",
    "                    smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=1, align='edge', color='skyblue')\n",
    "                    smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                    smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                    # Plot Raw PSTH\n",
    "                    raw_psth_ax.bar(displayed_time_array, displayed_counts, width=1, align='edge', color='gray')\n",
    "                    raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                    raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                    # Plot Raster\n",
    "                    for trial_index, trial in enumerate(spike_trains):\n",
    "                        spike_times = time_array[np.where(trial == 1)]\n",
    "                        raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1)\n",
    "                    raster_ax.set_title(f'{stim} Raster')\n",
    "                    raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        return fig\n",
    "\n",
    "    def plot_combined_psth_and_raster_changebinsize(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, bin_size=1, show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window, smoothing window, and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the PSTH and raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        max_smoothed_counts = 0\n",
    "        max_raw_counts = 0\n",
    "\n",
    "        all_smoothed_counts = []\n",
    "        all_raw_counts = []\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    all_smoothed_counts.append(smoothed_counts)\n",
    "                    all_raw_counts.append(displayed_counts)\n",
    "\n",
    "                    max_smoothed_counts = max(max_smoothed_counts, np.max(smoothed_counts))\n",
    "                    max_raw_counts = max(max_raw_counts, np.max(displayed_counts))\n",
    "\n",
    "        # Set y-limits based on the maximum counts across all stimulations\n",
    "        ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "        ylimit_raw = 1.1 * max_raw_counts\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    # Plot Smoothed PSTH\n",
    "                    smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=bin_size, align='edge', color='skyblue')\n",
    "                    smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                    smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "                    smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "                    # Plot Raw PSTH\n",
    "                    raw_psth_ax.bar(displayed_time_array, displayed_counts, width=bin_size, align='edge', color='gray')\n",
    "                    raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                    raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "                    raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "                    # Plot Raster\n",
    "                    for trial_index, trial in enumerate(binned_spike_trains):\n",
    "                        spike_times = new_time_array[np.where(trial > 0)]\n",
    "                        spike_times = spike_times[(spike_times >= displayed_time_array[0]) & (spike_times < displayed_time_array[-1])]\n",
    "                        raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "                    raster_ax.set_title(f'{stim} Raster')\n",
    "                    raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "    def extract_psth_and_raster_data(self, groupname, recordingname, cid, time_window=None, bin_size=1, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Extracts the PSTH and raster data for a specific cid within a specific group and recording across all stimulations.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to extract the PSTH and raster data.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the extracted data. The structure of the dictionary is as follows:\n",
    "            \n",
    "            {\n",
    "                'groupname': str,  # The name of the group\n",
    "                'recordingname': str,  # The name of the recording\n",
    "                'cid': str,  # The cell ID\n",
    "                'stim_labels': list of str,  # List of stimulation labels\n",
    "                'time_array': numpy.ndarray,  # Array of time points based on the specified bin size (1D array, length depends on bin size)\n",
    "                'data': {  # Nested dictionary containing data for each stimulation\n",
    "                    'stim_label_1': {  # Replace 'stim_label_1' with actual stimulation labels\n",
    "                        'spike_trains': numpy.ndarray,  # Binned spike trains (2D array, shape: [number of trials, length of new_time_array])\n",
    "                        'raw_counts': numpy.ndarray,  # Raw spike counts for each time bin (1D array, length depends on time window and bin size)\n",
    "                        'smoothed_counts': numpy.ndarray,  # Smoothed spike counts for each time bin (1D array, same length as raw_counts)\n",
    "                        'time_array': numpy.ndarray  # Array of time points for the specified time window (1D array, length depends on time window and bin size)\n",
    "                    },\n",
    "                    'stim_label_2': { ... },  # Repeat for each stimulation label\n",
    "                    ...\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        extracted_data = {\n",
    "            'groupname': groupname,\n",
    "            'recordingname': recordingname,\n",
    "            'cid': cid,\n",
    "            'stim_labels': stim_labels,\n",
    "            'time_array': new_time_array,\n",
    "            'data': {}\n",
    "        }\n",
    "\n",
    "        for stim in stim_labels:\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "                    \n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    extracted_data['data'][stim] = {\n",
    "                        'spike_trains': binned_spike_trains,\n",
    "                        'raw_counts': displayed_counts,\n",
    "                        'smoothed_counts': smoothed_counts,\n",
    "                        'time_array': displayed_time_array\n",
    "                    }\n",
    "\n",
    "        return extracted_data\n",
    "\n",
    "    def plot_psth_and_raster(self, extracted_data, show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots using the extracted data.\n",
    "\n",
    "        Args:\n",
    "            extracted_data (dict): The data extracted by the extract_psth_and_raster_data method. \n",
    "                                The structure of the dictionary should be:\n",
    "                                {\n",
    "                                    'groupname': str,\n",
    "                                    'recordingname': str,\n",
    "                                    'cid': str,\n",
    "                                    'stim_labels': list of str,\n",
    "                                    'time_array': numpy.ndarray,\n",
    "                                    'data': {\n",
    "                                        'stim_label_1': {\n",
    "                                            'spike_trains': numpy.ndarray,\n",
    "                                            'raw_counts': numpy.ndarray,\n",
    "                                            'smoothed_counts': numpy.ndarray,\n",
    "                                            'time_array': numpy.ndarray\n",
    "                                        },\n",
    "                                        ...\n",
    "                                    }\n",
    "                                }\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {extracted_data[\"cid\"]}, '\n",
    "                    f'Group: {extracted_data[\"groupname\"]}, Recording: {extracted_data[\"recordingname\"]}')\n",
    "\n",
    "        max_smoothed_counts = 0\n",
    "        max_raw_counts = 0\n",
    "\n",
    "        for stim, data in extracted_data['data'].items():\n",
    "            max_smoothed_counts = max(max_smoothed_counts, np.max(data['smoothed_counts']))\n",
    "            max_raw_counts = max(max_raw_counts, np.max(data['raw_counts']))\n",
    "\n",
    "        ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "        ylimit_raw = 1.1 * max_raw_counts\n",
    "\n",
    "        for i, stim in enumerate(extracted_data['stim_labels']):\n",
    "            if stim in extracted_data['data']:\n",
    "                smoothed_psth_ax = axes[0, i]\n",
    "                raw_psth_ax = axes[1, i]\n",
    "                raster_ax = axes[2, i]\n",
    "                data = extracted_data['data'][stim]\n",
    "\n",
    "                # Plot Smoothed PSTH\n",
    "                smoothed_psth_ax.bar(data['time_array'], data['smoothed_counts'], width=1, align='edge', color='skyblue')\n",
    "                smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                smoothed_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "                # Plot Raw PSTH\n",
    "                raw_psth_ax.bar(data['time_array'], data['raw_counts'], width=1, align='edge', color='gray')\n",
    "                raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                raw_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "                # Plot Raster\n",
    "                for trial_index, trial in enumerate(data['spike_trains']):\n",
    "                    spike_times = extracted_data['time_array'][np.where(trial > 0)]\n",
    "                    spike_times = spike_times[(spike_times >= data['time_array'][0]) & (spike_times < data['time_array'][-1])]\n",
    "                    raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "                raster_ax.set_title(f'{stim} Raster')\n",
    "                raster_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot if requested\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def process_and_store_psth_raster_data(self, is_single_unit=None, cell_type=None, stim_responsivity=None, \n",
    "                                        time_window=None, bin_size=1, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid', extracts the PSTH and raster data,\n",
    "        and stores the results in a structured format.\n",
    "\n",
    "        Args:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary containing the extracted data for each group, recording, and cid.\n",
    "                The structure of the dictionary is as follows:\n",
    "                {\n",
    "                    'groupname_1': {\n",
    "                        'recordingname_1': {\n",
    "                            'cid_1': extracted_data,\n",
    "                            'cid_2': extracted_data,\n",
    "                            ...\n",
    "                        },\n",
    "                        'recordingname_2': { ... },\n",
    "                        ...\n",
    "                    },\n",
    "                    'groupname_2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        stored_data = {}\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "                is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "\n",
    "            # Extract the PSTH and raster data\n",
    "            extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                            time_window=time_window, bin_size=bin_size, smoothing_window=smoothing_window)\n",
    "\n",
    "            # Add the parameters used to the extracted data\n",
    "            extracted_data['bin_size'] = bin_size\n",
    "            extracted_data['smoothing_window'] = smoothing_window\n",
    "            extracted_data['time_window'] = time_window\n",
    "\n",
    "            # Ensure the nested dictionary structure is created\n",
    "            if groupname not in stored_data:\n",
    "                stored_data[groupname] = {}\n",
    "            if recordingname not in stored_data[groupname]:\n",
    "                stored_data[groupname][recordingname] = {}\n",
    "            \n",
    "            stored_data[groupname][recordingname][cid] = extracted_data\n",
    "\n",
    "        return stored_data\n",
    "\n",
    "    def plot_all_psth_and_raster(self, stored_data, output_dir='/Volumes/MannySSD/figures', folder_name='PSTH_and_Rasters_RS_SUA',file_format='svg', show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots using the stored data for each unique cid per recording per group.\n",
    "\n",
    "        Args:\n",
    "            stored_data (dict): The nested dictionary containing the extracted data.\n",
    "                                The structure of the dictionary should be:\n",
    "                                {\n",
    "                                    'groupname_1': {\n",
    "                                        'recordingname_1': {\n",
    "                                            'cid_1': {\n",
    "                                                'data': extracted_data,\n",
    "                                                'bin_size': int,\n",
    "                                                'smoothing_window': int,\n",
    "                                                'time_window': tuple,\n",
    "                                            },\n",
    "                                            'cid_2': { ... },\n",
    "                                            ...\n",
    "                                        },\n",
    "                                        'recordingname_2': { ... },\n",
    "                                        ...\n",
    "                                    },\n",
    "                                    'groupname_2': { ... },\n",
    "                                    ...\n",
    "                                }\n",
    "            output_dir (str): The root directory where the figures will be saved.\n",
    "            folder_name (str): The folder name to be created within the output directory.\n",
    "            file_format (str, optional): The format in which to save the figures. Default is 'svg'.\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        # Validate the file format\n",
    "        if file_format not in ['svg', 'png']:\n",
    "            raise ValueError(\"file_format must be either 'svg' or 'png'\")\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Create the folder within the output directory\n",
    "        save_dir = os.path.join(output_dir, folder_name)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        for groupname, recordings in stored_data.items():\n",
    "            group_dir = os.path.join(save_dir, groupname)\n",
    "            if not os.path.exists(group_dir):\n",
    "                os.makedirs(group_dir)\n",
    "\n",
    "            for recordingname, cids in recordings.items():\n",
    "                recording_dir = os.path.join(group_dir, recordingname)\n",
    "                if not os.path.exists(recording_dir):\n",
    "                    os.makedirs(recording_dir)\n",
    "\n",
    "                for cid, extracted_data in cids.items():\n",
    "                    fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "                    fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, '\n",
    "                                f'Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "                    max_smoothed_counts = 0\n",
    "                    max_raw_counts = 0\n",
    "\n",
    "                    for stim, data in extracted_data['data'].items():\n",
    "                        max_smoothed_counts = max(max_smoothed_counts, np.max(data['smoothed_counts']))\n",
    "                        max_raw_counts = max(max_raw_counts, np.max(data['raw_counts']))\n",
    "\n",
    "                    ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "                    ylimit_raw = 1.1 * max_raw_counts\n",
    "                    bin_size = extracted_data['bin_size']\n",
    "\n",
    "                    for i, stim in enumerate(extracted_data['stim_labels']):\n",
    "                        if stim in extracted_data['data']:\n",
    "                            smoothed_psth_ax = axes[0, i]\n",
    "                            raw_psth_ax = axes[1, i]\n",
    "                            raster_ax = axes[2, i]\n",
    "                            data = extracted_data['data'][stim]\n",
    "\n",
    "                            # Plot Smoothed PSTH\n",
    "                            smoothed_psth_ax.bar(data['time_array'], data['smoothed_counts'], width=bin_size, align='edge', color='skyblue')\n",
    "                            smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                            smoothed_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                            smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "                            # Plot Raw PSTH\n",
    "                            raw_psth_ax.bar(data['time_array'], data['raw_counts'], width=bin_size, align='edge', color='gray')\n",
    "                            raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                            raw_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                            raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "                            # Plot Raster\n",
    "                            for trial_index, trial in enumerate(data['spike_trains']):\n",
    "                                spike_times = extracted_data['time_array'][np.where(trial > 0)]\n",
    "                                spike_times = spike_times[(spike_times >= data['time_array'][0]) & (spike_times < data['time_array'][-1])]\n",
    "                                raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "                            raster_ax.set_title(f'{stim} Raster')\n",
    "                            raster_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    if show:\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        save_path = os.path.join(recording_dir, f'psth_raster_{cid}.{file_format}')\n",
    "                        fig.savefig(save_path, transparent=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_baseline_and_response(self, groupname, recordingname, cid, baseline_window=(-500, 0), response_window=(0, 50)):\n",
    "        \"\"\"\n",
    "        Calculates the mean baseline firing rate, mean response magnitude, and z-scored magnitude for each stimulation.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID.\n",
    "            baseline_window (tuple): The time window for calculating the baseline firing rate. Default is (-500, 0).\n",
    "            response_window (tuple): The time window for calculating the response magnitude. Default is (0, 50).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the calculated values for each stimulation.\n",
    "                {\n",
    "                    'stim1': {\n",
    "                        'mean_baseline_firing_rate': float,\n",
    "                        'mean_response_magnitude': float,\n",
    "                        'z_scored_magnitude': float\n",
    "                    },\n",
    "                    'stim2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        # Get the full time array from the eed object\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        \n",
    "        # Set a fixed bin size of 1 ms for accurate baseline and response calculation\n",
    "        bin_size = 1\n",
    "        \n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Create masks for baseline and response windows based on the new time array\n",
    "        baseline_indices = (new_time_array >= baseline_window[0]) & (new_time_array < baseline_window[1])\n",
    "        response_indices = (new_time_array >= response_window[0]) & (new_time_array < response_window[1])\n",
    "\n",
    "        # Dictionary to store results for each stimulation\n",
    "        stim_results = {}\n",
    "\n",
    "        # Extract PSTH and raster data with the fixed bin size and no smoothing\n",
    "        extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                        time_window=None, bin_size=bin_size, smoothing_window=1)\n",
    "\n",
    "        # Iterate over each stimulation label in the extracted data\n",
    "        for stim, data in extracted_data['data'].items():\n",
    "            # Get the spike trains for the current stimulation\n",
    "            spike_trains = data['spike_trains']\n",
    "            \n",
    "            # Calculate the total number of spikes in the baseline and response windows for each trial\n",
    "            baseline_spikes = spike_trains[:, baseline_indices].sum(axis=1)\n",
    "            response_spikes = spike_trains[:, response_indices].sum(axis=1)\n",
    "            \n",
    "            # Calculate the mean number of spikes during the baseline and response windows\n",
    "            mean_baseline_firing_rate = baseline_spikes.mean()\n",
    "            mean_response_magnitude = response_spikes.mean() - mean_baseline_firing_rate\n",
    "            \n",
    "            # Calculate the standard deviation of the baseline firing rates across trials\n",
    "            baseline_sd = np.std(baseline_spikes)\n",
    "            \n",
    "            # Calculate the z-scored magnitude by dividing the mean response magnitude by the baseline standard deviation\n",
    "            z_scored_magnitude = mean_response_magnitude / baseline_sd if baseline_sd > 0 else 0\n",
    "\n",
    "            # Store the results for the current stimulation in the dictionary\n",
    "            stim_results[stim] = {\n",
    "                'mean_baseline_firing_rate': mean_baseline_firing_rate,\n",
    "                'mean_response_magnitude': mean_response_magnitude,\n",
    "                'z_scored_magnitude': z_scored_magnitude\n",
    "            }\n",
    "\n",
    "        # Return the dictionary containing results for all stimulations\n",
    "        return stim_results\n",
    "\n",
    "    def calculate_population_psth(self, groupname, recordingname, cid, baseline_window=(-500, 0), bin_size=1):\n",
    "        \"\"\"\n",
    "        Calculates the population PSTH by subtracting the overall prestimulus baseline spike rate from every bin value.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID.\n",
    "            baseline_window (tuple): The time window for calculating the baseline firing rate. Default is (-500, 0).\n",
    "            bin_size (int): The size of the bins in milliseconds for the PSTH. Default is 1ms.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the adjusted PSTH values for each stimulation.\n",
    "                {\n",
    "                    'stim1': adjusted_psth_array,\n",
    "                    'stim2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        # Get the full time array from the eed object\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        \n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Create a mask for the baseline window based on the new time array\n",
    "        baseline_indices = (new_time_array >= baseline_window[0]) & (new_time_array < baseline_window[1])\n",
    "\n",
    "        # Dictionary to store adjusted PSTH results for each stimulation\n",
    "        stim_results = {}\n",
    "\n",
    "        # Extract PSTH and raster data with the specified bin size and no smoothing\n",
    "        extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                        time_window=None, bin_size=bin_size, smoothing_window=1)\n",
    "\n",
    "        # Iterate over each stimulation label in the extracted data\n",
    "        for stim, data in extracted_data['data'].items():\n",
    "            # Get the spike trains for the current stimulation\n",
    "            spike_trains = data['spike_trains']\n",
    "            \n",
    "            # Calculate the total number of spikes in the baseline window for each trial\n",
    "            baseline_spikes = spike_trains[:, baseline_indices].sum(axis=1)\n",
    "            \n",
    "            # Calculate the mean baseline firing rate\n",
    "            mean_baseline_firing_rate = baseline_spikes.mean()\n",
    "\n",
    "            # Calculate the PSTH by summing spike counts across trials for each time bin\n",
    "            psth_counts = np.sum(spike_trains, axis=0)\n",
    "            \n",
    "            # Subtract the mean baseline firing rate from each bin value in the PSTH\n",
    "            adjusted_psth = psth_counts - mean_baseline_firing_rate\n",
    "            \n",
    "            # Store the adjusted PSTH values for the current stimulation\n",
    "            stim_results[stim] = adjusted_psth\n",
    "\n",
    "        return stim_results\n",
    "\n",
    "    def process_and_store_psth_raster_data_comprehensive(self, is_single_unit=None, cell_type=None, stim_responsivity=None, \n",
    "                                                        time_window=None, bin_size=1, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid', extracts the PSTH and raster data,\n",
    "        calculates the baseline and response magnitudes, and stores the results in a structured format.\n",
    "\n",
    "        Args:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary containing the extracted data for each group, recording, and cid.\n",
    "                The structure of the dictionary is as follows:\n",
    "                {\n",
    "                    'groupname_1': {\n",
    "                        'recordingname_1': {\n",
    "                            'cid_1': {\n",
    "                                'data': extracted_data,\n",
    "                                'bin_size': int,\n",
    "                                'smoothing_window': int,\n",
    "                                'time_window': tuple,\n",
    "                                'stimulation_results': dict,\n",
    "                                'population_psth': dict\n",
    "                            },\n",
    "                            'cid_2': { ... },\n",
    "                            ...\n",
    "                        },\n",
    "                        'recordingname_2': { ... },\n",
    "                        ...\n",
    "                    },\n",
    "                    'groupname_2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        stored_data = {}\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "                is_single_unit=is_single_unit, \n",
    "                cell_type=cell_type, \n",
    "                stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "\n",
    "            # Extract the PSTH and raster data\n",
    "            extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                            time_window=time_window, bin_size=bin_size, smoothing_window=smoothing_window)\n",
    "\n",
    "            \n",
    "            # Calculate baseline and response magnitudes using consistent binning and smoothing parameters store the \n",
    "            # mean_baseline_firing_rate, mean_response_magnitude, z_scored_magnitude per stimulation recoreded at each cid \n",
    "            # for 1ms bins and smoothing window of 1 \n",
    "            # Calculate baseline and response magnitudes for each stimulation\n",
    "            stim_results = self.calculate_baseline_and_response(groupname=groupname, recordingname=recordingname, cid=cid)\n",
    "\n",
    "            # Calculate population PSTH for each stimulation\n",
    "            population_psth = self.calculate_population_psth(groupname=groupname, recordingname=recordingname, cid=cid)\n",
    "\n",
    "            # Add the parameters and calculated values to the extracted data\n",
    "            extracted_data['bin_size'] = bin_size\n",
    "            extracted_data['smoothing_window'] = smoothing_window\n",
    "            extracted_data['time_window'] = time_window\n",
    "            extracted_data['stimulation_results'] = stim_results\n",
    "            extracted_data['population_psth'] = population_psth\n",
    "\n",
    "            # Ensure the nested dictionary structure is created\n",
    "            if groupname not in stored_data:\n",
    "                stored_data[groupname] = {}\n",
    "            if recordingname not in stored_data[groupname]:\n",
    "                stored_data[groupname][recordingname] = {}\n",
    "            \n",
    "            stored_data[groupname][recordingname][cid] = extracted_data\n",
    "\n",
    "        return stored_data\n",
    "\n",
    "    def plot_population_psth(self, stored_data, groupname, time_window=(-500, 999), bin_size=1):\n",
    "        \"\"\"\n",
    "        Plots the mean and SEM population PSTHs using the formatted data for a specified group.\n",
    "\n",
    "        Args:\n",
    "            stored_data (dict): The nested dictionary containing the extracted data.\n",
    "            groupname (str): The name of the group.\n",
    "            time_window (tuple): The window of time to plot, within the range -500 to 999 ms. Default is (-500, 999).\n",
    "            bin_size (int): The size of the bins in milliseconds for the PSTH. Default is 1ms.\n",
    "        \"\"\"\n",
    "        # Initialize a dictionary to store population PSTHs for each stimulation\n",
    "        population_psths = {}\n",
    "\n",
    "        # Iterate through recordings and cell IDs within the specified group\n",
    "        for recordingname, cids in stored_data[groupname].items():\n",
    "            for cid, extracted_data in cids.items():\n",
    "                for stim, psth in extracted_data['population_psth'].items():\n",
    "                    if stim not in population_psths:\n",
    "                        population_psths[stim] = []\n",
    "                    population_psths[stim].append(psth)\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Create a mask for the time window to plot\n",
    "        time_mask = (new_time_array >= time_window[0]) & (new_time_array <= time_window[1])\n",
    "        plot_time_array = new_time_array[time_mask]\n",
    "\n",
    "        # Initialize variables to track the global y-axis limits\n",
    "        global_min, global_max = np.inf, -np.inf\n",
    "\n",
    "        # Calculate the mean and SEM for each stimulation to find global y-axis limits\n",
    "        mean_sems = {}\n",
    "        for stim, psth_list in population_psths.items():\n",
    "            if len(psth_list) > 0:\n",
    "                psth_array = np.array(psth_list)\n",
    "                mean_psth = np.mean(psth_array, axis=0)\n",
    "                sem_psth = np.std(psth_array, axis=0) / np.sqrt(psth_array.shape[0])\n",
    "                mean_sems[stim] = (mean_psth, sem_psth)\n",
    "                global_min = min(global_min, (mean_psth - sem_psth).min())\n",
    "                global_max = max(global_max, (mean_psth + sem_psth).max())\n",
    "\n",
    "        # Plot the mean and SEM for each stimulation in a 1x4 grid\n",
    "        fig, axes = plt.subplots(1, len(population_psths), figsize=(20, 5), sharey=True)\n",
    "        if len(population_psths) == 1:\n",
    "            axes = [axes]  # Ensure axes is always iterable\n",
    "\n",
    "        for i, (stim, (mean_psth, sem_psth)) in enumerate(mean_sems.items()):\n",
    "            axes[i].plot(plot_time_array, mean_psth[time_mask], label=f'{stim} Mean PSTH')\n",
    "            axes[i].fill_between(plot_time_array, mean_psth[time_mask] - sem_psth[time_mask], \n",
    "                                mean_psth[time_mask] + sem_psth[time_mask], alpha=0.5, label=f'{stim} SEM')\n",
    "            axes[i].set_title(f'Mean and SEM Population PSTH for {stim}')\n",
    "            axes[i].set_xlabel('Time (ms)')\n",
    "            axes[i].set_ylabel('Firing Rate (spikes/bin)')\n",
    "            axes[i].set_ylim(global_min, global_max)\n",
    "            axes[i].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_psth_and_raster_data(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, bin_size=1):\n",
    "        \"\"\"\n",
    "        Computes PSTH and raster data for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window, smoothing window, and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to compute the PSTH and raster data.\n",
    "            time_window (tuple, optional): The window of time to consider, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the group name, recording name, cid, and the smoothed PSTH counts, raw PSTH counts, \n",
    "                raster spike times, and time arrays for each stimulation.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        results = {\n",
    "            'groupname': groupname,\n",
    "            'recordingname': recordingname,\n",
    "            'cid': cid,\n",
    "            'data': {\n",
    "                stim: {\n",
    "                    'smoothed_counts': None,\n",
    "                    'raw_counts': None,\n",
    "                    'raster_spike_times': [],\n",
    "                    'time_array': None\n",
    "                }\n",
    "                for stim in stim_labels\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    # Collect raster spike times\n",
    "                    raster_spike_times = []\n",
    "                    for trial in binned_spike_trains:\n",
    "                        spike_times = new_time_array[np.where(trial > 0)]\n",
    "                        spike_times = spike_times[(spike_times >= displayed_time_array[0]) & (spike_times < displayed_time_array[-1])]\n",
    "                        raster_spike_times.append(spike_times)\n",
    "\n",
    "                    # Store results\n",
    "                    results['data'][stim]['smoothed_counts'] = smoothed_counts\n",
    "                    results['data'][stim]['raw_counts'] = displayed_counts\n",
    "                    results['data'][stim]['raster_spike_times'] = raster_spike_times\n",
    "                    results['data'][stim]['time_array'] = displayed_time_array\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_from_computed_data(self, computed_data, bin_size=1, show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots using precomputed data.\n",
    "\n",
    "        Args:\n",
    "            computed_data (dict): The precomputed data containing the group name, recording name, cid, \n",
    "                                smoothed PSTH counts, raw PSTH counts, raster spike times, and time arrays for each stimulation.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        groupname = computed_data['groupname']\n",
    "        recordingname = computed_data['recordingname']\n",
    "        cid = computed_data['cid']\n",
    "        data = computed_data['data']\n",
    "\n",
    "        stim_labels = list(data.keys())\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        max_smoothed_counts = max(np.max(data[stim]['smoothed_counts']) for stim in stim_labels)\n",
    "        max_raw_counts = max(np.max(data[stim]['raw_counts']) for stim in stim_labels)\n",
    "\n",
    "        ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "        ylimit_raw = 1.1 * max_raw_counts\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "\n",
    "            smoothed_counts = data[stim]['smoothed_counts']\n",
    "            raw_counts = data[stim]['raw_counts']\n",
    "            raster_spike_times = data[stim]['raster_spike_times']\n",
    "            displayed_time_array = data[stim]['time_array']\n",
    "\n",
    "            # Plot Smoothed PSTH\n",
    "            smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=bin_size, align='edge', color='skyblue')\n",
    "            smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "            smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "            smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "            # Plot Raw PSTH\n",
    "            raw_psth_ax.bar(displayed_time_array, raw_counts, width=bin_size, align='edge', color='gray')\n",
    "            raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "            raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "            raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "            # Plot Raster\n",
    "            for trial_index, spike_times in enumerate(raster_spike_times):\n",
    "                raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "            raster_ax.set_title(f'{stim} Raster')\n",
    "            raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def compute_population_psth_and_raster_data(self, groupname, cell_type, is_single_unit=1.0, time_window=None, smoothing_window=5, bin_size=1):\n",
    "        \"\"\"\n",
    "        Computes population-level PSTH data for a specific cell type within a specific group across all recordings,\n",
    "        with an optional custom time window, smoothing window, and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            cell_type (str): The cell type for which to compute the population PSTH data.\n",
    "            is_single_unit (float, optional): Filter for single units. Default is 1.0.\n",
    "            time_window (tuple, optional): The window of time to consider, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH. Default is 1ms.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the group name, cell type, and the smoothed PSTH counts, raw PSTH counts, \n",
    "                and time arrays for each stimulation.\n",
    "        \"\"\"\n",
    "        # Filter data to get the relevant cells\n",
    "        channel = self.get_filtered_data('basic_metrics', is_single_unit=is_single_unit, cell_type=cell_type, groupname=groupname)\n",
    "\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        results = {\n",
    "            'groupname': groupname,\n",
    "            'cell_type': cell_type,\n",
    "            'data': {\n",
    "                stim: {\n",
    "                    'smoothed_counts': np.zeros(len(new_time_array)),\n",
    "                    'raw_counts': np.zeros(len(new_time_array)),\n",
    "                    'time_array': new_time_array\n",
    "                }\n",
    "                for stim in stim_labels\n",
    "            }\n",
    "        }\n",
    "\n",
    "        num_cells = 0\n",
    "\n",
    "        # Loop through each cell and recording\n",
    "        for _, row in channel.iterrows():\n",
    "            recordingname = row['recordingname']\n",
    "            cid = row['cid']\n",
    "            cell_data = self.compute_psth_and_raster_data(groupname, recordingname, cid, time_window, smoothing_window, bin_size)\n",
    "            cell_data = cell_data['data']\n",
    "\n",
    "            for stim in stim_labels:\n",
    "                if cell_data[stim]['smoothed_counts'] is not None:\n",
    "                    results['data'][stim]['smoothed_counts'] += cell_data[stim]['smoothed_counts']\n",
    "                if cell_data[stim]['raw_counts'] is not None:\n",
    "                    results['data'][stim]['raw_counts'] += cell_data[stim]['raw_counts']\n",
    "\n",
    "            num_cells += 1\n",
    "\n",
    "        # Compute the mean by dividing by the number of cells\n",
    "        if num_cells > 0:\n",
    "            for stim in stim_labels:\n",
    "                results['data'][stim]['smoothed_counts'] /= num_cells\n",
    "                results['data'][stim]['raw_counts'] /= num_cells\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def plot_combined_psth_and_raster_normalized(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, normalize=True, show=False):\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))\n",
    "        fig.suptitle(f'Normalized Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = [train for train in spike_trains[0] if np.any(train == 1)]  # Filter to include only trials with spikes\n",
    "\n",
    "                    if not spike_trains:\n",
    "                        for ax in axes[:, i]:  # Iterate over each subplot in the column\n",
    "                            ax.text(0.5, 0.5, 'No spikes detected', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "                            ax.set_axis_off()  # Optionally turn off the axis if no spikes are detected\n",
    "\n",
    "                    else:\n",
    "                        all_spikes = np.concatenate([np.where(train == 1)[0] for train in spike_trains])\n",
    "                        counts, _ = np.histogram(all_spikes, bins=len(time_array), range=(0, len(time_array)))\n",
    "                        number_of_trials = len(spike_trains)\n",
    "\n",
    "                        # Apply normalization if enabled\n",
    "                        normalized_counts = counts / number_of_trials if normalize else counts\n",
    "\n",
    "                        if time_window:\n",
    "                            start_idx = np.searchsorted(time_array, time_window[0])\n",
    "                            end_idx = np.searchsorted(time_array, time_window[1], side='right')\n",
    "                            displayed_time_array = time_array[start_idx:end_idx]\n",
    "                            displayed_counts = normalized_counts[start_idx:end_idx]\n",
    "                        else:\n",
    "                            displayed_time_array = time_array\n",
    "                            displayed_counts = normalized_counts\n",
    "\n",
    "                        # Smoothed PSTH\n",
    "                        smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "                        smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=1, align='edge', color='skyblue')\n",
    "                        smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                        # Raw PSTH\n",
    "                        raw_psth_ax.bar(displayed_time_array, displayed_counts, width=1, align='edge', color='gray')\n",
    "                        raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                        # Raster Plot\n",
    "                        for trial_index, trial in enumerate(spike_trains):\n",
    "                            spike_times = time_array[np.where(trial == 1)]\n",
    "                            raster_ax.eventplot(spike_times, lineoffsets=trial_index + 0.5, linelengths=1)\n",
    "                        raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if show: \n",
    "            plt.show()\n",
    "        return fig \n",
    "\n",
    "\n",
    "\n",
    "    def save_plots_for_all_units(self):\n",
    "        df = self.get_filtered_data('basic_metrics', is_single_unit=None, cell_type=None, stim_responsivity=None)\n",
    "        base_dir = '/Volumes/MannySSD/output_data_for_emx/saved_rasterplots'\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in df.groupby(['groupname', 'recordingname', 'cid']):\n",
    "            cid_dir = os.path.join(base_dir, groupname, recordingname, str(cid))\n",
    "            os.makedirs(cid_dir, exist_ok=True)\n",
    "\n",
    "            # Generate and save the standard plot\n",
    "            fig_standard = self.plot_combined_psth_and_raster(groupname, recordingname, cid, time_window=None, smoothing_window=8)\n",
    "            if fig_standard:  # Check if a figure was returned\n",
    "                fig_standard.savefig(os.path.join(cid_dir, 'standard_plot.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "                plt.close(fig_standard)\n",
    "\n",
    "            # Generate and save the normalized plot\n",
    "            fig_normalized = self.plot_combined_psth_and_raster_normalized(groupname, recordingname, cid, time_window=None, smoothing_window=8, normalize=True)\n",
    "            if fig_normalized:  # Check if a figure was returned\n",
    "                fig_normalized.savefig(os.path.join(cid_dir, 'normalized_plot.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "                plt.close(fig_normalized)\n",
    "\n",
    "    def calculate_psth_data(self, groupname, recordingname, cid, time_window=None, normalize=True, filter_empty_trials=True):\n",
    "        \"\"\"\n",
    "        Overview\n",
    "\n",
    "        The calculate_psth_data method is part of the DataFrameManager class.\n",
    "        It calculates peristimulus time histograms (PSTH) for specified neural recordings and cells. \n",
    "        The method handles multiple stimuli and can be configured to include all trials or only those with spikes, based on user preference. \n",
    "        This method does not plot the data but returns a structured dictionary containing counts and time arrays for further analysis or visualization.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "        groupname (str): The name of the experimental group. This parameter specifies which group the data should be retrieved from.\n",
    "        recordingname (str): The name of the recording session. It determines from which recording to fetch the spike data.\n",
    "        cid (str): Cell identifier. This specifies the neuron for which the PSTH will be calculated.\n",
    "        time_window (tuple of int, optional): A tuple representing the start and end of the time window in milliseconds within which to calculate the PSTH. If None, the entire span of time_array will be used. Default is None.\n",
    "        normalize (bool): Determines whether the spike counts should be normalized. If True, the counts are divided by the number of trials used in the calculation. The exact denominator depends on the filter_empty_trials parameter. Default is True.\n",
    "        filter_empty_trials (bool): Controls whether trials without any spikes should be included in the calculation. If True, only trials with spikes are considered. This affects both the counts and the normalization process. Default is True.\n",
    "        \n",
    "        Returns\n",
    "\n",
    "        psth_results (dict): A dictionary where keys are stimulus labels and values are dictionaries containing:\n",
    "        'counts' (numpy.array): An array of spike counts per time bin. This array is normalized if normalize is set to True.\n",
    "        'time_array' (numpy.array): The time points corresponding to the bins in 'counts'.\n",
    "        \n",
    "        Detailed Description\n",
    "\n",
    "        The method begins by accessing stimulus labels and relative time arrays from the trialTagsLabels and relative_time_ms attributes of the eed (Electrophysiology Extraction Data) instance, respectively. \n",
    "        It then iterates over each stimulus, fetching spike train data from the appropriate DataFrame. The spike trains are filtered based on the filter_empty_trials setting.\n",
    "\n",
    "        Spike trains are processed into histograms using numpy.histogram, with bin edges aligned to time_array. \n",
    "        If a time_window is provided, the method restricts the calculation to the specified window, adjusting both the counts and time arrays accordingly.\n",
    "\n",
    "        Normalization, when enabled, divides the counts by the number of trials that were used in the histogram calculation. \n",
    "        The choice of denominator is influenced by the filter_empty_trials flag: if True, it uses the count of trials with spikes; if False, it uses the total number of trials.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        psth_results = {}\n",
    "\n",
    "        for stim in stim_labels:\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid) \n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "                \n",
    "                # Get the laminar label for this cid\n",
    "                if condition.sum() == 0:\n",
    "                    print(f\"No data found for Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "                    return None\n",
    "                \n",
    "                #get the laminar label for this cid\n",
    "                label = df.loc[condition, 'LaminarLabel'].values[0]\n",
    "                \n",
    "                \n",
    "                \n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # Get the array of spike trains\n",
    "                    if filter_empty_trials:\n",
    "                        spike_trains = [train for train in spike_trains if np.any(train == 1)]  # Only include trials with spikes\n",
    "\n",
    "                    if len(spike_trains) == 0:  # Checking if the list is empty\n",
    "                        psth_results[stim] = {'counts': [], 'time_array': [], 'LaminarLabel': label}\n",
    "                    else:\n",
    "                        all_spikes = np.concatenate([np.where(train == 1)[0] for train in spike_trains])\n",
    "                        counts, _ = np.histogram(all_spikes, bins=len(time_array), range=(0, len(time_array)))\n",
    "                        \n",
    "                        if normalize:\n",
    "                            number_of_trials = len(spike_trains) if filter_empty_trials else len(spike_trains[0])\n",
    "                            normalized_counts = counts / number_of_trials\n",
    "                        else:\n",
    "                            normalized_counts = counts\n",
    "\n",
    "                        if time_window:\n",
    "                            start_idx = np.searchsorted(time_array, time_window[0])\n",
    "                            end_idx = np.searchsorted(time_array, time_window[1], side='right')\n",
    "                            displayed_time_array = time_array[start_idx:end_idx]\n",
    "                            displayed_counts = normalized_counts[start_idx:end_idx]\n",
    "                        else:\n",
    "                            displayed_time_array = time_array\n",
    "                            displayed_counts = normalized_counts\n",
    "\n",
    "                        psth_results[stim] = {'counts': displayed_counts, 'time_array': displayed_time_array, 'LaminarLabel': label}\n",
    "\n",
    "        return psth_results\n",
    "\n",
    "    def iterate_by_group_recording_cid(self, is_single_unit=None, cell_type=None, stim_responsivity=None):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid' after applying specified filters.\n",
    "        \n",
    "        This generator function fetches data using predefined criteria, applies additional filters, and yields\n",
    "        each subset of the data grouped by 'groupname', 'recordingname', and 'cid'. It is designed to facilitate\n",
    "        the processing of large datasets by providing chunks of data one group at a time, which can be particularly\n",
    "        useful for processing steps that do not require the complete dataset to be held in memory.\n",
    "\n",
    "        Parameters:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "        \n",
    "        Yields:\n",
    "            tuple: A tuple containing (groupname, recordingname, cid) as a tuple and the corresponding group DataFrame. \n",
    "                This allows for further processing of the data specific to each group.\n",
    "\n",
    "        Example:\n",
    "            for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(cell_type='pyramidal'):\n",
    "                # Process each group DataFrame here\n",
    "                print(groupname, recordingname, cid, len(group_df))\n",
    "        \"\"\"\n",
    "        # Fetch the data with potential filters applied\n",
    "        df = self.get_filtered_data('basic_metrics', is_single_unit=is_single_unit, \n",
    "                                    cell_type=cell_type, stim_responsivity=stim_responsivity)\n",
    "        \n",
    "        # Group the data by 'groupname', 'recordingname', and 'cid'\n",
    "        for (groupname, recordingname, cid), group_df in df.groupby(['groupname', 'recordingname', 'cid']):\n",
    "            yield (groupname, recordingname, cid), group_df\n",
    "\n",
    "    def calculate_all_psths(self, is_single_unit=None, cell_type=None, stim_responsivity=None, time_window=None, normalize=True, filter_empty_trials=True):\n",
    "        \"\"\"\n",
    "        Calculates PSTH for all cells across specified groups and recordings, aggregates the results into two DataFrames.\n",
    "        \n",
    "        One DataFrame contains the means of 'counts' over the entire duration for each stimulation. The other DataFrame contains detailed PSTH data for \n",
    "        each stimulation including both 'counts' and 'time_array', which can be selectively extracted based on a 'time_window'.\n",
    "\n",
    "        Parameters:\n",
    "            is_single_unit (bool, optional): Filter based on whether cells are considered single units.\n",
    "            cell_type (str, optional): Filter based on cell type.\n",
    "            stim_responsivity (bool, optional): Filter based on cell responsiveness to stimuli.\n",
    "            time_window (tuple of int, optional): Specific time window to calculate PSTH data over. If None, the full range is used.\n",
    "            normalize (bool): Whether to normalize the spike counts.\n",
    "            filter_empty_trials (bool): Whether to include only trials with spikes.\n",
    "\n",
    "        Returns:\n",
    "            tuple of DataFrames: (mean_df, detailed_df)\n",
    "                mean_df: DataFrame with columns 'mean_stimulation', 'Stimulation', 'Group', 'cid', 'recordingname'.\n",
    "                detailed_df: DataFrame with columns 'Stimulation', 'cid', 'groupname', 'recordingname', 'counts', 'time_array'.\n",
    "        \"\"\"\n",
    "        mean_results = []\n",
    "        detailed_results = []\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # Get stimulation labels from the eed object\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "            is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "            \n",
    "            psth_results = self.calculate_psth_data(groupname, recordingname, cid, time_window=time_window, \n",
    "                                                    normalize=normalize, filter_empty_trials=filter_empty_trials)\n",
    "            if psth_results is None:\n",
    "                continue\n",
    "            \n",
    "            # Process each stimulation's PSTH results\n",
    "            for stim_label in stim_labels:\n",
    "                \n",
    "                if stim_label in psth_results:\n",
    "                    counts = psth_results[stim_label]['counts']\n",
    "                    time_array = psth_results[stim_label]['time_array']\n",
    "                    \n",
    "                     # Get 'LaminarLabel' with a default value if it's not found\n",
    "                    laminar_label = psth_results[stim_label].get('LaminarLabel', 'DefaultLabel') #added 'LaminarLabel' if it's not found, it will be assigned 'DefaultLabel'\n",
    "                    #print(stim_label, laminar_label) #use print to check if the laminar label is being correctly assigned to the stim_label\n",
    "\n",
    "                    # Ensure counts is an array and has elements before calculating mean\n",
    "                    if isinstance(counts, np.ndarray) and counts.size > 0:\n",
    "                        mean_stimulation = np.mean(counts)\n",
    "                        #instead find the max value of the counts array and store it in mean_stimulation\n",
    "                        #mean_stimulation = np.max(counts)\n",
    "                    else:\n",
    "                        mean_stimulation = np.nan\n",
    "\n",
    "                    # Append to results for mean DataFrame\n",
    "                    mean_results.append({\n",
    "                        'Group': groupname,\n",
    "                        'Stimulation': stim_label,\n",
    "                        'mean_stimulation': mean_stimulation,\n",
    "                        'cid': cid,\n",
    "                        'LaminarLabel': laminar_label, #added 'LaminarLabel\n",
    "                        'recordingname': recordingname,\n",
    "                 \n",
    "                    })\n",
    "\n",
    "                    # Append to results for detailed DataFrame\n",
    "                    detailed_results.append({\n",
    "                        'Stimulation': stim_label,\n",
    "                        'cid': cid,\n",
    "                        'LaminarLabel': laminar_label, #added 'LaminarLabel\n",
    "                        'groupname': groupname,\n",
    "                        'recordingname': recordingname,\n",
    "                        'counts': counts,\n",
    "                        'time_array': time_array, \n",
    "                    })\n",
    "\n",
    "        # Create DataFrames from the aggregated results\n",
    "        mean_df = pd.DataFrame(mean_results)\n",
    "        detailed_df = pd.DataFrame(detailed_results)\n",
    "        return mean_df, detailed_df, psth_results\n",
    "    \n",
    "    def plot_mean_stimulation_box_and_strip(self, mean_df, groups=None, stimulations=None, show_outliers=True, hue_order=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations using the mean stimulation data.\n",
    "\n",
    "        Args:\n",
    "            mean_df (DataFrame): The DataFrame containing 'mean_stimulation' along with 'Group' and 'Stimulation' columns.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers in the stripplot.\n",
    "            hue_order (list, optional): Order of the hue levels, specifying how groups should be ordered in the plot.\n",
    "\n",
    "        Usage:\n",
    "            mean_df, _ = whisker_df_manager.calculate_all_psths(...)\n",
    "            whisker_df_manager.plot_mean_stimulation_box_and_strip(mean_df, ...)\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Filter by specified groups and stimulations if provided\n",
    "        if groups:\n",
    "            mean_df = mean_df[mean_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            mean_df = mean_df[mean_df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', **stripplot_kwargs)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        plt.show()\n",
    "               \n",
    "    def remove_outliers_by_stimulation(self, df, value_column='mean_stimulation'):\n",
    "        \"\"\"\n",
    "        Removes outliers within each stimulation group based on the interquartile range (IQR).\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the data, expected to have a 'Stimulation' column.\n",
    "            value_column (str): The name of the column from which outliers will be removed.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: DataFrame with outliers removed within each stimulation group.\n",
    "        \"\"\"\n",
    "        # Create an empty DataFrame to store results after removing outliers\n",
    "        channel = pd.DataFrame()\n",
    "        \n",
    "        # Process each stimulation group separately\n",
    "        for stim in df['Stimulation'].unique():\n",
    "            sub_df = df[df['Stimulation'] == stim]\n",
    "            Q1 = sub_df[value_column].quantile(0.25)\n",
    "            Q3 = sub_df[value_column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Filter out outliers and append the results to the filtered DataFrame\n",
    "            result_df = sub_df[(sub_df[value_column] >= lower_bound) & (sub_df[value_column] <= upper_bound)]\n",
    "            filtered_df = pd.concat([filtered_df, result_df], ignore_index=True)\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_mean_stimulation_box_and_strip2(self, mean_df, groups=None, stimulations=None, hue_order=None, remove_outliers_option=False, ylim=None, laminar_labels=None, directory=None, file_name=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations using the mean stimulation data, with an option to exclude outliers.\n",
    "        Args:\n",
    "            mean_df (DataFrame): The DataFrame containing 'mean_stimulation' along with 'Group' and 'Stimulation' columns.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            hue_order (list, optional): Order of the hue levels, specifying how groups should be ordered in the plot.\n",
    "            remove_outliers_option (bool, optional): If True, outliers will be removed before plotting.\n",
    "            laminar_labels (list of str, optional): List of laminar labels to include in the plot. '['IG, 'SG','L4']\n",
    "        \"\"\"\n",
    "        # Filter the DataFrame by groups and stimulations if provided\n",
    "        print(\"Initial mean_df shape:\", mean_df.shape)\n",
    "        print(\"mean_df columns:\", mean_df.columns)\n",
    "        \n",
    "        if groups:\n",
    "            mean_df = mean_df[mean_df['Group'].isin(groups)]\n",
    "            print(\"After filtering by groups, mean_df shape:\", mean_df.shape)\n",
    "        if stimulations:\n",
    "            mean_df = mean_df[mean_df['Stimulation'].isin(stimulations)]\n",
    "            print(\"After filtering by stimulations, mean_df shape:\", mean_df.shape)\n",
    "        if laminar_labels:\n",
    "            mean_df = mean_df[mean_df['LaminarLabel'].isin(laminar_labels)]\n",
    "            print(\"After filtering by laminar labels, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            mean_df = self.remove_outliers_by_stimulation(mean_df, 'mean_stimulation')\n",
    "            print(\"After removing outliers, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if mean_df.empty:\n",
    "            raise ValueError(\"The DataFrame is empty after filtering. Check the input parameters and data.\")\n",
    "        \n",
    "        stats_df = self.run_group_comparisons(mean_df, group_column='Group', value_column='mean_stimulation', stim_column='Stimulation')\n",
    "        \n",
    "        # Convert the mean_stimulation column to Hz by multiplying by 1000\n",
    "        mean_df['mean_stimulation'] = mean_df['mean_stimulation']*1000\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', \n",
    "                        boxprops={'edgecolor': 'k', 'linewidth': 2},\n",
    "                        medianprops={'color': 'k', 'linewidth': 2},\n",
    "                        whiskerprops={'color': 'k', 'linewidth': 2},\n",
    "                        capprops={'linewidth': 0},  # Hide the caps\n",
    "                        showfliers=not remove_outliers_option,\n",
    "                        palette={'No_CTZ': '#797979', 'CTZ': '#5a00c2'},\n",
    "                        hue_order=hue_order,\n",
    "                        width=0.75)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', \n",
    "                    linewidth=0.6, size=6, alpha=0.7, jitter=True, dodge=True, marker='o',\n",
    "                    palette={'No_CTZ': '#79797933', 'CTZ': '#5a00c233'},  # Lighter colors\n",
    "                    hue_order=hue_order)\n",
    "        \n",
    "        # Adjust opacity of the box fill\n",
    "        for patch in ax.artists:\n",
    "            r, g, b, a = patch.get_facecolor()\n",
    "            patch.set_facecolor((r, g, b, .1))  # Set the fill alpha to 10%\n",
    "        \n",
    "        # Removing other components of the boxplot\n",
    "        for line in ax.lines:\n",
    "            # Only keep the median and the whiskers visible:\n",
    "            # The boxplot consists of 6 lines per group: 0 and 1 are the box, 2 is the median,\n",
    "            # 3 and 4 are the whiskers, 5 is the cap. This might slightly vary depending on Seaborn's implementation.\n",
    "            if line.get_linestyle() != '-':  # Only preserve the median line\n",
    "                line.set_linewidth(0)\n",
    "        \n",
    "        # Set the y-axis limits if specified\n",
    "        if ylim:\n",
    "            plt.ylim(ylim) #ylim is a tuple (min, max)\n",
    "        \n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return stats_df, mean_df\n",
    "\n",
    "    def plot_mean_sem_line2(self, mean_df, groups=None, stimulations=None, hue_order=None, remove_outliers_option=False, ylim=None, laminar_labels=None, directory=None, file_name=None):\n",
    "        \"\"\"\n",
    "        Plots mean and SEM line plots for specified groups and stimulations using the mean stimulation data, with an option to exclude outliers.\n",
    "\n",
    "        Args:\n",
    "            mean_df (DataFrame): The DataFrame containing 'mean_stimulation' along with 'Group' and 'Stimulation' columns.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            hue_order (list, optional): Order of the hue levels, specifying how groups should be ordered in the plot.\n",
    "            remove_outliers_option (bool, optional): If True, outliers will be removed before plotting.\n",
    "            ylim (tuple, optional): Tuple specifying the y-axis limits (min, max).\n",
    "            laminar_labels (list of str, optional): List of laminar labels to include in the plot. '['IG, 'SG','L4']\n",
    "        \"\"\"\n",
    "        # Filter the DataFrame by groups and stimulations if provided\n",
    "        print(\"Initial mean_df shape:\", mean_df.shape)\n",
    "        print(\"mean_df columns:\", mean_df.columns)\n",
    "        \n",
    "        if groups:\n",
    "            mean_df = mean_df[mean_df['Group'].isin(groups)]\n",
    "            print(\"After filtering by groups, mean_df shape:\", mean_df.shape)\n",
    "        if stimulations:\n",
    "            mean_df = mean_df[mean_df['Stimulation'].isin(stimulations)]\n",
    "            print(\"After filtering by stimulations, mean_df shape:\", mean_df.shape)\n",
    "        if laminar_labels:\n",
    "            mean_df = mean_df[mean_df['LaminarLabel'].isin(laminar_labels)]\n",
    "            print(\"After filtering by laminar labels, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            mean_df = self.remove_outliers_by_stimulation(mean_df, 'mean_stimulation')\n",
    "            print(\"After removing outliers, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if mean_df.empty:\n",
    "            raise ValueError(\"The DataFrame is empty after filtering. Check the input parameters and data.\")\n",
    "        \n",
    "        # Compute means and SEMs\n",
    "        summary_df = mean_df.groupby(['Group', 'Stimulation']).agg(\n",
    "            mean_value=('mean_stimulation', 'mean'),\n",
    "            sem_value=('mean_stimulation', 'sem')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Convert the mean_stimulation column to Hz by multiplying by 1000\n",
    "        summary_df['mean_value'] = summary_df['mean_value'] * 1000\n",
    "        summary_df['sem_value'] = summary_df['sem_value'] * 1000\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        for group in groups:\n",
    "            group_data = summary_df[summary_df['Group'] == group]\n",
    "            plt.errorbar(group_data['Stimulation'], group_data['mean_value'], yerr=group_data['sem_value'],\n",
    "                        label=group, color=group_colors[group], capsize=5, marker='o', linestyle='-')\n",
    "        \n",
    "        # Set the y-axis limits if specified\n",
    "        if ylim:\n",
    "            plt.ylim(ylim)\n",
    "        \n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation (Hz)')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        plt.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return summary_df, mean_df\n",
    "\n",
    "\n",
    "    def plot_group_cell_distribution(self):\n",
    "        # Dictionary to store the counts for each group and recording\n",
    "        group_counts = {}\n",
    "        recording_count_per_group = {}  # To store the number of recordings per group\n",
    "        stim_responsivity_counts = {}   # To store StimResponsivity counts for SUA and MUA separately\n",
    "        combined_stim_responsivity_counts = {}  # To store combined StimResponsivity counts\n",
    "\n",
    "        # Iterate over each cell grouping\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid():\n",
    "            if groupname not in group_counts:\n",
    "                group_counts[groupname] = {'SUA': {'FS': 0, 'RS': 0}, 'MUA': {'FS': 0, 'RS': 0}}\n",
    "                stim_responsivity_counts[groupname] = {'SUA': {'-1.0': 0, '0.0': 0, '1.0': 0}, 'MUA': {'-1.0': 0, '0.0': 0, '1.0': 0}}\n",
    "                combined_stim_responsivity_counts[groupname] = {'-1.0': 0, '0.0': 0, '1.0': 0}\n",
    "                recording_count_per_group[groupname] = set()\n",
    "\n",
    "            recording_count_per_group[groupname].add(recordingname)\n",
    "\n",
    "            # Summarize the data across recordings\n",
    "            for index, row in group_df.iterrows():\n",
    "                unit_type = 'SUA' if row['IsSingleUnit'] == 1.0 else 'MUA'\n",
    "                cell_type = row['Cell_Type']\n",
    "                stim_responsivity = str(row['StimResponsivity'])\n",
    "                if cell_type in ['FS', 'RS']:\n",
    "                    group_counts[groupname][unit_type][cell_type] += 1\n",
    "                if stim_responsivity in ['-1.0', '0.0', '1.0']:\n",
    "                    stim_responsivity_counts[groupname][unit_type][stim_responsivity] += 1\n",
    "                    combined_stim_responsivity_counts[groupname][stim_responsivity] += 1\n",
    "\n",
    "        # Plotting\n",
    "        plt.rcParams.update({'font.size': 14})  # Increase the base font size\n",
    "        for groupname in group_counts:\n",
    "            fig, axs = plt.subplots(1, 5, figsize=(24, 5))  # Adjust subplot for 5 charts\n",
    "            total_recordings = len(recording_count_per_group[groupname])\n",
    "            total_units = sum(sum(sub_counts.values()) for sub_counts in group_counts[groupname].values())\n",
    "            sua_total = sum(group_counts[groupname]['SUA'].values())\n",
    "            mua_total = sum(group_counts[groupname]['MUA'].values())\n",
    "\n",
    "            # Titles and SUA/MUA Distribution\n",
    "            fig.suptitle(f'Group: {groupname} | Mice: {total_recordings} | Total Units: {total_units} (SUA: {sua_total}, MUA: {mua_total})',\n",
    "                        fontsize=16)\n",
    "\n",
    "            for i, (unit_type, sub_counts) in enumerate(group_counts[groupname].items()):\n",
    "                labels = [f'{ct} ({n})' for ct, n in sub_counts.items()]\n",
    "                sizes = sub_counts.values()\n",
    "                if any(sizes):\n",
    "                    axs[i].pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "                            startangle=90)\n",
    "                    axs[i].set_title(f'{unit_type} Distribution', fontsize=14)\n",
    "\n",
    "            # StimResponsivity Distribution for SUA and MUA\n",
    "            for i, unit_type in enumerate(['SUA', 'MUA']):\n",
    "                labels = [f'{resp} ({count})' for resp, count in stim_responsivity_counts[groupname][unit_type].items()]\n",
    "                sizes = stim_responsivity_counts[groupname][unit_type].values()\n",
    "                if any(sizes):\n",
    "                    axs[i+2].pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "                                startangle=90)\n",
    "                    axs[i+2].set_title(f'{unit_type} Stim Responsivity', fontsize=14)\n",
    "\n",
    "            # Combined StimResponsivity Distribution\n",
    "            labels = [f'{resp} ({count})' for resp, count in combined_stim_responsivity_counts[groupname].items()]\n",
    "            sizes = combined_stim_responsivity_counts[groupname].values()\n",
    "            if any(sizes):\n",
    "                axs[4].pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "                        startangle=90)\n",
    "                axs[4].set_title('Combined Stim Responsivity', fontsize=14)\n",
    "\n",
    "            plt.tight_layout()  # Adjust layout\n",
    "            plt.show()\n",
    "\n",
    "    def run_group_comparisons(self, df, group_column='Group', value_column='mean_stimulation', stim_column='Stimulation'):\n",
    "        \"\"\"\n",
    "        Performs statistical comparisons between groups for each type of stimulation, checks for normality,\n",
    "        uses the appropriate non-parametric tests, and includes detailed descriptive statistics.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for stim in df[stim_column].unique():\n",
    "            sub_df = df[df[stim_column] == stim]\n",
    "            groups = sub_df[group_column].unique()\n",
    "\n",
    "            if len(groups) < 2:\n",
    "                continue  # Skip if not enough groups for comparison\n",
    "\n",
    "            group_data = [sub_df[sub_df[group_column] == g][value_column].dropna() for g in groups]\n",
    "\n",
    "            # Skip normality test if any group has less than 3 data points\n",
    "            normality_results = []\n",
    "            normality_p_values = []\n",
    "            for data in group_data:\n",
    "                if len(data) < 3:\n",
    "                    normality_results.append(None)\n",
    "                    normality_p_values.append(None)\n",
    "                else:\n",
    "                    result = shapiro(data)\n",
    "                    normality_results.append(result)\n",
    "                    normality_p_values.append(result.pvalue)\n",
    "\n",
    "            # Choose the appropriate statistical test based on group count\n",
    "            if len(groups) == 2:\n",
    "                stat, p_value = mannwhitneyu(*group_data)\n",
    "                test_used = 'Mann-Whitney U'\n",
    "            elif len(groups) > 2:\n",
    "                stat, p_value = kruskal(*group_data)\n",
    "                test_used = 'Kruskal-Wallis'\n",
    "                # Note: If using Kruskal-Wallis, consider post-hoc tests for detailed group comparisons\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            descriptive_stats = [{\n",
    "                'mean': data.mean(),\n",
    "                'SD': data.std(),\n",
    "                'median': data.median(),\n",
    "                'range_min': data.min(),\n",
    "                'range_max': data.max(),\n",
    "            } for data in group_data]\n",
    "\n",
    "            result_entry = {\n",
    "                'Stimulation': stim,\n",
    "                'Test Used': test_used,\n",
    "                'Test Statistic': stat,\n",
    "                'p-value': p_value,\n",
    "                **{f'N Group{i+1}': len(data) for i, data in enumerate(group_data)},\n",
    "                **{f'Normality p-value Group{i+1}': p for i, p in enumerate(normality_p_values) if p is not None},\n",
    "                **{f'{stat_key} Group{i+1}': stat_val for i, stats in enumerate(descriptive_stats) for stat_key, stat_val in stats.items()}\n",
    "            }\n",
    "\n",
    "            results.append(result_entry)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def prepare_for_boxplot_default(self, dataframe_key, value_column, group_label='groupname'):\n",
    "        \"\"\"\n",
    "        Organizes data into a DataFrame suitable for plotting boxplots by extracting values\n",
    "        from a specified column in detailed DataFrames, including labels for group.\n",
    "\n",
    "        Parameters:\n",
    "            dataframe_key (str): Key to access the specific DataFrame.\n",
    "            value_column (str): Column name from which to extract the value for plotting.\n",
    "            group_label (str): Column name to use as label for grouping in the plot. For example, 'groupname'\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with columns for the specified 'value_column', and 'Group'.\n",
    "        \"\"\"\n",
    "        # Access the specified DataFrame\n",
    "        df = self.dataframes[dataframe_key]\n",
    "\n",
    "        boxplot_data = []\n",
    "\n",
    "        # Iterate over each stored DataFrame key (group)\n",
    "        for group in df[group_label].unique():\n",
    "            group_df = df[df[group_label] == group]\n",
    "\n",
    "            # Check for empty DataFrame\n",
    "            if not group_df.empty:\n",
    "                # Extract specified value column and corresponding group label\n",
    "                for index, row in group_df.iterrows():\n",
    "                    boxplot_data.append({\n",
    "                        value_column: row[value_column],\n",
    "                        'Group': group\n",
    "                    })\n",
    "\n",
    "        # Convert list of data to DataFrame\n",
    "        boxplot_df = pd.DataFrame(boxplot_data)\n",
    "\n",
    "        return boxplot_df\n",
    "    \n",
    "    def unpack_based_on_stim(self, dataframe_key, list_column, cell_type_filter=None, is_single_unit=True, \n",
    "                             stim_responsivity_filter=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Unpacks values from a specified list column in the DataFrame if they are four elements long,\n",
    "        and labels each value according to provided trial tags stored in eed. Includes group and additional filtering options.\n",
    "\n",
    "        Parameters:\n",
    "            dataframe_key (str): Key to access the specific DataFrame.\n",
    "            list_column (str): Column name from which to extract numpy.ndarray for unpacking.\n",
    "            cell_type_filter (str, optional): Filters rows based on 'Cell_Type' (e.g., 'FS' or 'RS').\n",
    "            is_single_unit (bool, optional): Filters rows where 'IsSingleUnit' is 1.0 (True) or 0.0 (False).\n",
    "            stim_responsivity_filter (int, optional): Filters rows based on 'StimResponsivity' (-1, 0, 1).\n",
    "            modulation_label (stf, optional): Column name to use, its a string: 'positive', 'negative' or 'none'\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with unpacked values labeled by stimulation, including additional columns.\n",
    "        \"\"\"\n",
    "        df = self.dataframes[dataframe_key]\n",
    "        \n",
    "        trial_tags_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "\n",
    "        # Convert and filter data as per user requirements\n",
    "        df['IsSingleUnit'] = df['IsSingleUnit'].astype(bool)\n",
    "        df['StimResponsivity'] = df['StimResponsivity'].astype(int)\n",
    "\n",
    "        if cell_type_filter is not None:\n",
    "            df = df[df['Cell_Type'] == cell_type_filter]\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity_filter is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity_filter]\n",
    "        if modulation_label is not None:\n",
    "            df = df[df['ModulationIndex'] == modulation_label]\n",
    "        \n",
    "        unpacked_data = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            values = row[list_column]\n",
    "            if isinstance(values, np.ndarray) and len(values) == 4:\n",
    "                for i, value in enumerate(values):\n",
    "                    unpacked_data.append({\n",
    "                        'Group': row['groupname'],\n",
    "                        'Stimulation': trial_tags_labels[i],\n",
    "                        list_column: value,\n",
    "                        'Cell_Type': row['Cell_Type'],\n",
    "                        'IsSingleUnit': row['IsSingleUnit'],\n",
    "                        'StimResponsivity': row['StimResponsivity'],\n",
    "                        'recordingname': row['recordingname'],\n",
    "                        'cid': row['cid'],\n",
    "                        'ModulationIndex': row['ModulationIndex']\n",
    "                    })\n",
    "\n",
    "        unpacked_df = pd.DataFrame(unpacked_data)\n",
    "\n",
    "        return unpacked_df\n",
    "    \n",
    "    def plot_box_and_strip_default(self, groups=None, stimulations=None, show_outliers=True, hue_order=None, dataframe_key=None, list_column=None, \n",
    "                                   cell_type_filter=None, is_single_unit=None, stim_responsivity_filter=None, \n",
    "                                   remove_outliers_option=False, directory=None, file_name=None, \n",
    "                                   modulation_label=None, firstspike_latency=True):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations from the unpacked DataFrame,\n",
    "        with color adjustments made directly in the plotting calls. Designed to work with unpacked DataFrame structure.\n",
    "\n",
    "        Args:\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "            modululation_label (str, optional): Filter for 'positive', 'negative' or 'none' modulation\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Prepare data for boxplot (already prepared by a previous method)\n",
    "        boxplot_df = self.unpack_based_on_stim(dataframe_key=dataframe_key, list_column=list_column, cell_type_filter=cell_type_filter, \n",
    "                                               is_single_unit=is_single_unit, stim_responsivity_filter=stim_responsivity_filter, \n",
    "                                               modulation_label=modulation_label, firstspike_latency=firstspike_latency)\n",
    "        # Filter by specified groups and stimulations\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            boxplot_df = self.remove_outliers_by_stimulation_default(boxplot_df, value_column=list_column)\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='Stimulation', y=list_column, hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=boxplot_df, x='Stimulation', y=list_column, hue='Group', **stripplot_kwargs)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel(list_column)\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def plot_mean_sem_line_default(self, groups=None, stimulations=None, show_outliers=True, hue_order=None, dataframe_key=None, list_column=None, cell_type_filter=None, is_single_unit=None, stim_responsivity_filter=None, remove_outliers_option=False, directory=None, file_name=None):\n",
    "        \"\"\"\n",
    "        Plots mean and SEM line plots for specified groups and stimulations from the unpacked DataFrame.\n",
    "\n",
    "        Args:\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "            dataframe_key (str, optional): Key for accessing the correct DataFrame.\n",
    "            list_column (str, optional): Column name for the values to be plotted.\n",
    "            cell_type_filter (str, optional): Filter for cell type.\n",
    "            is_single_unit (bool, optional): Filter for single units.\n",
    "            stim_responsivity_filter (str, optional): Filter for stimulus responsivity.\n",
    "            remove_outliers_option (bool, optional): Whether to remove outliers.\n",
    "            directory (str, optional): Directory to save the plot.\n",
    "            file_name (str, optional): File name to save the plot.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Prepare data for plotting (already prepared by a previous method)\n",
    "        plot_df = self.unpack_based_on_stim(dataframe_key=dataframe_key, list_column=list_column, cell_type_filter=cell_type_filter, is_single_unit=is_single_unit, stim_responsivity_filter=stim_responsivity_filter)\n",
    "\n",
    "        # Filter by specified groups and stimulations\n",
    "        if groups:\n",
    "            plot_df = plot_df[plot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            plot_df = plot_df[plot_df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            plot_df = self.remove_outliers_by_stimulation_default(plot_df, value_column=list_column)\n",
    "\n",
    "        # Compute means and SEMs\n",
    "        summary_df = plot_df.groupby(['Group', 'Stimulation']).agg(\n",
    "            mean_value=(list_column, 'mean'),\n",
    "            sem_value=(list_column, 'sem')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for group in groups:\n",
    "            group_data = summary_df[summary_df['Group'] == group]\n",
    "            plt.errorbar(group_data['Stimulation'], group_data['mean_value'], yerr=group_data['sem_value'],\n",
    "                        label=group, color=group_colors[group], capsize=5, marker='o', linestyle='-')\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Mean and SEM of Stimulation Responses Across Groups')\n",
    "        plt.ylabel(list_column)\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        plt.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def remove_outliers_by_stimulation_default(self, df, value_column=None):\n",
    "        \"\"\"\n",
    "        Removes outliers within each stimulation group based on the interquartile range (IQR).\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the data, expected to have a 'Stimulation' column.\n",
    "            value_column (str): The name of the column from which outliers will be removed.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: DataFrame with outliers removed within each stimulation group.\n",
    "        \"\"\"\n",
    "        # Create an empty DataFrame to store results after removing outliers\n",
    "        filtered_df = pd.DataFrame()\n",
    "        \n",
    "        # Process each stimulation group separately\n",
    "        for stim in df['Stimulation'].unique():\n",
    "            sub_df = df[df['Stimulation'] == stim]\n",
    "            Q1 = sub_df[value_column].quantile(0.25)\n",
    "            Q3 = sub_df[value_column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Filter out outliers and append the results to the filtered DataFrame\n",
    "            result_df = sub_df[(sub_df[value_column] >= lower_bound) & (sub_df[value_column] <= upper_bound)]\n",
    "            filtered_df = pd.concat([filtered_df, result_df], ignore_index=True)\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def extract_spike_trains(self, recording_name):\n",
    "        # Filter the data for a specific recording and only single units\n",
    "        filtered_data = self.data[(self.data['recordingname'] == recording_name) & (self.data['IsSingleUnit'] == 1.0)]\n",
    "        \n",
    "        # Extract the SpikeTrains_for_PSTHs for these filtered entries\n",
    "        spike_trains = filtered_data['SpikeTrains_for_PSTHs'].tolist()\n",
    "        \n",
    "        # Optional: Convert any additional processing on spike trains here\n",
    "        # For example, converting to numpy array, etc.\n",
    "        \n",
    "        return spike_trains\n",
    "    \n",
    "    def simplify_and_filter_dataframe(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Simplify and filter the DataFrame based on IsSingleUnit, Cell_Type, and StimResponsivity.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                           If None, no filtering by StimResponsivity.\n",
    "        modulation_type (str or None): Filter for 'positive', 'negative', or 'none'. \n",
    "                                            If None, no filtering by modulation type.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The simplified and filtered DataFrame.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure ModulationIndex column is present\n",
    "        self.label_modulation_index(df_name)\n",
    "\n",
    "        # Start with the full DataFrame\n",
    "        df = self.dataframes[df_name]\n",
    "\n",
    "        # Print the first few entries and data types for debugging\n",
    "        #print(\"Original DataFrame:\")\n",
    "        #print(df.head())\n",
    "        #print(\"\\nData types of the columns:\")\n",
    "        #print(df.dtypes)\n",
    "        \n",
    "\n",
    "        # Convert numpy arrays with single values to scalar values\n",
    "        def extract_single_value(x):\n",
    "            if isinstance(x, np.ndarray) and x.ndim > 0:\n",
    "                return x[0]\n",
    "            return x\n",
    "\n",
    "        for col in ['IsSingleUnit', 'StimResponsivity', 'ModulationIndex', 'Template_Channel']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(extract_single_value)\n",
    "\n",
    "        # Print the simplified DataFrame for debugging\n",
    "        #print(\"\\nSimplified DataFrame:\")\n",
    "        #print(df.head())\n",
    "        #print(\"\\nData types of the columns after simplification:\")\n",
    "        #print(df.dtypes)\n",
    "\n",
    "        # Filter by cell type if provided\n",
    "        if cell_type:\n",
    "            df = df[df['Cell_Type'] == cell_type]\n",
    "            #print(f\"\\nDataFrame after filtering by Cell_Type='{cell_type}':\")\n",
    "            #print(df)\n",
    "\n",
    "        # Filter by IsSingleUnit if not None\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "            #print(f\"\\nDataFrame after filtering by IsSingleUnit={is_single_unit}:\")\n",
    "            #print(df)\n",
    "\n",
    "        # Filter by StimResponsivity if not None\n",
    "        if stim_responsivity is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "            #print(f\"\\nDataFrame after filtering by StimResponsivity={stim_responsivity}:\")\n",
    "            #print(df)\n",
    "        \n",
    "        # Filter by group name if provided\n",
    "        if group_name:\n",
    "            df = df[df['groupname'] == group_name]\n",
    "            #print(f\"\\nDataFrame after filtering by groupname='{group_name}':\")\n",
    "            #print(df)\n",
    "        \n",
    "        # Filter by modulation type if provided\n",
    "        if modulation_type:\n",
    "            df = df[df['ModulationIndex'] == modulation_type]\n",
    "            print(f\"Filtered by modulation type='{modulation_type}': {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_plotting_data(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Prepare data for plotting the modulation index as a function of electrode location.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and prepare.\n",
    "        group_name (str or None): Name of the group to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        modulation_type (str or None): Filter for 'positive', 'negative', or 'none'. If None, no filtering by modulation type.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame ready for plotting.\n",
    "        \"\"\"\n",
    "        # Ensure the original numeric ModulationIndex column is present and labeled correctly\n",
    "        df = self.simplify_and_filter_dataframe(df_name, group_name, is_single_unit, cell_type, stim_responsivity, modulation_type)\n",
    "        if df is None or df.empty:\n",
    "            print(\"No data to prepare for plotting.\")\n",
    "            return None\n",
    "\n",
    "        # Define the electrode order\n",
    "        electrodes_order = [14, 20, 16, 18, 1, 31, 3, 29, 5, 27, 7, 25, 9, 23, 11, 21, 13, 19, 15, 17, 12, 22, 10, 24, 8, 26, 6, 28, 4, 30, 2, 32]\n",
    "\n",
    "        # Create a mapping from Template_Channel to electrode order index\n",
    "        channel_mapping = {electrode: idx for idx, electrode in enumerate(electrodes_order)}\n",
    "\n",
    "        # Map Template_Channel to electrode order index\n",
    "        df['ElectrodeOrder'] = df['Template_Channel'].map(channel_mapping)\n",
    "\n",
    "        # Drop rows where the mapping resulted in NaN values\n",
    "        df = df.dropna(subset=['ElectrodeOrder'])\n",
    "\n",
    "        # Print the prepared DataFrame for debugging\n",
    "        print(\"\\nPrepared DataFrame for plotting:\")\n",
    "        #print(df[['ElectrodeOrder', 'ModulationIndex_Numeric', 'MeanFR_baseline', 'groupname', 'StimProb', 'MeanFR_stim'  ]].head())  # Use the new numeric column\n",
    "\n",
    "        return df[['ElectrodeOrder', 'ModulationIndex_Numeric', 'groupname', 'MeanFR_baseline', 'StimProb', 'MeanFR_stim' ]].sort_values('ElectrodeOrder')\n",
    "\n",
    "\n",
    "    def plot_modulation_index(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        group_name (str or None): Name of the group to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        \"\"\"\n",
    "        # Prepare the data for plotting\n",
    "        plotting_data = self.prepare_plotting_data(df_name, group_name, is_single_unit, cell_type, stim_responsivity)\n",
    "        if plotting_data is None or plotting_data.empty:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Drop rows with NaN in relevant columns\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Convert 'MeanFR_baseline' from array-like to scalar values\n",
    "        plotting_data['MeanFR_baseline'] = plotting_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "        # Convert to numeric and ensure correct length\n",
    "        plotting_data['MeanFR_baseline'] = pd.to_numeric(plotting_data['MeanFR_baseline'], errors='coerce')\n",
    "        plotting_data['ModulationIndex_Numeric'] = pd.to_numeric(plotting_data['ModulationIndex_Numeric'], errors='coerce')\n",
    "        plotting_data['ElectrodeOrder'] = pd.to_numeric(plotting_data['ElectrodeOrder'], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of these columns have NaNs after conversion\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Ensure all columns have the same length\n",
    "        if not (len(plotting_data['MeanFR_baseline']) == len(plotting_data['ModulationIndex_Numeric']) == len(plotting_data['ElectrodeOrder'])):\n",
    "            print(\"Mismatch in data sizes after cleaning.\")\n",
    "            return\n",
    "\n",
    "        # Normalize size values if necessary (optional step)\n",
    "        size_values = plotting_data['MeanFR_baseline'].values\n",
    "        size_values = size_values / np.max(size_values) * 200  # Normalize to range suitable for plotting\n",
    "\n",
    "        # Plot the modulation index\n",
    "        plt.figure(figsize=(2, 6))\n",
    "        scatter = plt.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['ElectrodeOrder'], \n",
    "                            s=size_values,  # Use normalized sizes\n",
    "                            c=plotting_data['ModulationIndex_Numeric'], \n",
    "                            cmap='bwr', alpha=0.6)\n",
    "        \n",
    "        \n",
    "        # Enforce the color bar is always from -1 to 1 \n",
    "        plt.clim(-1, 1)\n",
    "        plt.colorbar(scatter, label='Modulation Index')\n",
    "        plt.xlabel('Spontaneous MI')\n",
    "        plt.ylabel('Electrode Order')\n",
    "        plt.title(f'{cell_type} multi-units from {group_name}')\n",
    "        plt.gca().invert_yaxis()  # To match the order from the provided list\n",
    "\n",
    "        \n",
    "        \n",
    "        # Ensure the y-axis has all the electrodes listed \n",
    "        plt.xlim(-1.5, 1.5)\n",
    "        plt.grid(False)\n",
    "        plt.yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Save the plot as a SVG file\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        # Make the file name more descriptive based on the user input, if single unit or multi unit or if stim responsive is provided in the file name \n",
    "        if is_single_unit == 1.0:\n",
    "            file_name = f'{group_name}_single_units'\n",
    "        elif is_single_unit == 0.0:\n",
    "            file_name = f'{group_name}_multi_units'\n",
    "        else:\n",
    "            file_name = f'{group_name}_all_units'\n",
    "        \n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def label_modulation_index(self, df_name):\n",
    "        \"\"\"\n",
    "        Label the ModulationIndex as 'positive', 'negative', or 'none'.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to label.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with a new column 'ModulationIndex'.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "        if 'ModulationIndex' not in df.columns:\n",
    "            df['ModulationIndex'] = df['ModulationIndex'].apply(lambda x: 'positive' if x > 0.3 else ('negative' if x < -0.3 else 'none'))\n",
    "        return df\n",
    "    \n",
    "    def label_modulation_index2(self, df):\n",
    "        \"\"\"\n",
    "        Label the ModulationIndex as 'positive', 'negative', or 'none'.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to label.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with a new column 'ModulationIndex'.\n",
    "        \"\"\"\n",
    "        df['ModulationIndex'] = df['ModulationIndex'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'none'))\n",
    "        return df\n",
    "\n",
    "    def plot_modulation_index_density(self, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Compare the density of cells across electrode numbers between two groups.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                           If None, no filtering by StimResponsivity.\n",
    "        modulation_type (str or None): Filter for 'positive', 'negative', or 'none'. If None, no filtering by modulation type.\n",
    "        \"\"\"\n",
    "        # Prepare the data for both groups\n",
    "        df_ctz = self.prepare_plotting_data(df_name, group_name=self.eed.group_names[0], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity, modulation_type=modulation_type)\n",
    "        df_no_ctz = self.prepare_plotting_data(df_name, group_name=self.eed.group_names[1], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity, modulation_type=modulation_type)\n",
    "        \n",
    "        if df_ctz is None or df_ctz.empty or df_no_ctz is None or df_no_ctz.empty:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the bins are consistent for both groups\n",
    "        bins = np.linspace(df_ctz['ElectrodeOrder'].min(), df_ctz['ElectrodeOrder'].max(), len(df_ctz['ElectrodeOrder'].unique()))\n",
    "\n",
    "        # Plot the density of cells across electrode numbers comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        sns.histplot(df_ctz, y='ElectrodeOrder', bins=bins, color='blue', alpha=0.5, label='CTZ', kde=True, common_norm=False)\n",
    "        sns.histplot(df_no_ctz, y='ElectrodeOrder', bins=bins, color='red', alpha=0.5, label='No CTZ', kde=False, common_norm=False)\n",
    "\n",
    "        plt.xlabel('Density')\n",
    "        plt.ylabel('Electrode Order')\n",
    "        plt.title(f'{cell_type} multi-units: CTZ vs No CTZ')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def add_modulation_label_column(self, df_name):\n",
    "        \"\"\"\n",
    "        Add the ModulationIndex column to the specified DataFrame in self.dataframes.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to which the column should be added.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "        df = self.label_modulation_index2(df)\n",
    "        self.dataframes[df_name] = df\n",
    "        print(f\"Added ModulationIndex column to '{df_name}' DataFrame.\")\n",
    "        return df\n",
    "     \n",
    "    def filter_dataframe(self, df_name, modulation_type):\n",
    "        \"\"\"\n",
    "        Filter the DataFrame to retain only specific entries based on ModulationIndex.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        modulation_type (str): The type of modulation to retain ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The filtered DataFrame.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "\n",
    "        if 'ModulationIndex' not in df.columns:\n",
    "            print(\"ModulationIndex column not found in the DataFrame.\")\n",
    "            return None\n",
    "\n",
    "        # Filter the DataFrame based on ModulationIndex\n",
    "        filtered_df = df[df['ModulationIndex'] == modulation_type]\n",
    "        \n",
    "        return filtered_df\n",
    " \n",
    "    def save_filtered_dataframe(self, df_name, modulation_type):\n",
    "        \"\"\"\n",
    "        Save the filtered DataFrame to self.dataframes and reset the index.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and save.\n",
    "        modulation_type (str): The type of modulation to retain ('positive', 'negative', 'none').\n",
    "        \"\"\"\n",
    "        filtered_df = self.filter_dataframe(df_name, modulation_type)\n",
    "        if filtered_df is not None:\n",
    "            filtered_df = filtered_df.reset_index(drop=True)  # Reset the index\n",
    "            self.dataframes[df_name] = filtered_df\n",
    "            print(f\"Filtered DataFrame with {modulation_type} modulation saved to '{df_name}'.\")\n",
    "        else:\n",
    "            print(\"Filtered DataFrame was not saved.\")\n",
    "\n",
    "    def create_psth_dataframe_2(self):\n",
    "        \"\"\"\n",
    "        Creates and stores a DataFrame for each stimulation type using the 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns from the base PSTH DataFrame. \n",
    "        Each DataFrame is stored as an attribute of the DataFrameManager under a name that corresponds to the stimulation type.\n",
    "\n",
    "        \"\"\"\n",
    "        # Create the base dataframe for PSTH analysis with modulation filtering\n",
    "        self.create_dataframe(['Cell_Type', \n",
    "            'LaminarLabel', \n",
    "            'IsSingleUnit', \n",
    "            'StimResponsivity', \n",
    "            'SpikeTrains_for_PSTHs', \n",
    "            'PSTHs_conv', \n",
    "            'ModulationIndex',\n",
    "            'FirstSpikeLatency', \n",
    "            'FirstSpikeLatency_Reliability', \n",
    "            'TroughToPeak_duration', \n",
    "            'SpikeHalfWidth', \n",
    "            'PeakToPeak_ratio', \n",
    "            'peak1_normalized_amplitude', \n",
    "            'Peak1ToTrough_ratio', \n",
    "            'Peak2ToTrough_ratio', \n",
    "            'Template_Channel', \n",
    "            'MeanFR_baseline', \n",
    "            'Normalized_Template_Waveform', \n",
    "            'UnNormalized_Template_Waveform',\n",
    "            'SpikeTimes_all', \n",
    "            'ISI_baseline_CV', \n",
    "            'ISI_baseline_vec', \n",
    "            'ISI_pdf_peak_xy', \n",
    "            'ISI_pdf_x', \n",
    "            'ISI_pdf_y',\n",
    "            'StimProb', \n",
    "            'MeanFR_stim', \n",
    "            'PeakEvokedFR', \n",
    "            'PeakEvokedFR_Latency', \n",
    "            'FanoFactor_baseline', \n",
    "            'FanoFactor_stim', \n",
    "            'MeanFR_inst_stim', \n",
    "            'ISI_violations_percent', \n",
    "            'Recording_Duration', \n",
    "            'Sampling_Frequency'], 'psth_dataframe')\n",
    "        # Extracting trial tags\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        # Process each label\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            # Retrieve spike trains and PSTHs for each label and create a new DataFrame\n",
    "            df_name = f'psth_dataframe_{label}'\n",
    "            self.dataframes[df_name] = self.dataframes['psth_dataframe'].copy()\n",
    "            self.dataframes[df_name]['SpikeTrains_for_PSTHs'] = self.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'].apply(lambda x: x[index] if isinstance(x, (list, np.ndarray)) else None)\n",
    "            self.dataframes[df_name]['PSTHs_conv'] = self.dataframes['psth_dataframe']['PSTHs_conv'].apply(lambda x: x[index] if isinstance(x, (list, np.ndarray)) else None)\n",
    "        \n",
    "\n",
    "    def plot_group_comparison(self, stim_labels, cell_type=None, is_single_unit=None, stim_responsivity=None, groupnames=None, modulation_label=None, time_range=None, smoothing_window=10, ylim=None):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the shaded area\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        plot_idx = 0\n",
    "        for groupname in groupnames:\n",
    "            for stim_label in stim_labels:\n",
    "                df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label)\n",
    "                \n",
    "                # Print the number of units that match the filter\n",
    "                print(f\"Number of units that match the filter for group {groupname} and stimulation {stim_label}: {df.shape[0]}\")\n",
    "\n",
    "                if df.empty:\n",
    "                    print(f\"No data matches the specified filters for group {groupname} and stimulation {stim_label}.\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Get relative time array for x-axis\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                \n",
    "                # Create time mask and adjust time array\n",
    "                time_mask = self.create_time_mask(time_array, time_range)\n",
    "                time_array = time_array[time_mask]\n",
    "\n",
    "                all_psths = []\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    individual_psth = row['PSTHs_conv']\n",
    "                    # Apply time mask to individual PSTH\n",
    "                    individual_psth = np.array(individual_psth)[time_mask]\n",
    "                    \n",
    "                    # Apply a smoothing window\n",
    "                    window = np.ones(smoothing_window) / smoothing_window  # 10ms window of smoothing\n",
    "                    individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "                    \n",
    "                    all_psths.append(individual_psth)\n",
    "                    # Plot the individual PSTH with lighter color\n",
    "                    color = lightened_colors.get(groupname, '#CCCCCC')\n",
    "                    axes[plot_idx].plot(time_array, individual_psth, color=color, linewidth=1, alpha=0.5)\n",
    "\n",
    "                # Calculate the mean PSTH\n",
    "                all_psths = np.array(all_psths)\n",
    "                mean_psth = np.mean(all_psths, axis=0)\n",
    "                # Apply a smoothing window to the mean PSTH\n",
    "                mean_psth = np.convolve(mean_psth, window, mode='same')\n",
    "                # Plot the mean PSTH with a thicker line\n",
    "                color = group_colors.get(groupname, 'black')\n",
    "                axes[plot_idx].plot(time_array, mean_psth, label='Mean PSTH', color=color, linewidth=2)\n",
    "\n",
    "                axes[plot_idx].set_title(f'Group: {groupname}, Stimulation: {stim_label}')\n",
    "                axes[plot_idx].set_xlabel('Time (ms)')\n",
    "                axes[plot_idx].set_ylabel('Spike Rate')\n",
    "                axes[plot_idx].legend().set_visible(False)\n",
    "                \n",
    "                plot_idx += 1\n",
    "                #specify the ylimit for all the subplots\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_group_comparison_meansem(self, stim_labels, cell_type=None, is_single_unit=None, stim_responsivity=None, groupnames=None, modulation_label=None, time_range=None, smoothing_window=10, ylim=None, directory=None, file_name=None):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the shaded area\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        plot_idx = 0\n",
    "        for groupname in groupnames:\n",
    "            for stim_label in stim_labels:\n",
    "                df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label)\n",
    "                \n",
    "                # Print the number of units that match the filter\n",
    "                print(f\"Number of units that match the filter for group {groupname} and stimulation {stim_label}: {df.shape[0]}\")\n",
    "\n",
    "                if df.empty:\n",
    "                    print(f\"No data matches the specified filters for group {groupname} and stimulation {stim_label}.\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Get relative time array for x-axis\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                \n",
    "                # Create time mask and adjust time array\n",
    "                time_mask = self.create_time_mask(time_array, time_range)\n",
    "                time_array = time_array[time_mask]\n",
    "\n",
    "                all_psths = []\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    individual_psth = row['PSTHs_conv']\n",
    "                    # Apply time mask to individual PSTH\n",
    "                    individual_psth = np.array(individual_psth)[time_mask]\n",
    "                    \n",
    "                    # Apply a smoothing window\n",
    "                    window = np.ones(smoothing_window) / smoothing_window  # 10ms window of smoothing\n",
    "                    individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "                    \n",
    "                    \n",
    "                    all_psths.append(individual_psth)\n",
    "\n",
    "                # Calculate the mean and SEM PSTH\n",
    "                all_psths = np.array(all_psths)\n",
    "                mean_psth = np.mean(all_psths, axis=0)\n",
    "                sem_psth = np.std(all_psths, axis=0) / np.sqrt(all_psths.shape[0])\n",
    "\n",
    "                # Plot the mean PSTH with SEM as shaded error bars\n",
    "                color = group_colors.get(groupname, 'black')\n",
    "                shaded_color = lightened_colors.get(groupname, 'gray')\n",
    "                axes[plot_idx].plot(time_array, mean_psth, label='Mean PSTH', color=color, linewidth=2)\n",
    "                axes[plot_idx].fill_between(time_array, mean_psth - sem_psth, mean_psth + sem_psth, color=shaded_color, alpha=0.3)\n",
    "\n",
    "                axes[plot_idx].set_title(f'Group: {groupname}, Stimulation: {stim_label}')\n",
    "                axes[plot_idx].set_xlabel('Time (ms)')\n",
    "                axes[plot_idx].set_ylabel('Spike Rate')\n",
    "                axes[plot_idx].legend().set_visible(False)\n",
    "                \n",
    "                plot_idx += 1\n",
    "        #specify the ylimit for all the subplots\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        fig.savefig(file_path, format='svg', transparent=True)\n",
    "    \n",
    "    def plot_group_comparison_1v1(self, stim_labels, cell_type=None, is_single_unit=None, stim_responsivity=None, groupnames=None, modulation_label=None, time_range=None, smoothing_window=10, ylim=None, recordingnames=None):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the shaded area\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        plot_idx = 0\n",
    "        for group_idx, groupname in enumerate(groupnames):\n",
    "            recordingname = recordingnames[group_idx]\n",
    "            for stim_label in stim_labels:\n",
    "                df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label, recordingname)\n",
    "                \n",
    "                # Print the number of units that match the filter\n",
    "                print(f\"Number of units that match the filter for group {groupname}, recording {recordingname}, and stimulation {stim_label}: {df.shape[0]}\")\n",
    "\n",
    "                if df.empty:\n",
    "                    print(f\"No data matches the specified filters for group {groupname}, recording {recordingname}, and stimulation {stim_label}.\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Get relative time array for x-axis\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                \n",
    "                # Create time mask and adjust time array\n",
    "                time_mask = self.create_time_mask(time_array, time_range)\n",
    "                time_array = time_array[time_mask]\n",
    "\n",
    "                all_psths = []\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    individual_psth = row['PSTHs_conv']\n",
    "                    # Apply time mask to individual PSTH\n",
    "                    individual_psth = np.array(individual_psth)[time_mask]\n",
    "                    \n",
    "                    # Apply a smoothing window\n",
    "                    window = np.ones(smoothing_window) / smoothing_window  # 10ms window of smoothing\n",
    "                    individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "                    \n",
    "                    all_psths.append(individual_psth)\n",
    "                    # Plot the individual PSTH with lighter color\n",
    "                    color = lightened_colors.get(groupname, '#CCCCCC')\n",
    "                    axes[plot_idx].plot(time_array, individual_psth, color=color, linewidth=1, alpha=0.5)\n",
    "\n",
    "                # Calculate the mean PSTH\n",
    "                all_psths = np.array(all_psths)\n",
    "                mean_psth = np.mean(all_psths, axis=0)\n",
    "                # Apply a smoothing window to the mean PSTH\n",
    "                mean_psth = np.convolve(mean_psth, window, mode='same')\n",
    "                # Plot the mean PSTH with a thicker line\n",
    "                color = group_colors.get(groupname, 'black')\n",
    "                axes[plot_idx].plot(time_array, mean_psth, label='Mean PSTH', color=color, linewidth=2)\n",
    "\n",
    "                axes[plot_idx].set_title(f'Group: {groupname}, Recording: {recordingname}, Stimulation: {stim_label}')\n",
    "                axes[plot_idx].set_xlabel('Time (ms)')\n",
    "                axes[plot_idx].set_ylabel('Spike Rate')\n",
    "                axes[plot_idx].legend().set_visible(False)\n",
    "                \n",
    "                plot_idx += 1\n",
    "        \n",
    "        # Set y-limit for all subplots\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def process_waveforms(self, cell_type, groupname=None, stim_responsivity=None, modulation_label=None, is_single_unit=None):\n",
    "        \"\"\"\n",
    "        Processes waveforms for the specified cell type and optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        cell_type (str): 'FS' or 'RS'.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (waveforms, mean_waveform, sem_waveform)\n",
    "        \"\"\"\n",
    "        df = self.get_filtered_data(\n",
    "            df_name='basic_metrics', \n",
    "            is_single_unit=is_single_unit,\n",
    "            cell_type=cell_type, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            groupname=groupname, \n",
    "            modulation_label=modulation_label, \n",
    "            spike_cutoff=spike_cutoff)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No data available for cell type: {cell_type}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        waveforms = df['UnNormalized_Template_Waveform'].values\n",
    "        mean_waveform = np.mean(np.stack(waveforms), axis=0)\n",
    "        sem_waveform = np.std(np.stack(waveforms), axis=0) / np.sqrt(len(waveforms))\n",
    "        \n",
    "        #convet data to microvolts conversion_factor = 0.25  # µV per unit\n",
    "        waveforms = waveforms * 0.25\n",
    "        mean_waveform = mean_waveform * 0.25\n",
    "        sem_waveform = sem_waveform * 0.25\n",
    "        \n",
    "        \n",
    "        return waveforms, mean_waveform, sem_waveform\n",
    "    \n",
    "    def align_waveforms_on_negative_peak(self, waveforms):\n",
    "        \"\"\"\n",
    "        Aligns waveforms based on the most negative deflection (peak). \n",
    "\n",
    "        Parameters:\n",
    "        waveforms (list of numpy.ndarray): A list of 1D numpy arrays where each array is a waveform.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: A 2D array of aligned waveforms with NaNs added to handle alignment.\n",
    "        \"\"\"\n",
    "        # Find the index of the most negative value in each waveform\n",
    "        peak_indices = [np.argmin(waveform) for waveform in waveforms]\n",
    "\n",
    "        # Determine the maximum shift needed to align the peaks\n",
    "        max_shift = max(peak_indices)\n",
    "\n",
    "        # Determine the length of the waveforms (assuming they are all the same length)\n",
    "        waveform_length = waveforms[0].shape[0]\n",
    "\n",
    "        # Initialize an array to hold the aligned waveforms with NaNs\n",
    "        aligned_waveforms = np.full((len(waveforms), waveform_length + max_shift), np.nan)\n",
    "\n",
    "        # Align each waveform\n",
    "        for i, waveform in enumerate(waveforms):\n",
    "            shift = max_shift - peak_indices[i]\n",
    "            aligned_waveforms[i, shift:shift + waveform_length] = waveform\n",
    "\n",
    "        return aligned_waveforms\n",
    "    \n",
    "    def process_waveforms(self, cell_type, groupname=None, stim_responsivity=None, modulation_label=None,  is_single_unit= None, align_on_negative_peak=True):\n",
    "        \"\"\"\n",
    "        Processes waveforms for the specified cell type and optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        cell_type (str): 'FS' or 'RS'.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        align_on_negative_peak (bool): If True, align waveforms on the most negative deflection before returning.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (waveforms, mean_waveform, sem_waveform)\n",
    "        \"\"\"\n",
    "        df = self.get_filtered_data(\n",
    "            df_name= 'basic_metrics', \n",
    "            is_single_unit=is_single_unit, \n",
    "            cell_type=cell_type, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            groupname=groupname, \n",
    "            modulation_label=modulation_label\n",
    "        )\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No data available for cell type: {cell_type}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        waveforms = df['UnNormalized_Template_Waveform'].values\n",
    "\n",
    "        if align_on_negative_peak:\n",
    "            waveforms = self.align_waveforms_on_negative_peak(waveforms)\n",
    "        \n",
    "        mean_waveform = np.nanmean(waveforms, axis=0)\n",
    "        sem_waveform = np.nanstd(waveforms, axis=0) / np.sqrt(np.sum(~np.isnan(waveforms), axis=0))\n",
    "        \n",
    "        #convert data to microvolts conversion_factor = 0.25  # µV per unit, speficic for blackrock data\n",
    "        waveforms = waveforms * 0.25\n",
    "        mean_waveform = mean_waveform * 0.25\n",
    "        sem_waveform = sem_waveform * 0.25\n",
    "        \n",
    "        return waveforms, mean_waveform, sem_waveform\n",
    "    \n",
    "    \n",
    "    def plot_waveforms(self, cell_type, color, label, groupname=None, stim_responsivity=None, modulation_label=None, is_single_unit=None, ylim=None):\n",
    "        \"\"\"\n",
    "        Plots individual and mean waveforms for the specified cell type with optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        cell_type (str): 'FS' or 'RS'.\n",
    "        color (str): Color for the plot.\n",
    "        label (str): Label for the mean waveform plot.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        ylim (tuple or None): Set y-axis limits. If None, no limit is set.\n",
    "        is_single_units (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        waveforms, mean_waveform, _ = self.process_waveforms(\n",
    "            cell_type, \n",
    "            groupname=groupname, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            modulation_label=modulation_label, \n",
    "            is_single_unit=is_single_unit\n",
    "        )\n",
    "        \n",
    "        if waveforms is None:\n",
    "            return\n",
    "        \n",
    "        # Plot individual waveforms\n",
    "        for waveform in waveforms:\n",
    "            plt.plot(waveform, color=color, alpha=0.1)\n",
    "        \n",
    "        # Plot mean waveform\n",
    "        plt.plot(mean_waveform, color=color, label=label, linewidth=2)\n",
    "        \n",
    "        # Add ylim range if provided\n",
    "        if ylim:\n",
    "            plt.ylim(ylim)\n",
    "            \n",
    "        # Save the figure\n",
    "        plt.savefig('/Volumes/MannySSD/figures/waveforms.svg', transparent=True)\n",
    "\n",
    "    def plot_trough_to_peak_histogram(self, df_name='basic_metrics', groupname=None, is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Plots histograms of Trough-to-Peak duration for FS and RS cells, with options to filter data.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): The name of the DataFrame to use (default is 'basic_metrics').\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        None: The function plots and saves the histogram.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve filtered data for FS and RS cells\n",
    "        fs_df = self.get_filtered_data(df_name, is_single_unit=is_single_unit, cell_type='FS', stim_responsivity=stim_responsivity, groupname=groupname, modulation_label=modulation_label)\n",
    "        rs_df = self.get_filtered_data(df_name, is_single_unit=is_single_unit, cell_type='RS', stim_responsivity=stim_responsivity, groupname=groupname, modulation_label=modulation_label)\n",
    "        \n",
    "        #print the shape of the dataframes \n",
    "        print(f'For FS cells: {fs_df.shape} after filtering')\n",
    "        print(f'For RS cells: {rs_df.shape} after filtering')\n",
    "        \n",
    "        # Extract the Trough-to-Peak durations, dropping NaNs\n",
    "        fs_durations = fs_df['TroughToPeak_duration'].dropna()\n",
    "        rs_durations = rs_df['TroughToPeak_duration'].dropna()\n",
    "        \n",
    "        print(f'For FS cells: {fs_durations.shape} after dropping NaNs')\n",
    "        print(f'For RS cells: {rs_durations.shape} after dropping NaNs')\n",
    "        \n",
    "        # Determine the bins based on the combined range of both FS and RS durations\n",
    "        all_durations = np.concatenate((fs_durations, rs_durations))\n",
    "        bins = np.histogram_bin_edges(all_durations, bins=30)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot histograms\n",
    "        plt.hist(fs_durations, bins=bins, alpha=0.7, color='darkgoldenrod', label='FS')\n",
    "        plt.hist(rs_durations, bins=bins, alpha=0.7, color='sienna', label='RS')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Trough-to-Peak Duration Histogram')\n",
    "        plt.xlabel('Trough-to-Peak Duration (ms)')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.legend()\n",
    "    \n",
    "    def plot_combined_waveforms(self, groupname=None, stim_responsivity=None, modulation_label=None,  is_single_unit=None, ylim=None):\n",
    "        \"\"\"\n",
    "        Plots combined waveforms for FS and RS cells with optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        ylim (tuple or None): Set y-axis limits. If None, no limit is set.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot FS waveforms\n",
    "        self.plot_waveforms('FS', color='darkgoldenrod', label='FS Mean', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label, is_single_unit=is_single_unit, ylim=ylim)\n",
    "        \n",
    "        # Plot RS waveforms\n",
    "        self.plot_waveforms('RS', color='sienna', label='RS Mean', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label,  is_single_unit=is_single_unit, ylim=ylim)\n",
    "        \n",
    "        plt.title('Combined Waveforms of FS and RS Units')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Normalized Amplitude')\n",
    "        plt.legend()\n",
    "        \n",
    "        # CONTROL THE X LIMITS\n",
    "        plt.xlim(30, 120)\n",
    "        plt.ylim(-2800, 1000)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig('/Volumes/MannySSD/figures/combined_waveforms.svg', transparent=True)\n",
    "\n",
    "    def plot_combined_waveforms_meansem(self, groupname=None, stim_responsivity=None, modulation_label=None, is_single_unit=None, ylim=None):\n",
    "        \"\"\"\n",
    "        Plots combined mean and SEM waveforms for FS and RS cells with optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "        ylim (tuple or None): Set y-axis limits. If None, no limit is set.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        fs_waveforms, fs_mean_waveform, fs_sem_waveform = self.process_waveforms(\n",
    "            'FS', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label, is_single_unit=is_single_unit\n",
    "        )\n",
    "        rs_waveforms, rs_mean_waveform, rs_sem_waveform = self.process_waveforms(\n",
    "            'RS', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label, is_single_unit=is_single_unit\n",
    "        )\n",
    "        \n",
    "        if fs_waveforms is None and rs_waveforms is None:\n",
    "            print(\"No data available for both FS and RS units.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot FS mean and SEM\n",
    "        if fs_waveforms is not None:\n",
    "            plt.plot(fs_mean_waveform, color='darkgoldenrod', label='FS Mean', linewidth=2)\n",
    "            plt.fill_between(np.arange(len(fs_mean_waveform)), fs_mean_waveform - fs_sem_waveform, fs_mean_waveform + fs_sem_waveform, color='darkgoldenrod', alpha=0.3)\n",
    "        \n",
    "        # Plot RS mean and SEM\n",
    "        if rs_waveforms is not None:\n",
    "            plt.plot(rs_mean_waveform, color='sienna', label='RS Mean', linewidth=2)\n",
    "            plt.fill_between(np.arange(len(rs_mean_waveform)), rs_mean_waveform - rs_sem_waveform, rs_mean_waveform + rs_sem_waveform, color='sienna', alpha=0.3)\n",
    "        \n",
    "        plt.title('Mean and SEM of FS and RS Units')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Normalized Amplitude')\n",
    "        plt.legend()\n",
    "        \n",
    "        # CONTROL THE X LIMITS\n",
    "        plt.xlim(30, 120)\n",
    "        plt.ylim(-2000, 500)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig('/Volumes/MannySSD/figures/mean_sem_waveforms.svg', transparent=True)\n",
    " \n",
    "        \n",
    "        \n",
    "    def process_and_store_analysis_data(self, is_single_unit=None, cell_type=None, stim_responsivity=None, \n",
    "                                        time_window=None, bin_size=1, smoothing_window=5, analysis_functions=None):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid', extracts the PSTH data,\n",
    "        performs specified analyses, and stores the results in a structured format.\n",
    "\n",
    "        Args:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            analysis_functions (list, optional): List of functions to apply to the extracted data. Each function should\n",
    "                                                 take the extracted data as input and return a dictionary with results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary containing the extracted data and analysis results for each group, recording, and cid.\n",
    "        \"\"\"\n",
    "        stored_data = {}\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "                is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "\n",
    "            # Extract the PSTH data\n",
    "            extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                            time_window=time_window, bin_size=bin_size, smoothing_window=smoothing_window)\n",
    "\n",
    "            # Perform additional analyses\n",
    "            if analysis_functions:\n",
    "                for func in analysis_functions:\n",
    "                    analysis_results = func(extracted_data)\n",
    "                    extracted_data.update(analysis_results)\n",
    "\n",
    "            # Add the parameters used to the extracted data\n",
    "            extracted_data['bin_size'] = bin_size\n",
    "            extracted_data['smoothing_window'] = smoothing_window\n",
    "            extracted_data['time_window'] = time_window\n",
    "\n",
    "            # Ensure the nested dictionary structure is created\n",
    "            if groupname not in stored_data:\n",
    "                stored_data[groupname] = {}\n",
    "            if recordingname not in stored_data[groupname]:\n",
    "                stored_data[groupname][recordingname] = {}\n",
    "            \n",
    "            stored_data[groupname][recordingname][cid] = extracted_data\n",
    "\n",
    "        return stored_data\n",
    "\n",
    "### this will be now the new methods for plotting distributuions\n",
    "    def plot_stim_prob_distribution(self, df_name, groupname=None, is_single_unit=None, cell_type=None, xlim_range=None, include_no_change=True, plot_stacked_bar=True):\n",
    "        \"\"\"\n",
    "        Plot the step plots for the StimResponsivity unique labels (-1.0, 0.0, 1.0)\n",
    "        showing the distribution of StimProb AUROC values, and print the total counts for each category.\n",
    "        Also, plot a stacked bar chart visualizing the proportions of the three groups.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        xlim_range (tuple or None): Tuple specifying the lower and upper bounds of the x-axis. \n",
    "                                    If None, the full range of [0, 1] is used.\n",
    "        include_no_change (bool): If True, include the \"No Change\" group in the plot. Default is True.\n",
    "        plot_stacked_bar (bool): If True, plot a stacked bar chart showing the proportions of the three groups. Default is True.\n",
    "        \"\"\"\n",
    "        # Define StimResponsivity labels and their corresponding colors\n",
    "        stim_responsivity_labels = [-1.0, 0.0, 1.0]\n",
    "        colors = ['red', 'grey', 'green']\n",
    "        labels = ['Decrease', 'No Change', 'Increase']\n",
    "        \n",
    "        # Prepare bin edges for the histogram\n",
    "        bin_edges = np.concatenate([np.arange(0, 0.5, 0.01), np.arange(0.501, 1.01, 0.01)])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Dictionary to store total counts\n",
    "        total_counts = {'Decrease': 0, 'No Change': 0, 'Increase': 0}\n",
    "\n",
    "        # Step 1: Get the filtered and categorized DataFrame\n",
    "        filtered_df = self.prepare_StimProb_distribution(df_name, is_single_unit, cell_type, groupname=groupname)\n",
    "        \n",
    "        # Step 2: Iterate over each category and count and plot the AUROC values\n",
    "        for label, stim_label in zip(labels, stim_responsivity_labels):\n",
    "            # Count the number of entries in the current category\n",
    "            total_counts[label] = filtered_df[filtered_df['StimResponsivity'] == stim_label].shape[0]\n",
    "\n",
    "            # Plot only if it's either \"Decrease\" or \"Increase\" or if \"No Change\" is included\n",
    "            if label == 'No Change' and not include_no_change:\n",
    "                continue  # Skip plotting for \"No Change\" if not included\n",
    "\n",
    "            # Extract AUROC values for the current category without additional filtering\n",
    "            auroc_values = filtered_df[filtered_df['StimResponsivity'] == stim_label]['StimProb'].apply(lambda x: x[0]).tolist()\n",
    "\n",
    "            # Calculate the histogram\n",
    "            hist, _ = np.histogram(auroc_values, bins=bin_edges)\n",
    "            \n",
    "            # Create the step plot\n",
    "            plt.step(bin_edges[:-1], hist, where='post', label=f'{label} (StimResponsivity={stim_label})', color=colors[labels.index(label)])\n",
    "            \n",
    "            # Add light fill for \"Decrease\"\n",
    "            if stim_label == -1.0:\n",
    "                plt.fill_between(bin_edges[:-1], hist, step='post', color=colors[labels.index(label)], alpha=0.1)\n",
    "        \n",
    "        # Add legend and title to the plot\n",
    "        plt.legend()\n",
    "        plt.title('StimProb AUROC Distribution by StimResponsivity')\n",
    "        plt.xlabel('AUROC Value')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Print the total counts for each category\n",
    "        print(f\"Total counts - Decrease: {total_counts['Decrease']}, No Change: {total_counts['No Change']}, Increase: {total_counts['Increase']}\")\n",
    "\n",
    "        # Set x-axis limits based on the user input\n",
    "        if xlim_range is not None:\n",
    "            plt.xlim(xlim_range)\n",
    "        else:\n",
    "            plt.xlim([0, 1])  # Default full range\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Step 3: Plot the stacked bar chart if requested\n",
    "        if plot_stacked_bar:\n",
    "            sizes = [total_counts['Decrease'], total_counts['No Change'], total_counts['Increase']]\n",
    "            colors_bar = ['red', 'grey', 'green']\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(['StimResponsivity'], sizes[0], color=colors_bar[0], label='Decrease')\n",
    "            plt.bar(['StimResponsivity'], sizes[1], bottom=sizes[0], color=colors_bar[1], label='No Change')\n",
    "            plt.bar(['StimResponsivity'], sizes[2], bottom=sizes[0] + sizes[1], color=colors_bar[2], label='Increase')\n",
    "            \n",
    "            plt.title('Proportions of StimResponsivity Categories')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_StimProb_distribution(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Prepare a distribution of AUROC values from the 'StimProb' column of the filtered DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Filtered DataFrame with StimProb AUROC values.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Apply the StimResponsivity logic\n",
    "        filtered_df['StimResponsivity'] = filtered_df.apply(determine_stim_responsivity, axis=1)\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_cumulative_probability(self, df_name, group1, group2, is_single_unit=None, cell_type=None):\n",
    "        \"\"\"\n",
    "        Plot the cumulative probability of baseline firing rates between two groups.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        group1 (str): Name of the first group.\n",
    "        group2 (str): Name of the second group.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Define groups and colors\n",
    "        groups = [group1, group2]\n",
    "        colors = ['blue', 'orange']\n",
    "        \n",
    "        for group, color in zip(groups, colors):\n",
    "            # Get filtered data for the current group\n",
    "            filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, groupname=group)\n",
    "            \n",
    "            # Extract baseline firing rates\n",
    "            baseline_firing_rates = filtered_df['MeanFR_baseline'].dropna()\n",
    "            \n",
    "            # Calculate cumulative distribution\n",
    "            sorted_fr = np.sort(baseline_firing_rates)\n",
    "            cum_prob = np.arange(1, len(sorted_fr) + 1) / len(sorted_fr)\n",
    "            \n",
    "            # Plot cumulative probability\n",
    "            plt.plot(sorted_fr, cum_prob, label=f'{group} (n={len(sorted_fr)})', color=color)\n",
    "        \n",
    "        # Add legend and title\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Cumulative Probability of Baseline Firing Rates')\n",
    "        plt.xlabel('Mean Baseline Firing Rate')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    def generate_summary_statistics(self, column_name, df_name='basic_metrics', handle_arrays=True, array_element=0):\n",
    "        \"\"\"\n",
    "        Generates summary statistics for a specified column in the DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        column_name (str): The name of the column to generate statistics for.\n",
    "        df_name (str): The name of the DataFrame to use (default is 'basic_metrics').\n",
    "        handle_arrays (bool): Whether to handle numpy arrays by extracting a specific element.\n",
    "        array_element (int): The index of the element to extract from numpy arrays (default is 0).\n",
    "        \n",
    "        Returns:\n",
    "        summary (dict): A dictionary with counts of unique values in the specified column.\n",
    "        \"\"\"\n",
    "        # Ensure the DataFrame exists\n",
    "        if df_name not in self.dataframes:\n",
    "            raise ValueError(f\"DataFrame '{df_name}' does not exist in self.dataframes.\")\n",
    "        \n",
    "        df = self.dataframes[df_name]\n",
    "        \n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in DataFrame '{df_name}'.\")\n",
    "        \n",
    "        # Access the column data\n",
    "        column_data = df[column_name]\n",
    "        \n",
    "        # If the column might contain numpy arrays or lists, and handle_arrays is True,\n",
    "        # process the column to extract a specific element from these arrays/lists.\n",
    "        if handle_arrays:\n",
    "            # Apply the extract_array_element function to each element in the column.\n",
    "            # This replaces numpy arrays or lists with the specific element (defined by array_element)\n",
    "            # or leaves other data types unchanged.\n",
    "            column_data = column_data.apply(extract_array_element, args=(array_element,))\n",
    "        \n",
    "        # Calculate the value counts for the processed column data.\n",
    "        # This will count how often each unique value appears in the column.\n",
    "        summary = column_data.value_counts().to_dict()\n",
    "        \n",
    "        # Return the summary as a dictionary, where the keys are unique values in the column\n",
    "        # and the values are the counts of how often these unique values appear.\n",
    "        return summary\n",
    "    \n",
    "    def inspect_raw_data(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Inspect the raw data to understand the distribution of cells before any transformations.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to inspect.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Prints out the information about the data.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Print out the basic information\n",
    "        print(\"Total number of cells:\", len(filtered_df))\n",
    "        print(filtered_df.head())  # Show the first few rows of the DataFrame\n",
    "\n",
    "        # Display the distribution of AUROC values\n",
    "        print(\"AUROC values distribution:\")\n",
    "        print(filtered_df['StimProb'].apply(lambda x: x[0]).describe())\n",
    "\n",
    "        # Display the initial distribution of ModulationIndex\n",
    "        print(\"Initial ModulationIndex distribution:\")\n",
    "        print(filtered_df['ModulationIndex'].value_counts())\n",
    "        \n",
    "    def inspect_categorized_data(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Categorize the data using determine_stim_responsivity and inspect the resulting distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to inspect.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Prints out the categorized information.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Apply the StimResponsivity logic\n",
    "        filtered_df['StimResponsivity'] = filtered_df.apply(determine_stim_responsivity, axis=1)\n",
    "        \n",
    "        # Print out the basic information after categorization\n",
    "        print(\"Total number of cells after categorization:\", len(filtered_df))\n",
    "        print(filtered_df.head())  # Show the first few rows of the categorized DataFrame\n",
    "\n",
    "        # Display the distribution of StimResponsivity labels\n",
    "        print(\"Distribution of StimResponsivity labels:\")\n",
    "        print(filtered_df['StimResponsivity'].value_counts())\n",
    "\n",
    "    def check_stim_responsivity_distribution_before_after(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Check the distribution of the 'StimResponsivity' column before and after applying the categorization logic.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to inspect.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Prints out the distributions before and after the categorization.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Check initial distribution of StimResponsivity\n",
    "        print(\"Initial StimResponsivity distribution:\")\n",
    "        print(filtered_df['StimResponsivity'].value_counts())\n",
    "\n",
    "        # Check how many cells have an AUROC lower CI > 0.5\n",
    "        auroc_lower_ci = filtered_df['StimProb'].apply(lambda x: x[1] if len(x) > 1 else None)\n",
    "        responsive_cells = auroc_lower_ci > 0.5\n",
    "        print(f\"Number of cells with AUROC lower CI > 0.5: {responsive_cells.sum()} / {len(filtered_df)}\")\n",
    "        \n",
    "        # Apply the StimResponsivity logic\n",
    "        filtered_df['StimResponsivity_New'] = filtered_df.apply(determine_stim_responsivity, axis=1)\n",
    "        \n",
    "        # Check the distribution of StimResponsivity after categorization\n",
    "        print(\"StimResponsivity distribution after categorization:\")\n",
    "        print(filtered_df['StimResponsivity_New'].value_counts())\n",
    "        \n",
    "        \n",
    "        #### plot distrubution without beign redundant\n",
    "\n",
    "    def plot_raw_auroc_distribution(self, df_name, is_single_unit=None, cell_type=None, groupname=None, xlim_range=None):\n",
    "        \"\"\"\n",
    "        Plot the distribution of AUROC values, color-coded based on the 'StimResponsivity' labels.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        xlim_range (tuple or None): Tuple specifying the lower and upper bounds of the x-axis. \n",
    "                                    If None, the full range of [0, 1] is used.\n",
    "        \n",
    "        Returns:\n",
    "        None: Displays the plot.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Define StimResponsivity labels and their corresponding colors\n",
    "        stim_responsivity_labels = [-1.0, 0.0, 1.0]\n",
    "        colors = ['red', 'grey', 'green']\n",
    "        labels = ['Decrease', 'No Change', 'Increase']\n",
    "        \n",
    "        # Prepare bin edges for the histogram\n",
    "        bin_edges = np.linspace(0, 1, 51)  # 50 bins between 0 and 1\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Dictionary to store total counts\n",
    "        total_counts = {'Decrease': 0, 'No Change': 0, 'Increase': 0}\n",
    "\n",
    "        # Iterate over each category and plot the AUROC values\n",
    "        for label, stim_label in zip(labels, stim_responsivity_labels):\n",
    "            # Extract AUROC values for the current category\n",
    "            auroc_values = filtered_df[filtered_df['StimResponsivity'] == stim_label]['StimProb'].apply(lambda x: x[0]).tolist()\n",
    "            total_counts[label] = len(auroc_values)\n",
    "            \n",
    "            # Calculate the histogram\n",
    "            hist, _ = np.histogram(auroc_values, bins=bin_edges)\n",
    "            \n",
    "            # Create the step plot\n",
    "            plt.step(bin_edges[:-1], hist, where='post', label=f'{label} (StimResponsivity={stim_label})', color=colors[labels.index(label)])\n",
    "            \n",
    "            # Add light fill\n",
    "            plt.fill_between(bin_edges[:-1], hist, step='post', color=colors[labels.index(label)], alpha=0.1)\n",
    "        \n",
    "        # Add legend and title to the plot\n",
    "        plt.legend()\n",
    "        plt.title('AUROC Distribution by StimResponsivity')\n",
    "        plt.xlabel('AUROC Value')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Set x-axis limits based on the user input\n",
    "        if xlim_range is not None:\n",
    "            plt.xlim(xlim_range)\n",
    "        else:\n",
    "            plt.xlim([0, 1])  # Default full range\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Print the total counts for each category\n",
    "        print(f\"Total counts - Decrease: {total_counts['Decrease']}, No Change: {total_counts['No Change']}, Increase: {total_counts['Increase']}\")\n",
    "\n",
    "        \n",
    "    def plot_modulation_index_distribution(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Plot the distribution of ModulationIndex values directly from the original DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Displays the plot.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Count the occurrences of each ModulationIndex category\n",
    "        modulation_index_counts = filtered_df['ModulationIndex'].value_counts()\n",
    "\n",
    "        # Plot the bar chart of ModulationIndex counts\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        modulation_index_counts.plot(kind='bar', color=['green', 'red', 'grey'])\n",
    "        plt.title('Distribution of ModulationIndex Categories')\n",
    "        plt.xlabel('ModulationIndex')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_modulation_index_with_firing_rate_color_and_size(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location, with firing rates represented by both color and size of circles.\n",
    "        Circles have black outlines and are slightly offset horizontally to reduce overlap. Horizontal lines connect each point to x=0.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        group_name (str or None): Name of the group to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "        \"\"\"\n",
    "        # Prepare the data for plotting\n",
    "        plotting_data = self.prepare_plotting_data(df_name, group_name, is_single_unit, cell_type, stim_responsivity)\n",
    "        if plotting_data is None or plotting_data.empty:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Data preparation (same as before)\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "        plotting_data['MeanFR_baseline'] = plotting_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "        plotting_data['MeanFR_baseline'] = pd.to_numeric(plotting_data['MeanFR_baseline'], errors='coerce')\n",
    "        plotting_data['ModulationIndex_Numeric'] = pd.to_numeric(plotting_data['ModulationIndex_Numeric'], errors='coerce')\n",
    "        plotting_data['ElectrodeOrder'] = pd.to_numeric(plotting_data['ElectrodeOrder'], errors='coerce')\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Calculate sizes for scatter points\n",
    "        min_size = 10\n",
    "        max_size = 200\n",
    "        sizes = min_size + (plotting_data['MeanFR_baseline'] / plotting_data['MeanFR_baseline'].max()) * (max_size - min_size)\n",
    "\n",
    "        # Add horizontal jitter\n",
    "        plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(figsize=(4, 8))\n",
    "\n",
    "        # Plot horizontal lines from x=0 to each point\n",
    "        for _, row in plotting_data.iterrows():\n",
    "            ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                    color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "        # Plot the modulation index points\n",
    "        scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                            c=plotting_data['MeanFR_baseline'], cmap='hot', \n",
    "                            s=sizes, alpha=0.8, vmin=0, edgecolors='black', linewidths=0.5)\n",
    "        \n",
    "        # Add colorbar for firing rates\n",
    "        cbar = plt.colorbar(scatter, label='Firing Rate (Hz)')\n",
    "        max_fr = plotting_data['MeanFR_baseline'].max()\n",
    "        cbar.set_ticks([0, max_fr])\n",
    "        cbar.set_ticklabels(['0', f'{max_fr:.2f}'])\n",
    "\n",
    "        # Add a vertical line at x=0\n",
    "        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax.set_xlabel('Spontaneous MI')\n",
    "        ax.set_ylabel('Electrode Order')\n",
    "        ax.set_title(f'{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units from {group_name}')\n",
    "        ax.invert_yaxis()  # To match the order from the provided list\n",
    "\n",
    "        ax.set_xlim(-1.5, 1.5)\n",
    "        ax.grid(False)\n",
    "        ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        file_name = f'{group_name}_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_firing_rate_color_size_jittered_hot'\n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity is not None:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    def plot_modulation_index_with_firing_rate_color_and_size_groupcomparison(self, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_fr_threshold=0.001):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location for CTZ and No_CTZ groups side by side.\n",
    "        Firing rates are represented by both color and size of circles, with customizable size controls.\n",
    "        Circles have black outlines and are slightly offset horizontally to reduce overlap. \n",
    "        Horizontal lines connect each point to x=0.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "        size_multiplier (float): Multiplier for circle sizes. Default is 1.\n",
    "        min_size (float): Minimum size for circles with firing rates below the threshold. Default is 20.\n",
    "        min_fr_threshold (float): Firing rate threshold below which circles will have the minimum size. Default is 0.001.\n",
    "        \"\"\"\n",
    "        groups = ['CTZ', 'No_CTZ']\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(6, 8), sharey=True)\n",
    "\n",
    "        all_data = []\n",
    "        for group in groups:\n",
    "            data = self.prepare_plotting_data(df_name, group, is_single_unit, cell_type, stim_responsivity)\n",
    "            if data is not None and not data.empty:\n",
    "                all_data.append(data)\n",
    "\n",
    "        if not all_data:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Combine all data for global color scaling\n",
    "        combined_data = pd.concat(all_data)\n",
    "        \n",
    "        # Convert MeanFR_baseline to scalar values\n",
    "        combined_data['MeanFR_baseline'] = combined_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "        combined_data['MeanFR_baseline'] = pd.to_numeric(combined_data['MeanFR_baseline'], errors='coerce')\n",
    "        \n",
    "        max_fr = combined_data['MeanFR_baseline'].max()\n",
    "\n",
    "        # Define size calculation function\n",
    "        def calculate_size(fr):\n",
    "            if fr <= min_fr_threshold:\n",
    "                return min_size\n",
    "            else:\n",
    "                return min_size + (fr - min_fr_threshold) * size_multiplier\n",
    "\n",
    "        for ax, group in zip(axes, groups):\n",
    "            plotting_data = combined_data[combined_data['groupname'] == group]\n",
    "\n",
    "            if plotting_data.empty:\n",
    "                ax.text(0.5, 0.5, f'No data for {group}', ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "\n",
    "            # Add horizontal jitter\n",
    "            plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "            # Calculate sizes\n",
    "            plotting_data['sizes'] = plotting_data['MeanFR_baseline'].apply(calculate_size)\n",
    "\n",
    "            # Plot horizontal lines from x=0 to each point\n",
    "            for _, row in plotting_data.iterrows():\n",
    "                ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                        color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "            # Plot the modulation index points\n",
    "            scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                                c=plotting_data['MeanFR_baseline'], cmap='hot', \n",
    "                                s=plotting_data['sizes'], alpha=0.8, vmin=0, vmax=max_fr, \n",
    "                                edgecolors='black', linewidths=0.5)\n",
    "\n",
    "            # Add a vertical line at x=0\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "            ax.set_xlabel('Spontaneous MI')\n",
    "            ax.set_title(f'{group}\\n{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units')\n",
    "            ax.invert_yaxis()  # To match the order from the provided list\n",
    "            ax.set_xlim(-1.0, 1.0)\n",
    "            ax.grid(False)\n",
    "            ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Add a common y-label\n",
    "        fig.text(0.04, 0.5, 'Electrode Order', va='center', rotation='vertical')\n",
    "\n",
    "        # Add colorbar for firing rates\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "        cbar.set_label('Firing Rate (Hz)')\n",
    "        cbar.set_ticks([0, max_fr])\n",
    "        cbar.set_ticklabels(['0', f'{max_fr:.2f}'])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.9)  # Make room for the colorbar\n",
    "\n",
    "        # Save the plot\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        file_name = f'CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_firing_rate_color_size_jittered_hot'\n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity is not None:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    def plot_modulation_index_with_firing_rate_color_and_size_groupcomparison_evoked(self, df_name, stim_index=3, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_fr_threshold=0.001):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location for CTZ and No_CTZ groups side by side,\n",
    "        using evoked firing rates from MeanFR_stim.\n",
    "        Firing rates are represented by both color and size of circles, with a minimum size for rates below a threshold.\n",
    "        Circles have black outlines and are slightly offset horizontally to reduce overlap. \n",
    "        Horizontal lines connect each point to x=0.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        stim_index (int): Index of the stimulus intensity to use (0-3, where 3 is max stimulation). Default is 3.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "        size_multiplier (float): Multiplier for circle sizes. Default is 1.\n",
    "        min_size (float): Minimum size for circles with firing rates below the threshold. Default is 20.\n",
    "        min_fr_threshold (float): Firing rate threshold below which circles will have the minimum size. Default is 0.001.\n",
    "        \"\"\"\n",
    "        groups = ['CTZ', 'No_CTZ']\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 8), sharey=True)\n",
    "\n",
    "        all_data = []\n",
    "        for group in groups:\n",
    "            data = self.prepare_plotting_data(df_name, group, is_single_unit, cell_type, stim_responsivity)\n",
    "            if data is not None and not data.empty:\n",
    "                all_data.append(data)\n",
    "\n",
    "        if not all_data:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Combine all data for global color scaling\n",
    "        combined_data = pd.concat(all_data)\n",
    "        \n",
    "        # Extract the specified stim_index from MeanFR_stim\n",
    "        combined_data['EvokeFR'] = combined_data['MeanFR_stim'].apply(lambda x: x[stim_index] if isinstance(x, (list, np.ndarray)) and len(x) > stim_index else np.nan)\n",
    "        combined_data['EvokeFR'] = pd.to_numeric(combined_data['EvokeFR'], errors='coerce')\n",
    "        \n",
    "        max_fr = combined_data['EvokeFR'].max()\n",
    "\n",
    "        # Define size calculation function\n",
    "        def calculate_size(fr):\n",
    "            if fr <= min_fr_threshold:\n",
    "                return min_size\n",
    "            else:\n",
    "                return min_size + (fr - min_fr_threshold) * size_multiplier\n",
    "\n",
    "        for ax, group in zip(axes, groups):\n",
    "            plotting_data = combined_data[combined_data['groupname'] == group]\n",
    "\n",
    "            if plotting_data.empty:\n",
    "                ax.text(0.5, 0.5, f'No data for {group}', ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "\n",
    "            # Add horizontal jitter\n",
    "            plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "            # Calculate sizes\n",
    "            plotting_data['sizes'] = plotting_data['EvokeFR'].apply(calculate_size)\n",
    "\n",
    "            # Plot horizontal lines from x=0 to each point\n",
    "            for _, row in plotting_data.iterrows():\n",
    "                ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                        color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "            # Plot the modulation index points\n",
    "            scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                                c=plotting_data['EvokeFR'], cmap='hot', \n",
    "                                s=plotting_data['sizes'], alpha=0.8, vmin=0, vmax=max_fr, \n",
    "                                edgecolors='black', linewidths=0.5)\n",
    "\n",
    "            # Add a vertical line at x=0\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "            ax.set_xlabel('Spontaneous MI')\n",
    "            ax.set_title(f'{group}\\n{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units\\nStim Index: {stim_index}')\n",
    "            ax.invert_yaxis()  # To match the order from the provided list\n",
    "            ax.set_xlim(-1.0, 1.0)\n",
    "            ax.grid(False)\n",
    "            ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Add a common y-label\n",
    "        fig.text(0.04, 0.5, 'Electrode Order', va='center', rotation='vertical')\n",
    "\n",
    "        # Add colorbar for firing rates\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "        cbar.set_label('Evoked Firing Rate (Hz)')\n",
    "        cbar.set_ticks([0, max_fr])\n",
    "        cbar.set_ticklabels(['0', f'{max_fr:.2f}'])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.9)  # Make room for the colorbar\n",
    "\n",
    "        # Save the plot\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        file_name = f'CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_evoked_FR_stim{stim_index}_color_size_jittered_hot'\n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity is not None:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "    \n",
    "    def plot_optogenetic_psth_comparison(self, group1, group2, opto_freq, \n",
    "                                        ax=None, cell_type=None, is_single_unit=None, \n",
    "                                        stim_responsivity=None, modulation_label=None,\n",
    "                                        time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison for optogenetic experiments on the provided axes object \n",
    "        or creates a new figure if not provided.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # DataFrame to store all traces\n",
    "        all_traces_df = pd.DataFrame()\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group in zip([df1, df2], [group1, group2]):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = opto_freq\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "        # Set plot attributes\n",
    "        ax.set_title(f'PSTH Comparison of {opto_freq} between {group1} and {group2}')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Average Spike Rate')\n",
    "        ax.legend()\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "\n",
    "    def compare_optogenetic_groups(self, group1, group2, opto_freq, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Compares PSTH data between two specified groups for a given optogenetic stimulation frequency.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): The first group name for comparison.\n",
    "            group2 (str): The second group name for comparison.\n",
    "            opto_freq (str): The optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Specific cell type to filter by.\n",
    "            is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0).\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "            modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two pandas DataFrames representing the filtered data for each group.\n",
    "        \"\"\"\n",
    "        df_name = f'psth_dataframe_{opto_freq}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the optogenetic stimulation: {opto_freq}\")\n",
    "            return None, None\n",
    "\n",
    "        base_df = self.dataframes[df_name]\n",
    "\n",
    "        # Print the shape of the pre-filtered DataFrame\n",
    "        print(f\"Pre-filtered DataFrame shape: {base_df.shape}\")\n",
    "\n",
    "        # Define a helper function to create a mask for a given group\n",
    "        def create_mask(group):\n",
    "            mask = (base_df['groupname'] == group)\n",
    "            if cell_type is not None:\n",
    "                mask &= (base_df['Cell_Type'] == cell_type)\n",
    "            if is_single_unit is not None:\n",
    "                mask &= (base_df['IsSingleUnit'] == is_single_unit)\n",
    "            if stim_responsivity is not None:\n",
    "                mask &= (base_df['StimResponsivity'] == stim_responsivity)\n",
    "            if modulation_label is not None:\n",
    "                if modulation_label not in ['positive', 'negative', 'none']:\n",
    "                    raise ValueError(\"Modulation label must be one of 'positive', 'negative', or 'none'.\")\n",
    "                mask &= (base_df['ModulationIndex'] == modulation_label)\n",
    "            return mask\n",
    "\n",
    "        # Create masks for each group\n",
    "        mask1 = create_mask(group1)\n",
    "        mask2 = create_mask(group2)\n",
    "\n",
    "        # Filter the DataFrame for each group\n",
    "        channel1 = base_df[mask1]\n",
    "        channel2 = base_df[mask2]\n",
    "\n",
    "        # Print the shape of the filtered DataFrames\n",
    "        print(f\"Filtered df1 (group {group1}) shape: {channel1.shape}\")\n",
    "        print(f\"Filtered df2 (group {group2}) shape: {channel2.shape}\")\n",
    "\n",
    "        return channel1, channel2\n",
    "    \n",
    "    def plot_optogenetic_psth_comparison_subplot(self, group1, group2, opto_freq, \n",
    "                                                cell_type=None, is_single_unit=None, \n",
    "                                                stim_responsivity=None, modulation_label=None,\n",
    "                                                time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison for optogenetic experiments in a 1x2 subplot,\n",
    "        with each group plotted separately.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Create a new figure with 1x2 subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5), sharey=True)\n",
    "        axes = [ax1, ax2]\n",
    "\n",
    "        # DataFrame to store all traces\n",
    "        all_traces_df = pd.DataFrame()\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group, ax in zip([df1, df2], [group1, group2], axes):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = opto_freq\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "            # Set plot attributes for each subplot\n",
    "            ax.set_title(f'{group} - {opto_freq}')\n",
    "            ax.set_xlabel('Time (ms)')\n",
    "            ax.set_ylabel('Average Spike Rate')\n",
    "            ax.legend()\n",
    "\n",
    "        # Set overall title\n",
    "        fig.suptitle(f'PSTH Comparison of {opto_freq} between {group1} and {group2}', fontsize=16)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "\n",
    "    def plot_optogenetic_psth_comparison_subplot_with_led(self, group1, group2, opto_freq, \n",
    "                                                        cell_type=None, is_single_unit=None, \n",
    "                                                        stim_responsivity=None, modulation_label=None,\n",
    "                                                        time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison for optogenetic experiments in a 1x2 subplot,\n",
    "        with each group plotted separately and the LED signal on a shared third axis.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq]\n",
    "        led_array = led_array[time_mask]  # Apply the same time mask to LED signal\n",
    "\n",
    "        # Create a new figure with 1x2 subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5), sharey=True)\n",
    "        axes = [ax1, ax2]\n",
    "\n",
    "        # Create a third axis for LED signal, shared between the two subplots\n",
    "        ax_led = ax1.twinx()\n",
    "        ax2.twinx().set_ylabel('')  # Hide y-axis label for the second LED axis\n",
    "\n",
    "        # DataFrame to store all traces\n",
    "        all_traces_df = pd.DataFrame()\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group, ax in zip([df1, df2], [group1, group2], axes):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = opto_freq\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "            # Set plot attributes for each subplot\n",
    "            ax.set_title(f'{group} - {opto_freq}')\n",
    "            ax.set_xlabel('Time (ms)')\n",
    "            ax.set_ylabel('Average Spike Rate')\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "        # Plot LED signal on both subplots\n",
    "        ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "        ax2.twinx().plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "        # Set y-axis label for LED signal\n",
    "        ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "        ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Add legend for LED signal\n",
    "        ax_led.legend(loc='upper right')\n",
    "\n",
    "        # Set overall title\n",
    "        fig.suptitle(f'PSTH Comparison of {opto_freq} between {group1} and {group2}', fontsize=16)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "\n",
    "    def plot_optogenetic_psth_per_recording(self, group1, group2, opto_freq, \n",
    "                                            cell_type=None, is_single_unit=None, \n",
    "                                            stim_responsivity=None, modulation_label=None,\n",
    "                                            time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots PSTHs for optogenetic experiments, filtered at the recording level for each group,\n",
    "        saves the results in a specified directory structure, and ensures comparable y-axes between groups.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty and df2.empty:\n",
    "            print(\"Both groups have no data after filtering.\")\n",
    "            return\n",
    "        \n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq]\n",
    "        led_array = led_array[time_mask]  # Apply the same time mask to LED signal\n",
    "\n",
    "        # Base directory for saving figures\n",
    "        base_dir = \"/Volumes/MannySSD/figures/opto_psths_perrecording\"\n",
    "\n",
    "        # Find global y-axis limits\n",
    "        all_psth_data = pd.concat([df1['PSTHs_conv'], df2['PSTHs_conv']])\n",
    "        all_psth_data = all_psth_data.apply(lambda x: np.array(x)[time_mask])\n",
    "        global_ymin = all_psth_data.apply(np.min).min()\n",
    "        global_ymax = all_psth_data.apply(np.max).max()\n",
    "\n",
    "        # Process each group\n",
    "        for group, df in zip([group1, group2], [df1, df2]):\n",
    "            if df.empty:\n",
    "                print(f\"No data for group {group} after filtering.\")\n",
    "                continue\n",
    "\n",
    "            # Create group directory\n",
    "            group_dir = os.path.join(base_dir, group)\n",
    "            os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "            # Process each unique recording\n",
    "            for recording in df['recordingname'].unique():\n",
    "                recording_df = df[df['recordingname'] == recording]\n",
    "                \n",
    "                # Create recording directory\n",
    "                recording_dir = os.path.join(group_dir, recording)\n",
    "                os.makedirs(recording_dir, exist_ok=True)\n",
    "\n",
    "                # Create a new figure for this recording\n",
    "                fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "                # Plot PSTH data\n",
    "                data = recording_df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "                if smoothing_window:\n",
    "                    window = np.ones(smoothing_window) / smoothing_window\n",
    "                    data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "                mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "                if plot_mode == 'sem':\n",
    "                    sem = data.apply(pd.Series).sem(axis=0)\n",
    "                    ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)\n",
    "\n",
    "                elif plot_mode == 'traces':\n",
    "                    for trace in data:\n",
    "                        ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)\n",
    "\n",
    "                ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])\n",
    "\n",
    "                # Plot LED signal\n",
    "                ax_led = ax.twinx()\n",
    "                ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "\n",
    "                # Set plot attributes\n",
    "                ax.set_title(f'{group} - {opto_freq} - Recording: {recording}')\n",
    "                ax.set_xlabel('Time (ms)')\n",
    "                ax.set_ylabel('Average Spike Rate')\n",
    "                ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "                ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "                # Set y-axis limits to be the same for all plots\n",
    "                ax.set_ylim(global_ymin, global_ymax)\n",
    "\n",
    "                # Add legends\n",
    "                ax.legend(loc='upper left')\n",
    "                ax_led.legend(loc='upper right')\n",
    "\n",
    "                # Save the figure\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Create filename with user input parameters\n",
    "                filename = f'PSTH_{opto_freq}'\n",
    "                if cell_type:\n",
    "                    filename += f'_celltype-{cell_type}'\n",
    "                if is_single_unit is not None:\n",
    "                    filename += f'_singleunit-{is_single_unit}'\n",
    "                if stim_responsivity is not None:\n",
    "                    filename += f'_stimresp-{stim_responsivity}'\n",
    "                if time_range:\n",
    "                    filename += f'_timerange-{time_range[0]}to{time_range[1]}'\n",
    "                filename += '.png'\n",
    "                \n",
    "                fig_path = os.path.join(recording_dir, filename)\n",
    "                plt.savefig(fig_path)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"Saved figure for {group} - {recording} at {fig_path}\")\n",
    "\n",
    "        print(\"Finished processing all recordings.\")\n",
    "\n",
    "    def plot_optogenetic_psth_comparison_grid(self, group1, group2, opto_freq, \n",
    "                                            cell_types=['RS', 'FS'], is_single_unit=None, \n",
    "                                            stim_responsivity=None, modulation_label=None,\n",
    "                                            time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a 2x2 grid PSTH comparison for optogenetic experiments,\n",
    "        with each subplot showing a specific group and cell type combination.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name (e.g., 'CTZ').\n",
    "            group2 (str): Second group name (e.g., 'No_CTZ').\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_types (list): List of cell types to plot (default: ['RS', 'FS']).\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            group1: '#5a00c2',  # CTZ\n",
    "            group2: '#797979'   # No_CTZ\n",
    "        }\n",
    "\n",
    "        # Create a new figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16), sharex=True, sharey=True)\n",
    "\n",
    "        # Define the mapping of group and cell type to subplot position\n",
    "        plot_mapping = {\n",
    "            (group1, 'RS'): (0, 0),  # Top left\n",
    "            (group2, 'RS'): (0, 1),  # Top right\n",
    "            (group1, 'FS'): (1, 0),  # Bottom left\n",
    "            (group2, 'FS'): (1, 1)   # Bottom right\n",
    "        }\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq][time_mask]\n",
    "\n",
    "        # Process and plot data for each combination of group and cell type\n",
    "        for (group, cell_type), (row, col) in plot_mapping.items():\n",
    "            ax = axes[row, col]\n",
    "\n",
    "            # Fetch data for the current group and cell type\n",
    "            df = self.compare_optogenetic_groups(group, group, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)[0]\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data after filtering for {group} - {cell_type}.\")\n",
    "                continue\n",
    "\n",
    "            # Create a third axis for LED signal\n",
    "            ax_led = ax.twinx()\n",
    "\n",
    "            # Process data\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "            # Plot LED signal\n",
    "            ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "\n",
    "            # Set plot attributes\n",
    "            ax.set_title(f'{group} - {cell_type}')\n",
    "            if row == 1:  # Bottom row\n",
    "                ax.set_xlabel('Time (ms)')\n",
    "            if col == 0:  # Left column\n",
    "                ax.set_ylabel('Average Spike Rate')\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "            # Set y-axis label for LED signal only for the rightmost subplots\n",
    "            if col == 1:\n",
    "                ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "                ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "            else:\n",
    "                ax_led.set_ylabel('')\n",
    "\n",
    "        # Set overall title\n",
    "        fig.suptitle(f'PSTH Comparison of {opto_freq} between {group1} and {group2} for {\" and \".join(cell_types)} cells', fontsize=16)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #save the plot in the normal directory\n",
    "        plt.savefig(f'/Volumes/MannySSD/figures/opto_psths_comparison/CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_evoked_FR.svg', format='svg', dpi=300, bbox_inches='tight', transparent=True)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def plot_interactive_psth_comparison_grid(self, group1, group2, opto_freq, \n",
    "                                            cell_types=['RS', 'FS'], is_single_unit=None, \n",
    "                                            stim_responsivity=None, modulation_label=None,\n",
    "                                            time_range=None, smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots an interactive 2x2 grid PSTH comparison for optogenetic experiments using Plotly,\n",
    "        with each subplot showing a specific group and cell type combination, and LED signal on a separate axis.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name (e.g., 'CTZ').\n",
    "            group2 (str): Second group name (e.g., 'No_CTZ').\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_types (list): List of cell types to plot (default: ['RS', 'FS']).\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-200, 800)).\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            group1: 'rgba(90, 0, 194, 0.7)',  # CTZ (semi-transparent purple)\n",
    "            group2: 'rgba(121, 121, 121, 0.7)'  # No_CTZ (semi-transparent gray)\n",
    "        }\n",
    "\n",
    "        # Create subplot titles\n",
    "        subplot_titles = [\n",
    "            f'{group1} - RS', f'{group2} - RS',\n",
    "            f'{group1} - FS', f'{group2} - FS'\n",
    "        ]\n",
    "\n",
    "        # Create a 2x2 subplot layout with secondary y-axes\n",
    "        fig = make_subplots(rows=2, cols=2, shared_xaxes=True, shared_yaxes=True,\n",
    "                            subplot_titles=subplot_titles,\n",
    "                            specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n",
    "                                [{\"secondary_y\": True}, {\"secondary_y\": True}]])\n",
    "\n",
    "        # Define the mapping of group and cell type to subplot position\n",
    "        plot_mapping = {\n",
    "            (group1, 'RS'): (1, 1),  # Top left\n",
    "            (group2, 'RS'): (1, 2),  # Top right\n",
    "            (group1, 'FS'): (2, 1),  # Bottom left\n",
    "            (group2, 'FS'): (2, 2)   # Bottom right\n",
    "        }\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq][time_mask]\n",
    "\n",
    "        # Process and plot data for each combination of group and cell type\n",
    "        for (group, cell_type), (row, col) in plot_mapping.items():\n",
    "            # Fetch data for the current group and cell type\n",
    "            df = self.compare_optogenetic_groups(group, group, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)[0]\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data after filtering for {group} - {cell_type}.\")\n",
    "                continue\n",
    "\n",
    "            # Process data\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            # Plot individual traces\n",
    "            for idx, (trace, groupname, recordingname, cid) in enumerate(zip(data, df['groupname'], df['recordingname'], df['cid'])):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=time_array,\n",
    "                        y=trace,\n",
    "                        mode='lines',\n",
    "                        line=dict(color=group_colors[group]),\n",
    "                        name=f'{group} - {cell_type}',\n",
    "                        legendgroup=f'{group} - {cell_type}',\n",
    "                        showlegend=idx == 0,  # Show legend only for the first trace of each group\n",
    "                        hoverinfo='text',\n",
    "                        hovertext=[f'Group: {groupname}<br>Recording: {recordingname}<br>CID: {cid}<br>Time: {t:.2f} ms<br>Rate: {r:.2f}' for t, r in zip(time_array, trace)],\n",
    "                        customdata=np.column_stack((np.full(len(time_array), groupname), \n",
    "                                                    np.full(len(time_array), recordingname), \n",
    "                                                    np.full(len(time_array), cid)))\n",
    "                    ),\n",
    "                    row=row, col=col, secondary_y=False\n",
    "                )\n",
    "\n",
    "            # Plot mean PSTH\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=time_array,\n",
    "                    y=mean_psth,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='black', width=2),\n",
    "                    name=f'Mean {group} - {cell_type}',\n",
    "                    legendgroup=f'Mean {group} - {cell_type}',\n",
    "                ),\n",
    "                row=row, col=col, secondary_y=False\n",
    "            )\n",
    "\n",
    "            # Plot LED signal on secondary y-axis\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=time_array,\n",
    "                    y=led_array,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='blue', width=1.5),\n",
    "                    name='LED Signal',\n",
    "                    legendgroup='LED Signal',\n",
    "                    showlegend=row == 1 and col == 1,  # Show legend only once for LED signal\n",
    "                ),\n",
    "                row=row, col=col, secondary_y=True\n",
    "            )\n",
    "\n",
    "            # Update axes\n",
    "            fig.update_xaxes(title_text=\"Time (ms)\", row=row, col=col)\n",
    "            fig.update_yaxes(title_text=\"Average Spike Rate\" if col == 1 else \"\", row=row, col=col, secondary_y=False)\n",
    "            fig.update_yaxes(title_text=\"LED Signal (a.u.)\" if col == 2 else \"\", row=row, col=col, secondary_y=True)\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'PSTH Comparison of {opto_freq} between {group1} and {group2} for {\" and \".join(cell_types)} cells',\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "\n",
    "        # Add custom JavaScript for trace highlighting\n",
    "        fig.update_layout(\n",
    "            updatemenus=[\n",
    "                dict(\n",
    "                    type=\"buttons\",\n",
    "                    direction=\"left\",\n",
    "                    buttons=[\n",
    "                        dict(\n",
    "                            args=[{\"visible\": [True] * len(fig.data)}],\n",
    "                            label=\"Show All\",\n",
    "                            method=\"restyle\"\n",
    "                        ),\n",
    "                        dict(\n",
    "                            args=[{\"visible\": \"legendonly\"}],\n",
    "                            label=\"Hide All\",\n",
    "                            method=\"restyle\"\n",
    "                        )\n",
    "                    ],\n",
    "                    pad={\"r\": 10, \"t\": 10},\n",
    "                    showactive=False,\n",
    "                    x=0.11,\n",
    "                    xanchor=\"left\",\n",
    "                    y=1.1,\n",
    "                    yanchor=\"top\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "def extract_array_element(value, array_element=0):\n",
    "    \"\"\"\n",
    "    Extracts the scalar value from a numpy array or list.\n",
    "    \n",
    "    Parameters:\n",
    "    value: The value from which to extract the element. This can be of any type, \n",
    "           but we are particularly interested in cases where it's a numpy array or list.\n",
    "    array_element (int): The index of the element to extract from the array or list (default is 0).\n",
    "    \n",
    "    Returns:\n",
    "    The extracted element if the value is a numpy array or list. \n",
    "    If the value is a scalar within a numpy array, it extracts and returns the scalar.\n",
    "    Otherwise, it returns the original value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the value is a numpy array with a single element, extract that element.\n",
    "    if isinstance(value, np.ndarray) and value.size == 1:\n",
    "        return value.item()\n",
    "    \n",
    "    # If the value is a numpy array or list with multiple elements, extract the specified element.\n",
    "    elif isinstance(value, (np.ndarray, list)) and len(value) > array_element:\n",
    "        return value[array_element]\n",
    "    \n",
    "    # If the value is not a numpy array or list, or if it's not possible to extract a valid element, \n",
    "    # return the value as-is.\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "def determine_stim_responsivity(row):\n",
    "    \"\"\"\n",
    "    Determine the StimResponsivity based on the lower bound of the AUROC confidence interval and ModulationIndex.\n",
    "    \n",
    "    Parameters:\n",
    "    row (Series): A row of the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    int: StimResponsivity value (1 for increase/positive modulation, -1 for decrease/negative modulation, 0 for no significant change).\n",
    "    \"\"\"\n",
    "    # Access the lower bound of the AUROC confidence interval\n",
    "    auroc_lower_bound = row['StimProb'][1] if len(row['StimProb']) > 1 else None\n",
    "    modulation_index = row['ModulationIndex']\n",
    "    \n",
    "    print(f\"AUROC lower bound: {auroc_lower_bound}, Modulation Index: {modulation_index}\")\n",
    "\n",
    "    # Step 1: Determine if the neuron is sensory responsive\n",
    "    if auroc_lower_bound is not None and auroc_lower_bound > 0.5:\n",
    "        # Step 2: Categorize modulation if the neuron is sensory responsive\n",
    "        if modulation_index == 'positive':\n",
    "            return 1  # Sensory responsive and positively modulated (Increase)\n",
    "        elif modulation_index == 'negative':\n",
    "            return -1  # Sensory responsive and negatively modulated (Decrease)\n",
    "    return 0  # No significant change or non-responsive\n",
    "\n",
    "def example_analysis_function(extracted_data):\n",
    "    \"\"\"\n",
    "    Example analysis function that calculates the mean spike count for each stimulus.\n",
    "\n",
    "    Args:\n",
    "        extracted_data (dict): The extracted data from the PSTH and raster data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results = {'mean_spike_counts': {}}\n",
    "    for stim, data in extracted_data['data'].items():\n",
    "        mean_spike_count = np.mean(data['raw_counts'])\n",
    "        analysis_results['mean_spike_counts'][stim] = mean_spike_count\n",
    "    return analysis_results\n",
    "\n",
    "def analyze_sensory_responsiveness(extracted_data, baseline_window=(-500, 0), response_window=(0, 50), analysis_window=(0, 200)):\n",
    "    \"\"\"\n",
    "    Analyze the sensory responsiveness of neurons by calculating the sensory response magnitude, z-scoring it,\n",
    "    and performing a nonparametric permutation test to determine if a neuron is modulated by the stimulus.\n",
    "\n",
    "    Args:\n",
    "        extracted_data (dict): The extracted data from the PSTH and raster data.\n",
    "        baseline_window (tuple): The window (in ms) to calculate the baseline firing rate. Default is (-500, 0).\n",
    "        response_window (tuple): The window (in ms) to calculate the response magnitude. Default is (0, 50).\n",
    "        analysis_window (tuple): The window (in ms) to analyze modulation. Default is (0, 200).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the sensory response analysis results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for stim, data in extracted_data['data'].items():\n",
    "        spike_trains = data['spike_trains']\n",
    "        time_array = data['time_array']\n",
    "\n",
    "        # Calculate baseline firing rate\n",
    "        baseline_start_idx = np.searchsorted(time_array, baseline_window[0])\n",
    "        baseline_end_idx = np.searchsorted(time_array, baseline_window[1])\n",
    "        baseline_rates = spike_trains[:, baseline_start_idx:baseline_end_idx].mean(axis=1)\n",
    "\n",
    "        # Calculate response magnitude\n",
    "        response_start_idx = np.searchsorted(time_array, response_window[0])\n",
    "        response_end_idx = np.searchsorted(time_array, response_window[1])\n",
    "        response_rates = spike_trains[:, response_start_idx:response_end_idx].mean(axis=1)\n",
    "        response_magnitude = response_rates.mean() - baseline_rates.mean()\n",
    "\n",
    "        # Calculate z-scored magnitude\n",
    "        baseline_sd = baseline_rates.std()\n",
    "        z_scored_magnitude = response_magnitude / baseline_sd if baseline_sd != 0 else np.nan\n",
    "\n",
    "        # Permutation test for modulation analysis\n",
    "        analysis_start_idx = np.searchsorted(time_array, analysis_window[0])\n",
    "        analysis_end_idx = np.searchsorted(time_array, analysis_window[1])\n",
    "        pre_stimulus_rates = spike_trains[:, baseline_start_idx:baseline_end_idx].mean(axis=1)\n",
    "        post_stimulus_rates = spike_trains[:, analysis_start_idx:analysis_end_idx].mean(axis=1)\n",
    "\n",
    "        def permutation_test_func(x, y):\n",
    "            return x.mean() - y.mean()\n",
    "\n",
    "        permutation_result = permutation_test((pre_stimulus_rates, post_stimulus_rates),\n",
    "                                              statistic=permutation_test_func,\n",
    "                                              n_resamples=10000,\n",
    "                                              alternative='two-sided')\n",
    "\n",
    "        results[stim] = {\n",
    "            'response_magnitude': response_magnitude,\n",
    "            'z_scored_magnitude': z_scored_magnitude,\n",
    "            'permutation_p_value': permutation_result.pvalue\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import mat73\n",
    "class CCGHandler:\n",
    "    def __init__(self, eed_object, ccg_directory):\n",
    "        \"\"\"\n",
    "        Initializes the CCGHandler class with an object of ExtractEphysData and a directory for CCG files.\n",
    "\n",
    "        Parameters:\n",
    "            eed_object (ExtractEphysData): An instance of ExtractEphysData which provides\n",
    "                                           access to electrophysiological data methods and attributes.\n",
    "            \n",
    "            ccg_directory (str): The directory path where the CCG files are stored.\n",
    "        \"\"\"\n",
    "        self.eed = eed_object\n",
    "        self.ccg_directory = ccg_directory\n",
    "        self.ccg = {}\n",
    "        self.load_ccg_files()\n",
    "        self.ccg['MonoConnectionsTable'] = self.unpack_mono_connections()\n",
    "        self.integrate_ccgs() # Integrate CCG data into the MonoConnectionsTable DataFrame and delete the MonoConnCCGs data attribute\n",
    "    \n",
    "    def load_ccg_files(self):\n",
    "        \"\"\"\n",
    "        Loads all required .mat files from the specified CCG directory and sets them as attributes under the `ccg` dictionary.\n",
    "\n",
    "        Required Files:\n",
    "            MonoConnCCGs.mat - Contains CCG data for monosynaptic connections.\n",
    "            \n",
    "            MonoConnectionsTable.mat - Contains a struct MonoConnectionsTable converted into a struct by group,\n",
    "            and recording all connection data; accessed using the key 'nested_struct'.\n",
    "            \n",
    "            PointwiseABs.mat - Contains 95% pointwise acceptacne bands for CCG.\n",
    "            SimultaneousABs.mat - Contains 95% pointwise acceptacne bands for CCG.\n",
    "            \n",
    "            t.mat - Contains the time array for the x axis of the CCG.\n",
    "\n",
    "        The data from each file is accessed by stripping the '.mat' and accessing the corresponding key in the loaded dictionary, except for specific exceptions noted.\n",
    "        \"\"\"\n",
    "        required_files = [\n",
    "            'MonoConnCCGs.mat',\n",
    "            'MonoConnectionsTable.mat',\n",
    "            'PointwiseABs.mat',\n",
    "            'SimultaneousABs.mat',\n",
    "            't.mat'\n",
    "        ]\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(self.ccg_directory):\n",
    "            print(f\"CCG directory not found: {self.ccg_directory}\")\n",
    "            return\n",
    "\n",
    "        # Load each required .mat file from the directory\n",
    "        for file_name in required_files:\n",
    "            file_path = os.path.join(self.ccg_directory, file_name)\n",
    "            print(f\"Loading CCG data from: {file_path}\")\n",
    "            try:\n",
    "                # Load the file and extract the data using the base filename as the key\n",
    "                data = mat73.loadmat(file_path)\n",
    "                base_key = file_name.replace('.mat', '')\n",
    "                # Handle special case for 'MonoConnectionsTable'\n",
    "                if base_key == 'MonoConnectionsTable':\n",
    "                    self.ccg[base_key] = data['nested_struct']\n",
    "                elif base_key in data:\n",
    "                    self.ccg[base_key] = data[base_key]\n",
    "                else:\n",
    "                    print(f\"Expected key '{base_key}' not found in {file_name}\")\n",
    "                    self.ccg[base_key] = None\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                self.ccg[base_key] = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "                self.ccg[base_key] = None\n",
    "    \n",
    "    def unpack_mono_connections(self):\n",
    "        \"\"\"\n",
    "        Unpacks the MonoConnectionsTable and assembles the data into a comprehensive DataFrame.\n",
    "        This method iterates over each group and recording in the MonoConnectionsTable, which\n",
    "        is structured by group and recording name. Each entry represents connectivity data between\n",
    "        neuron pairs, including various metrics and identifiers.\n",
    "\n",
    "        The resulting DataFrame compiles all the connectivity data across different groups and recordings,\n",
    "        with scalar values extracted from lists for each metric, providing a holistic view of the monosynaptic connections analyzed.\n",
    "\n",
    "        Adjustments:\n",
    "        - Converts 'Significance' from [1.0, 0.0] to True/False.\n",
    "        - Maps 'StimResp_A' and 'StimResp_B' from 'nr', '-', '+' to 0, -1, 1 respectively for easier filtering.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame containing all the unpacked connectivity data with scalar values.\n",
    "        \"\"\"\n",
    "        # Initialize an empty list to collect data\n",
    "        all_data = []\n",
    "\n",
    "        # Mapping for stimulus response\n",
    "        stim_resp_mapping = {'nr': 0, '-': -1, '+': 1}\n",
    "\n",
    "        # Navigate through each group and recording\n",
    "        for groupname, recordings in self.ccg['MonoConnectionsTable'].items():\n",
    "            for recording, data in recordings.items():\n",
    "                for item in data:\n",
    "                    # Flatten the dictionary and add groupname and recording info\n",
    "                    item_data = {}\n",
    "                    for key, val in item.items():\n",
    "                        if isinstance(val, list) and len(val) == 1:\n",
    "                            if key == 'Significance':\n",
    "                                # Convert Significance to boolean\n",
    "                                item_data[key] = bool(val[0])\n",
    "                            elif key in ['StimResp_A', 'StimResp_B']:\n",
    "                                # Map StimResp_A and StimResp_B\n",
    "                                item_data[key] = stim_resp_mapping[val[0]]\n",
    "                            else:\n",
    "                                item_data[key] = val[0]\n",
    "                        else:\n",
    "                            item_data[key] = val\n",
    "                    item_data['groupname'] = groupname\n",
    "                    item_data['recording'] = recording\n",
    "                    all_data.append(item_data)\n",
    "        return pd.DataFrame(all_data)\n",
    "      \n",
    "    def integrate_ccgs(self):\n",
    "        \"\"\"\n",
    "        Integrates the CCG data from 'MonoConnCCGs' into the 'MonoConnectionsTable' DataFrame.\n",
    "        Each row in 'MonoConnectionsTable' corresponds to a column in 'MonoConnCCGs', which contains\n",
    "        the time points for the CCGs in 1ms bins. This method adds these CCGs as a new column 'ccgs' in\n",
    "        the DataFrame, ensuring each pair's CCGs are correctly aligned with the DataFrame rows.\n",
    "\n",
    "        Post-process:\n",
    "        - Deletes the original 'MonoConnCCGs' data from the 'ccg' attribute to save memory and avoid redundancy.\n",
    "        \"\"\"\n",
    "        # Retrieve the DataFrame and the CCG data\n",
    "        df = self.ccg['MonoConnectionsTable']\n",
    "        ccgs_data = self.ccg['MonoConnCCGs']\n",
    "\n",
    "        # Ensure that the rows of the DataFrame match the columns in the CCGs data\n",
    "        if df.shape[0] != ccgs_data.shape[1]:\n",
    "            raise ValueError(\"Mismatch between DataFrame rows and CCG data columns.\")\n",
    "\n",
    "        # Add the CCGs as a new column to the DataFrame\n",
    "        df['ccgs'] = [ccgs_data[:, i] for i in range(ccgs_data.shape[1])]\n",
    "\n",
    "        # Update the 'MonoConnectionsTable' with the new DataFrame\n",
    "        self.ccg['MonoConnectionsTable'] = df\n",
    "\n",
    "        # Remove the 'MonoConnCCGs' data\n",
    "        del self.ccg['MonoConnCCGs']\n",
    "        \n",
    "    def access_ccgs(self, groupname=None, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, limit=None):\n",
    "        \"\"\"\n",
    "        Accesses CCGs from 'MonoConnectionsTable' based on specified criteria.\n",
    "        Returns a filtered DataFrame containing CCGs that meet the specified criteria.\n",
    "\n",
    "        Parameters:\n",
    "            groupname (str or None): The group name to filter by; defaults to None, which includes all groups.\n",
    "            significance (bool): Filter connections by significance; defaults to True, meaning only significant connections are included.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs\n",
    "            EorI (list of str or None): Filter connections by a list of excitatory/inhibitory statuses; defaults to None, which includes all statuses (e.g 'E', 'I', 'NS', 'S').\n",
    "            connectiontype (str, list of str, or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS' or ['FS->RS', 'RS->FS']); defaults to None, which includes all types.\n",
    "            response (str or None): Filters connections based on responsiveness; 'responsive' for either StimResp_A being 1 or 'nonresponsive' for both being 0.\n",
    "            layers (list of str or None): Filter connections by layer (e.g., 'IG->L4', 'L4->L4'); defaults to None, which includes all layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include; defaults to 150.\n",
    "            threshold (bool): Filter connections by threshold; defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A filtered DataFrame containing CCGs that meet the specified criteria.\n",
    "        \"\"\"\n",
    "        df = self.ccg['MonoConnectionsTable']\n",
    "\n",
    "        # Filter by group name if specified\n",
    "        if groupname is not None:\n",
    "            df = df[df['groupname'] == groupname]\n",
    "\n",
    "        # Filter by significance\n",
    "        if significance:\n",
    "            df = df[df['Significance'] == True]\n",
    "\n",
    "        # Filter by it being sig based on 7*SD of peak relative to noise \n",
    "        if sd_sig: \n",
    "            df = df[df['sd_sig'] == True]\n",
    "\n",
    "        # Filter by EorI status if specified\n",
    "        if EorI is not None:\n",
    "            df = df[df['EorI'].isin(EorI)]\n",
    "\n",
    "        # Filter by connection type if specified\n",
    "        if connectiontype is not None:\n",
    "            if isinstance(connectiontype, list):\n",
    "                df = df[df['CellTypes'].isin(connectiontype)]\n",
    "            else:\n",
    "                df = df[df['CellTypes'] == connectiontype]\n",
    "\n",
    "        # Filter by response if specified\n",
    "        if response:\n",
    "            if response == 'responsive':\n",
    "                df = df[(df['StimResp_A'] == 1)]\n",
    "            elif response == 'nonresponsive':\n",
    "                df = df[(df['StimResp_A'] == 0)]\n",
    "\n",
    "        # Filter by layers if specified\n",
    "        if layers is not None:\n",
    "            df = df[df['layers'].isin(layers)]\n",
    "\n",
    "        # Filter by threshold if specified\n",
    "        if limit:\n",
    "            df = df[df['threshold'] == limit]\n",
    "\n",
    "        # Filter by minimum number of spike pairs\n",
    "        df = df[df['N_SpikePairs'] >= min_spike_pairs]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def plot_ccgs(self, groupname=None, significance=True, EorI=None, connectiontype=None):\n",
    "        \"\"\"\n",
    "        Assembles the CCGs for connections based on specified criteria and plots\n",
    "        individual CCG traces. Returns the DataFrame containing the filtered connections.\n",
    "        \n",
    "        Parameters:\n",
    "            groupname (str or None): The group name to filter by; defaults to None.\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses (e.g 'E', 'I', 'NS', 'S').\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "        \"\"\"\n",
    "        # Access the filtered data\n",
    "        df = self.access_ccgs(groupname=groupname, significance=significance, EorI=EorI, connectiontype=connectiontype)\n",
    "\n",
    "        # Retrieve the 'ccgs' column which contains the CCG data arrays\n",
    "        ccgs_data = df['ccgs'].tolist() # Convert the column to a list of arrays which represent the CCGs of each connection\n",
    "        #calculate the mean of the ccgs\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "\n",
    "        # Plotting each CCG\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        for ccg in ccgs_data:\n",
    "            ax.plot(time_points, ccg)\n",
    "        ax.set_title('CCGs Plot')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_mean_ccgs(self, groupname=None, significance=True, EorI=None, connectiontype=None):\n",
    "        \"\"\"\n",
    "        Plots mean CCGs for connections based on specified criteria.\n",
    "        \n",
    "        Parameters:\n",
    "            groupname (str or None): The group name to filter by; defaults to None.\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses (e.g 'E', 'I', 'NS', 'S').\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "        \"\"\"\n",
    "        # Access the filtered data\n",
    "        df = self.access_ccgs(groupname=groupname, significance=significance, EorI=EorI, connectiontype=connectiontype)\n",
    "\n",
    "        # Retrieve the 'ccgs' column which contains the CCG data arrays\n",
    "        ccgs_data = df['ccgs'].tolist() # Convert the column to a list of arrays which represent the CCGs of each connection\n",
    "        \n",
    "        #calculate the mean of the ccgs\n",
    "        mean_ccg = np.mean(ccgs_data, axis=0)\n",
    "        #calculte the SEM of the ccgs\n",
    "        sem_ccg = np.std(ccgs_data, axis=0) / np.sqrt(len(ccgs_data))\n",
    "\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "        \n",
    "\n",
    "        # Plotting the mean CCG\n",
    "        fig, ax = plt.subplots(figsize=(10, 6)) #plot the mean CCG and shade the SEM region\n",
    "        ax.plot(time_points, mean_ccg, color='b', label='Mean CCG')\n",
    "        ax.fill_between(time_points, mean_ccg-sem_ccg, mean_ccg+sem_ccg, color='b', alpha=0.2, label='SEM')\n",
    "        ax.set_title('Mean CCGs Plot')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_group_ccgs(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, xlim=None, front_group='CTZ',limit=None, directory=None, file_name=None):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Plots overlaid mean CCGs for the 'No_CTZ' and 'CTZ' groups with their SEM as shaded regions.\n",
    "        The SEM shading uses a lighter version of the group colors.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses.\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "            response (str or None): Filters connections based on responsiveness; 'responsive' for either StimResp_A being 1 or 'nonresponsive' for both being 0.\n",
    "            layers (list of str or None): Filter connections by layer (e.g., 'IG->L4', 'L4->L4'); defaults to None, which includes all layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include; defaults to 150.\n",
    "            xlim (tuple of int or None): The x-axis limits for the plot; defaults to None.\n",
    "            \n",
    "        \"\"\"\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        # Define lighter versions of the colors for SEM shading\n",
    "        sem_colors = {\n",
    "            'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "            'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5)) #figsize is (width, height)\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            ccgs_data = df['ccgs'].tolist()\n",
    "\n",
    "            if len(ccgs_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "            \n",
    "            mean_ccg = np.mean(ccgs_data, axis=0)\n",
    "            sem_ccg = np.std(ccgs_data, axis=0) / np.sqrt(len(ccgs_data))\n",
    "\n",
    "            # Plotting the mean CCG\n",
    "            ax.plot(time_points, mean_ccg, color=group_colors[group], label=f'Mean CCG {group}', linewidth=2)\n",
    "            ax.fill_between(time_points, mean_ccg - sem_ccg, mean_ccg + sem_ccg, color=sem_colors[group], label=f'SEM {group}')\n",
    "            \n",
    "\n",
    "        ax.set_title('Comparison of Mean CCGs by Group')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "        \n",
    "        #change the x-axis limits if specified\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #hide legend\n",
    "        ax.legend().set_visible(False)\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def prepare_for_boxplot_ccg(self, significance=True, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, xlim=None):\n",
    "        \"\"\"\n",
    "        Organizes data into a DataFrame suitable for plotting boxplots by unique 'CellTypes' and group.\n",
    "        Uses filtered data from 'access_ccgs' method based on provided parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses.\n",
    "            connectiontype (str or None): Filter connections by type; defaults to None, which includes all types.\n",
    "        \n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with columns for the value to plot, 'CellTypes', and 'Group'.\n",
    "        \"\"\"\n",
    "        boxplot_data = []\n",
    "\n",
    "        # Access filtered data using the 'access_ccgs' method\n",
    "        df = self.access_ccgs(significance=significance, EorI=EorI, connectiontype=connectiontype)\n",
    "\n",
    "        if not df.empty:\n",
    "            unique_cell_types = df['CellTypes'].unique()  # Get unique 'CellTypes'\n",
    "            for cell_type in unique_cell_types:\n",
    "                # Filter DataFrame for each cell type\n",
    "                cell_type_df = df[df['CellTypes'] == cell_type]\n",
    "                \n",
    "                # Collect data for each cell type entry\n",
    "                for index, row in cell_type_df.iterrows():\n",
    "                    boxplot_data.append({\n",
    "                        'Value': row['ccgs'],  # Adjust this key as needed\n",
    "                        'CellTypes': cell_type,\n",
    "                        'Group': row['group']  # Assuming 'group' column exists\n",
    "                    })\n",
    "\n",
    "        # Convert list of data to DataFrame\n",
    "        boxplot_df = pd.DataFrame(boxplot_data)\n",
    "        return boxplot_df\n",
    "\n",
    "    def plot_group_ccgs_traces(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, xlim=None, front_group='CTZ', limit=None):\n",
    "        \"\"\"\n",
    "        Plots overlaid mean CCGs for the 'No_CTZ' and 'CTZ' groups along with individual CCG traces.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses.\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "            response (str or None): Filters connections based on responsiveness; 'responsive' for either StimResp_A being 1 or 'nonresponsive' for both being 0.\n",
    "            layers (list of str or None): Filter connections by layer (e.g., 'IG->L4', 'L4->L4'); defaults to None, which includes all layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include; defaults to 150.\n",
    "            xlim (tuple of int or None): The x-axis limits for the plot; defaults to None.\n",
    "        \"\"\"\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            ccgs_data = df['ccgs'].tolist()\n",
    "\n",
    "            if len(ccgs_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "            \n",
    "            mean_ccg = np.mean(ccgs_data, axis=0)\n",
    "\n",
    "            # Plotting the mean CCG\n",
    "            ax.plot(time_points, mean_ccg, color=group_colors[group], label=f'Mean CCG {group}', linewidth=3)\n",
    "\n",
    "            # Plotting individual traces\n",
    "            for trace in ccgs_data:\n",
    "                ax.plot(time_points, trace, color=group_colors[group], alpha=0.3, linewidth=0.5)  # Lower alpha for individual traces\n",
    "\n",
    "        ax.set_title('Comparison of Mean CCGs by Group with Individual Traces')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "\n",
    "        # Change the x-axis limits if specified\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "\n",
    "        # Hide legend\n",
    "        ax.legend().set_visible(False)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_box_and_strip_ccg(self, groups=None, cell_types=None, show_outliers=True, hue_order=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and cell types, with color adjustments made directly in the plotting calls.\n",
    "\n",
    "        Args:\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            cell_types (list of str, optional): List of cell types to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: mcolors.to_rgba(v, alpha=0.5) for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Prepare data for boxplot\n",
    "        boxplot_df = self.prepare_for_boxplot_ccg()\n",
    "\n",
    "        # Filter by specified groups and cell types\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if cell_types:\n",
    "            boxplot_df = boxplot_df[boxplot_df['CellTypes'].isin(cell_types)]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='CellTypes', y='Value', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(cell_types)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=boxplot_df, x='CellTypes', y='Value', hue='Group', **stripplot_kwargs)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Values Across Groups and Cell Types')\n",
    "        plt.ylabel('Value')\n",
    "        plt.xlabel('Cell Type')\n",
    "        ax.legend(title='Group')\n",
    "        plt.show()\n",
    "        \n",
    "    def is_significant_ccg(self, sd_multiplier=7):\n",
    "        # Access the time array\n",
    "        t = self.ccg['t']\n",
    "        \n",
    "        # Create an empty list to store significance results temporarily\n",
    "        significance_results = []\n",
    "\n",
    "        # Iterate over all CCG entries\n",
    "        for ccg_values in self.ccg['MonoConnectionsTable']['ccgs']:\n",
    "            # Define the noise distribution from the flanks of the CCG\n",
    "            noise_mask = (np.abs(t) >= 50) & (np.abs(t) <= 100)\n",
    "            noise_values = ccg_values[noise_mask]\n",
    "\n",
    "            # Calculate the mean and standard deviation of the noise distribution\n",
    "            noise_mean = np.mean(noise_values)\n",
    "            noise_std = np.std(noise_values)\n",
    "\n",
    "            # Define the peak detection region within ±10ms of zero\n",
    "            peak_mask = (t >= -10) & (t <= 10)\n",
    "            peak_values = ccg_values[peak_mask]\n",
    "\n",
    "            # Find the peak value in the defined region\n",
    "            peak = np.max(peak_values)\n",
    "\n",
    "            # Check if the peak is more than the specified multiplier of standard deviations above the noise mean\n",
    "            significant = peak > noise_mean + sd_multiplier * noise_std\n",
    "\n",
    "            # Append the significance result\n",
    "            significance_results.append(significant)\n",
    "\n",
    "        # Safely add the 'sd_sig' column to the DataFrame\n",
    "        self.ccg['MonoConnectionsTable']['sd_sig'] = significance_results\n",
    "\n",
    "        return self.ccg['MonoConnectionsTable']['sd_sig']\n",
    "    \n",
    "    def plot_group_excess_synchrony(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150,limit=None, directory=None, file_name=None, front_group='CTZ'):\n",
    "\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        # Define lighter versions of the colors for SEM shading\n",
    "        sem_colors = {\n",
    "            'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "            'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            excess_synchrony_data = df['ExcessSynchrony'].tolist()\n",
    "\n",
    "            if len(excess_synchrony_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "\n",
    "            mean_excess_synchrony = np.mean(excess_synchrony_data)\n",
    "            sem_excess_synchrony = np.std(excess_synchrony_data) / np.sqrt(len(excess_synchrony_data))\n",
    "\n",
    "            # Plotting the mean excess synchrony\n",
    "            ax.bar(group, mean_excess_synchrony, yerr=sem_excess_synchrony, color=group_colors[group], alpha=0.7, label=f'{group}')\n",
    "\n",
    "        ax.set_title('Comparison of Mean Excess Synchrony by Group')\n",
    "        ax.set_ylabel('Excess Synchrony')\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_group_excess_synchrony(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150):\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Summary statistics\n",
    "        no_ctz_summary = no_ctz_excess_synchrony.describe()\n",
    "        ctz_summary = ctz_excess_synchrony.describe()\n",
    "\n",
    "        # Perform Mann-Whitney U test\n",
    "        stat, p_value = mannwhitneyu(no_ctz_excess_synchrony, ctz_excess_synchrony)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        # Print Mann-Whitney U test results\n",
    "        print(\"\\nMann-Whitney U Test Results:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        # Return results for further use if needed\n",
    "        return {\n",
    "            'no_ctz_excess_synchrony': no_ctz_excess_synchrony, #returns the filtered data for No_CTZ group\n",
    "            'ctz_excess_synchrony': ctz_excess_synchrony, #returns the filtered data for CTZ group\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'mannwhitneyu_stat': stat,\n",
    "            'mannwhitneyu_p_value': p_value\n",
    "        }\n",
    "\n",
    "    def compare_group_excess_synchrony_welch(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150):\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        no_ctz_summary = no_ctz_excess_synchrony.describe()\n",
    "        ctz_summary = ctz_excess_synchrony.describe()\n",
    "\n",
    "        stat, p_value = ttest_ind(no_ctz_excess_synchrony, ctz_excess_synchrony, equal_var=False)\n",
    "\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        print(\"\\nWelch's t-test Results:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'ttest_stat': stat,\n",
    "            'ttest_p_value': p_value\n",
    "        }\n",
    "\n",
    "    def compare_group_excess_synchrony_ks(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150):\n",
    "        from scipy.stats import ks_2samp\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        no_ctz_summary = no_ctz_excess_synchrony.describe()\n",
    "        ctz_summary = ctz_excess_synchrony.describe()\n",
    "\n",
    "        stat, p_value = ks_2samp(no_ctz_excess_synchrony, ctz_excess_synchrony)\n",
    "\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        print(\"\\nKolmogorov-Smirnov Test Results:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'ks_stat': stat,\n",
    "            'ks_p_value': p_value\n",
    "        }\n",
    "        \n",
    "    def permutation_test(self, data1, data2, num_permutations=10000):\n",
    "        # Calculate the observed difference in means\n",
    "        observed_diff = np.mean(data1) - np.mean(data2)\n",
    "        \n",
    "        # Combine the data\n",
    "        combined_data = np.concatenate([data1, data2])\n",
    "        \n",
    "        # Perform permutations\n",
    "        perm_diffs = np.zeros(num_permutations)\n",
    "        for i in range(num_permutations):\n",
    "            np.random.shuffle(combined_data)\n",
    "            perm_data1 = combined_data[:len(data1)]\n",
    "            perm_data2 = combined_data[len(data1):]\n",
    "            perm_diffs[i] = np.mean(perm_data1) - np.mean(perm_data2)\n",
    "        \n",
    "        # Calculate the p-value\n",
    "        p_value = np.sum(np.abs(perm_diffs) >= np.abs(observed_diff)) / num_permutations\n",
    "        \n",
    "        return observed_diff, p_value, perm_diffs\n",
    "\n",
    "    def compare_group_excess_synchrony_permutation(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, num_permutations=10000, limit=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Compares group excess synchrony between No_CTZ and CTZ groups using a permutation test,\n",
    "        and saves the results as a DataFrame. Handles cases where one group may have no data.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance.\n",
    "            sd_sig (bool): Filter connections based on SD significance.\n",
    "            EorI (list of str): Filter connections by excitatory/inhibitory status.\n",
    "            connectiontype (str): Filter connections by type.\n",
    "            response (str): Filter connections by responsiveness.\n",
    "            layers (list of str): Filter connections by layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs.\n",
    "            num_permutations (int): Number of permutations for the permutation test.\n",
    "            limit (bool): Filter connections by threshold.\n",
    "            save_path (str): Path to save the results as a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame containing summary statistics and permutation test results.\n",
    "        \"\"\"\n",
    "\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        # Check if either group has no data\n",
    "        if len(no_ctz_excess_synchrony) == 0:\n",
    "            no_ctz_summary = pd.Series([None] * 8, index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "        else:\n",
    "            no_ctz_summary = pd.Series(no_ctz_excess_synchrony).describe()\n",
    "\n",
    "        if len(ctz_excess_synchrony) == 0:\n",
    "            ctz_summary = pd.Series([None] * 8, index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "        else:\n",
    "            ctz_summary = pd.Series(ctz_excess_synchrony).describe()\n",
    "\n",
    "        # Perform the permutation test only if both groups have data\n",
    "        if len(no_ctz_excess_synchrony) > 0 and len(ctz_excess_synchrony) > 0:\n",
    "            observed_diff, p_value, perm_diffs = self.permutation_test(no_ctz_excess_synchrony, ctz_excess_synchrony, num_permutations)\n",
    "        else:\n",
    "            observed_diff = None\n",
    "            p_value = None\n",
    "            perm_diffs = []\n",
    "\n",
    "        # Ensure all lists are the same length or adjust appropriately\n",
    "        stats_labels = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "        no_ctz_values = no_ctz_summary.values\n",
    "        ctz_values = ctz_summary.values\n",
    "\n",
    "        # Create a DataFrame to store the summary statistics\n",
    "        results_df = pd.DataFrame({\n",
    "            'Statistic': stats_labels,\n",
    "            'No_CTZ': no_ctz_values,\n",
    "            'CTZ': ctz_values\n",
    "        })\n",
    "\n",
    "        # Add the permutation test results to the DataFrame\n",
    "        perm_results_df = pd.DataFrame({\n",
    "            'Statistic': ['Observed Difference', 'P-Value'],\n",
    "            'No_CTZ': [observed_diff, None],  # Observed difference applies to both groups, but we'll store it under No_CTZ\n",
    "            'CTZ': [None, p_value]  # P-Value is only meaningful for the comparison\n",
    "        })\n",
    "\n",
    "        # Combine the results into a single DataFrame\n",
    "        results_df = pd.concat([results_df, perm_results_df], ignore_index=True)\n",
    "\n",
    "        # Save the DataFrame to a CSV file if a save path is provided\n",
    "        if save_path:\n",
    "            results_df.to_csv(save_path, index=False)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return results_df\n",
    "\n",
    "    def compare_group_excess_synchrony_permutation_quartile(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, num_permutations=10000, quartile='top', limit=None):\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Calculate quartiles\n",
    "        no_ctz_quartiles = no_ctz_excess_synchrony.quantile([0.25, 0.5, 0.75])\n",
    "        ctz_quartiles = ctz_excess_synchrony.quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "        # Filter by specified quartile\n",
    "        if quartile == 'top':\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony[no_ctz_excess_synchrony >= no_ctz_quartiles[0.75]]\n",
    "            ctz_filtered = ctz_excess_synchrony[ctz_excess_synchrony >= ctz_quartiles[0.75]]\n",
    "        elif quartile == 'bottom':\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony[no_ctz_excess_synchrony <= no_ctz_quartiles[0.25]]\n",
    "            ctz_filtered = ctz_excess_synchrony[ctz_excess_synchrony <= ctz_quartiles[0.25]]\n",
    "        elif quartile == 'middle':\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony[(no_ctz_excess_synchrony > no_ctz_quartiles[0.25]) & (no_ctz_excess_synchrony < no_ctz_quartiles[0.75])]\n",
    "            ctz_filtered = ctz_excess_synchrony[(ctz_excess_synchrony > ctz_quartiles[0.25]) & (ctz_excess_synchrony < ctz_quartiles[0.75])]\n",
    "        else:\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony\n",
    "            ctz_filtered = ctz_excess_synchrony\n",
    "\n",
    "        no_ctz_summary = no_ctz_filtered.describe()\n",
    "        ctz_summary = ctz_filtered.describe()\n",
    "\n",
    "        # Perform the permutation test\n",
    "        observed_diff, p_value, perm_diffs = self.permutation_test(no_ctz_filtered, ctz_filtered, num_permutations)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        # Print Permutation Test results\n",
    "        print(\"\\nPermutation Test Results:\")\n",
    "        print(f\"Observed Difference: {observed_diff}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        # Return results for further use if needed\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'observed_diff': observed_diff,\n",
    "            'perm_diffs': perm_diffs,\n",
    "            'perm_p_value': p_value\n",
    "        }\n",
    "\n",
    "    def run_permutation_test_between_groups(self, band_name, condition, quantile_low=None, quantile_high=None, cell_type=None, cell_type2=None, n_permutations=10000):\n",
    "        \"\"\"\n",
    "        Runs a permutation test to compare the PPC2 values between groups for a specified frequency band.\n",
    "\n",
    "        Parameters:\n",
    "        band_name (str): The name of the frequency band to filter by (e.g., 'alpha', 'beta').\n",
    "        condition (bool): The condition to filter by (True for evoked, False for spontaneous).\n",
    "        quantile_low (float): The lower quantile threshold for filtering.\n",
    "        quantile_high (float): The upper quantile threshold for filtering.\n",
    "        cell_type (str): The cell type to filter by.\n",
    "        cell_type2 (int): The secondary cell type to filter by.\n",
    "        n_permutations (int): The number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        float: The p-value from the permutation test.\n",
    "        \"\"\"\n",
    "        filtered_df = self.filter_by_frequency_band(band_name)\n",
    "\n",
    "        # Filter by condition\n",
    "        filtered_df = filtered_df[filtered_df['Condition'] == condition]\n",
    "\n",
    "        # Filter by cell type if specified\n",
    "        if cell_type:\n",
    "            filtered_df = filtered_df[filtered_df['CellType'] == cell_type]\n",
    "        if cell_type2 is not None:\n",
    "            filtered_df = filtered_df[filtered_df['CellType2'] == cell_type2]\n",
    "\n",
    "        # Apply quantile filtering if specified\n",
    "        if quantile_low is not None and quantile_high is not None:\n",
    "            filtered_df = self.filter_data_by_quantile(filtered_df, quantile_low, quantile_high)\n",
    "\n",
    "        # Aggregate PPC2 values per CellID and frequency band midpoint\n",
    "        aggregated_df = filtered_df.groupby(['CellID', 'Group']).agg({'PPC2': 'mean'}).reset_index()\n",
    "\n",
    "        # Separate the data by groups\n",
    "        df_ctz = aggregated_df[aggregated_df['Group'] == 'CTZ']['PPC2']\n",
    "        df_no_ctz = aggregated_df[aggregated_df['Group'] == 'No_CTZ']['PPC2']\n",
    "\n",
    "        # Calculate observed difference in means\n",
    "        observed_diff = np.mean(df_ctz) - np.mean(df_no_ctz)\n",
    "\n",
    "        # Concatenate the data for permutation\n",
    "        all_data = np.concatenate([df_ctz, df_no_ctz])\n",
    "        n_ctz = len(df_ctz)\n",
    "        n_no_ctz = len(df_no_ctz)\n",
    "\n",
    "        # Perform permutations\n",
    "        perm_diffs = np.zeros(n_permutations)\n",
    "        for i in range(n_permutations):\n",
    "            np.random.shuffle(all_data)\n",
    "            perm_ctz = all_data[:n_ctz]\n",
    "            perm_no_ctz = all_data[n_ctz:]\n",
    "            perm_diffs[i] = np.mean(perm_ctz) - np.mean(perm_no_ctz)\n",
    "\n",
    "        # Calculate p-value\n",
    "        p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n",
    "        return p_value\n",
    "    \n",
    "    def filter_by_ccg_threshold(self, time_range, threshold_range):\n",
    "        \"\"\"\n",
    "        Filters CCGs based on whether their values exceed a given threshold within a specified time range.\n",
    "        Adds a new column 'threshold' indicating True or False based on the condition.\n",
    "\n",
    "        Parameters:\n",
    "            time_range (int or tuple): An integer specifying a single time point, or a tuple specifying the start and end of the time range to check (e.g., (-5, 0) or (0, 1)).\n",
    "            threshold_range (tuple): A tuple specifying the lower and upper bounds of the threshold (e.g., (0, 7)).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with a new 'threshold' column.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure 'MonoConnectionsTable' exists in 'self.ccg'\n",
    "        if 'MonoConnectionsTable' not in self.ccg:\n",
    "            raise KeyError(\"'MonoConnectionsTable' not found in 'self.ccg'\")\n",
    "\n",
    "        # Extract the MonoConnectionsTable DataFrame\n",
    "        df = self.ccg['MonoConnectionsTable'].copy()\n",
    "\n",
    "        # Ensure 'ccgs' and 't' exist in 'self.ccg'\n",
    "        if 'ccgs' not in df or 't' not in self.ccg:\n",
    "            raise KeyError(\"'ccgs' or 't' not found in 'self.ccg'\")\n",
    "\n",
    "        # Extract time points and CCG values\n",
    "        time_points = self.ccg['t']\n",
    "        ccgs = df['ccgs']\n",
    "\n",
    "        # Convert time points to integers for ease of indexing\n",
    "        time_points = time_points.astype(int)\n",
    "\n",
    "        # Determine the time range\n",
    "        if isinstance(time_range, int):\n",
    "            start_time = end_time = time_range\n",
    "        else:\n",
    "            start_time, end_time = time_range\n",
    "\n",
    "        # Find the indices of the time range\n",
    "        start_idx = (time_points >= start_time).argmax()\n",
    "        end_idx = (time_points <= end_time).argmin() + 1\n",
    "\n",
    "        # Ensure the indices are valid\n",
    "        if start_idx > end_idx:\n",
    "            raise ValueError(\"Invalid time range specified\")\n",
    "\n",
    "        # Initialize the 'threshold' column with False\n",
    "        df['threshold'] = False\n",
    "\n",
    "        # Unpack the threshold range\n",
    "        lower_threshold, upper_threshold = threshold_range\n",
    "\n",
    "        # Check CCG values within the specified time and threshold ranges\n",
    "        for index, row in df.iterrows():\n",
    "            ccg_values = ccgs.iloc[index][start_idx:end_idx]\n",
    "            if any((ccg_values > lower_threshold) & (ccg_values < upper_threshold)):\n",
    "                df.at[index, 'threshold'] = True\n",
    "\n",
    "        # Update the MonoConnectionsTable in self.ccg\n",
    "        self.ccg['MonoConnectionsTable'] = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def nullify_specific_rows(self, indices):\n",
    "        \"\"\"\n",
    "        Sets all values of specific rows in 'MonoConnectionsTable' to NaN based on the provided indices.\n",
    "        \n",
    "        Parameters:\n",
    "            indices (list): List of indices of the rows to be nullified.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with the specified rows set to NaN.\n",
    "        \"\"\"\n",
    "        # Ensure 'MonoConnectionsTable' exists in 'self.ccg'\n",
    "        if 'MonoConnectionsTable' not in self.ccg:\n",
    "            raise KeyError(\"'MonoConnectionsTable' not found in 'self.ccg'\")\n",
    "\n",
    "        # Extract the MonoConnectionsTable DataFrame\n",
    "        df = self.ccg['MonoConnectionsTable']\n",
    "\n",
    "        # Check if indices are valid\n",
    "        invalid_indices = [i for i in indices if i not in df.index]\n",
    "        if invalid_indices:\n",
    "            raise KeyError(f\"Invalid indices: {invalid_indices}\")\n",
    "\n",
    "        # Set specified rows to NaN\n",
    "        df.loc[indices] = float('nan')\n",
    "\n",
    "        # Update the MonoConnectionsTable in self.ccg\n",
    "        self.ccg['MonoConnectionsTable'] = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def plot_group_excess_synchrony_with_dots(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, limit=None, directory=None, file_name=None, front_group='CTZ'):\n",
    "        \"\"\"\n",
    "        Plots the mean excess synchrony by group with SEM as error bars and overlays the underlying data as color-matched dots.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs.\n",
    "            EorI (list of str): Filter connections by excitatory/inhibitory status.\n",
    "            connectiontype (str): Filter connections by type (e.g., 'FS->RS').\n",
    "            response (str): Filter connections based on responsiveness.\n",
    "            layers (list of str): Filter connections by layer.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include.\n",
    "            limit (int): Limit the number of connections to include.\n",
    "            directory (str): Directory to save the plot.\n",
    "            file_name (str): File name to save the plot.\n",
    "            front_group (str): Group to be plotted in front.\n",
    "        \"\"\"\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        # Define lighter versions of the colors for SEM shading\n",
    "        sem_colors = {\n",
    "            'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "            'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            excess_synchrony_data = df['ExcessSynchrony'].tolist()\n",
    "\n",
    "            if len(excess_synchrony_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "\n",
    "            mean_excess_synchrony = np.mean(excess_synchrony_data)\n",
    "            sem_excess_synchrony = np.std(excess_synchrony_data) / np.sqrt(len(excess_synchrony_data))\n",
    "\n",
    "            # Plotting the mean excess synchrony\n",
    "            ax.bar(group, mean_excess_synchrony, yerr=sem_excess_synchrony, color=group_colors[group], alpha=0.3, label=f'{group}')\n",
    "            \n",
    "            # Overlaying the underlying data as dots\n",
    "            x_positions = np.full(len(excess_synchrony_data), group)  # x-positions are the same for each group\n",
    "            ax.scatter(x_positions, excess_synchrony_data, color=group_colors[group], alpha=0.7, edgecolor=None)\n",
    "\n",
    "        ax.set_title('Comparison of Mean Excess Synchrony by Group')\n",
    "        ax.set_ylabel('Excess Synchrony')\n",
    "        ax.legend()\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_group_DCW_with_dots(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, limit=None, directory=None, file_name=None, front_group='CTZ'):\n",
    "            \"\"\"\n",
    "            Plots the mean excess synchrony by group with SEM as error bars and overlays the underlying data as color-matched dots.\n",
    "\n",
    "            Parameters:\n",
    "                significance (bool): Filter connections by significance.\n",
    "                sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs.\n",
    "                EorI (list of str): Filter connections by excitatory/inhibitory status.\n",
    "                connectiontype (str): Filter connections by type (e.g., 'FS->RS').\n",
    "                response (str): Filter connections based on responsiveness.\n",
    "                layers (list of str): Filter connections by layer.\n",
    "                min_spike_pairs (int): Minimum number of spike pairs to include.\n",
    "                limit (int): Limit the number of connections to include.\n",
    "                directory (str): Directory to save the plot.\n",
    "                file_name (str): File name to save the plot.\n",
    "                front_group (str): Group to be plotted in front.\n",
    "            \"\"\"\n",
    "            group_colors = {\n",
    "                'No_CTZ': '#797979',  # Grey color\n",
    "                'CTZ': '#5a00c2'  # Purple color\n",
    "            }\n",
    "\n",
    "            # Define lighter versions of the colors for SEM shading\n",
    "            sem_colors = {\n",
    "                'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "                'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "            }\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "            groups = ['No_CTZ', 'CTZ']\n",
    "            # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "            groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "            for group in groups:\n",
    "                df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "                excess_synchrony_data = df['DCW'].tolist()\n",
    "\n",
    "                if len(excess_synchrony_data) == 0:\n",
    "                    continue  # Skip if no data to plot\n",
    "\n",
    "                mean_excess_synchrony = np.mean(excess_synchrony_data)\n",
    "                sem_excess_synchrony = np.std(excess_synchrony_data) / np.sqrt(len(excess_synchrony_data))\n",
    "\n",
    "                # Plotting the mean excess synchrony\n",
    "                ax.bar(group, mean_excess_synchrony, yerr=sem_excess_synchrony, color=group_colors[group], alpha=0.3, label=f'{group}')\n",
    "                \n",
    "                # Overlaying the underlying data as dots\n",
    "                x_positions = np.full(len(excess_synchrony_data), group)  # x-positions are the same for each group\n",
    "                ax.scatter(x_positions, excess_synchrony_data, color=group_colors[group], alpha=0.7, edgecolor=None)\n",
    "\n",
    "            ax.set_title('Comparison of DCW by Group')\n",
    "            ax.set_ylabel('DCW')\n",
    "            ax.legend()\n",
    "\n",
    "            # Prompt user for directory and file name if not provided\n",
    "            if directory is None:\n",
    "                directory = input(\"Please enter the directory to save the plot: \")\n",
    "            if file_name is None:\n",
    "                file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "            # Create directory if it does not exist\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "            # Save the figure as an SVG file in the specified directory\n",
    "            file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "            plt.savefig(file_path, format='svg')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def compare_group_DCW_permutation(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, num_permutations=10000, limit=None):\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['DCW'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        ctz_excess_synchrony = ctz_df['DCW'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        no_ctz_summary = pd.Series(no_ctz_excess_synchrony).describe()\n",
    "        ctz_summary = pd.Series(ctz_excess_synchrony).describe()\n",
    "\n",
    "        # Perform the permutation test\n",
    "        observed_diff, p_value, perm_diffs = self.permutation_test(no_ctz_excess_synchrony, ctz_excess_synchrony, num_permutations)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        # Print Permutation Test results\n",
    "        print(\"\\nPermutation Test Results:\")\n",
    "        print(f\"Observed Difference: {observed_diff}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        # Return results for further use if needed\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'observed_diff': observed_diff,\n",
    "            'perm_diffs': perm_diffs,\n",
    "            'perm_p_value': p_value\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mat73\n",
    "import os\n",
    "\n",
    "class PhaseDataHandler:\n",
    "    \n",
    "    FREQUENCY_BANDS = {\n",
    "        'alpha': (8, 15),\n",
    "        'beta': (15, 30),\n",
    "        'gamma': (30, 55),\n",
    "        'high_gamma': (66, 120)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, eed_object, phase_directory):\n",
    "        self.eed = eed_object\n",
    "        self.phase_directory = phase_directory\n",
    "        self.phase = {}\n",
    "        self.load_phase_files()\n",
    "        self.unpack_phase_data()  # This will now directly update self.phase\n",
    "    \n",
    "    def load_phase_files(self):\n",
    "        \"\"\"\n",
    "        Loads all required .mat files from the specified CCG directory and sets them as attributes under the `ccg` dictionary.\n",
    "\n",
    "        Required Files:\n",
    "            \n",
    "        The data from each file is accessed by stripping the '.mat' and accessing the corresponding key in the loaded dictionary, except for specific exceptions noted.\n",
    "        \"\"\"\n",
    "        required_files = [\n",
    "            'phase_data_out.mat'\n",
    "        ]\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(self.phase_directory):\n",
    "            print(f\"Phase directory not found: {self.phase_directory}\")\n",
    "            return\n",
    "\n",
    "        # Load each required .mat file from the directory\n",
    "        for file_name in required_files:\n",
    "            file_path = os.path.join(self.phase_directory, file_name)\n",
    "            print(f\"Loading Phase data from: {file_path}\")\n",
    "            try:\n",
    "                # Load the file and extract the data using the base filename as the key\n",
    "                data = mat73.loadmat(file_path)\n",
    "                base_key = file_name.replace('.mat', '')\n",
    "                # Handle special case for 'MonoConnectionsTable'\n",
    "                if base_key == 'phase_data_out':\n",
    "                    self.phase[base_key] = data['phase_data_out']\n",
    "                elif base_key in data:\n",
    "                    self.phase[base_key] = data[base_key]\n",
    "                else:\n",
    "                    print(f\"Expected key '{base_key}' not found in {file_name}\")\n",
    "                    self.phase[base_key] = None\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                self.phase[base_key] = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "                self.phase[base_key] = None\n",
    "                \n",
    "    def unpack_phase_data(self):\n",
    "        frames = []\n",
    "        for groupname, group_data in self.phase['phase_data_out'].items():\n",
    "            for recording, records in group_data.items():\n",
    "                df = pd.DataFrame(records)\n",
    "                df['Group'] = groupname\n",
    "                df['Recording'] = recording\n",
    "\n",
    "                # Renaming group entries\n",
    "                df['Group'] = df['Group'].replace({'Control': 'No_CTZ', 'ES': 'CTZ'})\n",
    "\n",
    "                # Data cleaning and conversion\n",
    "                df['Condition'] = df['Condition'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "                df['Condition'] = df['Condition'].map({'Stim': True, 'No Stim': False})\n",
    "                df['CellType2'] = df['CellType2'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "                df['CellType2'] = df['CellType2'].map({'+FS': 1, 'nsFS': 0, '-FS': -1, '+RS': 1, 'nsRS': 0, '-RS': -1})\n",
    "\n",
    "                # Flatten all columns that contain lists to scalar if they contain only one value\n",
    "                for col in df.columns:\n",
    "                    if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                        df[col] = df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "                frames.append(df)\n",
    "\n",
    "        # Replace self.phase with the cleaned DataFrame\n",
    "        self.phase = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "    def plot_ctz_noctz_ppc2(self, cell_type=None, cell_type2=None, show_individual_points=False, smoothing_window=1, min_spikes_evoked=1, min_spikes_spontaneous=1):\n",
    "        # Prepare the subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle('PPC2 Analysis for CTZ and No_CTZ Groups with SEM')\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the SEM face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Function to apply simple moving average for smoothing\n",
    "        def smooth_data(y, window_size):\n",
    "            if window_size <= 1:\n",
    "                return y\n",
    "            window = np.ones(int(window_size)) / float(window_size)\n",
    "            return np.convolve(y, window, 'same')\n",
    "\n",
    "        # Common plotting function for a given condition\n",
    "        def plot_data(df, group, ax, title):\n",
    "            # Filter by cell type if specified\n",
    "            if cell_type:\n",
    "                df = df[df['CellType'] == cell_type]\n",
    "            if cell_type2:\n",
    "                df = df[df['CellType2'] == cell_type2]\n",
    "\n",
    "            # Group by frequency band midpoint\n",
    "            grouped = df.groupby('fband_midpoint')\n",
    "            \n",
    "            # Plot individual data points if toggled on\n",
    "            if show_individual_points:\n",
    "                for fband_midpoint, group_data in grouped:\n",
    "                    ax.plot([fband_midpoint] * len(group_data), group_data['PPC2'], color=lightened_colors[group], alpha=0.7, markersize=4)\n",
    "            \n",
    "            # Calculate mean and SEM\n",
    "            mean_values = grouped['PPC2'].mean()\n",
    "            sem_values = grouped['PPC2'].sem()\n",
    "\n",
    "            # Smooth the mean and SEM if smoothing_window is greater than 1\n",
    "            x_values = mean_values.index\n",
    "            mean_values_smoothed = smooth_data(mean_values, smoothing_window)\n",
    "            sem_values_smoothed = smooth_data(sem_values, smoothing_window)\n",
    "\n",
    "            # Plot mean line\n",
    "            ax.plot(x_values, mean_values_smoothed, linestyle='-', color=group_colors[group], linewidth=3, label=f'Mean {group}')\n",
    "            # Plot SEM as shaded area\n",
    "            ax.fill_between(x_values, (mean_values_smoothed - sem_values_smoothed), (mean_values_smoothed + sem_values_smoothed), color=lightened_colors[group], alpha=0.1)\n",
    "            \n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Frequency Band Midpoint (Hz)')\n",
    "            ax.set_ylabel('PPC2')\n",
    "            ax.grid(False)  # Turn off grid lines\n",
    "            ax.legend()\n",
    "\n",
    "        # Filter data for both groups\n",
    "        df_ctz = self.phase[(self.phase['Group'] == 'CTZ')]\n",
    "        df_no_ctz = self.phase[(self.phase['Group'] == 'No_CTZ')]\n",
    "\n",
    "        #filter data based on spikes \n",
    "        list_of_df = [df_ctz, df_no_ctz]\n",
    "        for df in list_of_df:\n",
    "            df = self.filter_by_spikes(df, min_spikes_spontaneous=min_spikes_spontaneous, min_spikes_evoked=min_spikes_evoked)\n",
    "            #rename the corected df\n",
    "            if df.equals(df_ctz):\n",
    "                df_ctz = df\n",
    "            else:\n",
    "                df_no_ctz = df\n",
    "\n",
    "        \n",
    "        # Separate the data based on the 'Condition'\n",
    "        conditions = [False, True]  # False for Spontaneous, True for Evoked\n",
    "        titles = ['Spontaneous', 'Evoked']\n",
    "\n",
    "        for i, cond in enumerate(conditions):\n",
    "            # Filter by condition for each group\n",
    "            df_ctz_cond = df_ctz[df_ctz['Condition'] == cond]\n",
    "            df_no_ctz_cond = df_no_ctz[df_no_ctz['Condition'] == cond]\n",
    "\n",
    "            # Plot CTZ group data\n",
    "            plot_data(df_ctz_cond, 'CTZ', axes[i], titles[i])\n",
    "\n",
    "            # Plot No_CTZ group data\n",
    "            plot_data(df_no_ctz_cond, 'No_CTZ', axes[i], titles[i])\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "        plt.show()\n",
    "        \n",
    "    def filter_by_spikes(self, df, min_spikes_spontaneous, min_spikes_evoked):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame based on the 'Condition' column with two different\n",
    "        minimum spike thresholds for Spontaneous and Evoked conditions.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): The DataFrame to filter.\n",
    "        min_spikes_spontaneous (int): Minimum number of spikes for Spontaneous condition.\n",
    "        min_spikes_evoked (int): Minimum number of spikes for Evoked condition.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A new DataFrame filtered according to the specified thresholds.\n",
    "        \"\"\"\n",
    "        # Filter for Spontaneous condition (Condition == False)\n",
    "        spontaneous_df = df[(df['Condition'] == False) & (df['NumSpikes'] >= min_spikes_spontaneous)]\n",
    "        \n",
    "        # Filter for Evoked condition (Condition == True)\n",
    "        evoked_df = df[(df['Condition'] == True) & (df['NumSpikes'] >= min_spikes_evoked)]\n",
    "        \n",
    "        # Concatenate the two filtered DataFrames\n",
    "        filtered_df = pd.concat([spontaneous_df, evoked_df])\n",
    "\n",
    "        return filtered_df\n",
    "    \n",
    "    def filter_data_by_quantile(self, df, quantile_low, quantile_high, column='PPC2'):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame based on the quantile range of a specified column.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): The DataFrame to filter.\n",
    "        quantile_low (float): The lower quantile threshold (e.g., 0.25 for the 25th percentile).\n",
    "        quantile_high (float): The upper quantile threshold (e.g., 0.75 for the 75th percentile).\n",
    "        column (str): The column to apply the quantile filter on.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A new DataFrame filtered according to the specified quantiles.\n",
    "        \"\"\"\n",
    "        lower_bound = df[column].quantile(quantile_low)\n",
    "        upper_bound = df[column].quantile(quantile_high)\n",
    "        filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_quantile_filtered_ppc2(self, quantile_low=0.05, quantile_high=0.95, show_individual_points=False, \n",
    "                                    smoothing_window=1, min_spikes_evoked=5, min_spikes_spontaneous=50):\n",
    "        \"\"\"\n",
    "        Plots PPC2 data with quantile filtering, and saves the plots dynamically in subfolders based on cell type and sensory response.\n",
    "\n",
    "        Parameters:\n",
    "            quantile_low (float): The lower quantile threshold for filtering.\n",
    "            quantile_high (float): The upper quantile threshold for filtering.\n",
    "            show_individual_points (bool): Whether to show individual data points in the plot.\n",
    "            smoothing_window (int): Window size for smoothing the data.\n",
    "            min_spikes_evoked (int): Minimum number of spikes for evoked condition filtering.\n",
    "            min_spikes_spontaneous (int): Minimum number of spikes for spontaneous condition filtering.\n",
    "\n",
    "        Returns:\n",
    "            None: Plots are saved in the specified directory with the appropriate subfolder structure.\n",
    "        \"\"\"\n",
    "\n",
    "        # Default directory\n",
    "        base_directory = '/Volumes/MannySSD/figures/thesis_final_figs/ppc2/'\n",
    "\n",
    "        # Assume these attributes are set within the object\n",
    "        cell_types = ['FS', 'RS']  # Example: or dynamically extracted from your data\n",
    "        sensory_responses = [1, 0]  # 1 for Sensory Response, 0 for Non-Sensory Response\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the SEM face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Function to apply simple moving average for smoothing\n",
    "        def smooth_data(y, window_size):\n",
    "            if window_size <= 1:\n",
    "                return y\n",
    "            window = np.ones(int(window_size)) / float(window_size)\n",
    "            return np.convolve(y, window, 'same')\n",
    "\n",
    "        # Common plotting function for a given condition\n",
    "        def plot_data(df, group, ax, title):\n",
    "            # Apply quantile filtering\n",
    "            df = self.filter_data_by_quantile(df, quantile_low, quantile_high)\n",
    "\n",
    "            # Group by frequency band midpoint\n",
    "            grouped = df.groupby('fband_midpoint')\n",
    "\n",
    "            # Plot individual data points if toggled on\n",
    "            if show_individual_points:\n",
    "                for fband_midpoint, group_data in grouped:\n",
    "                    ax.plot([fband_midpoint] * len(group_data), group_data['PPC2'], color=lightened_colors[group], alpha=0.7, markersize=4)\n",
    "\n",
    "            # Calculate mean and SEM\n",
    "            mean_values = grouped['PPC2'].mean()\n",
    "            sem_values = grouped['PPC2'].sem()\n",
    "\n",
    "            # Smooth the mean and SEM if smoothing_window is greater than 1\n",
    "            x_values = mean_values.index\n",
    "            mean_values_smoothed = smooth_data(mean_values, smoothing_window)\n",
    "            sem_values_smoothed = smooth_data(sem_values, smoothing_window)\n",
    "\n",
    "            # Plot mean line\n",
    "            ax.plot(x_values, mean_values_smoothed, linestyle='-', color=group_colors[group], linewidth=3, label=f'Mean {group}')\n",
    "            # Plot SEM as shaded area\n",
    "            ax.fill_between(x_values, (mean_values_smoothed - sem_values_smoothed), (mean_values_smoothed + sem_values_smoothed), color=lightened_colors[group], alpha=0.1)\n",
    "\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Frequency Band Midpoint (Hz)')\n",
    "            ax.set_ylabel('PPC2')\n",
    "            ax.grid(False)  # Turn off grid lines\n",
    "            ax.legend()\n",
    "\n",
    "        # Iterate through all combinations of cell types and sensory responses\n",
    "        for cell_type in cell_types:\n",
    "            for cell_type2 in sensory_responses:\n",
    "                cell_type2_label = 'Sensory Response' if cell_type2 == 1 else 'Non-Sensory Response'\n",
    "\n",
    "                # Filter the DataFrame based on the current cell type and sensory response\n",
    "                df_ctz_filtered = self.phase[(self.phase['Group'] == 'CTZ') & (self.phase['CellType'] == cell_type) & (self.phase['CellType2'] == cell_type2)]\n",
    "                df_no_ctz_filtered = self.phase[(self.phase['Group'] == 'No_CTZ') & (self.phase['CellType'] == cell_type) & (self.phase['CellType2'] == cell_type2)]\n",
    "\n",
    "                # Prepare the subplots (1 row by 2 columns for Spontaneous vs Evoked)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "                fig.suptitle(f'PPC2 Analysis for {cell_type} - {cell_type2_label}', fontsize=16)\n",
    "\n",
    "                conditions = [False, True]  # False for Spontaneous, True for Evoked\n",
    "                titles = ['Spontaneous', 'Evoked']\n",
    "\n",
    "                for i, cond in enumerate(conditions):\n",
    "                    # Filter by condition for each group\n",
    "                    df_ctz_cond = df_ctz_filtered[df_ctz_filtered['Condition'] == cond]\n",
    "                    df_no_ctz_cond = df_no_ctz_filtered[df_no_ctz_filtered['Condition'] == cond]\n",
    "\n",
    "                    # Plot CTZ group data\n",
    "                    plot_data(df_ctz_cond, 'CTZ', axes[i], titles[i])\n",
    "\n",
    "                    # Plot No_CTZ group data\n",
    "                    plot_data(df_no_ctz_cond, 'No_CTZ', axes[i], titles[i])\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "\n",
    "                # Create the directory structure based on cell type and sensory response\n",
    "                subfolder = os.path.join(base_directory, cell_type, cell_type2_label)\n",
    "                os.makedirs(subfolder, exist_ok=True)\n",
    "\n",
    "                # Generate the filename based on cell type and sensory response\n",
    "                file_name = f'{cell_type}_{cell_type2_label}_ppc2'\n",
    "                file_path = os.path.join(subfolder, f'{file_name}.svg')\n",
    "\n",
    "                # Save the figure as an SVG file in the specified subfolder\n",
    "                fig.savefig(file_path, format='svg', transparent=True)\n",
    "                plt.close(fig)  # Close the figure after saving to free up memory\n",
    "\n",
    "    def filter_by_frequency_band(self, band_name):\n",
    "        \"\"\"\n",
    "        Filters the data based on the specified frequency band.\n",
    "\n",
    "        Parameters:\n",
    "        band_name (str): The name of the frequency band to filter by (e.g., 'alpha', 'beta').\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A DataFrame filtered by the specified frequency band.\n",
    "        \"\"\"\n",
    "        if band_name not in self.FREQUENCY_BANDS:\n",
    "            raise ValueError(f\"Invalid band name: {band_name}. Valid names are: {list(self.FREQUENCY_BANDS.keys())}\")\n",
    "\n",
    "        low, high = self.FREQUENCY_BANDS[band_name]\n",
    "        filtered_df = self.phase[(self.phase['fband_midpoint'] >= low) & (self.phase['fband_midpoint'] < high)]\n",
    "        return filtered_df\n",
    "    \n",
    "    def compare_frequency_bands_between_conditions(self, frequency_bands=None, cell_types1=None, cell_types2=None, quantile_low=None, quantile_high=None, show_outliers=None, hue_order=None):\n",
    "        \"\"\"\n",
    "        Compares the data between groups for all specified frequency bands and conditions, and saves the plots dynamically.\n",
    "\n",
    "        Parameters:\n",
    "            frequency_bands (list of str): List of frequency bands to compare (e.g., ['alpha', 'beta']).\n",
    "            cell_types1 (list of str): List of primary cell types to filter by (e.g., ['RS', 'FS']).\n",
    "            cell_types2 (list of int): List of secondary cell types to filter by (1 for Sensory Response, 0 for Non-Sensory Response).\n",
    "            quantile_low (float): The lower quantile threshold for filtering.\n",
    "            quantile_high (float): The upper quantile threshold for filtering.\n",
    "            show_outliers (bool): Whether to show outliers in the boxplot.\n",
    "            hue_order (list of str): Order of the hue levels in the plot.\n",
    "\n",
    "        Returns:\n",
    "            None: Plots are saved in the specified directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Base directory\n",
    "        base_directory = '/Volumes/MannySSD/figures/thesis_final_figs/ppc2/'\n",
    "\n",
    "        # Initialize default parameters if not provided\n",
    "        if frequency_bands is None:\n",
    "            frequency_bands = ['alpha', 'beta', 'gamma', 'high_gamma']\n",
    "        if cell_types1 is None:\n",
    "            cell_types1 = ['RS', 'FS']\n",
    "        if cell_types2 is None:\n",
    "            cell_types2 = [1, 0]\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,  # Removed dodge=True from the sns.stripplot() call\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Loop over each combination of cell types\n",
    "        for cell1 in cell_types1:\n",
    "            for cell2 in cell_types2:\n",
    "                cell_type2_label = 'Sensory Response' if cell2 == 1 else 'Non-Sensory Response'\n",
    "\n",
    "                # Create subfolder path based on hierarchy\n",
    "                subfolder = os.path.join(base_directory, cell1, cell_type2_label)\n",
    "                os.makedirs(subfolder, exist_ok=True)\n",
    "\n",
    "                # Prepare the 1x2 plot\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "                fig.suptitle(f'{cell1} - {cell_type2_label} Comparison Across Frequency Bands')\n",
    "\n",
    "                conditions = [False, True]  # False for Spontaneous, True for Evoked\n",
    "                titles = ['Spontaneous', 'Evoked']\n",
    "\n",
    "                for i, cond in enumerate(conditions):\n",
    "                    # Filter data for each condition\n",
    "                    filtered_dfs = []\n",
    "                    for band in frequency_bands:\n",
    "                        filtered_df = self.filter_by_frequency_band(band)\n",
    "                        filtered_df = filtered_df[filtered_df['Condition'] == cond]\n",
    "                        if cell1:\n",
    "                            filtered_df = filtered_df[filtered_df['CellType'] == cell1]\n",
    "                        if cell2 is not None:\n",
    "                            filtered_df = filtered_df[filtered_df['CellType2'] == cell2]\n",
    "\n",
    "                        # Apply quantile filtering if specified\n",
    "                        if quantile_low is not None and quantile_high is not None:\n",
    "                            filtered_df = self.filter_data_by_quantile(filtered_df, quantile_low, quantile_high)\n",
    "\n",
    "                        filtered_df['Frequency Band'] = band  # Add frequency band to DataFrame\n",
    "                        filtered_dfs.append(filtered_df)\n",
    "\n",
    "                    # Concatenate data across frequency bands\n",
    "                    combined_df = pd.concat(filtered_dfs)\n",
    "\n",
    "                    # Aggregate PPC2 values per CellID, Group, and Frequency Band\n",
    "                    aggregated_df = combined_df.groupby(['CellID', 'Group', 'Frequency Band']).agg({'PPC2': 'mean'}).reset_index()\n",
    "\n",
    "                    # Plot the comparison for this condition across all frequency bands\n",
    "                    sns.boxplot(data=aggregated_df, x='Frequency Band', y='PPC2', hue='Group', **boxplot_kwargs, ax=axes[i])\n",
    "                    sns.stripplot(data=aggregated_df, x='Frequency Band', y='PPC2', hue='Group', **stripplot_kwargs, ax=axes[i])\n",
    "\n",
    "                    axes[i].set_title(titles[i])\n",
    "                    axes[i].set_xlabel('Frequency Band')\n",
    "                    if i == 0:\n",
    "                        axes[i].set_ylabel('PPC2')\n",
    "                    axes[i].legend(title='Group')\n",
    "\n",
    "                # Save the figure\n",
    "                file_name = f'{cell1}_{cell_type2_label}_FrequencyBandComparison.svg'\n",
    "                file_path = os.path.join(subfolder, file_name)\n",
    "                fig.savefig(file_path, format='svg', transparent=True)\n",
    "                plt.close(fig)  # Close the figure after saving to free up memory\n",
    "\n",
    "    def run_permutation_test_between_groups(self, band_name, condition, quantile_low=None, quantile_high=None, cell_type=None, cell_type2=None, min_num_spikes=None, n_permutations=10000):\n",
    "        \"\"\"\n",
    "        Runs a permutation test to compare the PPC2 values between groups for a specified frequency band.\n",
    "\n",
    "        Parameters:\n",
    "        band_name (str): The name of the frequency band to filter by (e.g., 'alpha', 'beta').\n",
    "        condition (bool): The condition to filter by (True for evoked, False for spontaneous).\n",
    "        quantile_low (float): The lower quantile threshold for filtering.\n",
    "        quantile_high (float): The upper quantile threshold for filtering.\n",
    "        cell_type (str): The cell type to filter by.\n",
    "        cell_type2 (int): The secondary cell type to filter by.\n",
    "        min_num_spikes (int): The minimum number of spikes to filter by.\n",
    "        n_permutations (int): The number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        float: The p-value from the permutation test.\n",
    "        \"\"\"\n",
    "        filtered_df = self.filter_by_frequency_band(band_name)\n",
    "\n",
    "        # Filter by condition\n",
    "        filtered_df = filtered_df[filtered_df['Condition'] == condition]\n",
    "\n",
    "        # Filter by cell type if specified\n",
    "        if cell_type:\n",
    "            filtered_df = filtered_df[filtered_df['CellType'] == cell_type]\n",
    "        if cell_type2 is not None:\n",
    "            filtered_df = filtered_df[filtered_df['CellType2'] == cell_type2]\n",
    "\n",
    "        # Filter by minimum number of spikes if specified\n",
    "        if min_num_spikes is not None:\n",
    "            filtered_df = filtered_df[filtered_df['NumSpikes'] >= min_num_spikes]\n",
    "\n",
    "        # Apply quantile filtering if specified\n",
    "        if quantile_low is not None and quantile_high is not None:\n",
    "            filtered_df = self.filter_data_by_quantile(filtered_df, quantile_low, quantile_high)\n",
    "\n",
    "        # Aggregate PPC2 values per CellID and frequency band midpoint\n",
    "        aggregated_df = filtered_df.groupby(['CellID', 'Group']).agg({'PPC2': 'mean'}).reset_index()\n",
    "\n",
    "        # Separate the data by groups\n",
    "        df_ctz = aggregated_df[aggregated_df['Group'] == 'CTZ']['PPC2']\n",
    "        df_no_ctz = aggregated_df[aggregated_df['Group'] == 'No_CTZ']['PPC2']\n",
    "\n",
    "        # Calculate observed difference in means\n",
    "        observed_diff = np.mean(df_ctz) - np.mean(df_no_ctz)\n",
    "\n",
    "        # Concatenate the data for permutation\n",
    "        all_data = np.concatenate([df_ctz, df_no_ctz])\n",
    "        n_ctz = len(df_ctz)\n",
    "        n_no_ctz = len(df_no_ctz)\n",
    "\n",
    "        # Perform permutations\n",
    "        perm_diffs = np.zeros(n_permutations)\n",
    "        for i in range(n_permutations):\n",
    "            np.random.shuffle(all_data)\n",
    "            perm_ctz = all_data[:n_ctz]\n",
    "            perm_no_ctz = all_data[n_ctz:]\n",
    "            perm_diffs[i] = np.mean(perm_ctz) - np.mean(perm_no_ctz)\n",
    "\n",
    "        # Calculate p-value\n",
    "        p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CV_Analysis:\n",
    "    def __init__(self, eed_object):\n",
    "        \"\"\"\n",
    "        Initialize the CV_Analysis class.\n",
    "\n",
    "        Parameters:\n",
    "            eed_object (ExtractEphysData): An instance of ExtractEphysData which provides\n",
    "                                           access to electrophysiological data methods and attributes.\n",
    "        \"\"\"\n",
    "        self.eed = eed_object\n",
    "\n",
    "    def get_spike_times_for_cell(self, groupname, recordingname, cid):\n",
    "        \"\"\"\n",
    "        Retrieves the spike times for a specific cell based on groupname, recordingname, and cid.\n",
    "\n",
    "        Parameters:\n",
    "            groupname (str): The group name identifier.\n",
    "            recordingname (str): The recording name identifier.\n",
    "            cid (int): The unique cell identifier.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The spike times in samples for the specified cell.\n",
    "        \"\"\"\n",
    "        df = self.eed.dataframes['basic_metrics']\n",
    "        row = df[(df['groupname'] == groupname) & \n",
    "                 (df['recordingname'] == recordingname) & \n",
    "                 (df['cid'] == cid)]\n",
    "        \n",
    "        if not row.empty:\n",
    "            return row['SpikeTimes_all'].values[0]\n",
    "        else:\n",
    "            raise ValueError(\"No matching cell found.\")\n",
    "\n",
    "    def calculate_moving_window_firing_rate(self, spike_times, window_duration_seconds=180, step_size_seconds=1):\n",
    "        \"\"\"\n",
    "        Calculates the firing rate across a moving window of a specified duration.\n",
    "\n",
    "        Parameters:\n",
    "            spike_times (np.array): The spike times in samples for a particular cell.\n",
    "            window_duration_seconds (int): The duration of the moving window in seconds (default is 180 seconds).\n",
    "            step_size_seconds (int): The step size for the moving window in seconds (default is 1 second).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the window start time (in seconds) as the key and the firing rate as the value.\n",
    "        \"\"\"\n",
    "        sampling_rate = 30000  # 30 kHz\n",
    "        window_size_samples = int(window_duration_seconds * sampling_rate)\n",
    "        step_size_samples = int(step_size_seconds * sampling_rate)\n",
    "\n",
    "        max_time_samples = int(spike_times[-1])\n",
    "        firing_rate_dict = {}\n",
    "\n",
    "        for start_time in range(0, max_time_samples - window_size_samples + 1, step_size_samples):\n",
    "            end_time = start_time + window_size_samples\n",
    "            spikes_in_window = np.sum((spike_times >= start_time) & (spike_times < end_time))\n",
    "            firing_rate = spikes_in_window / window_duration_seconds\n",
    "            firing_rate_dict[start_time / sampling_rate] = firing_rate\n",
    "\n",
    "        return firing_rate_dict\n",
    "\n",
    "    def analyze_single_cell(self, groupname, recordingname, cid):\n",
    "        \"\"\"\n",
    "        Analyzes a single cell, calculating the moving window firing rate.\n",
    "\n",
    "        Parameters:\n",
    "            groupname (str): The group name identifier.\n",
    "            recordingname (str): The recording name identifier.\n",
    "            cid (int): The unique cell identifier.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the moving window firing rate over time.\n",
    "        \"\"\"\n",
    "        spike_times = self.get_spike_times_for_cell(groupname, recordingname, cid)\n",
    "        firing_rate_dict = self.calculate_moving_window_firing_rate(spike_times)\n",
    "        return firing_rate_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the data for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class with the path to your .mat file \n",
    "#whisker = ExtractEphysData('/Volumes/MannySSD/', 'all_data_20ms_99CI_FINAL.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organoid = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid/', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rbp4 opto and whikser starts here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker = ExtractEphysData('/Volumes/MannySSD/', 'all_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for opto data\n",
    "rbp4_opto = ExtractEphysData('/Volumes/MannySSD/rbp4_led/','all_data.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager = DataFrameManager(rbp4_opto)\n",
    "rbp4_opto_opto_df_manager.create_psth_dataframe_opto()\n",
    "rbp4_opto_opto_df_manager.dataframes['psth_dataframe_8 Hz LED']['PSTHs_raw'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1, df2 = rbp4_opto_opto_df_manager.compare_groups('CTZ', 'No_CTZ', '8 Hz LED', cell_type=None, is_single_unit=None, stim_responsivity=None) \n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = rbp4_opto_opto_df_manager.extract_stim_signals_opto()\n",
    "x['8 Hz LED'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.plot_rasters_for_cid('CTZ','ctz_2847_rbp4_optostim','cid131')\n",
    "rbp4_opto_opto_df_manager.plot_combined_psth_and_raster('CTZ','ctz_2847_rbp4_optostim','cid131', time_window=None)\n",
    "rbp4_opto_opto_df_manager.plot_combined_psth_and_raster_normalized('CTZ','ctz_2847_rbp4_optostim','cid131', time_window=None, smoothing_window=5, normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.create_dataframe(['Cell_Type', 'LaminarLabel','IsSingleUnit', 'StimResponsivity', 'SpikeTrains_for_PSTHs', 'PSTHs_raw'], 'psth_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.eed.trialTagsLabels['trialTagsLabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.plot_psth_comparison('CTZ', 'No_CTZ', '8 Hz LED', cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-15,250), plot_mode='traces', smoothing_window=10)\n",
    "rbp4_opto_opto_df_manager.plot_psth_comparison('CTZ', 'No_CTZ', '8 Hz LED', cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-15,250), plot_mode='mean', smoothing_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.plot_psth_with_stim_opto('CTZ','No_CTZ',  stim_label='8 Hz LED', cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-15,600), plot_mode='traces', smoothing_window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_opto_stim_responses(df, group_col='groupname', baseline_col='MeanFR_baseline', stim_col='MeanFR_stim', jitter=0.1, plot_individual_points=True):\n",
    "    \"\"\"\n",
    "    Plot the mean, SEM, and optionally individual points with jitter of responses for baseline and stimulation conditions \n",
    "    for two groups present in the DataFrame, along with the count of observations.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data to plot.\n",
    "    group_col (str): Name of the column in df that contains group names.\n",
    "    baseline_col (str): Name of the column in df that contains baseline firing rates.\n",
    "    stim_col (str): Name of the column in df that contains stimulated firing rates.\n",
    "    jitter (float): The width of the jitter around the main x position.\n",
    "    plot_individual_points (bool): Whether to plot individual points with jitter.\n",
    "    \"\"\"\n",
    "    conditions = ['Baseline', 'Stimulation']\n",
    "    groups = df[group_col].unique()\n",
    "    colors = ['blue', 'orange']  # Specify more colors if there are more than two groups\n",
    "    means = {group: [] for group in groups}\n",
    "    sems = {group: [] for group in groups}\n",
    "    counts = {group: [] for group in groups}\n",
    "\n",
    "    # Plotting\n",
    "    x = np.arange(len(conditions))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "    offset = width / (len(groups) * 2)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for i, group in enumerate(groups):\n",
    "        group_data = df[df[group_col] == group]\n",
    "        for j, cond in enumerate([baseline_col, stim_col]):\n",
    "            responses = group_data[cond]\n",
    "            means[group].append(np.median(responses))\n",
    "            sems[group].append(np.std(responses) / np.sqrt(len(responses)))\n",
    "            counts[group].append(len(responses))\n",
    "            \n",
    "            if plot_individual_points:\n",
    "                # Add individual points with jitter\n",
    "                jittered_x = x[j] - offset + i * offset * 2 + np.random.uniform(-jitter, jitter, size=len(responses))\n",
    "                ax.plot(jittered_x, responses, 'o', color=colors[i], alpha=0.2)\n",
    "        \n",
    "        # Add mean and SEM points without horizontal caps\n",
    "        ax.errorbar(x - offset + i * offset * 2, means[group], yerr=sems[group], fmt='o', color=colors[i],\n",
    "                    label=f'{group} (N={counts[group][0]})', capsize=0, elinewidth=2, capthick=2, markersize=5)\n",
    "\n",
    "    # Add some text for labels, title, and custom x-axis tick labels, etc.\n",
    "    ax.set_xlabel('Condition')\n",
    "    ax.set_ylabel('Firing Rate (Hz)')\n",
    "    ax.set_title('Comparison of Baseline vs. Stimulation Responses by Group')\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(conditions)\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def compare_group_firing_rates_with_stats(df, group_col='groupname', baseline_col='MeanFR_baseline', stim_col='PeakEvokedFR'):\n",
    "    \"\"\"\n",
    "    Performs Mann-Whitney U tests to compare baseline and stimulation firing rates between groups\n",
    "    and includes descriptive statistics for each group.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the data to analyze.\n",
    "    group_col (str): Name of the column in df that contains group names.\n",
    "    baseline_col (str): Name of the column in df that contains baseline firing rates.\n",
    "    stim_col (str): Name of the column in df that contains peak evoked firing rates.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with descriptive statistics and Mann-Whitney U test results.\n",
    "    \"\"\"\n",
    "    # Group the DataFrame by 'groupname' and extract values\n",
    "    grouped = df.groupby(group_col)\n",
    "\n",
    "    group_stats = []\n",
    "    for group_name, group_df in grouped:\n",
    "        for condition in [baseline_col, stim_col]:\n",
    "            # Extract values and compute statistics\n",
    "            values = np.array([val.item() for val in group_df[condition] if isinstance(val, np.ndarray)])\n",
    "            group_stats.append({\n",
    "                'Group': group_name,\n",
    "                'Condition': condition,\n",
    "                'N': len(values),\n",
    "                'Mean': np.mean(values),\n",
    "                'Median': np.median(values),\n",
    "                'SD': np.std(values)\n",
    "            })\n",
    "\n",
    "    stats_df = pd.DataFrame(group_stats)\n",
    "\n",
    "    # Calculate Mann-Whitney U tests between groups\n",
    "    test_results = []\n",
    "    groups = list(grouped.groups.keys())\n",
    "    for i in range(len(groups)):\n",
    "        for j in range(i+1, len(groups)):\n",
    "            group1_name, group2_name = groups[i], groups[j]\n",
    "            for condition in [baseline_col, stim_col]:\n",
    "                data1 = np.array([val.item() for val in grouped.get_group(group1_name)[condition] if isinstance(val, np.ndarray)])\n",
    "                data2 = np.array([val.item() for val in grouped.get_group(group2_name)[condition] if isinstance(val, np.ndarray)])\n",
    "                stat, p_value = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "                #change the \n",
    "                \n",
    "                test_results.append({\n",
    "                    'Group1': group1_name,\n",
    "                    'Group2': group2_name,\n",
    "                    'Condition': condition,\n",
    "                    'U-Statistic': stat,\n",
    "                    'p-Value': p_value,\n",
    "                    f'{group1_name} N': len(data1),\n",
    "                    f'{group1_name} Mean': np.mean(data1),\n",
    "                    f'{group1_name} Median': np.median(data1),\n",
    "                    f'{group1_name} SD': np.std(data1),\n",
    "                    f'{group2_name} N': len(data2),\n",
    "                    f'{group2_name} Mean': np.mean(data2),\n",
    "                    f'{group2_name} Median': np.median(data2),\n",
    "                    f'{group2_name} SD': np.std(data2)\n",
    "                })\n",
    "\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    return stats_df, results_df\n",
    "\n",
    "def compare_CTZ_vs_NoCTZ(df, group_col, value_col):\n",
    "    # Filter data for the two specific groups\n",
    "    ctz_data = df[df[group_col] == 'CTZ'][value_col].to_numpy()\n",
    "    no_ctz_data = df[df[group_col] == 'No_CTZ'][value_col].to_numpy()\n",
    "\n",
    "\n",
    "        # Calculate Welch's t-test\n",
    "    stat, p_value = stats.ttest_ind(ctz_data, no_ctz_data, equal_var=False, alternative='greater')\n",
    "        \n",
    "        # Collect results\n",
    "    results = {\n",
    "            'Group1': 'CTZ',\n",
    "            'Group2': 'No_CTZ',\n",
    "            'P-Value': p_value,\n",
    "            'CTZ_Mean': ctz_data.mean(),\n",
    "            'No_CTZ_Mean': no_ctz_data.mean(),\n",
    "            'CTZ_SD': ctz_data.std(),\n",
    "            'No_CTZ_SD': no_ctz_data.std()\n",
    "        }\n",
    "\n",
    "    return pd.DataFrame([results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager = DataFrameManager(rbp4_opto)\n",
    "rbp4_opto_opto_df_manager.eed\n",
    "rbp4_opto_opto_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'StimResponsivity', 'MeanFR_baseline', 'MeanFR_stim', 'PeakEvokedFR', 'PeakEvokedFR_Latency', 'FanoFactor_baseline', 'FanoFactor_stim'], 'basic_metrics')\n",
    "\n",
    "###fast spiking sua for opto \n",
    "fastspiking_opto_df = rbp4_opto_opto_df_manager.get_filtered_data('basic_metrics', is_single_unit=None,  cell_type='FS', stim_responsivity=1.0)\n",
    "#fastspiking_opto_df = fastspiking_opto_df[fastspiking_opto_df['MeanFR_baseline'] > 0.01]\n",
    "plot_opto_stim_responses(fastspiking_opto_df, baseline_col='MeanFR_baseline', stim_col='MeanFR_stim', jitter=0.1, plot_individual_points=True)\n",
    "\n",
    "\n",
    "regularspiking_opto_df = rbp4_opto_opto_df_manager.get_filtered_data('basic_metrics', is_single_unit=1.0, cell_type='RS', stim_responsivity=1.0)\n",
    "#regularspiking_opto_df = regularspiking_opto_df[regularspiking_opto_df['MeanFR_baseline'] > 0.01]\n",
    "plot_opto_stim_responses(regularspiking_opto_df, baseline_col='MeanFR_baseline', stim_col='PeakEvokedFR', jitter=0.1, plot_individual_points=True)\n",
    "\n",
    "\n",
    "mua_opto_df = rbp4_opto_opto_df_manager.get_filtered_data('basic_metrics', is_single_unit=None, cell_type=None, stim_responsivity=1.0)\n",
    "plot_opto_stim_responses(mua_opto_df, baseline_col='MeanFR_baseline', stim_col='MeanFR_stim', jitter=0.1, plot_individual_points=False)\n",
    "\n",
    "mua_opto_df = rbp4_opto_opto_df_manager.get_filtered_data('basic_metrics', is_single_unit=0.0, cell_type=None, stim_responsivity=1.0)\n",
    "plot_opto_stim_responses(mua_opto_df, baseline_col='MeanFR_baseline', stim_col='MeanFR_stim', jitter=0.1, plot_individual_points=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_opto_opto_df_manager.dataframes['basic_metrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type='FS', is_single_unit=1.0, stim_responsivity=1.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=None, show_outliers=True)\n",
    "\n",
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=None, show_outliers=True)\n",
    "\n",
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type=None, is_single_unit=None, stim_responsivity=0.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=None, show_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastspiking_opto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularspiking_opto_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns that should be numeric\n",
    "numeric_columns = ['IsSingleUnit', 'StimResponsivity', 'MeanFR_baseline', 'MeanFR_stim',\n",
    "                   'PeakEvokedFR', 'PeakEvokedFR_Latency', 'FanoFactor_baseline', 'FanoFactor_stim']\n",
    "\n",
    "# Convert these columns to numeric, coercing errors to NaN\n",
    "for col in numeric_columns:\n",
    "    regularspiking_opto_df[col] = pd.to_numeric(regularspiking_opto_df[col], errors='coerce')\n",
    "\n",
    "# After converting, it's good to check if there are any NaNs that were introduced\n",
    "print(regularspiking_opto_df[numeric_columns].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularspiking_opto_df['MeanFR_stim'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start the CV analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisker CCG starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize the ccg handler\n",
    "CrossCorrObj = CCGHandler(whisker, '/Volumes/MannySSD/Data/ccgs')\n",
    "#CrossCorrObj = CCGHandler(whisker, '/Volumes/MannySSD/new_ccgs_manny')\n",
    "CrossCorrObj.ccg['MonoConnectionsTable'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make plot and run stats on CCGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossCorrObj.ccg['MonoConnectionsTable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossCorrObj.ccg['MonoConnectionsTable'].columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the unique celltypes in the ccg table\n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['CellTypes'].unique()\n",
    "print(f'The unique cell types in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"CellTypes\"].unique()}')\n",
    "\n",
    "#print the unique Significance in the ccg table\n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['Significance'].unique()\n",
    "print(f'The unique significance values in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"Significance\"].unique()}')\n",
    "\n",
    "#print the unique sd_sig in the ccg table\n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['sd_sig'].unique()\n",
    "print(f'The unique sd_sig values in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"sd_sig\"].unique()}')\n",
    "\n",
    "#print the unique groupnames in the ccg table\n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['groupname'].unique()\n",
    "print(f'The unique groupnames in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"groupname\"].unique()}')\n",
    "\n",
    "#print the unique StimResp_A\n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['StimResp_A'].unique()\n",
    "print(f'The unique StimResp_A values in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"StimResp_A\"].unique()}')\n",
    "\n",
    "#print the unique StimResp_B\n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['StimResp_B'].unique()\n",
    "print(f'The unique StimResp_B values in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"StimResp_B\"].unique()}')\n",
    "\n",
    "#print the unique layers \n",
    "CrossCorrObj.ccg['MonoConnectionsTable']['layers'].unique()\n",
    "print(f'The unique layers in the CCG table are: {CrossCorrObj.ccg[\"MonoConnectionsTable\"][\"layers\"].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossCorrObj.ccg['PointwiseABs'].shape \n",
    "#print the details of what the shap emeans \n",
    "print(f'The shape of the pointwise abs is {CrossCorrObj.ccg[\"PointwiseABs\"].shape}') # (101, 4878, 2), (time, pairs, and positive and negative PointwiseABs)\n",
    "print(f'The shape represents time, pairs, and positive and negative PointwiseABs') \n",
    "\n",
    "CrossCorrObj.ccg['SimultaneousABs'].shape\n",
    "print(f'The shape of the simultaneous abs is {CrossCorrObj.ccg[\"SimultaneousABs\"].shape}') # (101, 4878, 2), (time, pairs, and positive and negative SimultaneousABs)\n",
    "print(f'The shapre represents time, pairs, and positive and negative SimultaneousABs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ccg(CrossCorrObj, index=0, filter_criteria=None):\n",
    "    # Filter data if criteria are provided\n",
    "    if filter_criteria:\n",
    "        mask = np.ones(len(CrossCorrObj.ccg['MonoConnectionsTable']), dtype=bool)\n",
    "        for key, value in filter_criteria.items():\n",
    "            mask &= CrossCorrObj.ccg['MonoConnectionsTable'][key] == value\n",
    "        filtered_indices = np.where(mask)[0]\n",
    "        if len(filtered_indices) == 0:\n",
    "            print(\"No data matches the filter criteria.\")\n",
    "            return\n",
    "        index = filtered_indices[0]  # Use the first matching index\n",
    "\n",
    "    # Extract data\n",
    "    ccf = CrossCorrObj.ccg['MonoConnectionsTable']['ccgs'][index]\n",
    "    negative_pointwise_ab = CrossCorrObj.ccg['PointwiseABs'][:, index, 0]\n",
    "    positive_pointwise_ab = CrossCorrObj.ccg['PointwiseABs'][:, index, 1]\n",
    "    negative_simultaneous_ab = CrossCorrObj.ccg['SimultaneousABs'][:, index, 0]\n",
    "    positive_simultaneous_ab = CrossCorrObj.ccg['SimultaneousABs'][:, index, 1]\n",
    "    time = CrossCorrObj.ccg['t']\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(time, ccf, label='CCF', color='black')\n",
    "    ax.plot(time, negative_pointwise_ab, label='Negative Pointwise AB', color='red', linestyle='--')\n",
    "    ax.plot(time, positive_pointwise_ab, label='Positive Pointwise AB', color='red', linestyle='--')\n",
    "    ax.plot(time, negative_simultaneous_ab, label='Negative Simultaneous AB', color='blue', linestyle='-.')\n",
    "    ax.plot(time, positive_simultaneous_ab, label='Positive Simultaneous AB', color='blue', linestyle='-.')\n",
    "\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.set_title(f'CCG Analysis for Index {index}')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print some information about the plotted data\n",
    "    print(f\"Plotted CCG for index {index}\")\n",
    "    print(f\"Cell Types: {CrossCorrObj.ccg['MonoConnectionsTable']['CellTypes'][index]}\")\n",
    "    print(f\"Significance: {CrossCorrObj.ccg['MonoConnectionsTable']['Significance'][index]}\")\n",
    "    print(f\"Group Name: {CrossCorrObj.ccg['MonoConnectionsTable']['groupname'][index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ccg(CrossCorrObj)\n",
    "\n",
    "# Example with filtering:\n",
    "filter_criteria = {\n",
    "    'CellTypes': 'RS->RS',\n",
    "   'Significance': True,\n",
    "    'groupname': 'CTZ'\n",
    "}\n",
    "\n",
    "plot_ccg(CrossCorrObj, filter_criteria=filter_criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot figures and run stats, save dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossCorrObj.is_significant_ccg()\n",
    "\n",
    "\n",
    "# Define the conditions you want to iterate over\n",
    "responses = ['responsive', 'nonresponsive']\n",
    "connection_types = ['RS->RS', 'FS->FS']\n",
    "\n",
    "# Other parameters remain consistent\n",
    "min_spike_pairs = 1000  # minimum number of spike pairs (wass 1000)upda\n",
    "sd_sig = True  # use standard deviation to determine significance\n",
    "significance = ['S']  # determine if the ccg is significant based on the old method\n",
    "EorI = None\n",
    "#layers = ['SG->SG', 'SG->IG', 'IG->SG']\n",
    "\n",
    "layers = None\n",
    "\n",
    "# Base directory for all figures\n",
    "base_directory = '/Volumes/MannySSD/figures/synchrony_oddpairs2'\n",
    "\n",
    "# Loop over the response types and connection types\n",
    "for connectiontype in connection_types:\n",
    "    # Create a top-level folder for each connection type\n",
    "    connection_folder = os.path.join(base_directory, connectiontype.replace('->', '_'))\n",
    "    os.makedirs(connection_folder, exist_ok=True)\n",
    "\n",
    "    for response in responses:\n",
    "        # Create subfolders within each connection type folder for each response type\n",
    "        response_folder = os.path.join(connection_folder, response)\n",
    "        os.makedirs(response_folder, exist_ok=True)\n",
    "\n",
    "        # Dynamically create file names based on the current conditions\n",
    "        file_name_base = f\"{connectiontype.replace('->', '_')}_excess_synchrony_{min_spike_pairs}_{sd_sig}_{response}_{layers}_{significance}_{EorI}\"\n",
    "        file_name_dots = f\"{file_name_base}_dots\"\n",
    "        file_name_no_dots = f\"{file_name_base}_no_dots\"\n",
    "\n",
    "        # Plotting and saving figures\n",
    "        CrossCorrObj.plot_group_ccgs_traces(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=(-10,10), front_group='No_CTZ', limit=None)\n",
    "\n",
    "        CrossCorrObj.plot_group_ccgs(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=(-50,50), limit=None, front_group='No_CTZ', directory=response_folder, file_name=file_name_base)\n",
    "\n",
    "        CrossCorrObj.plot_group_ccgs(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=None, limit=None, front_group='No_CTZ', directory=response_folder, file_name=f\"{file_name_base}_full\")\n",
    "\n",
    "        CrossCorrObj.plot_group_ccgs(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=(-10,10), limit=None, front_group='No_CTZ', directory=response_folder, file_name=f\"{file_name_base}_zoomed\")\n",
    "\n",
    "        CrossCorrObj.plot_group_excess_synchrony(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=None, directory=response_folder, file_name=file_name_no_dots)\n",
    "\n",
    "        CrossCorrObj.plot_group_excess_synchrony_with_dots(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=None, directory=response_folder, file_name=file_name_dots)\n",
    "\n",
    "        # Perform the permutation test and store results\n",
    "        results_perm = CrossCorrObj.compare_group_excess_synchrony_permutation(\n",
    "            significance=significance,\n",
    "            sd_sig=sd_sig,\n",
    "            EorI=EorI,\n",
    "            connectiontype=connectiontype,\n",
    "            response=response,\n",
    "            layers=layers,\n",
    "            min_spike_pairs=min_spike_pairs,\n",
    "            num_permutations=10000, \n",
    "            limit=None\n",
    "        )\n",
    "\n",
    "        # Save the permutation results in the appropriate folder\n",
    "        results_perm.to_csv(os.path.join(response_folder, f\"{file_name_base}_results.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSFS_CTZ_synchdf = CrossCorrObj.access_ccgs('CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "FSFS_NoCTZ_synchdf = CrossCorrObj.access_ccgs('No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "RSRS_CTZ_synchdf = CrossCorrObj.access_ccgs('CTZ', significance=significance, sd_sig=sd_sig,  EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "RSRS_NoCTZ_synchdf = CrossCorrObj.access_ccgs('No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this section is for directed weights E/I -- no differences to be seen here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CrossCorrObj.is_significant_ccg()\n",
    "\n",
    "min_spike_pairs = 50; #minimum number of spike pairs\n",
    "sd_sig = None #use standard deviation to determine significance\n",
    "significance = None  #determine if the ccg is significant based old method\n",
    "response = 'responsive' #response to filter by 'responsive' or 'nonresponsive'\n",
    "connectiontype=['FS->FS']\n",
    "EorI=['S']\n",
    "layers = None\n",
    "\n",
    "# change file name to allow more than one connection type\n",
    "file_name = connectiontype[0].replace('->','_') + str(min_spike_pairs)+str(sd_sig)+response+str(layers)+str(significance)+str(EorI)\n",
    "file_name_dots = connectiontype[0].replace('->','_') + str(min_spike_pairs)+str(sd_sig)+response+str(layers)+str(significance)+str(EorI)+'_dots'\n",
    "\n",
    "\n",
    "CrossCorrObj.plot_group_ccgs_traces(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=(-5,5), front_group='No_CTZ', limit=None)\n",
    "CrossCorrObj.plot_group_ccgs(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=(-50,50), limit=None, front_group='No_CTZ', directory='/Volumes/MannySSD/figures/synchrony', file_name=file_name)\n",
    "CrossCorrObj.plot_group_ccgs(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=None, limit=None, front_group='No_CTZ', directory='/Volumes/MannySSD/figures/synchrony', file_name=file_name+'_full')\n",
    "CrossCorrObj.plot_group_ccgs(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, xlim=(-4,4), limit=None, front_group='No_CTZ', directory='/Volumes/MannySSD/figures/synchrony', file_name=file_name+'_zoomed')\n",
    "CrossCorrObj.plot_group_DCW_with_dots(significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=None, directory='/Volumes/MannySSD/figures/synchrony', file_name=file_name_dots)\n",
    "\n",
    "#dcw_results = CrossCorrObj.compare_group_DCW_permutation(\n",
    "#    significance=significance,\n",
    "#    sd_sig=sd_sig,\n",
    "#    EorI=EorI,\n",
    "#    connectiontype=connectiontype,\n",
    "#    response=response,\n",
    "#    layers=layers,\n",
    "#    min_spike_pairs=min_spike_pairs,\n",
    "#    num_permutations=10000, \n",
    "#    limit=None\n",
    "#)\n",
    "\n",
    "results_perm = CrossCorrObj.compare_group_excess_synchrony_permutation(\n",
    "    significance=significance,\n",
    "    sd_sig=sd_sig,\n",
    "    EorI=EorI,\n",
    "    connectiontype=connectiontype,\n",
    "    response=response,\n",
    "    layers=layers,\n",
    "    min_spike_pairs=min_spike_pairs,\n",
    "    num_permutations=10000, \n",
    "    limit=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisker Dataframe initializer Starts Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intialize whisker df manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager = DataFrameManager(whisker)\n",
    "whisker_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'StimResponsivity', \n",
    "                                     'StimProb', 'MeanFR_baseline', 'MeanFR_stim','LaminarLabel',\n",
    "                                     'PeakEvokedFR', 'PeakEvokedFR_Latency', 'FanoFactor_baseline', \n",
    "                                     'FanoFactor_stim', 'SpikeTimes_all', 'MeanFR_inst_stim', 'Template_Channel', \n",
    "                                     'ModulationIndex', 'Normalized_Template_Waveform', 'TroughToPeak_duration', \n",
    "                                     'SpikeHalfWidth', 'UnNormalized_Template_Waveform', 'ISI_violations_percent', \n",
    "                                     'Recording_Duration', 'Sampling_Frequency', 'SpikeTimes_all', 'FirstSpikeLatency', \n",
    "                                     'Stim_Intensity', 'Stim_Onsets_samples', 'Stim_Offsets_samples'], 'basic_metrics')\n",
    "whisker_df_manager.create_psth_dataframe_2() #create the psth dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burst analysis: The run_mannwhitneyu_test and the plot_burst_counts_by_laminar are modiofied from the above, but it works for now, might need to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bursts(row):\n",
    "    \"\"\"\n",
    "    This function identifies burst events and associates them with the corresponding stimulation intensity.\n",
    "    \n",
    "    Parameters:\n",
    "    row: A single row from the DataFrame containing spike times, sampling frequency, and stimulation details.\n",
    "\n",
    "    Returns:\n",
    "    bursts: A list of all identified bursts, where each burst is a list of spike times in samples.\n",
    "    bursts_with_stim: A list of tuples, where each tuple contains:\n",
    "        - A list of spike times that form a burst.\n",
    "        - The corresponding stimulation intensity during which the burst occurred, if applicable.\n",
    "    \"\"\"\n",
    "    spike_times = np.array(row['SpikeTimes_all'])  # Extract spike times in samples from the row\n",
    "    sampling_freq = row['Sampling_Frequency']  # Extract the sampling frequency in Hz\n",
    "    stim_intensities = row['Stim_Intensity']  # Extract the stimulus intensities (1, 2, 3, 4 for Zero, Low, Mid, Max)\n",
    "    stim_onsets = np.array(row['Stim_Onsets_samples'])  # Extract stimulation onset times in samples\n",
    "    stim_offsets = np.array(row['Stim_Offsets_samples'])  # Extract stimulation offset times in samples\n",
    "    \n",
    "    # Convert spike times from samples to milliseconds\n",
    "    time_in_ms = spike_times / sampling_freq * 1000  \n",
    "    \n",
    "    # Calculate Interspike Intervals (ISIs) in milliseconds\n",
    "    isi = np.diff(time_in_ms)\n",
    "    \n",
    "    # Identify burst events: ISIs ≤ 10 ms\n",
    "    burst_mask = isi <= 10  # Boolean array where True indicates that the ISI is part of a burst\n",
    "    \n",
    "    # Initialize lists to store bursts and bursts with associated stimulation information\n",
    "    bursts = []  # This will store all identified bursts\n",
    "    bursts_with_stim = []  # This will store bursts along with their corresponding stimulation intensity\n",
    "    current_burst = []  # Temporarily holds the spike times forming a burst\n",
    "    \n",
    "    for i in range(len(burst_mask)):\n",
    "        if burst_mask[i]:  # If the ISI at index i indicates a burst (ISI ≤ 10 ms)\n",
    "            if not current_burst:  # If no burst is currently being tracked\n",
    "                current_burst.append(spike_times[i])  # Start the burst with the current spike time\n",
    "            current_burst.append(spike_times[i + 1])  # Add the next spike time to the current burst\n",
    "            \n",
    "            # If this is the last ISI and it's part of a burst, finalize the burst\n",
    "            if i == len(burst_mask) - 1:\n",
    "                bursts.append(current_burst)  # Store the burst in the overall list of bursts\n",
    "                bursts_with_stim.append((current_burst, get_stimulation_for_burst(current_burst, stim_onsets, stim_offsets, stim_intensities)))\n",
    "        else:\n",
    "            if current_burst:  # If a burst is being tracked and this ISI ends the burst\n",
    "                bursts.append(current_burst)  # Store the burst in the overall list of bursts\n",
    "                bursts_with_stim.append((current_burst, get_stimulation_for_burst(current_burst, stim_onsets, stim_offsets, stim_intensities)))\n",
    "                current_burst = []  # Reset for the next potential burst\n",
    "    \n",
    "    # The 'bursts' list now contains all identified bursts (each as a list of spike times in samples).\n",
    "    # The 'bursts_with_stim' list contains tuples, where each tuple includes:\n",
    "    #   - A list of spike times forming a burst.\n",
    "    #   - The stimulation intensity (1, 2, 3, 4) during which the burst occurred.\n",
    "    \n",
    "    return bursts, bursts_with_stim\n",
    "\n",
    "def get_stimulation_for_burst(burst, stim_onsets, stim_offsets, stim_intensities):\n",
    "    \"\"\"\n",
    "    Helper function to determine which stimulation intensity a burst occurred during.\n",
    "    \n",
    "    Parameters:\n",
    "    burst: A list of spike times representing a burst.\n",
    "    stim_onsets: An array of stimulation onset times in samples.\n",
    "    stim_offsets: An array of stimulation offset times in samples.\n",
    "    stim_intensities: An array of stimulation intensities corresponding to the onsets and offsets.\n",
    "\n",
    "    Returns:\n",
    "    The stimulation intensity (1, 2, 3, 4) during which the burst occurred, or None if it doesn't match any stimulation window.\n",
    "    \"\"\"\n",
    "    burst_start = burst[0]  # The start of the burst (first spike time in the burst)\n",
    "    burst_end = burst[-1]  # The end of the burst (last spike time in the burst)\n",
    "    \n",
    "    # Loop through all stimulation periods\n",
    "    for onset, offset, intensity in zip(stim_onsets, stim_offsets, stim_intensities):\n",
    "        if burst_start >= onset and burst_end <= offset:  # If the burst fits within a stimulation period\n",
    "            return intensity  # Return the corresponding stimulation intensity\n",
    "    \n",
    "    return None  # If the burst doesn't fit within any stimulation period, return None\n",
    "\n",
    "def count_bursts_by_stimulation(row):\n",
    "    \"\"\"\n",
    "    Counts the number of segments with bursts during no stimulation and stimulation.\n",
    "\n",
    "    Parameters:\n",
    "    row: A single row from the DataFrame containing the bursts with stimulation information.\n",
    "\n",
    "    Returns:\n",
    "    A tuple containing the counts:\n",
    "        - Count of segments with bursts during no stimulation (None or 1.0)\n",
    "        - Count of segments with bursts during stimulation (2.0, 3.0, 4.0)\n",
    "    \"\"\"\n",
    "    bursts_with_stim = row['Bursts_with_Stim']\n",
    "    \n",
    "    # Initialize counts\n",
    "    no_stim_count = 0\n",
    "    stim_count = 0\n",
    "    \n",
    "    # Iterate through the bursts_with_stim list\n",
    "    for burst, stim in bursts_with_stim:\n",
    "        if stim is None or stim == 1.0:\n",
    "            no_stim_count += 1\n",
    "        elif stim in [2.0, 3.0, 4.0]:\n",
    "            stim_count += 1\n",
    "    \n",
    "    return no_stim_count, stim_count\n",
    "\n",
    "def count_bursts_by_cid_and_laminar(row):\n",
    "    \"\"\"\n",
    "    Counts the number of segments with bursts during no stimulation and stimulation per cid and LaminarLabel.\n",
    "\n",
    "    Parameters:\n",
    "    row: A single row from the DataFrame containing the bursts with stimulation information.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing:\n",
    "        - groupname: The group (CTZ or No_CTZ)\n",
    "        - recordingname: The recording name\n",
    "        - cid: The cell ID\n",
    "        - LaminarLabel: The laminar label (SG, L4, IG)\n",
    "        - No_Stim_Burst_Count: Count of segments with bursts during no stimulation (None or 1.0)\n",
    "        - Stim_Burst_Count: Count of segments with bursts during stimulation (2.0, 3.0, 4.0)\n",
    "    \"\"\"\n",
    "    bursts_with_stim = row['Bursts_with_Stim']\n",
    "    \n",
    "    # Initialize counts\n",
    "    no_stim_count = 0\n",
    "    stim_count = 0\n",
    "    \n",
    "    # Iterate through the bursts_with_stim list\n",
    "    for burst, stim in bursts_with_stim:\n",
    "        if stim is None or stim == 1.0:\n",
    "            no_stim_count += 1\n",
    "        elif stim in [2.0, 3.0, 4.0]:\n",
    "            stim_count += 1\n",
    "    \n",
    "    return pd.Series({\n",
    "        'groupname': row['groupname'],\n",
    "        'recordingname': row['recordingname'],\n",
    "        'cid': row['cid'],\n",
    "        'LaminarLabel': row['LaminarLabel'],\n",
    "        'Cell_Type': row['Cell_Type'],\n",
    "        'ModulationIndex': row['ModulationIndex'],\n",
    "        'IsSingleUnit': row['IsSingleUnit'],\n",
    "        'StimResponsivity': row['StimResponsivity'],\n",
    "        'StimProb': row['StimProb'],\n",
    "        'No_Stim_Burst_Count': no_stim_count,\n",
    "        'Stim_Burst_Count': stim_count\n",
    "    })\n",
    "\n",
    "def plot_burst_counts_by_laminar(melted_data, show_outliers=True):\n",
    "    \"\"\"\n",
    "    Plots the burst counts broken down by LaminarLabel and optionally omits outliers.\n",
    "\n",
    "    Parameters:\n",
    "    melted_data: The melted DataFrame containing burst counts.\n",
    "    show_outliers: Boolean indicating whether to display outliers in the box plot (default is True).\n",
    "    \"\"\"\n",
    "    # Define the color palette\n",
    "    palette = {\"CTZ\": \"purple\", \"No_CTZ\": \"grey\"}\n",
    "\n",
    "    # Specify the order of the LaminarLabels\n",
    "    laminar_order = ['SG', 'L4', 'IG']\n",
    "\n",
    "    # Specify the hue order to ensure consistent plotting\n",
    "    hue_order = [\"CTZ\", \"No_CTZ\"]\n",
    "\n",
    "    # Create a FacetGrid to plot data for each LaminarLabel separately\n",
    "    g = sns.FacetGrid(melted_data, col=\"LaminarLabel\", height=5, aspect=1.5, col_order=laminar_order)\n",
    "\n",
    "    # Map the boxplot to the grid, with control over outlier display\n",
    "    g.map(sns.boxplot, \"Condition\", \"Burst_Count\", \"groupname\", \n",
    "          palette=palette, hue_order=hue_order, \n",
    "          order=['No_Stim_Burst_Count', 'Stim_Burst_Count'], showfliers=show_outliers)\n",
    "\n",
    "    # Set the labels and title\n",
    "    g.set_axis_labels('Condition', 'Burst Count')\n",
    "    g.set_titles(col_template='{col_name}')\n",
    "    g.add_legend(title='Group')\n",
    "\n",
    "    # Adjust the layout and show the plot\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    g.fig.suptitle('Comparison of Burst Counts by Laminar Label between CTZ and No_CTZ Groups')\n",
    "    plt.show()\n",
    "#\n",
    "# \n",
    "# \n",
    "# Apply the burst calculation function to each row in the DataFrame\n",
    "whisker_df_manager.dataframes['basic_metrics'][['Bursts', 'Bursts_with_Stim']] = whisker_df_manager.dataframes['basic_metrics'].apply(calculate_bursts, axis=1, result_type='expand')\n",
    "\n",
    "# Apply the function to count bursts for no stimulation and stimulation conditions\n",
    "whisker_df_manager.dataframes['basic_metrics'][['No_Stim_Burst_Count', 'Stim_Burst_Count']] = whisker_df_manager.dataframes['basic_metrics'].apply(count_bursts_by_stimulation, axis=1, result_type='expand')\n",
    "\n",
    "# Apply the function to calculate burst counts per cid and LaminarLabel\n",
    "burst_counts_per_cid_laminar = whisker_df_manager.dataframes['basic_metrics'].apply(count_bursts_by_cid_and_laminar, axis=1)\n",
    "\n",
    "# Prepare the data for plotting\n",
    "#melted_data_laminar = pd.melt(burst_counts_per_cid_laminar, id_vars=['groupname', 'recordingname', 'cid', 'LaminarLabel'], \n",
    "#                              value_vars=['No_Stim_Burst_Count', 'Stim_Burst_Count'],\n",
    "#                              var_name='Condition', value_name='Burst_Count')\n",
    "\n",
    "\n",
    "# Example usage with the option to hide outliers\n",
    "#plot_burst_counts_by_laminar(melted_data_laminar, show_outliers=False)\n",
    "\n",
    "\n",
    "def filter_and_melt_data(df, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_index=None):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on the specified criteria and melts it for plotting,\n",
    "    while ensuring that all necessary columns are carried over.\n",
    "\n",
    "    Parameters:\n",
    "    df: The DataFrame containing burst data.\n",
    "    cell_type: Filter based on 'Cell_Type' ('FS', 'RS', or None for no filtering).\n",
    "    is_single_unit: Filter based on 'IsSingleUnit' (1.0, 0.0, or None for no filtering).\n",
    "    stim_responsivity: Filter based on 'StimResponsivity' (-1.0, 0.0, 1.0, or None for no filtering).\n",
    "    modulation_index: Filter based on 'ModulationIndex' ('positive', 'negative', 'none', 'modulated', or None for no filtering).\n",
    "\n",
    "    Returns:\n",
    "    melted_data: The melted DataFrame, ready for plotting.\n",
    "    \"\"\"\n",
    "    # Apply filters based on user input\n",
    "    if cell_type is not None:\n",
    "        df = df[df['Cell_Type'] == cell_type]\n",
    "    \n",
    "    if is_single_unit is not None:\n",
    "        df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "    \n",
    "    if stim_responsivity is not None:\n",
    "        df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "    \n",
    "    if modulation_index is not None:\n",
    "        if modulation_index == 'modulated':\n",
    "            df = df[df['ModulationIndex'].isin(['positive', 'negative'])]\n",
    "        else:\n",
    "            df = df[df['ModulationIndex'] == modulation_index]\n",
    "\n",
    "    # Melt the DataFrame for plotting, ensuring all necessary columns are carried over\n",
    "    melted_data = pd.melt(\n",
    "        df,\n",
    "        id_vars=['groupname', 'recordingname', 'cid', 'LaminarLabel', 'Cell_Type', \n",
    "                 'IsSingleUnit', 'StimResponsivity', 'ModulationIndex'],\n",
    "        value_vars=['No_Stim_Burst_Count', 'Stim_Burst_Count'],\n",
    "        var_name='Condition', value_name='Burst_Count'\n",
    "    )\n",
    "    \n",
    "    return melted_data\n",
    "\n",
    "melted_data_laminar_filtered = filter_and_melt_data(\n",
    "    burst_counts_per_cid_laminar,\n",
    "    cell_type=None,\n",
    "    is_single_unit=0.0,\n",
    "    stim_responsivity=None,\n",
    "    modulation_index='modulated'  # Using the new \"modulated\" option\n",
    ")\n",
    "# Now you can plot the filtered data\n",
    "plot_burst_counts_by_laminar(melted_data_laminar_filtered, show_outliers=False)\n",
    "\n",
    "\n",
    "def run_mannwhitneyu_test(data, laminar_label, condition, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_index=None):\n",
    "    \"\"\"\n",
    "    Perform the Mann-Whitney U test between CTZ and No_CTZ for a given LaminarLabel and Condition,\n",
    "    with optional filtering based on Cell_Type, IsSingleUnit, StimResponsivity, and ModulationIndex.\n",
    "\n",
    "    Parameters:\n",
    "    data: The melted DataFrame containing burst counts.\n",
    "    laminar_label: The specific LaminarLabel to filter by (e.g., 'SG', 'L4', 'IG').\n",
    "    condition: The specific condition to filter by ('No_Stim_Burst_Count' or 'Stim_Burst_Count').\n",
    "    cell_type: Filter based on 'Cell_Type' ('FS', 'RS', or None for no filtering).\n",
    "    is_single_unit: Filter based on 'IsSingleUnit' (1.0, 0.0, or None for no filtering).\n",
    "    stim_responsivity: Filter based on 'StimResponsivity' (-1.0, 0.0, 1.0, or None for no filtering).\n",
    "    modulation_index: Filter based on 'ModulationIndex' ('positive', 'negative', 'none', 'modulated', or None for no filtering).\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the test results: U statistic, p-value, and whether the result is significant.\n",
    "    \"\"\"\n",
    "    # Filter the data for the specified LaminarLabel and Condition\n",
    "    subset = data[(data['LaminarLabel'] == laminar_label) & (data['Condition'] == condition)]\n",
    "    \n",
    "    # Apply additional filters based on user input\n",
    "    if cell_type is not None:\n",
    "        subset = subset[subset['Cell_Type'] == cell_type]\n",
    "    \n",
    "    if is_single_unit is not None:\n",
    "        subset = subset[subset['IsSingleUnit'] == is_single_unit]\n",
    "    \n",
    "    if stim_responsivity is not None:\n",
    "        subset = subset[subset['StimResponsivity'] == stim_responsivity]\n",
    "    \n",
    "    if modulation_index is not None:\n",
    "        if modulation_index == 'modulated':\n",
    "            subset = subset[subset['ModulationIndex'].isin(['positive', 'negative'])]\n",
    "        else:\n",
    "            subset = subset[subset['ModulationIndex'] == modulation_index]\n",
    "\n",
    "    # Separate the data into the two groups\n",
    "    ctz_data = subset[subset['groupname'] == 'CTZ']['Burst_Count']\n",
    "    no_ctz_data = subset[subset['groupname'] == 'No_CTZ']['Burst_Count']\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    u_stat, p_value = mannwhitneyu(ctz_data, no_ctz_data, alternative='two-sided')\n",
    "    \n",
    "    # Determine significance (typically p < 0.05 is considered significant)\n",
    "    significance = p_value < 0.05\n",
    "    \n",
    "    return {\n",
    "        'LaminarLabel': laminar_label,\n",
    "        'Condition': condition,\n",
    "        'Cell_Type': cell_type,\n",
    "        'IsSingleUnit': is_single_unit,\n",
    "        'StimResponsivity': stim_responsivity,\n",
    "        'ModulationIndex': modulation_index,\n",
    "        'U Statistic': u_stat,\n",
    "        'p-value': p_value,\n",
    "        'Significant': significance\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage with filtering and running the test for each combination of LaminarLabel and Condition\n",
    "results = []\n",
    "for laminar_label in ['SG', 'L4', 'IG']:\n",
    "    for condition in ['No_Stim_Burst_Count', 'Stim_Burst_Count']:\n",
    "        result = run_mannwhitneyu_test(\n",
    "            melted_data_laminar_filtered, \n",
    "            laminar_label, \n",
    "            condition, \n",
    "            cell_type=None,\n",
    "            is_single_unit=0.0,\n",
    "            stim_responsivity=None,\n",
    "            modulation_index='modulated')  # Using the new \"modulated\" option)\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "# Convert the results to a DataFrame for easier interpretation\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mannwhitneyu_test(data, laminar_label, condition, plot_filename, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_index=None):\n",
    "    \"\"\"\n",
    "    Perform the Mann-Whitney U test between CTZ and No_CTZ for a given LaminarLabel and Condition,\n",
    "    with optional filtering based on Cell_Type, IsSingleUnit, StimResponsivity, and ModulationIndex.\n",
    "    Also calculates descriptive statistics (N, mean, median, SD, min, max) for both groups.\n",
    "\n",
    "    Parameters:\n",
    "    data: The melted DataFrame containing burst counts.\n",
    "    laminar_label: The specific LaminarLabel to filter by (e.g., 'SG', 'L4', 'IG').\n",
    "    condition: The specific condition to filter by ('No_Stim_Burst_Count' or 'Stim_Burst_Count').\n",
    "    plot_filename: The name of the plot file that was saved.\n",
    "    cell_type: Filter based on 'Cell_Type' ('FS', 'RS', or None for no filtering).\n",
    "    is_single_unit: Filter based on 'IsSingleUnit' (1.0, 0.0, or None for no filtering).\n",
    "    stim_responsivity: Filter based on 'StimResponsivity' (-1.0, 0.0, 1.0, or None for no filtering).\n",
    "    modulation_index: Filter based on 'ModulationIndex' ('positive', 'negative', 'none', 'modulated', or None for no filtering).\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing the test results: U statistic, p-value, whether the result is significant,\n",
    "    and descriptive statistics for both No_CTZ and CTZ groups, and the name of the plot file.\n",
    "    \"\"\"\n",
    "    # Filter the data for the specified LaminarLabel and Condition\n",
    "    subset = data[(data['LaminarLabel'] == laminar_label) & (data['Condition'] == condition)]\n",
    "    \n",
    "    # Apply additional filters based on user input\n",
    "    if cell_type is not None:\n",
    "        subset = subset[subset['Cell_Type'] == cell_type]\n",
    "    \n",
    "    if is_single_unit is not None:\n",
    "        subset = subset[subset['IsSingleUnit'] == is_single_unit]\n",
    "    \n",
    "    if stim_responsivity is not None:\n",
    "        subset = subset[subset['StimResponsivity'] == stim_responsivity]\n",
    "    \n",
    "    if modulation_index is not None:\n",
    "        if modulation_index == 'modulated':\n",
    "            subset = subset[subset['ModulationIndex'].isin(['positive', 'negative'])]\n",
    "        else:\n",
    "            subset = subset[subset['ModulationIndex'] == modulation_index]\n",
    "\n",
    "    # Separate the data into the two groups\n",
    "    ctz_data = subset[subset['groupname'] == 'CTZ']['Burst_Count']\n",
    "    no_ctz_data = subset[subset['groupname'] == 'No_CTZ']['Burst_Count']\n",
    "    \n",
    "    # Check if both groups have data\n",
    "    if ctz_data.empty or no_ctz_data.empty:\n",
    "        return {\n",
    "            'LaminarLabel': laminar_label,\n",
    "            'Condition': condition,\n",
    "            'Cell_Type': cell_type,\n",
    "            'IsSingleUnit': is_single_unit,\n",
    "            'StimResponsivity': stim_responsivity,\n",
    "            'ModulationIndex': modulation_index,\n",
    "            'U Statistic': None,\n",
    "            'p-value': None,\n",
    "            'Significant': False,\n",
    "            'CTZ_N': len(ctz_data),\n",
    "            'CTZ_Mean': np.nan,\n",
    "            'CTZ_Median': np.nan,\n",
    "            'CTZ_SD': np.nan,\n",
    "            'CTZ_Min': np.nan,\n",
    "            'CTZ_Max': np.nan,\n",
    "            'No_CTZ_N': len(no_ctz_data),\n",
    "            'No_CTZ_Mean': np.nan,\n",
    "            'No_CTZ_Median': np.nan,\n",
    "            'No_CTZ_SD': np.nan,\n",
    "            'No_CTZ_Min': np.nan,\n",
    "            'No_CTZ_Max': np.nan,\n",
    "            'Note': f\"Test not performed due to insufficient data; Plot saved as {plot_filename}\"\n",
    "        }\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    u_stat, p_value = mannwhitneyu(ctz_data, no_ctz_data, alternative='two-sided')\n",
    "    \n",
    "    # Determine significance (typically p < 0.05 is considered significant)\n",
    "    significance = p_value < 0.05\n",
    "\n",
    "    # Calculate descriptive statistics for CTZ\n",
    "    ctz_mean = ctz_data.mean()\n",
    "    ctz_median = ctz_data.median()\n",
    "    ctz_sd = ctz_data.std()\n",
    "    ctz_min = ctz_data.min()\n",
    "    ctz_max = ctz_data.max()\n",
    "    ctz_n = len(ctz_data)\n",
    "    \n",
    "    # Calculate descriptive statistics for No_CTZ\n",
    "    no_ctz_mean = no_ctz_data.mean()\n",
    "    no_ctz_median = no_ctz_data.median()\n",
    "    no_ctz_sd = no_ctz_data.std()\n",
    "    no_ctz_min = no_ctz_data.min()\n",
    "    no_ctz_max = no_ctz_data.max()\n",
    "    no_ctz_n = len(no_ctz_data)\n",
    "    \n",
    "    return {\n",
    "        'LaminarLabel': laminar_label,\n",
    "        'Condition': condition,\n",
    "        'Cell_Type': cell_type,\n",
    "        'IsSingleUnit': is_single_unit,\n",
    "        'StimResponsivity': stim_responsivity,\n",
    "        'ModulationIndex': modulation_index,\n",
    "        'U Statistic': u_stat,\n",
    "        'p-value': p_value,\n",
    "        'Significant': significance,\n",
    "        'CTZ_N': ctz_n,\n",
    "        'CTZ_Mean': ctz_mean,\n",
    "        'CTZ_Median': ctz_median,\n",
    "        'CTZ_SD': ctz_sd,\n",
    "        'CTZ_Min': ctz_min,\n",
    "        'CTZ_Max': ctz_max,\n",
    "        'No_CTZ_N': no_ctz_n,\n",
    "        'No_CTZ_Mean': no_ctz_mean,\n",
    "        'No_CTZ_Median': no_ctz_median,\n",
    "        'No_CTZ_SD': no_ctz_sd,\n",
    "        'No_CTZ_Min': no_ctz_min,\n",
    "        'No_CTZ_Max': no_ctz_max,\n",
    "        'Note': f\"Plot saved as {plot_filename}\"\n",
    "    }\n",
    "def plot_burst_counts_by_laminar(melted_data, show_outliers=True):\n",
    "    \"\"\"\n",
    "    Plots the burst counts broken down by LaminarLabel and optionally omits outliers.\n",
    "\n",
    "    Parameters:\n",
    "    melted_data: The melted DataFrame containing burst counts.\n",
    "    show_outliers: Boolean indicating whether to display outliers in the box plot (default is True).\n",
    "\n",
    "    Returns:\n",
    "    g: The FacetGrid object, which can be used for saving the plot.\n",
    "    \"\"\"\n",
    "    # Define the color palette\n",
    "    palette = {\"CTZ\": \"purple\", \"No_CTZ\": \"grey\"}\n",
    "\n",
    "    # Specify the order of the LaminarLabels\n",
    "    laminar_order = ['SG', 'L4', 'IG']\n",
    "\n",
    "    # Specify the hue order to ensure consistent plotting\n",
    "    hue_order = [\"CTZ\", \"No_CTZ\"]\n",
    "\n",
    "    # Create a FacetGrid to plot data for each LaminarLabel separately\n",
    "    g = sns.FacetGrid(melted_data, col=\"LaminarLabel\", height=5, aspect=1.5, col_order=laminar_order)\n",
    "\n",
    "    # Map the boxplot to the grid, with control over outlier display\n",
    "    g.map(sns.boxplot, \"Condition\", \"Burst_Count\", \"groupname\", \n",
    "          palette=palette, hue_order=hue_order, \n",
    "          order=['No_Stim_Burst_Count', 'Stim_Burst_Count'], showfliers=show_outliers)\n",
    "\n",
    "    # Set the labels and title\n",
    "    g.set_axis_labels('Condition', 'Burst Count')\n",
    "    g.set_titles(col_template='{col_name}')\n",
    "    g.add_legend(title='Group')\n",
    "\n",
    "    # Adjust the layout\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    g.fig.suptitle('Comparison of Burst Counts by Laminar Label between CTZ and No_CTZ Groups')\n",
    "\n",
    "    return g  # Return the FacetGrid object for saving the plot\n",
    "\n",
    "# Define the output base directory\n",
    "output_base_dir = \"/Volumes/MannySSD/figures/laminaranalysis_bursts\"\n",
    "\n",
    "# Check if the base directory exists, and if not, create it\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Define possible values for each filter\n",
    "cell_types = [None]\n",
    "is_single_units = [0.0, None]\n",
    "stim_responsivities = [ 1.0, None]\n",
    "modulation_indices = ['positive', 'modulated', None]\n",
    "\n",
    "# Initialize a list to store all results\n",
    "all_results = []\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    for is_single_unit in is_single_units:\n",
    "        for stim_responsivity in stim_responsivities:\n",
    "            for modulation_index in modulation_indices:\n",
    "                # Filter and melt the data\n",
    "                melted_data_laminar_filtered = filter_and_melt_data(\n",
    "                    burst_counts_per_cid_laminar,\n",
    "                    cell_type=cell_type,\n",
    "                    is_single_unit=is_single_unit,\n",
    "                    stim_responsivity=stim_responsivity,\n",
    "                    modulation_index=modulation_index\n",
    "                )\n",
    "                \n",
    "                # Create a hierarchical directory structure for saving the plots\n",
    "                dir_name = f\"{cell_type}_{is_single_unit}_{stim_responsivity}_{modulation_index}\"\n",
    "                dir_name = dir_name.replace(\"None\", \"ALL\")  # Replace None with ALL for directory naming\n",
    "                plot_dir = os.path.join(output_base_dir, dir_name)\n",
    "                os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "                # Plot the data and save the plot\n",
    "                plot_filename = f\"plot_{cell_type}_{is_single_unit}_{stim_responsivity}_{modulation_index}.svg\"\n",
    "                plot_filepath = os.path.join(plot_dir, plot_filename)\n",
    "                g = plot_burst_counts_by_laminar(melted_data_laminar_filtered, show_outliers=False)\n",
    "                g.savefig(plot_filepath, format='svg', transparent=True)\n",
    "    \n",
    "                # Close the plot to prevent it from displaying in the notebook\n",
    "                plt.close(g.fig)\n",
    "\n",
    "                # Run the Mann-Whitney U test for each combination of LaminarLabel and Condition\n",
    "                for laminar_label in ['SG', 'L4', 'IG']:\n",
    "                    for condition in ['No_Stim_Burst_Count', 'Stim_Burst_Count']:\n",
    "                        result = run_mannwhitneyu_test(\n",
    "                            melted_data_laminar_filtered,\n",
    "                            laminar_label,\n",
    "                            condition,\n",
    "                            plot_filename,  # Pass the plot filename to the function\n",
    "                            cell_type=cell_type,\n",
    "                            is_single_unit=is_single_unit,\n",
    "                            stim_responsivity=stim_responsivity,\n",
    "                            modulation_index=modulation_index\n",
    "                        )\n",
    "                        all_results.append(result)\n",
    "\n",
    "# Convert all results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_filepath = os.path.join(output_base_dir, \"mannwhitneyu_results.csv\")\n",
    "results_df.to_csv(results_filepath, index=False)\n",
    "\n",
    "print(f\"Plots saved to {output_base_dir}\")\n",
    "print(f\"Statistical results saved to {results_filepath}\")\n",
    "# Convert all results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_filepath = os.path.join(output_base_dir, \"mannwhitneyu_results.csv\")\n",
    "results_df.to_csv(results_filepath, index=False)\n",
    "\n",
    "print(f\"Plots saved to {output_base_dir}\")\n",
    "print(f\"Statistical results saved to {results_filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#repeat this but make it have clear scentences\n",
    "# Print unique values in the 'Cell_Type' column\n",
    "unique_cell_types = whisker_df_manager.dataframes['basic_metrics']['Cell_Type'].unique()\n",
    "print(f\"Unique values in 'Cell_Type' column: {unique_cell_types}\")\n",
    "\n",
    "# Print unique values in the 'IsSingleUnit' column\n",
    "unique_is_single_units = whisker_df_manager.dataframes['basic_metrics']['IsSingleUnit'].unique()\n",
    "\n",
    "# Print unique values in the 'StimResponsivity' column\n",
    "unique_stim_responsivities = whisker_df_manager.dataframes['basic_metrics']['StimResponsivity'].unique()\n",
    "\n",
    "# Print unique values in the 'ModulationIndex' column\n",
    "unique_modulation_indices = whisker_df_manager.dataframes['basic_metrics']['ModulationIndex'].unique()\n",
    "\n",
    "# Print unique values in the 'LaminarLabel' column\n",
    "unique_laminar_labels = whisker_df_manager.dataframes['basic_metrics']['LaminarLabel'].unique()\n",
    "\n",
    "# Print unique values in the 'IsSingleUnit' column\n",
    "print(f\"Unique values in 'IsSingleUnit' column: {unique_is_single_units}\")\n",
    "print(f\"Unique values in 'StimResponsivity' column: {unique_stim_responsivities}\")\n",
    "print(f\"Unique values in 'ModulationIndex' column: {unique_modulation_indices}\")\n",
    "print(f\"Unique values in 'LaminarLabel' column: {unique_laminar_labels}\")\n",
    "print(f\"Unique values in 'StimResponsivity' column: {unique_stim_responsivities}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.dataframes['basic_metrics'] \n",
    "\n",
    "#find the CTZ groups with the most cells in them \n",
    "whisker_df_manager.dataframes['basic_metrics'].groupby(['groupname', 'recordingname']).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSCAnalysis:\n",
    "    def __init__(self, whisker_df_manager):\n",
    "        self.whisker_df_manager = whisker_df_manager\n",
    "        self.df = whisker_df_manager.dataframes['basic_metrics']\n",
    "        self.results_df = pd.DataFrame()\n",
    "        self.color_map = {'CTZ': 'purple', 'No_CTZ': 'grey'}\n",
    "        self.intensity_order = ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "    \n",
    "    def _bin_spikes(self, spike_times, stim_windows, bin_size, pre_stim_samples, post_stim_samples):\n",
    "        \"\"\"Bin spike times into a histogram for each stimulation window.\"\"\"\n",
    "        max_window_length = max(offset - onset + pre_stim_samples + post_stim_samples for onset, offset in stim_windows)\n",
    "        n_bins = int(np.ceil(max_window_length / bin_size))\n",
    "        \n",
    "        binned_counts = np.zeros((len(stim_windows), n_bins))\n",
    "        \n",
    "        for i, (onset, offset) in enumerate(stim_windows):\n",
    "            window_start = onset - pre_stim_samples\n",
    "            window_end = offset + post_stim_samples\n",
    "            window_spikes = spike_times[(spike_times >= window_start) & (spike_times < window_end)] - window_start\n",
    "            \n",
    "            binned_counts[i], _ = np.histogram(window_spikes, bins=n_bins, range=(0, max_window_length))\n",
    "        \n",
    "        return binned_counts\n",
    "    \n",
    "    def calculate_rsc(self, recording_name, bin_size_ms=50, stim_intensity=None, \n",
    "                      pre_stim_ms=0, post_stim_ms=0, cell_type=None, \n",
    "                      is_single_unit=None, stim_responsivity=None, \n",
    "                      modulation_index=None, laminar_label=None):\n",
    "        \"\"\"Calculate rSC correlations between cells for a specific recording and stimulation condition.\"\"\"\n",
    "        recording_df = self.df[self.df['recordingname'] == recording_name]\n",
    "        \n",
    "        # Get the groupname for this recording\n",
    "        groupname = recording_df['groupname'].iloc[0] if 'groupname' in recording_df.columns else 'Unknown'\n",
    "        \n",
    "        # Apply filters\n",
    "        if cell_type is not None:\n",
    "            recording_df = recording_df[recording_df['Cell_Type'] == cell_type]\n",
    "        if is_single_unit is not None:\n",
    "            recording_df = recording_df[recording_df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity is not None:\n",
    "            recording_df = recording_df[recording_df['StimResponsivity'] == stim_responsivity]\n",
    "        if modulation_index is not None:\n",
    "            recording_df = recording_df[recording_df['ModulationIndex'] == modulation_index]\n",
    "        if laminar_label is not None:\n",
    "            recording_df = recording_df[recording_df['LaminarLabel'] == laminar_label]\n",
    "        \n",
    "        if recording_df.empty:\n",
    "            print(f\"Warning: No data available for the specified filters in recording {recording_name}\")\n",
    "            return None, None\n",
    "        \n",
    "        sampling_freq = recording_df['Sampling_Frequency'].iloc[0]\n",
    "        bin_size_samples = int(bin_size_ms * sampling_freq / 1000)\n",
    "        pre_stim_samples = int(pre_stim_ms * sampling_freq / 1000)\n",
    "        post_stim_samples = int(post_stim_ms * sampling_freq / 1000)\n",
    "        \n",
    "        binned_spikes = {}\n",
    "        cell_info = {}\n",
    "        \n",
    "        for _, row in recording_df.iterrows():\n",
    "            cid = row['cid']\n",
    "            spike_times = row['SpikeTimes_all']\n",
    "            stim_onsets = row['Stim_Onsets_samples']\n",
    "            stim_offsets = row['Stim_Offsets_samples']\n",
    "            stim_intensities = row['Stim_Intensity']\n",
    "            \n",
    "            # Filter stimulation windows based on intensity\n",
    "            if stim_intensity is not None:\n",
    "                stim_windows = [(onset, offset) for onset, offset, intensity in \n",
    "                                zip(stim_onsets, stim_offsets, stim_intensities) \n",
    "                                if intensity == stim_intensity]\n",
    "            else:\n",
    "                stim_windows = list(zip(stim_onsets, stim_offsets))\n",
    "            \n",
    "            if not stim_windows:\n",
    "                print(f\"Warning: No stimulation windows found for cell {cid} in recording {recording_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Bin the spikes\n",
    "            binned_counts = self._bin_spikes(spike_times, stim_windows, bin_size_samples, pre_stim_samples, post_stim_samples)\n",
    "            binned_spikes[cid] = binned_counts.flatten()  # Flatten to 1D array for correlation calculation\n",
    "            \n",
    "            cell_info[cid] = {\n",
    "                'Cell_Type': row['Cell_Type'],\n",
    "                'LaminarLabel': row['LaminarLabel'],\n",
    "                'MeanFR_baseline': row['MeanFR_baseline'],\n",
    "                'MeanFR_stim': row['MeanFR_stim'],\n",
    "                'IsSingleUnit': row['IsSingleUnit'],\n",
    "                'StimResponsivity': row['StimResponsivity'],\n",
    "                'ModulationIndex': row['ModulationIndex']\n",
    "            }\n",
    "        \n",
    "        if len(binned_spikes) < 2:\n",
    "            print(f\"Warning: Not enough cells ({len(binned_spikes)}) for correlation analysis in recording {recording_name}\")\n",
    "            return None, None\n",
    "        \n",
    "        # Calculate correlations\n",
    "        cids = list(binned_spikes.keys())\n",
    "        num_cells = len(cids)\n",
    "        corr_matrix = np.zeros((num_cells, num_cells))\n",
    "        \n",
    "        for i in range(num_cells):\n",
    "            for j in range(i+1, num_cells):\n",
    "                corr, _ = pearsonr(binned_spikes[cids[i]], binned_spikes[cids[j]])\n",
    "                corr_matrix[i, j] = corr\n",
    "                corr_matrix[j, i] = corr\n",
    "        \n",
    "        corr_df = pd.DataFrame(corr_matrix, index=cids, columns=cids)\n",
    "        \n",
    "        # Update results dataframe\n",
    "        self._update_results_df(recording_name, corr_df, cell_info, bin_size_ms, stim_intensity, pre_stim_ms, post_stim_ms, groupname)\n",
    "        \n",
    "        return corr_df, cell_info\n",
    "\n",
    "    def _update_results_df(self, recording_name, corr_df, cell_info, bin_size_ms, stim_intensity, pre_stim_ms, post_stim_ms, groupname):\n",
    "        \"\"\"Update the results dataframe with the calculated correlations and metadata.\"\"\"\n",
    "        intensity_labels = {1: 'Zero', 2: 'Low', 3: 'Mid', 4: 'Max'}\n",
    "        stim_label = intensity_labels.get(stim_intensity, 'All')\n",
    "        \n",
    "        new_rows = []\n",
    "        for cid1 in corr_df.index:\n",
    "            for cid2 in corr_df.columns:\n",
    "                if cid1 < cid2:  # Avoid duplicates and self-correlations\n",
    "                    new_row = {\n",
    "                        'recording_name': recording_name,\n",
    "                        'groupname': groupname,  # Add groupname to the results\n",
    "                        'cid1': cid1,\n",
    "                        'cid2': cid2,\n",
    "                        'cell_type1': cell_info[cid1]['Cell_Type'],\n",
    "                        'cell_type2': cell_info[cid2]['Cell_Type'],\n",
    "                        'laminar_label1': cell_info[cid1]['LaminarLabel'],\n",
    "                        'laminar_label2': cell_info[cid2]['LaminarLabel'],\n",
    "                        'mean_fr_baseline1': cell_info[cid1]['MeanFR_baseline'],\n",
    "                        'mean_fr_baseline2': cell_info[cid2]['MeanFR_baseline'],\n",
    "                        'mean_fr_stim1': cell_info[cid1]['MeanFR_stim'],\n",
    "                        'mean_fr_stim2': cell_info[cid2]['MeanFR_stim'],\n",
    "                        'is_single_unit1': cell_info[cid1]['IsSingleUnit'],\n",
    "                        'is_single_unit2': cell_info[cid2]['IsSingleUnit'],\n",
    "                        'stim_responsivity1': cell_info[cid1]['StimResponsivity'],\n",
    "                        'stim_responsivity2': cell_info[cid2]['StimResponsivity'],\n",
    "                        'modulation_index1': cell_info[cid1]['ModulationIndex'],\n",
    "                        'modulation_index2': cell_info[cid2]['ModulationIndex'],\n",
    "                        'rsc': corr_df.loc[cid1, cid2],\n",
    "                        'bin_size_ms': bin_size_ms,\n",
    "                        'stim_intensity': stim_label,\n",
    "                        'pre_stim_ms': pre_stim_ms,\n",
    "                        'post_stim_ms': post_stim_ms\n",
    "                    }\n",
    "                    new_rows.append(new_row)\n",
    "        \n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        self.results_df = pd.concat([self.results_df, new_df], ignore_index=True)\n",
    "\n",
    "    def process_all_recordings(self, bin_size_ms=50, pre_stim_ms=50, post_stim_ms=100, **filter_params):\n",
    "        \"\"\"\n",
    "        Process all recordings and calculate rSC correlations for each stimulation intensity.\n",
    "        \n",
    "        :param bin_size_ms: Size of time bins in milliseconds (default: 50ms)\n",
    "        :param pre_stim_ms: Time before stimulation onset to include (in ms)\n",
    "        :param post_stim_ms: Time after stimulation offset to include (in ms)\n",
    "        :param filter_params: Additional filtering parameters (cell_type, is_single_unit, etc.)\n",
    "        \"\"\"\n",
    "        unique_recordings = self.df['recordingname'].unique()\n",
    "\n",
    "        for recording_name in unique_recordings:\n",
    "            print(f\"Processing recording: {recording_name}\")\n",
    "            for stim_intensity in range(1, 5):  # 1: Zero, 2: Low, 3: Mid, 4: Max\n",
    "                self.calculate_rsc(recording_name, bin_size_ms, stim_intensity, pre_stim_ms, post_stim_ms, **filter_params)\n",
    "\n",
    "    def get_results_df(self):\n",
    "        \"\"\"Return the results dataframe.\"\"\"\n",
    "        return self.results_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compare_groups_zero_max(self):\n",
    "        \"\"\"\n",
    "        Compare correlations between CTZ and No_CTZ groups for FS-FS, RS-RS, and FS-RS pairs\n",
    "        during Zero and Max trials.\n",
    "        \n",
    "        :return: matplotlib figure object with the plotted box plots\n",
    "        \"\"\"\n",
    "        if 'groupname' not in self.results_df.columns:\n",
    "            raise ValueError(\"The 'groupname' column is missing from the results DataFrame. \"\n",
    "                             \"Make sure to process the recordings before calling this method.\")\n",
    "\n",
    "        # Filter for Zero and Max trials\n",
    "        zero_max_df = self.results_df[self.results_df['stim_intensity'].isin(['Zero', 'Max'])]\n",
    "\n",
    "        # Create cell pair types\n",
    "        zero_max_df['pair_type'] = zero_max_df.apply(lambda row: \n",
    "            'FS-FS' if row['cell_type1'] == 'FS' and row['cell_type2'] == 'FS' else\n",
    "            'RS-RS' if row['cell_type1'] == 'RS' and row['cell_type2'] == 'RS' else\n",
    "            'FS-RS', axis=1)\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for group in zero_max_df['groupname'].unique():\n",
    "            group_df = zero_max_df[zero_max_df['groupname'] == group]\n",
    "            for intensity in ['Zero', 'Max']:\n",
    "                intensity_df = group_df[group_df['stim_intensity'] == intensity]\n",
    "                for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                    pair_data = intensity_df[intensity_df['pair_type'] == pair_type]['rsc']\n",
    "                    plot_data.extend([\n",
    "                        {'Group': group, 'Intensity': intensity, 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                        for rsc in pair_data\n",
    "                    ])\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Create the box plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        fig.suptitle('Comparison of RSC: CTZ vs No_CTZ')\n",
    "\n",
    "        for i, intensity in enumerate(['Zero', 'Max']):\n",
    "            sns.boxplot(x='Pair Type', y='RSC', hue='Group', \n",
    "                        data=plot_df[plot_df['Intensity'] == intensity], ax=axes[i])\n",
    "            axes[i].set_title(f'{intensity} Intensity')\n",
    "            axes[i].set_ylabel('RSC (Spike Count Correlation)')\n",
    "            if i == 1:  # Only show legend for the second plot\n",
    "                axes[i].legend(title='Group')\n",
    "            else:\n",
    "                axes[i].get_legend().remove()\n",
    "\n",
    "        # Perform statistical tests\n",
    "        for intensity in ['Zero', 'Max']:\n",
    "            print(f\"\\nStatistical tests for {intensity} intensity:\")\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                group_names = plot_df['Group'].unique()\n",
    "                if len(group_names) == 2:\n",
    "                    group1_data = plot_df[(plot_df['Group'] == group_names[0]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    group2_data = plot_df[(plot_df['Group'] == group_names[1]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    \n",
    "                    t_stat, p_value = ttest_ind(group1_data, group2_data)\n",
    "                    print(f\"{pair_type}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Expected 2 groups, but found {len(group_names)}. Skipping statistical test.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def compare_all_intensities(self):\n",
    "        \"\"\"\n",
    "        Compare correlations between CTZ and No_CTZ groups for FS-FS, RS-RS, and FS-RS pairs\n",
    "        across all four intensities.\n",
    "        \n",
    "        :return: matplotlib figure object with the plotted box plots\n",
    "        \"\"\"\n",
    "        if 'groupname' not in self.results_df.columns:\n",
    "            raise ValueError(\"The 'groupname' column is missing from the results DataFrame. \"\n",
    "                             \"Make sure to process the recordings before calling this method.\")\n",
    "\n",
    "        # Create cell pair types\n",
    "        self.results_df['pair_type'] = self.results_df.apply(lambda row: \n",
    "            'FS-FS' if row['cell_type1'] == 'FS' and row['cell_type2'] == 'FS' else\n",
    "            'RS-RS' if row['cell_type1'] == 'RS' and row['cell_type2'] == 'RS' else\n",
    "            'FS-RS', axis=1)\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for group in self.results_df['groupname'].unique():\n",
    "            group_df = self.results_df[self.results_df['groupname'] == group]\n",
    "            for intensity in self.intensity_order:\n",
    "                intensity_df = group_df[group_df['stim_intensity'] == intensity]\n",
    "                for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                    pair_data = intensity_df[intensity_df['pair_type'] == pair_type]['rsc']\n",
    "                    plot_data.extend([\n",
    "                        {'Group': group, 'Intensity': intensity, 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                        for rsc in pair_data\n",
    "                    ])\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Create the box plot\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        fig.suptitle('Comparison of RSC: CTZ vs No_CTZ (All Intensities)')\n",
    "\n",
    "        sns.boxplot(x='Intensity', y='RSC', hue='Group', \n",
    "                    data=plot_df, palette=self.color_map, \n",
    "                    order=self.intensity_order, \n",
    "                    ax=ax)\n",
    "        ax.set_ylabel('RSC (Spike Count Correlation)')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Perform statistical tests\n",
    "        for intensity in self.intensity_order:\n",
    "            print(f\"\\nStatistical tests for {intensity} intensity:\")\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                group_names = plot_df['Group'].unique()\n",
    "                if len(group_names) == 2:\n",
    "                    group1_data = plot_df[(plot_df['Group'] == group_names[0]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    group2_data = plot_df[(plot_df['Group'] == group_names[1]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    \n",
    "                    t_stat, p_value = ttest_ind(group1_data, group2_data)\n",
    "                    print(f\"{pair_type}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Expected 2 groups, but found {len(group_names)}. Skipping statistical test.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def compare_zero_vs_pooled(self):\n",
    "        \"\"\"\n",
    "        Compare correlations between CTZ and No_CTZ groups for FS-FS, RS-RS, and FS-RS pairs\n",
    "        for Zero intensity vs pooled Low/Mid/Max intensities.\n",
    "        \n",
    "        :return: matplotlib figure object with the plotted box plots\n",
    "        \"\"\"\n",
    "        if 'groupname' not in self.results_df.columns:\n",
    "            raise ValueError(\"The 'groupname' column is missing from the results DataFrame. \"\n",
    "                             \"Make sure to process the recordings before calling this method.\")\n",
    "\n",
    "        # Create cell pair types\n",
    "        self.results_df['pair_type'] = self.results_df.apply(lambda row: \n",
    "            'FS-FS' if row['cell_type1'] == 'FS' and row['cell_type2'] == 'FS' else\n",
    "            'RS-RS' if row['cell_type1'] == 'RS' and row['cell_type2'] == 'RS' else\n",
    "            'FS-RS', axis=1)\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for group in self.results_df['groupname'].unique():\n",
    "            group_df = self.results_df[self.results_df['groupname'] == group]\n",
    "            \n",
    "            # Zero intensity\n",
    "            zero_df = group_df[group_df['stim_intensity'] == 'Zero']\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                pair_data = zero_df[zero_df['pair_type'] == pair_type]['rsc']\n",
    "                plot_data.extend([\n",
    "                    {'Group': group, 'Intensity': 'Zero', 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                    for rsc in pair_data\n",
    "                ])\n",
    "            \n",
    "            # Pooled Low/Mid/Max intensities\n",
    "            pooled_df = group_df[group_df['stim_intensity'].isin(['Low', 'Mid', 'Max'])]\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                pair_data = pooled_df[pooled_df['pair_type'] == pair_type]['rsc']\n",
    "                plot_data.extend([\n",
    "                    {'Group': group, 'Intensity': 'Pooled', 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                    for rsc in pair_data\n",
    "                ])\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Create the box plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        fig.suptitle('Comparison of RSC: CTZ vs No_CTZ (Zero vs Pooled)')\n",
    "\n",
    "        sns.boxplot(x='Intensity', y='RSC', hue='Group', \n",
    "                    data=plot_df, palette=self.color_map, \n",
    "                    order=['Zero', 'Pooled'], \n",
    "                    ax=ax)\n",
    "        ax.set_ylabel('RSC (Spike Count Correlation)')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Perform statistical tests\n",
    "        for intensity in ['Zero', 'Pooled']:\n",
    "            print(f\"\\nStatistical tests for {intensity} intensity:\")\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                group_names = plot_df['Group'].unique()\n",
    "                if len(group_names) == 2:\n",
    "                    group1_data = plot_df[(plot_df['Group'] == group_names[0]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    group2_data = plot_df[(plot_df['Group'] == group_names[1]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    \n",
    "                    t_stat, p_value = ttest_ind(group1_data, group2_data)\n",
    "                    print(f\"{pair_type}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Expected 2 groups, but found {len(group_names)}. Skipping statistical test.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def apply_filters(self, df, filters):\n",
    "        \"\"\"\n",
    "        Apply filters to the DataFrame based on the provided filter dictionary.\n",
    "        \n",
    "        :param df: DataFrame to filter\n",
    "        :param filters: Dictionary of filters to apply\n",
    "        :return: Filtered DataFrame\n",
    "        \"\"\"\n",
    "        filtered_df = df.copy()\n",
    "        for column, values in filters.items():\n",
    "            if isinstance(values, (list, tuple, np.ndarray)):\n",
    "                filtered_df = filtered_df[filtered_df[column].isin(values)]\n",
    "            else:\n",
    "                filtered_df = filtered_df[filtered_df[column] == values]\n",
    "        return filtered_df\n",
    "\n",
    "    def compare_all_intensities(self, filters=None):\n",
    "        \"\"\"\n",
    "        Compare correlations between CTZ and No_CTZ groups for FS-FS, RS-RS, and FS-RS pairs\n",
    "        across all four intensities, with optional filtering.\n",
    "        \n",
    "        :param filters: Dictionary of filters to apply (e.g., {'IsSingleUnit': 1.0, 'StimResponsivity': [0.0, 1.0]})\n",
    "        :return: matplotlib figure object with the plotted box plots\n",
    "        \"\"\"\n",
    "        if 'groupname' not in self.results_df.columns:\n",
    "            raise ValueError(\"The 'groupname' column is missing from the results DataFrame. \"\n",
    "                             \"Make sure to process the recordings before calling this method.\")\n",
    "\n",
    "        # Apply filters if provided\n",
    "        df = self.results_df if filters is None else self.apply_filters(self.results_df, filters)\n",
    "\n",
    "        # Create cell pair types\n",
    "        df['pair_type'] = df.apply(lambda row: \n",
    "            'FS-FS' if row['cell_type1'] == 'FS' and row['cell_type2'] == 'FS' else\n",
    "            'RS-RS' if row['cell_type1'] == 'RS' and row['cell_type2'] == 'RS' else\n",
    "            'FS-RS', axis=1)\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for group in df['groupname'].unique():\n",
    "            group_df = df[df['groupname'] == group]\n",
    "            for intensity in self.intensity_order:\n",
    "                intensity_df = group_df[group_df['stim_intensity'] == intensity]\n",
    "                for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                    pair_data = intensity_df[intensity_df['pair_type'] == pair_type]['rsc']\n",
    "                    plot_data.extend([\n",
    "                        {'Group': group, 'Intensity': intensity, 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                        for rsc in pair_data\n",
    "                    ])\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Create the box plot\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        fig.suptitle('Comparison of RSC: CTZ vs No_CTZ (All Intensities)')\n",
    "\n",
    "        sns.boxplot(x='Intensity', y='RSC', hue='Group', \n",
    "                    data=plot_df, palette=self.color_map, \n",
    "                    order=self.intensity_order, \n",
    "                    ax=ax)\n",
    "        ax.set_ylabel('RSC (Spike Count Correlation)')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Add filter information to the plot\n",
    "        if filters:\n",
    "            filter_text = \"\\n\".join([f\"{k}: {v}\" for k, v in filters.items()])\n",
    "            plt.text(0.95, 0.05, f\"Filters applied:\\n{filter_text}\", \n",
    "                     horizontalalignment='right', verticalalignment='bottom', \n",
    "                     transform=ax.transAxes, fontsize=8, alpha=0.7)\n",
    "\n",
    "        # Perform statistical tests\n",
    "        for intensity in self.intensity_order:\n",
    "            print(f\"\\nStatistical tests for {intensity} intensity:\")\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                group_names = plot_df['Group'].unique()\n",
    "                if len(group_names) == 2:\n",
    "                    group1_data = plot_df[(plot_df['Group'] == group_names[0]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    group2_data = plot_df[(plot_df['Group'] == group_names[1]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    \n",
    "                    t_stat, p_value = ttest_ind(group1_data, group2_data)\n",
    "                    print(f\"{pair_type}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Expected 2 groups, but found {len(group_names)}. Skipping statistical test.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def compare_zero_vs_pooled(self, filters=None):\n",
    "        \"\"\"\n",
    "        Compare correlations between CTZ and No_CTZ groups for FS-FS, RS-RS, and FS-RS pairs\n",
    "        for Zero intensity vs pooled Low/Mid/Max intensities, with optional filtering.\n",
    "        \n",
    "        :param filters: Dictionary of filters to apply (e.g., {'IsSingleUnit': 1.0, 'StimResponsivity': [0.0, 1.0]})\n",
    "        :return: matplotlib figure object with the plotted box plots\n",
    "        \"\"\"\n",
    "        if 'groupname' not in self.results_df.columns:\n",
    "            raise ValueError(\"The 'groupname' column is missing from the results DataFrame. \"\n",
    "                             \"Make sure to process the recordings before calling this method.\")\n",
    "\n",
    "        # Apply filters if provided\n",
    "        df = self.results_df if filters is None else self.apply_filters(self.results_df, filters)\n",
    "\n",
    "        # Create cell pair types\n",
    "        df['pair_type'] = df.apply(lambda row: \n",
    "            'FS-FS' if row['cell_type1'] == 'FS' and row['cell_type2'] == 'FS' else\n",
    "            'RS-RS' if row['cell_type1'] == 'RS' and row['cell_type2'] == 'RS' else\n",
    "            'FS-RS', axis=1)\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for group in df['groupname'].unique():\n",
    "            group_df = df[df['groupname'] == group]\n",
    "            \n",
    "            # Zero intensity\n",
    "            zero_df = group_df[group_df['stim_intensity'] == 'Zero']\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                pair_data = zero_df[zero_df['pair_type'] == pair_type]['rsc']\n",
    "                plot_data.extend([\n",
    "                    {'Group': group, 'Intensity': 'Zero', 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                    for rsc in pair_data\n",
    "                ])\n",
    "            \n",
    "            # Pooled Low/Mid/Max intensities\n",
    "            pooled_df = group_df[group_df['stim_intensity'].isin(['Low', 'Mid', 'Max'])]\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                pair_data = pooled_df[pooled_df['pair_type'] == pair_type]['rsc']\n",
    "                plot_data.extend([\n",
    "                    {'Group': group, 'Intensity': 'Pooled', 'Pair Type': pair_type, 'RSC': rsc}\n",
    "                    for rsc in pair_data\n",
    "                ])\n",
    "\n",
    "        plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "        # Create the box plot\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        fig.suptitle('Comparison of RSC: CTZ vs No_CTZ (Zero vs Pooled)')\n",
    "\n",
    "        sns.boxplot(x='Intensity', y='RSC', hue='Group', \n",
    "                    data=plot_df, palette=self.color_map, \n",
    "                    order=['Zero', 'Pooled'], \n",
    "                    ax=ax)\n",
    "        ax.set_ylabel('RSC (Spike Count Correlation)')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Add filter information to the plot\n",
    "        if filters:\n",
    "            filter_text = \"\\n\".join([f\"{k}: {v}\" for k, v in filters.items()])\n",
    "            plt.text(0.95, 0.05, f\"Filters applied:\\n{filter_text}\", \n",
    "                     horizontalalignment='right', verticalalignment='bottom', \n",
    "                     transform=ax.transAxes, fontsize=8, alpha=0.7)\n",
    "\n",
    "        # Perform statistical tests\n",
    "        for intensity in ['Zero', 'Pooled']:\n",
    "            print(f\"\\nStatistical tests for {intensity} intensity:\")\n",
    "            for pair_type in ['FS-FS', 'RS-RS', 'FS-RS']:\n",
    "                group_names = plot_df['Group'].unique()\n",
    "                if len(group_names) == 2:\n",
    "                    group1_data = plot_df[(plot_df['Group'] == group_names[0]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    group2_data = plot_df[(plot_df['Group'] == group_names[1]) & \n",
    "                                          (plot_df['Intensity'] == intensity) & \n",
    "                                          (plot_df['Pair Type'] == pair_type)]['RSC']\n",
    "                    \n",
    "                    t_stat, p_value = ttest_ind(group1_data, group2_data)\n",
    "                    print(f\"{pair_type}: t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Warning: Expected 2 groups, but found {len(group_names)}. Skipping statistical test.\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "rsc_analyzer = RSCAnalysis(whisker_df_manager)\n",
    "rsc_analyzer.process_all_recordings()\n",
    "\n",
    "# Define filters\n",
    "filters = {\n",
    "    'IsSingleUnit': 1.0,\n",
    "    'StimResponsivity': [0.0, 1.0],\n",
    "    'ModulationIndex': 'positive',\n",
    "    'LaminarLabel': ['IG', 'SG']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Zero vs Pooled\n",
    "fig_pooled = rsc_analyzer.compare_zero_vs_pooled()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well need to revisit if you wan to work with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_analysis = CV_Analysis(whisker_df_manager)\n",
    "\n",
    "# Analyzing a single cell\n",
    "groupname = 'CTZ'\n",
    "recordingname = 'ctz_3836'\n",
    "cid = 'cid107'  # Replace with the actual CID of the cell you're interested in\n",
    "\n",
    "firing_rate_dict = cv_analysis.analyze_single_cell(groupname, recordingname, cid)\n",
    "\n",
    "#plot the firing_rate_dict where he results are stored in a dictionary where the keys are the start times of each window (in seconds), and the values are the corresponding firing rates.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(list(firing_rate_dict.keys()), list(firing_rate_dict.values()), color='b')\n",
    "plt.xlabel('Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only single units (IsSingleUnit == 1.0) and FS or RS cell types\n",
    "filtered_df = whisker_df_manager.dataframes['basic_metrics'][\n",
    "    (whisker_df_manager.dataframes['basic_metrics']['IsSingleUnit'] == 1.0) & \n",
    "    (whisker_df_manager.dataframes['basic_metrics']['Cell_Type'].isin(['FS', 'RS']))\n",
    "]\n",
    "\n",
    "# Define a function to extract the first element from a numpy array, or return the value as-is if it's already a scalar\n",
    "def extract_first_element(value):\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value.item()  # Extract the scalar from a single-element numpy array\n",
    "    return value\n",
    "\n",
    "# Apply this function to the relevant columns in the filtered DataFrame\n",
    "filtered_df['StimResponsivity'] = filtered_df['StimResponsivity'].apply(extract_first_element)\n",
    "filtered_df['ModulationIndex'] = filtered_df['ModulationIndex'].apply(extract_first_element)\n",
    "\n",
    "# Define the combinations\n",
    "groupnames = ['CTZ', 'No_CTZ']\n",
    "cell_types = ['FS', 'RS']\n",
    "stim_responsivity_values = [1.0, 0.0, -1.0]\n",
    "modulation_label_values = ['positive', 'negative', 'none']\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over all combinations\n",
    "for groupname in groupnames:\n",
    "    for cell_type in cell_types:\n",
    "        for stim_responsivity in stim_responsivity_values:\n",
    "            for modulation_label in modulation_label_values:\n",
    "                    # Apply detailed filtering logic\n",
    "                    filtered_specific_df = filtered_df[\n",
    "                        (filtered_df['groupname'] == groupname) &\n",
    "                        (filtered_df['Cell_Type'] == cell_type) &\n",
    "                        (filtered_df['StimResponsivity'] == stim_responsivity) &\n",
    "                        (filtered_df['ModulationIndex'] == modulation_label)\n",
    "                    ]\n",
    "                    \n",
    "                    # If there are no records matching the filter, skip to the next iteration\n",
    "                    if filtered_specific_df.empty:\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract the recording names and cell ids\n",
    "                    recordings_and_cells = filtered_specific_df[['recordingname', 'cid']]\n",
    "                    \n",
    "                    # Store the number and identity of the recordings and cells\n",
    "                    num_unique_recordings = recordings_and_cells['recordingname'].nunique()\n",
    "                    unique_recording_names = recordings_and_cells['recordingname'].unique()\n",
    "                    num_unique_cells = recordings_and_cells['cid'].nunique()\n",
    "                    unique_cell_ids = recordings_and_cells['cid'].unique()\n",
    "                    \n",
    "                    # Append the results to the list\n",
    "                    results.append({\n",
    "                        'groupname': groupname,\n",
    "                        'CellType': cell_type,\n",
    "                        'StimResponsivity': stim_responsivity,\n",
    "                        'ModulationIndex': modulation_label,\n",
    "                        'NumUniqueRecordings': num_unique_recordings,\n",
    "                        'UniqueRecordingNames': unique_recording_names,\n",
    "                        'NumUniqueCells': num_unique_cells,\n",
    "                        'UniqueCellIDs': unique_cell_ids\n",
    "                    })\n",
    "\n",
    "# Convert the results list into a DataFrame\n",
    "SUA_breakdown_df = pd.DataFrame(results)\n",
    "SUA_breakdown_df.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations/SUA_breakdown_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the breakdown of how the untis are being broken up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.generate_summary_statistics('IsSingleUnit', 'basic_metrics') # was the ISI violation less than 1.5%? If True then it is a single unit, if False then it is a multiunit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is important reminder as to how the data is being stored, notice the repetition, need to fix later, \n",
    "Use the print statement to help you remember how things are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this accesses the data from the dataframe['psths_dataframe_{label}]['SpikeTrains_for_PSTHs'][i] \n",
    "# where i is the index of the cell in the dataframe and returns numpy.ndarray (trial_n x time=1500 ms, 1ms bins) of the spike trains\n",
    "# in rows and time in columns. Columns will always be 1ms bins at 1500ms duration\n",
    "# zero\n",
    "print(type(whisker_df_manager.dataframes['psth_dataframe_Zero']['SpikeTrains_for_PSTHs'][0]))\n",
    "print(whisker_df_manager.dataframes['psth_dataframe_Zero']['SpikeTrains_for_PSTHs'][0].shape)\n",
    "# low\n",
    "print(type(whisker_df_manager.dataframes['psth_dataframe_Low']['SpikeTrains_for_PSTHs'][0]))\n",
    "print(whisker_df_manager.dataframes['psth_dataframe_Low']['SpikeTrains_for_PSTHs'][0].shape)\n",
    "# mid\n",
    "print(type(whisker_df_manager.dataframes['psth_dataframe_Mid']['SpikeTrains_for_PSTHs'][0]))\n",
    "print(whisker_df_manager.dataframes['psth_dataframe_Mid']['SpikeTrains_for_PSTHs'][0].shape)\n",
    "# max\n",
    "print(type(whisker_df_manager.dataframes['psth_dataframe_Max']['SpikeTrains_for_PSTHs'][0]))\n",
    "print(whisker_df_manager.dataframes['psth_dataframe_Max']['SpikeTrains_for_PSTHs'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a pandas.core.series.Series , contains the raw calculated PSTHs from Matlab \n",
    "print(type(whisker_df_manager.dataframes['psth_dataframe_Zero']['PSTHs_raw'])) \n",
    "# shape of (n, ) where n is the number of cells in the DataFrame\n",
    "print(whisker_df_manager.dataframes['psth_dataframe_Zero']['PSTHs_raw'].shape) \n",
    "\n",
    "#this is a numpy.ndarray, contains the raw calculated PSTHs from MATLAB for the first cell id in the DataFrame\n",
    "print(type(whisker_df_manager.dataframes['psth_dataframe_Zero']['PSTHs_raw'][0]))\n",
    "# shape is (n,) where n is the number of bins which is 1500 1 ms bins and the values are the calculated PSTHs \n",
    "print(whisker_df_manager.dataframes['psth_dataframe_Zero']['PSTHs_raw'][0].shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot Wavefornms (norm or unormalized) aplot raster, or rasters with PSTHS --> to to make more efficients to plot dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_and_plot_waveforms(df_manager, output_dir, show=False):\n",
    "    # Define the parameters to iterate over\n",
    "    cell_types = ['FS', 'RS']\n",
    "    is_single_units = [1.0, 0.0, None]\n",
    "    stim_responsivities = [1.0, 0.0, None]\n",
    "\n",
    "    # Iterate over each combination of parameters\n",
    "    for cell_type in cell_types:\n",
    "        for is_single_unit in is_single_units:\n",
    "            for stim_responsivity in stim_responsivities:\n",
    "                # Filter the DataFrame based on current parameters using the df_manager\n",
    "                df_filtered = df_manager.get_filtered_data(\n",
    "                    'basic_metrics', \n",
    "                    is_single_unit=is_single_unit, \n",
    "                    cell_type=cell_type, \n",
    "                    stim_responsivity=stim_responsivity\n",
    "                )\n",
    "                \n",
    "                # Skip iteration if the filtered DataFrame is empty\n",
    "                if df_filtered.empty:\n",
    "                    continue\n",
    "\n",
    "                # Define folder names based on the current parameters\n",
    "                is_single_unit_str = 'SUA' if is_single_unit == 1.0 else 'MUA' if is_single_unit == 0.0 else 'ALL'\n",
    "                stim_responsivity_str = 'sensory_responsive' if stim_responsivity == 1.0 else 'non_responsive' if stim_responsivity == 0.0 else 'ALL'\n",
    "                folder_suffix = f\"{cell_type}_{is_single_unit_str}_stim_{stim_responsivity_str}\"\n",
    "\n",
    "                # Plot normalized waveforms\n",
    "                iterate_and_plot_normalizedwaveforms(df_filtered, output_dir, f'{folder_suffix}_normalized_waveforms', show)\n",
    "\n",
    "                # Plot unnormalized waveforms\n",
    "                iterate_and_plot_unnormalizedwaveforms(df_filtered, output_dir, f'{folder_suffix}_unnormalized_waveforms', show)\n",
    "\n",
    "def iterate_and_plot_normalizedwaveforms(df, output_dir, folder_name, show=False):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create the folder within the output directory\n",
    "    save_dir = os.path.join(output_dir, folder_name)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    unique_combinations = df[['groupname', 'recordingname', 'cid']].drop_duplicates()\n",
    "\n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        groupname = row['groupname']\n",
    "        recordingname = row['recordingname']\n",
    "        cid = row['cid']\n",
    "\n",
    "        condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "        waveforms = df.loc[condition, 'Normalized_Template_Waveform'].values\n",
    "        \n",
    "\n",
    "        if waveforms.size > 0:\n",
    "            waveform = waveforms[0]\n",
    "            \n",
    "            #convert to microvolts by multiplying by 0.25\n",
    "            waveform = waveform * 0.25\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.plot(waveform)\n",
    "            ax.set_title(f'Normalized Template Waveform for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                save_path = os.path.join(save_dir, f'normalized_waveform_{groupname}_{recordingname}_{cid}.svg')\n",
    "                fig.savefig(save_path, transparent=True)\n",
    "            plt.close(fig)          \n",
    "\n",
    "def iterate_and_plot_unnormalizedwaveforms(df, output_dir, folder_name, show=False):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create the folder within the output directory\n",
    "    save_dir = os.path.join(output_dir, folder_name)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    unique_combinations = df[['groupname', 'recordingname', 'cid']].drop_duplicates()\n",
    "\n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        groupname = row['groupname']\n",
    "        recordingname = row['recordingname']\n",
    "        cid = row['cid']\n",
    "\n",
    "        condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "        waveforms = df.loc[condition, 'UnNormalized_Template_Waveform'].values\n",
    "        \n",
    "\n",
    "        if waveforms.size > 0:\n",
    "            waveform = waveforms[0]\n",
    "            \n",
    "            #convert to microvolts by multiplying by 0.25\n",
    "            waveform = waveform * 0.25\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.plot(waveform)\n",
    "            ax.set_title(f'UnNormalized Template Waveform for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                save_path = os.path.join(save_dir, f'unnormalized_waveform_{groupname}_{recordingname}_{cid}.svg')\n",
    "                fig.savefig(save_path, transparent=True)\n",
    "            plt.close(fig)\n",
    "\n",
    "# Example of how to call the function\n",
    "iterate_and_plot_waveforms(whisker_df_manager, '/Volumes/MannySSD/figures/waveforms', show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot rasters or rasters with Raw AND smmoothe PSTH for a specific recording and cid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('No_CTZ','veh_3062_2','cid109', time_window=(-10, 50), bin_size=1)\n",
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('No_CTZ','veh_3062_2','cid109', time_window=(-10, 50), bin_size=2)\n",
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('No_CTZ','veh_3062_2','cid109', time_window=(-10, 50), bin_size=2, filter_empty_trials=True)\n",
    "whisker_df_manager.plot_combined_psth_and_raster_changebinsize('No_CTZ','veh_3062_2','cid109', time_window=(-20, 50), smoothing_window=3, bin_size=1)\n",
    "whisker_df_manager.plot_combined_psth_and_raster_changebinsize('No_CTZ','veh_3062_2','cid109', time_window=(-100, 100), smoothing_window=3, bin_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the raster, raw and smoothed PSTHs in a nested way by group, recording, and cell id that meet the criteria dynamically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter sets\n",
    "cell_types = ['FS', 'RS']\n",
    "is_single_units = [True, False, None]  # True for SUA, False for MUA, None for ALL\n",
    "stim_responsivities = [1.0, 0.0, None]  # 1.0 for sensory_responsive, 0.0 for non_responsive, None for ALL\n",
    "\n",
    "# Map is_single_unit and stim_responsivity to meaningful strings\n",
    "is_single_unit_map = {True: 'SUA', False: 'MUA', None: 'ALL'}\n",
    "stim_responsivity_map = {1.0: 'sensory_responsive', 0.0: 'non_responsive', None: 'ALL'}\n",
    "\n",
    "# Loop over the parameter combinations\n",
    "for cell_type in cell_types:\n",
    "    for is_single_unit in is_single_units:\n",
    "        for stim_responsivity in stim_responsivities:\n",
    "            \n",
    "            # Process and store PSTH and raster data\n",
    "            stored_data = whisker_df_manager.process_and_store_psth_raster_data(\n",
    "                is_single_unit=is_single_unit, \n",
    "                cell_type=cell_type, \n",
    "                stim_responsivity=stim_responsivity, \n",
    "                time_window=(-100, 600), \n",
    "                bin_size=1, \n",
    "                smoothing_window=3\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Construct folder name\n",
    "            folder_name = f'PSTH_and_Rasters_{cell_type}_{is_single_unit_map[is_single_unit]}_{stim_responsivity_map[stim_responsivity]}'\n",
    "            \n",
    "            # Plot and save the PSTH and raster data\n",
    "            whisker_df_manager.plot_all_psth_and_raster(\n",
    "                stored_data, \n",
    "                output_dir='/Volumes/MannySSD/figures/psth_raster_data_wholestimulation', \n",
    "                folder_name=folder_name, \n",
    "                file_format='svg', \n",
    "                show=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUA or MUA plotting Then creates descriptive stat, runs kruskall, and permutation (sores in df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mass plot different combinations of stim for neurometric curves with box plots and mean/sem plots between groups(can specific any number of stimulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the box and line plots for all combinations of cell types, stim ranges, is_single_units, and stim_responsivities\n",
    "def dynamic_plot_saving(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations=None):\n",
    "    \"\"\"\n",
    "    Dynamically saves plots across different conditions for specified cell types and parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        base_directory (str): Base directory to save the plots.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "                    \n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', \n",
    "                        baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                        stim_responsivity=stim_responsivity, \n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Check if the required columns are present\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Define directory for saving plots\n",
    "                    directory = os.path.join(base_directory, 'boxplots_and_line', suffix)\n",
    "                    os.makedirs(directory, exist_ok=True)\n",
    "                    print(f\"Directory created: {directory}\")\n",
    "                    \n",
    "                    # Plot box and strip plots\n",
    "                    print(\"Plotting box and strip plots\")\n",
    "                    #whisker_df_manager.plot_box_and_strip(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=True,\n",
    "                    #    directory=directory, file_name=f'{suffix}_boxplot', ylim=None, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "                    \n",
    "                    whisker_df_manager.plot_box_and_strip_with_controls(df, \n",
    "                        groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=True, show_scatter=True, \n",
    "                        directory=directory, file_name=f'{suffix}_boxplot', \n",
    "                        modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Plot mean and SEM line plots\n",
    "                    #print(\"Plotting mean and SEM line plots\")\n",
    "                    #whisker_df_manager.plot_mean_and_sem_lineplot(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, directory=directory,\n",
    "                    #    file_name=f'{suffix}_lineplot', ylim=None, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "\n",
    "#all possible combinations of cell types, stim ranges, is_single_units, and stim_responsivities\n",
    "\n",
    "cell_types = ['FS', 'RS']\n",
    "stim_ranges =  [(0,20)]\n",
    "is_single_units = [1.0]\n",
    "stim_responsivities = [1.0]\n",
    "stimulations = ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "# includes all stimulations, default\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations'\n",
    "dynamic_plot_saving(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAMINAR: repeat this process but sperate it by laminar labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_plot_saving_laminar(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, laminarlabel, stimulations=None):\n",
    "    \"\"\"\n",
    "    Dynamically saves plots across different conditions for specified cell types, laminar labels, and parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS', 'None']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0,50), (0,20)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        base_directory (str): Base directory to save the plots.\n",
    "        laminarlabel (list of str): List of laminar labels (e.g., ['SG', 'IG', 'L4']).\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    required_columns = ['mean_stimulation', 'Stimulation', 'Group', 'LaminarLabel']\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    for laminar in laminarlabel:\n",
    "                        # Handle the 'None' case for cell_type\n",
    "                        if cell_type == 'None':\n",
    "                            suffix = f\"AllCells_IsSingleUnit_{str(is_single_unit) if is_single_unit is not None else 'All'}_IsStimResponsive_{str(stim_responsivity) if stim_responsivity is not None else 'All'}_StimRange_{stim_range[0]}to{stim_range[1]}_{laminar}\"\n",
    "                            cell_type_filter = None\n",
    "                        else:\n",
    "                            suffix = f\"{cell_type}_IsSingleUnit_{str(is_single_unit) if is_single_unit is not None else 'All'}_IsStimResponsive_{str(stim_responsivity) if stim_responsivity is not None else 'All'}_StimRange_{stim_range[0]}to{stim_range[1]}_{laminar}\"\n",
    "                            cell_type_filter = cell_type\n",
    "\n",
    "                        # Calculate basic stats\n",
    "                        print(f\"Calculating basic stats for: {suffix}\")\n",
    "                        whisker_df_manager.calculate_basic_stats(\n",
    "                            'CTZ', 'No_CTZ', \n",
    "                            baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                            cell_type=cell_type_filter, is_single_unit=is_single_unit, \n",
    "                            stim_responsivity=stim_responsivity, \n",
    "                            smoothing_window=3, modulation_label=None\n",
    "                        )\n",
    "                        \n",
    "                        # Prepare data for boxplot\n",
    "                        print(f\"Preparing data for boxplot, filtering by laminar label: {laminar}\")\n",
    "                        df = whisker_df_manager.prepare_for_boxplot()\n",
    "                        df = df[df['LaminarLabel'] == laminar]\n",
    "                        \n",
    "                        # If not 'All', further filter by cell type\n",
    "                        if cell_type_filter is not None:\n",
    "                            df = df[df['Cell_Type'] == cell_type_filter]\n",
    "\n",
    "                        print(f\"DataFrame shape after filtering: {df.shape}\")\n",
    "                        print(f\"DataFrame columns: {df.columns}\")\n",
    "                        print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                        # Check if the required columns are present\n",
    "                        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                        if missing_columns:\n",
    "                            print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Define directory for saving plots\n",
    "                        directory = os.path.join(base_directory, laminar, 'boxplots_and_line', suffix)\n",
    "                        os.makedirs(directory, exist_ok=True)\n",
    "                        print(f\"Directory created: {directory}\")\n",
    "                        \n",
    "                        # Plot box and strip plots\n",
    "                        print(\"Plotting box and strip plots\")\n",
    "                        whisker_df_manager.plot_box_and_strip(df, \n",
    "                            groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=True,\n",
    "                            directory=directory, file_name=f'{suffix}_boxplot', ylim=None, \n",
    "                            modulation_label=None\n",
    "                        )\n",
    "                        \n",
    "                        # Plot mean and SEM line plots\n",
    "                        print(\"Plotting mean and SEM line plots\")\n",
    "                        whisker_df_manager.plot_mean_and_sem_lineplot(df, \n",
    "                            groups=['CTZ', 'No_CTZ'], stimulations=stimulations, directory=directory,\n",
    "                            file_name=f'{suffix}_lineplot', ylim=None, \n",
    "                            modulation_label=None\n",
    "                        )\n",
    "                        \n",
    "cell_types = ['FS', 'RS', None]\n",
    "stim_ranges =  [(0, 500), (0,50), (0,20)]\n",
    "is_single_units = [1.0, 0.0,  None]\n",
    "stim_responsivities = [1.0, None]\n",
    "laminarlabel = ['SG', 'IG', 'L4']\n",
    "stimulations = ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "# includes all stimulations, default\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_laminar'\n",
    "dynamic_plot_saving_laminar(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, laminarlabel, stimulations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debug or mass run statsistical analysis across different combinations of stim for bow plots and mean/sem plots between groups as a function of stimualtion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL PSTHs (not laminar) only PSTHs and nothign else...look below for more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "def dynamic_statistical_analysis(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations=None):\n",
    "    \"\"\"\n",
    "    Dynamically collects descriptive statistics across different combinations for specified cell types and parameter combinations.\n",
    "    \n",
    "    # it currently filters based on modulation_label='positive' and firstspike_latency=True so keep that in mind\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        stimulations (list of str, optional): List of stimulations to include in the analysis. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'CellType': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'Group': [],\n",
    "        'N': [],\n",
    "        'Mean': [],\n",
    "        'Median': [],\n",
    "        'SD': [],\n",
    "        'SEM': [],\n",
    "        'CI Lower': [],\n",
    "        'CI Upper': [],\n",
    "        'Range': []\n",
    "    }\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "                    \n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', stim_label=None, \n",
    "                        baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                        stim_responsivity=stim_responsivity, \n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Filter data for the specified stimulations\n",
    "                    if stimulations:\n",
    "                        df = df[df['Stimulation'].isin(stimulations)]\n",
    "                    \n",
    "                    \n",
    "                    # Check if the required columns are present\n",
    "                    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Loop through each stimulation and perform analysis\n",
    "                    for stim in stimulations or df['Stimulation'].unique():\n",
    "                        for group in ['CTZ', 'No_CTZ']:\n",
    "                            group_data = df[(df['Stimulation'] == stim) & (df['Group'] == group)]['mean_stimulation']\n",
    "                            \n",
    "                            if group_data.empty:\n",
    "                                print(f\"Skipping {stim} for group {group} due to lack of data\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Calculate IQR\n",
    "                            Q1 = group_data.quantile(0.25)\n",
    "                            Q3 = group_data.quantile(0.75)\n",
    "                            IQR = Q3 - Q1\n",
    "                            \n",
    "                            print(f\"Before IQR filtering: count={group_data.count()}, mean={group_data.mean()}, std={group_data.std()}\")\n",
    "                            \n",
    "                            \n",
    "                            # Filter data within IQR\n",
    "                            filtered_data = group_data[(group_data >= Q1 - 1.5 * IQR) & (group_data <= Q3 + 1.5 * IQR)]\n",
    "                            \n",
    "                            print(f\"After IQR filtering: count={filtered_data.count()}, mean={filtered_data.mean()}, std={filtered_data.std()}\")\n",
    "\n",
    "                            if filtered_data.empty:\n",
    "                                print(f\"Skipping {stim} for group {group} due to all data being outside IQR\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Calculate descriptive statistics\n",
    "                            group_desc = filtered_data.describe()\n",
    "                            group_sem = sem(filtered_data, nan_policy='omit')\n",
    "                            confidence_level = 0.95\n",
    "                            degrees_freedom = len(filtered_data) - 1\n",
    "                            confidence_interval = t.interval(confidence_level, degrees_freedom, loc=group_desc['mean'], scale=group_sem)\n",
    "                            \n",
    "                            # Store results\n",
    "                            results['CellType'].append(cell_type)\n",
    "                            results['StimRange'].append(f\"{stim_range[0]}to{stim_range[1]}\")\n",
    "                            results['IsSingleUnit'].append(is_single_unit_str)\n",
    "                            results['StimResponsivity'].append(stim_responsivity_str)\n",
    "                            results['Stimulation'].append(stim)\n",
    "                            results['Group'].append(group)\n",
    "                            results['N'].append(group_desc['count'])\n",
    "                            results['Mean'].append(group_desc['mean'])\n",
    "                            results['Median'].append(filtered_data.median())\n",
    "                            results['SD'].append(group_desc['std'])\n",
    "                            results['SEM'].append(group_sem)\n",
    "                            results['CI Lower'].append(confidence_interval[0])\n",
    "                            results['CI Upper'].append(confidence_interval[1])\n",
    "                            results['Range'].append((filtered_data.min(), filtered_data.max()))\n",
    "    \n",
    "    # Convert results dictionary to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Results DataFrame:\\n{results_df}\")\n",
    "\n",
    "    return results_df\n",
    "def generate_actual_data(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations=None):\n",
    "    \"\"\"\n",
    "    Collect actual data across different combinations for specified cell types and parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        stimulations (list of str, optional): List of stimulations to include in the analysis. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    # Initialize list to collect actual data\n",
    "    actual_data = []\n",
    "\n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "\n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', stim_label=None,\n",
    "                        baseline_range=(-100, -1), stim_range=stim_range,\n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit,\n",
    "                        stim_responsivity=stim_responsivity,\n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "\n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Filter data for the specified stimulations\n",
    "                    if stimulations:\n",
    "                        df = df[df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "\n",
    "                    # Check if the required columns are present\n",
    "                    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Loop through each stimulation and perform analysis\n",
    "                    for stim in stimulations or df['Stimulation'].unique():\n",
    "                        for group in ['CTZ', 'No_CTZ']:\n",
    "                            group_data = df[(df['Stimulation'] == stim) & (df['Group'] == group)]\n",
    "                            \n",
    "                            if group_data.empty:\n",
    "                                print(f\"Skipping {stim} for group {group} due to lack of data\")\n",
    "                                continue\n",
    "\n",
    "                            # Filter data within IQR\n",
    "                            Q1 = group_data['mean_stimulation'].quantile(0.25)\n",
    "                            Q3 = group_data['mean_stimulation'].quantile(0.75)\n",
    "                            IQR = Q3 - Q1\n",
    "                            filtered_data = group_data[(group_data['mean_stimulation'] >= Q1 - 1.5 * IQR) & (group_data['mean_stimulation'] <= Q3 + 1.5 * IQR)]\n",
    "                            \n",
    "                            if filtered_data.empty:\n",
    "                                print(f\"Skipping {stim} for group {group} due to all data being outside IQR\")\n",
    "                                continue\n",
    "\n",
    "                            # Add additional columns for metadata\n",
    "                            filtered_data['CellType'] = cell_type\n",
    "                            filtered_data['StimRange'] = f\"{stim_range[0]}to{stim_range[1]}\"\n",
    "                            filtered_data['IsSingleUnit'] = is_single_unit_str\n",
    "                            filtered_data['StimResponsivity'] = stim_responsivity_str\n",
    "                            \n",
    "                            # Collect the filtered data\n",
    "                            actual_data.append(filtered_data)\n",
    "\n",
    "    # Combine all collected data into a single DataFrame\n",
    "    actual_data_df = pd.concat(actual_data, ignore_index=True)\n",
    "    print(f\"Actual Data DataFrame:\\n{actual_data_df}\")\n",
    "\n",
    "    return actual_data_df\n",
    "def kruskal_wallis_test(data, group_col, value_col):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis H-test for independent samples.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data.\n",
    "        group_col (str): Column name to group by (e.g., 'Group').\n",
    "        value_col (str): Column name containing the values (e.g., 'mean_stimulation').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Kruskal-Wallis H-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Group data by the specified column and extract values\n",
    "    groups = [group[value_col].values for name, group in data.groupby(group_col)]\n",
    "\n",
    "    # Perform Kruskal-Wallis test\n",
    "    h_stat, p_value = kruskal(*groups)\n",
    "\n",
    "    return h_stat, p_value\n",
    "def perform_kruskal_wallis_on_actual_data(actual_data_df, stimulations):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis test on the actual data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        actual_data_df (pd.DataFrame): DataFrame containing the actual data.\n",
    "        stimulations (list of str): List of stimulations to include in the analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the Kruskal-Wallis test results.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary for Kruskal-Wallis test results\n",
    "    kruskal_results = {\n",
    "        'CellType': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'N_CTZ': [],\n",
    "        'N_No_CTZ': [],\n",
    "        'Mean_CTZ': [],\n",
    "        'Mean_No_CTZ': [],\n",
    "        'H_statistic': [],\n",
    "        'p_value': []\n",
    "    }\n",
    "\n",
    "    for cell_type in actual_data_df['CellType'].unique():\n",
    "        for stim_range in actual_data_df['StimRange'].unique():\n",
    "            for is_single_unit in actual_data_df['IsSingleUnit'].unique():\n",
    "                for stim_responsivity in actual_data_df['StimResponsivity'].unique():\n",
    "                    for stim in stimulations:\n",
    "                        subset_df = actual_data_df[\n",
    "                            (actual_data_df['CellType'] == cell_type) &\n",
    "                            (actual_data_df['StimRange'] == stim_range) &\n",
    "                            (actual_data_df['IsSingleUnit'] == is_single_unit) &\n",
    "                            (actual_data_df['StimResponsivity'] == stim_responsivity) &\n",
    "                            (actual_data_df['Stimulation'] == stim)\n",
    "                        ]\n",
    "\n",
    "                        if subset_df.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Separate data for each group\n",
    "                        group_ctz = subset_df[subset_df['Group'] == 'CTZ']\n",
    "                        group_no_ctz = subset_df[subset_df['Group'] == 'No_CTZ']\n",
    "\n",
    "                        # Calculate N and mean for each group\n",
    "                        n_ctz = len(group_ctz)\n",
    "                        n_no_ctz = len(group_no_ctz)\n",
    "                        mean_ctz = group_ctz['mean_stimulation'].mean()\n",
    "                        mean_no_ctz = group_no_ctz['mean_stimulation'].mean()\n",
    "\n",
    "                        # Perform Kruskal-Wallis test\n",
    "                        h_stat, p_value = kruskal_wallis_test(subset_df, group_col='Group', value_col='mean_stimulation')\n",
    "\n",
    "                        # Store Kruskal-Wallis results\n",
    "                        kruskal_results['CellType'].append(cell_type)\n",
    "                        kruskal_results['StimRange'].append(stim_range)\n",
    "                        kruskal_results['IsSingleUnit'].append(is_single_unit)\n",
    "                        kruskal_results['StimResponsivity'].append(stim_responsivity)\n",
    "                        kruskal_results['Stimulation'].append(stim)\n",
    "                        kruskal_results['N_CTZ'].append(n_ctz)\n",
    "                        kruskal_results['N_No_CTZ'].append(n_no_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places\n",
    "                        mean_ctz = float(\"{:.3f}\".format(mean_ctz))\n",
    "                        kruskal_results['Mean_CTZ'].append(mean_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places \n",
    "                        mean_no_ctz = float(\"{:.3f}\".format(mean_no_ctz))\n",
    "                        kruskal_results['Mean_No_CTZ'].append(mean_no_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places for h_stat\n",
    "                        h_stat = float(\"{:.3f}\".format(h_stat))\n",
    "                        kruskal_results['H_statistic'].append(h_stat)\n",
    "                        \n",
    "                        \n",
    "                        #convert to a float with 3 decimal places\n",
    "                        p_value = float(\"{:.3f}\".format(p_value))\n",
    "                        kruskal_results['p_value'].append(p_value)\n",
    "                    \n",
    "\n",
    "    # Convert Kruskal-Wallis results dictionary to DataFrame\n",
    "    kruskal_results_df = pd.DataFrame(kruskal_results)\n",
    "    print(f\"Kruskal-Wallis Results DataFrame:\\n{kruskal_results_df}\")\n",
    "\n",
    "    return kruskal_results_df\n",
    "def permutation_test(data, group_col, value_col, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test for the specified groups.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data.\n",
    "        group_col (str): Column name to group by (e.g., 'Group').\n",
    "        value_col (str): Column name containing the values (e.g., 'mean_stimulation').\n",
    "        n_permutations (int): Number of permutations to perform.\n",
    "\n",
    "    Returns:\n",
    "        float: p-value from the permutation test.\n",
    "    \"\"\"\n",
    "    # Calculate the actual Kruskal-Wallis test statistic\n",
    "    groups = [group[value_col].values for name, group in data.groupby(group_col)]\n",
    "    actual_stat, _ = kruskal(*groups)\n",
    "    \n",
    "    # Combine all data values and shuffle group labels to generate the null distribution\n",
    "    combined_values = data[value_col].values\n",
    "    group_labels = data[group_col].values\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(group_labels)\n",
    "        permuted_groups = [combined_values[group_labels == label] for label in np.unique(group_labels)]\n",
    "        perm_stat, _ = kruskal(*permuted_groups)\n",
    "        permuted_stats.append(perm_stat)\n",
    "    \n",
    "    # Calculate the p-value as the proportion of permuted stats greater than or equal to the actual stat\n",
    "    permuted_stats = np.array(permuted_stats)\n",
    "    p_value = np.sum(permuted_stats >= actual_stat) / n_permutations\n",
    "\n",
    "    return p_value\n",
    "def perform_permutation_test_on_actual_data(actual_data_df, stimulations, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test on the actual data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        actual_data_df (pd.DataFrame): DataFrame containing the actual data.\n",
    "        stimulations (list of str): List of stimulations to include in the analysis.\n",
    "        n_permutations (int): Number of permutations to perform.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the permutation test results.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary for permutation test results\n",
    "    perm_results = {\n",
    "        'CellType': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'N_CTZ': [],\n",
    "        'N_No_CTZ': [],\n",
    "        'Mean_CTZ': [],\n",
    "        'Mean_No_CTZ': [],\n",
    "        'Permutation_p_value': []\n",
    "    }\n",
    "\n",
    "    for cell_type in actual_data_df['CellType'].unique():\n",
    "        for stim_range in actual_data_df['StimRange'].unique():\n",
    "            for is_single_unit in actual_data_df['IsSingleUnit'].unique():\n",
    "                for stim_responsivity in actual_data_df['StimResponsivity'].unique():\n",
    "                    for stim in stimulations:\n",
    "                        subset_df = actual_data_df[\n",
    "                            (actual_data_df['CellType'] == cell_type) &\n",
    "                            (actual_data_df['StimRange'] == stim_range) &\n",
    "                            (actual_data_df['IsSingleUnit'] == is_single_unit) &\n",
    "                            (actual_data_df['StimResponsivity'] == stim_responsivity) &\n",
    "                            (actual_data_df['Stimulation'] == stim)\n",
    "                        ]\n",
    "\n",
    "                        if subset_df.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Separate data for each group\n",
    "                        group_ctz = subset_df[subset_df['Group'] == 'CTZ']\n",
    "                        group_no_ctz = subset_df[subset_df['Group'] == 'No_CTZ']\n",
    "\n",
    "                        # Calculate N and mean for each group\n",
    "                        n_ctz = len(group_ctz)\n",
    "                        n_no_ctz = len(group_no_ctz)\n",
    "                        mean_ctz = group_ctz['mean_stimulation'].mean()\n",
    "                        mean_no_ctz = group_no_ctz['mean_stimulation'].mean()\n",
    "\n",
    "                        # Perform permutation test\n",
    "                        p_value = permutation_test(subset_df, group_col='Group', value_col='mean_stimulation', n_permutations=n_permutations)\n",
    "\n",
    "                        # Store permutation test results\n",
    "                        perm_results['CellType'].append(cell_type)\n",
    "                        perm_results['StimRange'].append(stim_range)\n",
    "                        perm_results['IsSingleUnit'].append(is_single_unit)\n",
    "                        perm_results['StimResponsivity'].append(stim_responsivity)\n",
    "                        perm_results['Stimulation'].append(stim)\n",
    "                        perm_results['N_CTZ'].append(n_ctz)\n",
    "                        perm_results['N_No_CTZ'].append(n_no_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places\n",
    "                        mean_ctz = float(\"{:.3f}\".format(mean_ctz))\n",
    "                        perm_results['Mean_CTZ'].append(mean_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places \n",
    "                        mean_no_ctz = float(\"{:.3f}\".format(mean_no_ctz))\n",
    "                        perm_results['Mean_No_CTZ'].append(mean_no_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places\n",
    "                        p_value = float(\"{:.3f}\".format(p_value))\n",
    "                        perm_results['Permutation_p_value'].append(p_value)\n",
    "\n",
    "    # Convert permutation test results dictionary to DataFrame\n",
    "    perm_results_df = pd.DataFrame(perm_results)\n",
    "    print(f\"Permutation Test Results DataFrame:\\n{perm_results_df}\")\n",
    "\n",
    "    return perm_results_df\n",
    "\n",
    "\n",
    "descriptive_stats_df = dynamic_statistical_analysis(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "input_df_4_kruskal_and_permutation = generate_actual_data(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "kruskal_results_df = perform_kruskal_wallis_on_actual_data(input_df_4_kruskal_and_permutation, stimulations)\n",
    "perm_results_df = perform_permutation_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations, n_permutations=1000)\n",
    "\n",
    "descriptive_stats_df.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations/descriptive_stats.csv', index=False)\n",
    "input_df_4_kruskal_and_permutation.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations/input_df_4_kruskal_and_permutation.csv', index=False)\n",
    "kruskal_results_df.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations/kruskal_results.csv', index=False)\n",
    "perm_results_df.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations/perm_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAMINAR: This section will run the analysis for PSTHs based on a laminar profiles, similar to the above but can handle the laminar label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "## dynamic_statistical_analysis and generate_actual_data in this cell is modified form the above to allow for \n",
    "## the inclusion of laminar labels in the analysis -- will need to change the function names \n",
    "def dynamic_statistical_analysis(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, laminarlabel, stimulations=None):\n",
    "    \"\"\"\n",
    "    Dynamically collects descriptive statistics across different combinations for specified cell types and parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS', 'None']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        laminarlabel (list of str): List of laminar labels (e.g., ['SG', 'IG', 'L4']).\n",
    "        stimulations (list of str, optional): List of stimulations to include in the analysis. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'Cell_Type': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'Group': [],\n",
    "        'N': [],\n",
    "        'Mean': [],\n",
    "        'Median': [],\n",
    "        'SD': [],\n",
    "        'SEM': [],\n",
    "        'CI Lower': [],\n",
    "        'CI Upper': [],\n",
    "        'Range': []\n",
    "    }\n",
    "\n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    for laminar in laminarlabel:\n",
    "                        # Prepare data for boxplot\n",
    "                        df = whisker_df_manager.prepare_for_boxplot()\n",
    "\n",
    "                        # Handle the 'None' case: if None, include both 'FS' and 'RS'\n",
    "                        if cell_type is not None:\n",
    "                            df = df[df['Cell_Type'] == cell_type]\n",
    "                            cell_type_str = cell_type\n",
    "                        else:\n",
    "                            cell_type_str = 'All'  # This means include all cell types, so no filtering\n",
    "\n",
    "                        if is_single_unit is not None:\n",
    "                            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "                            is_single_unit_str = str(is_single_unit)\n",
    "                        else:\n",
    "                            is_single_unit_str = 'All'\n",
    "\n",
    "                        if stim_responsivity is not None:\n",
    "                            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "                            stim_responsivity_str = str(stim_responsivity)\n",
    "                        else:\n",
    "                            stim_responsivity_str = 'All'\n",
    "\n",
    "                        # Filter by laminar label\n",
    "                        df = df[df['LaminarLabel'] == laminar]\n",
    "                        print(f\"DataFrame shape after filtering by laminar label '{laminar}': {df.shape}\")\n",
    "\n",
    "                        # Filter data for the specified stimulations\n",
    "                        if stimulations:\n",
    "                            df = df[df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "                        # Check if the required columns are present\n",
    "                        required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "                        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                        if missing_columns:\n",
    "                            print(f\"Skipping combination {cell_type_str} due to missing columns: {missing_columns}\")\n",
    "                            continue\n",
    "\n",
    "                        # Loop through each stimulation and perform analysis\n",
    "                        for stim in stimulations or df['Stimulation'].unique():\n",
    "                            for group in ['CTZ', 'No_CTZ']:\n",
    "                                group_data = df[(df['Stimulation'] == stim) & (df['Group'] == group)]['mean_stimulation']\n",
    "\n",
    "                                if group_data.empty:\n",
    "                                    print(f\"Skipping {stim} for group {group} due to lack of data\")\n",
    "                                    continue\n",
    "\n",
    "                                # Calculate IQR\n",
    "                                Q1 = group_data.quantile(0.25)\n",
    "                                Q3 = group_data.quantile(0.75)\n",
    "                                IQR = Q3 - Q1\n",
    "\n",
    "                                # Filter data within IQR\n",
    "                                #filtered_data = group_data[(group_data >= Q1 - 1.5 * IQR) & (group_data <= Q3 + 1.5 * IQR)]\n",
    "                                filtered_data = group_data \n",
    "\n",
    "                                if filtered_data.empty:\n",
    "                                    print(f\"Skipping {stim} for group {group} due to all data being outside IQR\")\n",
    "                                    continue\n",
    "\n",
    "                                # Calculate descriptive statistics\n",
    "                                group_desc = filtered_data.describe()\n",
    "                                group_sem = sem(filtered_data, nan_policy='omit')\n",
    "                                confidence_level = 0.95\n",
    "                                degrees_freedom = len(filtered_data) - 1\n",
    "                                confidence_interval = t.interval(confidence_level, degrees_freedom, loc=group_desc['mean'], scale=group_sem)\n",
    "\n",
    "                                # Store results\n",
    "                                results['Cell_Type'].append(cell_type_str)\n",
    "                                results['StimRange'].append(f\"{stim_range[0]}to{stim_range[1]}\")\n",
    "                                results['IsSingleUnit'].append(is_single_unit_str)\n",
    "                                results['StimResponsivity'].append(stim_responsivity_str)\n",
    "                                results['Stimulation'].append(stim)\n",
    "                                results['Group'].append(group)\n",
    "                                results['N'].append(group_desc['count'])\n",
    "                                results['Mean'].append(group_desc['mean'])\n",
    "                                results['Median'].append(filtered_data.median())\n",
    "                                results['SD'].append(group_desc['std'])\n",
    "                                results['SEM'].append(group_sem)\n",
    "                                results['CI Lower'].append(confidence_interval[0])\n",
    "                                results['CI Upper'].append(confidence_interval[1])\n",
    "                                results['Range'].append((filtered_data.min(), filtered_data.max()))\n",
    "\n",
    "    # Convert results dictionary to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Results DataFrame:\\n{results_df}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def generate_actual_data(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, laminarlabel, stimulations=None):\n",
    "    \"\"\"\n",
    "    Collect actual data across different combinations for specified cell types, laminar labels, and parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS', None] - None means include both 'FS' and 'RS').\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        laminarlabel (list of str): List of laminar labels (e.g., ['SG', 'IG', 'L4']).\n",
    "        stimulations (list of str, optional): List of stimulations to include in the analysis. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    # Initialize list to collect actual data\n",
    "    actual_data = []\n",
    "\n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    for laminar in laminarlabel:\n",
    "                        # Handle the 'None' case for cell_type\n",
    "                        df = whisker_df_manager.prepare_for_boxplot()\n",
    "\n",
    "                        # If cell_type is None, include all cell types ('FS' and 'RS')\n",
    "                        if cell_type is not None:\n",
    "                            df = df[df['Cell_Type'] == cell_type]\n",
    "                            cell_type_str = cell_type\n",
    "                        else:\n",
    "                            cell_type_str = 'All'  # Include all cell types, so no filtering\n",
    "\n",
    "                        if is_single_unit is not None:\n",
    "                            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "                            is_single_unit_str = str(is_single_unit)\n",
    "                        else:\n",
    "                            is_single_unit_str = 'All'\n",
    "\n",
    "                        if stim_responsivity is not None:\n",
    "                            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "                            stim_responsivity_str = str(stim_responsivity)\n",
    "                        else:\n",
    "                            stim_responsivity_str = 'All'\n",
    "\n",
    "                        # Filter by laminar label\n",
    "                        df = df[df['LaminarLabel'] == laminar]\n",
    "                        print(f\"DataFrame shape after filtering by laminar label '{laminar}': {df.shape}\")\n",
    "\n",
    "                        # If the DataFrame is empty after filtering, print a message and continue\n",
    "                        if df.empty:\n",
    "                            print(f\"No data available for laminar label '{laminar}' with current filters.\")\n",
    "                            continue\n",
    "\n",
    "                        # Filter data for the specified stimulations\n",
    "                        if stimulations:\n",
    "                            df = df[df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "                        # Check if the required columns are present\n",
    "                        required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "                        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                        if missing_columns:\n",
    "                            print(f\"Skipping combination {cell_type_str} due to missing columns: {missing_columns}\")\n",
    "                            continue\n",
    "\n",
    "                        # Loop through each stimulation and collect data\n",
    "                        for stim in stimulations or df['Stimulation'].unique():\n",
    "                            for group in ['CTZ', 'No_CTZ']:\n",
    "                                group_data = df[(df['Stimulation'] == stim) & (df['Group'] == group)]\n",
    "                                \n",
    "                                if group_data.empty:\n",
    "                                    print(f\"Skipping {stim} for group {group} due to lack of data\")\n",
    "                                    continue\n",
    "\n",
    "                                # Filter data within IQR\n",
    "                                Q1 = group_data['mean_stimulation'].quantile(0.25)\n",
    "                                Q3 = group_data['mean_stimulation'].quantile(0.75)\n",
    "                                IQR = Q3 - Q1\n",
    "                                #filtered_data = group_data[(group_data['mean_stimulation'] >= Q1 - 1.5 * IQR) & (group_data['mean_stimulation'] <= Q3 + 1.5 * IQR)]\n",
    "                                filtered_data = group_data\n",
    "                                if filtered_data.empty:\n",
    "                                    print(f\"Skipping {stim} for group {group} due to all data being outside IQR\")\n",
    "                                    continue\n",
    "\n",
    "                                # Add additional columns for metadata\n",
    "                                filtered_data['Cell_Type'] = cell_type_str\n",
    "                                filtered_data['StimRange'] = f\"{stim_range[0]}to{stim_range[1]}\"\n",
    "                                filtered_data['IsSingleUnit'] = is_single_unit_str\n",
    "                                filtered_data['StimResponsivity'] = stim_responsivity_str\n",
    "                                filtered_data['LaminarLabel'] = laminar  # Include laminar label\n",
    "                                \n",
    "                                # Collect the filtered data\n",
    "                                actual_data.append(filtered_data)\n",
    "\n",
    "    # Check if there's any data to concatenate\n",
    "    if not actual_data:\n",
    "        print(\"No data was collected, returning an empty DataFrame.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no data was collected\n",
    "\n",
    "    # Combine all collected data into a single DataFrame\n",
    "    actual_data_df = pd.concat(actual_data, ignore_index=True)\n",
    "    print(f\"Actual Data DataFrame:\\n{actual_data_df}\")\n",
    "\n",
    "    return actual_data_df\n",
    "\n",
    "\n",
    "def wilcoxon_rank_sum_test(data, group_col, value_col):\n",
    "    \"\"\"\n",
    "    Perform Wilcoxon rank-sum test (Mann-Whitney U test) for independent samples.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data.\n",
    "        group_col (str): Column name to group by (e.g., 'Group').\n",
    "        value_col (str): Column name containing the values (e.g., 'mean_stimulation').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Wilcoxon rank-sum U-statistic and p-value.\n",
    "    \"\"\"\n",
    "    # Ensure that there are exactly two groups to compare\n",
    "    unique_groups = data[group_col].unique()\n",
    "    if len(unique_groups) != 2:\n",
    "        raise ValueError(f\"Wilcoxon rank-sum test requires exactly two groups, but {len(unique_groups)} groups were found.\")\n",
    "\n",
    "    # Extract values for each group\n",
    "    group1_values = data[data[group_col] == unique_groups[0]][value_col].values\n",
    "    group2_values = data[data[group_col] == unique_groups[1]][value_col].values\n",
    "\n",
    "    # Perform the Wilcoxon rank-sum test\n",
    "    u_stat, p_value = mannwhitneyu(group1_values, group2_values, alternative='two-sided')\n",
    "\n",
    "    return u_stat, p_value\n",
    "def perform_wilcoxon_test_on_actual_data(actual_data_df, stimulations):\n",
    "    \"\"\"\n",
    "    Perform Wilcoxon rank-sum test on the actual data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        actual_data_df (pd.DataFrame): DataFrame containing the actual data.\n",
    "        stimulations (list of str): List of stimulations to include in the analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the Wilcoxon test results.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary for Wilcoxon test results\n",
    "    wilcoxon_results = {\n",
    "        'Cell_Type': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'N_CTZ': [],\n",
    "        'N_No_CTZ': [],\n",
    "        'Mean_CTZ': [],\n",
    "        'Mean_No_CTZ': [],\n",
    "        'U_statistic': [],\n",
    "        'p_value': []\n",
    "    }\n",
    "\n",
    "    for cell_type in actual_data_df['Cell_Type'].unique():\n",
    "        for stim_range in actual_data_df['StimRange'].unique():\n",
    "            for is_single_unit in actual_data_df['IsSingleUnit'].unique():\n",
    "                for stim_responsivity in actual_data_df['StimResponsivity'].unique():\n",
    "                    for stim in stimulations:\n",
    "                        subset_df = actual_data_df[\n",
    "                            (actual_data_df['Cell_Type'] == cell_type) &\n",
    "                            (actual_data_df['StimRange'] == stim_range) &\n",
    "                            (actual_data_df['IsSingleUnit'] == is_single_unit) &\n",
    "                            (actual_data_df['StimResponsivity'] == stim_responsivity) &\n",
    "                            (actual_data_df['Stimulation'] == stim)\n",
    "                        ]\n",
    "\n",
    "                        if subset_df.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Separate data for each group\n",
    "                        group_ctz = subset_df[subset_df['Group'] == 'CTZ']\n",
    "                        group_no_ctz = subset_df[subset_df['Group'] == 'No_CTZ']\n",
    "\n",
    "                        # Calculate N and mean for each group\n",
    "                        n_ctz = len(group_ctz)\n",
    "                        n_no_ctz = len(group_no_ctz)\n",
    "                        mean_ctz = group_ctz['mean_stimulation'].mean()\n",
    "                        mean_no_ctz = group_no_ctz['mean_stimulation'].mean()\n",
    "\n",
    "                        # Perform Wilcoxon rank-sum test\n",
    "                        u_stat, p_value = wilcoxon_rank_sum_test(subset_df, group_col='Group', value_col='mean_stimulation')\n",
    "\n",
    "                        # Store Wilcoxon test results\n",
    "                        wilcoxon_results['Cell_Type'].append(cell_type)\n",
    "                        wilcoxon_results['StimRange'].append(stim_range)\n",
    "                        wilcoxon_results['IsSingleUnit'].append(is_single_unit)\n",
    "                        wilcoxon_results['StimResponsivity'].append(stim_responsivity)\n",
    "                        wilcoxon_results['Stimulation'].append(stim)\n",
    "                        wilcoxon_results['N_CTZ'].append(n_ctz)\n",
    "                        wilcoxon_results['N_No_CTZ'].append(n_no_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places\n",
    "                        mean_ctz = float(\"{:.3f}\".format(mean_ctz))\n",
    "                        wilcoxon_results['Mean_CTZ'].append(mean_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places \n",
    "                        mean_no_ctz = float(\"{:.3f}\".format(mean_no_ctz))\n",
    "                        wilcoxon_results['Mean_No_CTZ'].append(mean_no_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places for u_stat\n",
    "                        u_stat = float(\"{:.3f}\".format(u_stat))\n",
    "                        wilcoxon_results['U_statistic'].append(u_stat)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places\n",
    "                        p_value = float(\"{:.3f}\".format(p_value))\n",
    "                        wilcoxon_results['p_value'].append(p_value)\n",
    "\n",
    "    # Convert Wilcoxon test results dictionary to DataFrame\n",
    "    wilcoxon_results_df = pd.DataFrame(wilcoxon_results)\n",
    "    print(f\"Wilcoxon Test Results DataFrame:\\n{wilcoxon_results_df}\")\n",
    "\n",
    "    return wilcoxon_results_df\n",
    "\n",
    "def kruskal_wallis_test(data, group_col, value_col):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis H-test for independent samples.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data.\n",
    "        group_col (str): Column name to group by (e.g., 'Group').\n",
    "        value_col (str): Column name containing the values (e.g., 'mean_stimulation').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Kruskal-Wallis H-statistic and p-value. If all values are identical, return NaN for both.\n",
    "    \"\"\"\n",
    "    # Group data by the specified column and extract values\n",
    "    groups = [group[value_col].values for name, group in data.groupby(group_col)]\n",
    "    \n",
    "    # Check if all values across groups are identical\n",
    "    all_values = np.concatenate(groups)\n",
    "    if np.all(all_values == all_values[0]):\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Perform Kruskal-Wallis test\n",
    "    h_stat, p_value = kruskal(*groups)\n",
    "\n",
    "    return h_stat, p_value\n",
    "\n",
    "\n",
    "def perform_kruskal_wallis_on_actual_data(actual_data_df, stimulations):\n",
    "    \"\"\"\n",
    "    Perform Kruskal-Wallis test on the actual data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        actual_data_df (pd.DataFrame): DataFrame containing the actual data.\n",
    "        stimulations (list of str): List of stimulations to include in the analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the Kruskal-Wallis test results.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary for Kruskal-Wallis test results\n",
    "    kruskal_results = {\n",
    "        'Cell_Type': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'N_CTZ': [],\n",
    "        'N_No_CTZ': [],\n",
    "        'Mean_CTZ': [],\n",
    "        'Mean_No_CTZ': [],\n",
    "        'H_statistic': [],\n",
    "        'p_value': []\n",
    "    }\n",
    "\n",
    "    for cell_type in actual_data_df['Cell_Type'].unique():\n",
    "        for stim_range in actual_data_df['StimRange'].unique():\n",
    "            for is_single_unit in actual_data_df['IsSingleUnit'].unique():\n",
    "                for stim_responsivity in actual_data_df['StimResponsivity'].unique():\n",
    "                    for stim in stimulations:\n",
    "                        # Create a subset of the data for the current combination\n",
    "                        subset_df = actual_data_df[\n",
    "                            ((actual_data_df['Cell_Type'] == cell_type) | (cell_type == 'All')) &\n",
    "                            (actual_data_df['StimRange'] == stim_range) &\n",
    "                            ((actual_data_df['IsSingleUnit'] == is_single_unit) | (is_single_unit == 'All')) &\n",
    "                            ((actual_data_df['StimResponsivity'] == stim_responsivity) | (stim_responsivity == 'All')) &\n",
    "                            (actual_data_df['Stimulation'] == stim)\n",
    "                        ]\n",
    "\n",
    "                        if subset_df.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Separate data for each group\n",
    "                        group_ctz = subset_df[subset_df['Group'] == 'CTZ']\n",
    "                        group_no_ctz = subset_df[subset_df['Group'] == 'No_CTZ']\n",
    "\n",
    "                        # Calculate N and mean for each group\n",
    "                        n_ctz = len(group_ctz)\n",
    "                        n_no_ctz = len(group_no_ctz)\n",
    "                        mean_ctz = group_ctz['mean_stimulation'].mean()\n",
    "                        mean_no_ctz = group_no_ctz['mean_stimulation'].mean()\n",
    "\n",
    "                        # Perform Kruskal-Wallis test\n",
    "                        h_stat, p_value = kruskal_wallis_test(subset_df, group_col='Group', value_col='mean_stimulation')\n",
    "\n",
    "                        # Store Kruskal-Wallis results\n",
    "                        kruskal_results['Cell_Type'].append(cell_type)\n",
    "                        kruskal_results['StimRange'].append(stim_range)\n",
    "                        kruskal_results['IsSingleUnit'].append(is_single_unit)\n",
    "                        kruskal_results['StimResponsivity'].append(stim_responsivity)\n",
    "                        kruskal_results['Stimulation'].append(stim)\n",
    "                        kruskal_results['N_CTZ'].append(n_ctz)\n",
    "                        kruskal_results['N_No_CTZ'].append(n_no_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places\n",
    "                        mean_ctz = float(\"{:.3f}\".format(mean_ctz))\n",
    "                        kruskal_results['Mean_CTZ'].append(mean_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places \n",
    "                        mean_no_ctz = float(\"{:.3f}\".format(mean_no_ctz))\n",
    "                        kruskal_results['Mean_No_CTZ'].append(mean_no_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places for h_stat\n",
    "                        h_stat = float(\"{:.3f}\".format(h_stat))\n",
    "                        kruskal_results['H_statistic'].append(h_stat)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places\n",
    "                        p_value = float(\"{:.3f}\".format(p_value))\n",
    "                        kruskal_results['p_value'].append(p_value)\n",
    "                    \n",
    "\n",
    "    # Convert Kruskal-Wallis results dictionary to DataFrame\n",
    "    kruskal_results_df = pd.DataFrame(kruskal_results)\n",
    "    print(f\"Kruskal-Wallis Results DataFrame:\\n{kruskal_results_df}\")\n",
    "\n",
    "    return kruskal_results_df\n",
    "\n",
    "\n",
    "def permutation_test(data, group_col, value_col, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test for the specified groups.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data.\n",
    "        group_col (str): Column name to group by (e.g., 'Group').\n",
    "        value_col (str): Column name containing the values (e.g., 'mean_stimulation').\n",
    "        n_permutations (int): Number of permutations to perform.\n",
    "\n",
    "    Returns:\n",
    "        float: p-value from the permutation test.\n",
    "    \"\"\"\n",
    "    # Calculate the actual Kruskal-Wallis test statistic\n",
    "    groups = [group[value_col].values for name, group in data.groupby(group_col)]\n",
    "    actual_stat, _ = kruskal(*groups)\n",
    "    \n",
    "    # Combine all data values and shuffle group labels to generate the null distribution\n",
    "    combined_values = data[value_col].values\n",
    "    group_labels = data[group_col].values\n",
    "    permuted_stats = []\n",
    "\n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(group_labels)\n",
    "        permuted_groups = [combined_values[group_labels == label] for label in np.unique(group_labels)]\n",
    "        perm_stat, _ = kruskal(*permuted_groups)\n",
    "        permuted_stats.append(perm_stat)\n",
    "    \n",
    "    # Calculate the p-value as the proportion of permuted stats greater than or equal to the actual stat\n",
    "    permuted_stats = np.array(permuted_stats)\n",
    "    p_value = np.sum(permuted_stats >= actual_stat) / n_permutations\n",
    "\n",
    "    return p_value\n",
    "def perform_permutation_test_on_actual_data(actual_data_df, stimulations, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Perform a permutation test on the actual data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        actual_data_df (pd.DataFrame): DataFrame containing the actual data.\n",
    "        stimulations (list of str): List of stimulations to include in the analysis.\n",
    "        n_permutations (int): Number of permutations to perform.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the permutation test results.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary for permutation test results\n",
    "    perm_results = {\n",
    "        'Cell_Type': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'N_CTZ': [],\n",
    "        'N_No_CTZ': [],\n",
    "        'Mean_CTZ': [],\n",
    "        'Mean_No_CTZ': [],\n",
    "        'Permutation_p_value': []\n",
    "    }\n",
    "\n",
    "    for cell_type in actual_data_df['Cell_Type'].unique():\n",
    "        for stim_range in actual_data_df['StimRange'].unique():\n",
    "            for is_single_unit in actual_data_df['IsSingleUnit'].unique():\n",
    "                for stim_responsivity in actual_data_df['StimResponsivity'].unique():\n",
    "                    for stim in stimulations:\n",
    "                        subset_df = actual_data_df[\n",
    "                            (actual_data_df['Cell_Type'] == cell_type) &\n",
    "                            (actual_data_df['StimRange'] == stim_range) &\n",
    "                            (actual_data_df['IsSingleUnit'] == is_single_unit) &\n",
    "                            (actual_data_df['StimResponsivity'] == stim_responsivity) &\n",
    "                            (actual_data_df['Stimulation'] == stim)\n",
    "                        ]\n",
    "\n",
    "                        if subset_df.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Separate data for each group\n",
    "                        group_ctz = subset_df[subset_df['Group'] == 'CTZ']\n",
    "                        group_no_ctz = subset_df[subset_df['Group'] == 'No_CTZ']\n",
    "\n",
    "                        # Calculate N and mean for each group\n",
    "                        n_ctz = len(group_ctz)\n",
    "                        n_no_ctz = len(group_no_ctz)\n",
    "                        mean_ctz = group_ctz['mean_stimulation'].mean()\n",
    "                        mean_no_ctz = group_no_ctz['mean_stimulation'].mean()\n",
    "\n",
    "                        # Perform permutation test\n",
    "                        p_value = permutation_test(subset_df, group_col='Group', value_col='mean_stimulation', n_permutations=n_permutations)\n",
    "\n",
    "                        # Store permutation test results\n",
    "                        perm_results['Cell_Type'].append(cell_type)\n",
    "                        perm_results['StimRange'].append(stim_range)\n",
    "                        perm_results['IsSingleUnit'].append(is_single_unit)\n",
    "                        perm_results['StimResponsivity'].append(stim_responsivity)\n",
    "                        perm_results['Stimulation'].append(stim)\n",
    "                        perm_results['N_CTZ'].append(n_ctz)\n",
    "                        perm_results['N_No_CTZ'].append(n_no_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places\n",
    "                        mean_ctz = float(\"{:.3f}\".format(mean_ctz))\n",
    "                        perm_results['Mean_CTZ'].append(mean_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places \n",
    "                        mean_no_ctz = float(\"{:.3f}\".format(mean_no_ctz))\n",
    "                        perm_results['Mean_No_CTZ'].append(mean_no_ctz)\n",
    "                        \n",
    "                        #convert to a float with 3 decimal places\n",
    "                        p_value = float(\"{:.3f}\".format(p_value))\n",
    "                        perm_results['Permutation_p_value'].append(p_value)\n",
    "\n",
    "    # Convert permutation test results dictionary to DataFrame\n",
    "    perm_results_df = pd.DataFrame(perm_results)\n",
    "    print(f\"Permutation Test Results DataFrame:\\n{perm_results_df}\")\n",
    "\n",
    "    return perm_results_df\n",
    "\n",
    "def bootstrap_statistic_difference(data, group_col, value_col, n_bootstraps=10000, alpha=0.05, statistic='mean'):\n",
    "    \"\"\"\n",
    "    Perform a bootstrap statistic-difference test between two groups (mean or median).\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing the data.\n",
    "        group_col (str): Column name to group by (e.g., 'Group').\n",
    "        value_col (str): Column name containing the values (e.g., 'mean_stimulation').\n",
    "        n_bootstraps (int): Number of bootstrap samples to generate.\n",
    "        alpha (float): Significance level for confidence intervals.\n",
    "        statistic (str): Statistic to use ('mean' or 'median').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Observed statistic difference, bootstrap statistic difference confidence interval, and p-value.\n",
    "    \"\"\"\n",
    "    # Ensure that there are exactly two groups to compare\n",
    "    unique_groups = data[group_col].unique()\n",
    "    if len(unique_groups) != 2:\n",
    "        raise ValueError(f\"Bootstrap statistic-difference test requires exactly two groups, but {len(unique_groups)} groups were found.\")\n",
    "    \n",
    "    # Extract values for each group\n",
    "    group1_values = data[data[group_col] == unique_groups[0]][value_col].values\n",
    "    group2_values = data[data[group_col] == unique_groups[1]][value_col].values\n",
    "    \n",
    "    # Select the appropriate statistic function\n",
    "    if statistic == 'mean':\n",
    "        stat_func = np.mean\n",
    "    elif statistic == 'median':\n",
    "        stat_func = np.median\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported statistic '{statistic}'. Use 'mean' or 'median'.\")\n",
    "    \n",
    "    # Calculate the observed statistic difference\n",
    "    observed_stat_diff = stat_func(group1_values) - stat_func(group2_values)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    stat_diffs = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        group1_sample = np.random.choice(group1_values, size=len(group1_values), replace=True)\n",
    "        group2_sample = np.random.choice(group2_values, size=len(group2_values), replace=True)\n",
    "        stat_diffs.append(stat_func(group1_sample) - stat_func(group2_sample))\n",
    "    \n",
    "    # Calculate the p-value\n",
    "    stat_diffs = np.array(stat_diffs)\n",
    "    p_value = np.sum(np.abs(stat_diffs) >= np.abs(observed_stat_diff)) / n_bootstraps\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    ci_lower = np.percentile(stat_diffs, 100 * alpha / 2)\n",
    "    ci_upper = np.percentile(stat_diffs, 100 * (1 - alpha / 2))\n",
    "    \n",
    "    return observed_stat_diff, (ci_lower, ci_upper), p_value\n",
    "def perform_bootstrap_statistic_test_on_actual_data(actual_data_df, stimulations, n_bootstraps=10000, alpha=0.05, statistic='mean'):\n",
    "    \"\"\"\n",
    "    Perform a bootstrap statistic-difference test (mean or median) on the actual data DataFrame.\n",
    "\n",
    "    Args:\n",
    "        actual_data_df (pd.DataFrame): DataFrame containing the actual data.\n",
    "        stimulations (list of str): List of stimulations to include in the analysis.\n",
    "        n_bootstraps (int): Number of bootstrap samples to generate.\n",
    "        alpha (float): Significance level for confidence intervals.\n",
    "        statistic (str): Statistic to use ('mean' or 'median').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the bootstrap test results.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary for bootstrap statistic-difference test results\n",
    "    bootstrap_results = {\n",
    "        'Cell_Type': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'N_CTZ': [],\n",
    "        'N_No_CTZ': [],\n",
    "        f'{statistic.capitalize()}_CTZ': [],\n",
    "        f'{statistic.capitalize()}_No_CTZ': [],\n",
    "        'Observed_Statistic_Difference': [],\n",
    "        'CI_Lower': [],\n",
    "        'CI_Upper': [],\n",
    "        'Bootstrap_p_value': []\n",
    "    }\n",
    "\n",
    "    for cell_type in actual_data_df['Cell_Type'].unique():\n",
    "        for stim_range in actual_data_df['StimRange'].unique():\n",
    "            for is_single_unit in actual_data_df['IsSingleUnit'].unique():\n",
    "                for stim_responsivity in actual_data_df['StimResponsivity'].unique():\n",
    "                    for stim in stimulations:\n",
    "                        subset_df = actual_data_df[\n",
    "                            (actual_data_df['Cell_Type'] == cell_type) &\n",
    "                            (actual_data_df['StimRange'] == stim_range) &\n",
    "                            (actual_data_df['IsSingleUnit'] == is_single_unit) &\n",
    "                            (actual_data_df['StimResponsivity'] == stim_responsivity) &\n",
    "                            (actual_data_df['Stimulation'] == stim)\n",
    "                        ]\n",
    "\n",
    "                        if subset_df.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Separate data for each group\n",
    "                        group_ctz = subset_df[subset_df['Group'] == 'CTZ']\n",
    "                        group_no_ctz = subset_df[subset_df['Group'] == 'No_CTZ']\n",
    "\n",
    "                        # Calculate N and statistic for each group\n",
    "                        n_ctz = len(group_ctz)\n",
    "                        n_no_ctz = len(group_no_ctz)\n",
    "                        stat_ctz = np.mean(group_ctz['mean_stimulation']) if statistic == 'mean' else np.median(group_ctz['mean_stimulation'])\n",
    "                        stat_no_ctz = np.mean(group_no_ctz['mean_stimulation']) if statistic == 'mean' else np.median(group_no_ctz['mean_stimulation'])\n",
    "\n",
    "                        # Perform bootstrap statistic-difference test\n",
    "                        observed_stat_diff, (ci_lower, ci_upper), p_value = bootstrap_statistic_difference(\n",
    "                            subset_df, group_col='Group', value_col='mean_stimulation', \n",
    "                            n_bootstraps=n_bootstraps, alpha=alpha, statistic=statistic\n",
    "                        )\n",
    "\n",
    "                        # Store bootstrap test results\n",
    "                        bootstrap_results['Cell_Type'].append(cell_type)\n",
    "                        bootstrap_results['StimRange'].append(stim_range)\n",
    "                        bootstrap_results['IsSingleUnit'].append(is_single_unit)\n",
    "                        bootstrap_results['StimResponsivity'].append(stim_responsivity)\n",
    "                        bootstrap_results['Stimulation'].append(stim)\n",
    "                        bootstrap_results['N_CTZ'].append(n_ctz)\n",
    "                        bootstrap_results['N_No_CTZ'].append(n_no_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places\n",
    "                        stat_ctz = float(\"{:.3f}\".format(stat_ctz))\n",
    "                        bootstrap_results[f'{statistic.capitalize()}_CTZ'].append(stat_ctz)\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places \n",
    "                        stat_no_ctz = float(\"{:.3f}\".format(stat_no_ctz))\n",
    "                        bootstrap_results[f'{statistic.capitalize()}_No_CTZ'].append(stat_no_ctz)\n",
    "                        \n",
    "                        # Store observed statistic difference and confidence interval\n",
    "                        bootstrap_results['Observed_Statistic_Difference'].append(float(\"{:.3f}\".format(observed_stat_diff)))\n",
    "                        bootstrap_results['CI_Lower'].append(float(\"{:.3f}\".format(ci_lower)))\n",
    "                        bootstrap_results['CI_Upper'].append(float(\"{:.3f}\".format(ci_upper)))\n",
    "                        \n",
    "                        # Convert to a float with 3 decimal places\n",
    "                        p_value = float(\"{:.3f}\".format(p_value))\n",
    "                        bootstrap_results['Bootstrap_p_value'].append(p_value)\n",
    "\n",
    "    # Convert bootstrap test results dictionary to DataFrame\n",
    "    bootstrap_results_df = pd.DataFrame(bootstrap_results)\n",
    "    print(f\"Bootstrap {statistic.capitalize()}-Difference Test Results DataFrame:\\n{bootstrap_results_df}\")\n",
    "\n",
    "    return bootstrap_results_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The same approach can be applied to the other functions for generating data and performing tests. \n",
    "# Now, loop through each laminar label and call the statistical analysis functions, saving the results dynamically.\n",
    "cell_types = ['FS', 'RS', None]\n",
    "laminarlabel = ['SG', 'IG', 'L4']\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_laminar_NOIQR'\n",
    "stimulations = ['Zero', 'Low', 'Mid', 'Max']  # Correct stimulation labels\n",
    "stim_ranges =  [(0, 500), (0,50), (0,20)]\n",
    "is_single_units = [1.0, 0.0,  None]\n",
    "stim_responsivities = [1.0, None]\n",
    "\n",
    "\n",
    "for laminar in laminarlabel:\n",
    "    print(f\"Running analysis for laminar label: {laminar}\")\n",
    "    \n",
    "    # Run the descriptive statistics\n",
    "    #descriptive_stats_df = dynamic_statistical_analysis(whisker_df_manager, cell_types, stim_ranges, is_single_units, stim_responsivities, [laminar], stimulations)\n",
    "    \n",
    "    \n",
    "    # Generate actual data\n",
    "    print(f\"Generating actual data for laminar label: {laminar}\")\n",
    "    input_df_4_kruskal_and_permutation = generate_actual_data(whisker_df_manager, \n",
    "                                                              cell_types, \n",
    "                                                              stim_ranges, \n",
    "                                                              is_single_units, \n",
    "                                                              stim_responsivities, \n",
    "                                                              [laminar],\n",
    "                                                              stimulations)\n",
    "    \n",
    "    \n",
    "    ### run statistical test\n",
    "    \n",
    "    #perform bootstrap test\n",
    "    #bootstrap_results_df = perform_bootstrap_statistic_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations, n_bootstraps=10000, alpha=0.05, statistic='median')\n",
    "    \n",
    "    # Perform Kruskal-Wallis test\n",
    "    kruskal_results_df = perform_kruskal_wallis_on_actual_data(input_df_4_kruskal_and_permutation, stimulations)\n",
    "    \n",
    "    # perform Wilcoxon rank-sum test\n",
    "    #wilcoxon_results_df = perform_wilcoxon_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations)\n",
    "    \n",
    "    # Perform permutation test\n",
    "    #perm_results_df = perform_permutation_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations, n_permutations=1000)\n",
    "    \n",
    "    # Save the results dynamically based on laminar label\n",
    "    laminar_directory = os.path.join(base_directory, laminar)\n",
    "    os.makedirs(laminar_directory, exist_ok=True)\n",
    "    \n",
    "    #descriptive_stats_df.to_csv(os.path.join(laminar_directory, f'descriptive_stats_{laminar}.csv'), index=False)\n",
    "    input_df_4_kruskal_and_permutation.to_csv(os.path.join(laminar_directory, f'input_df_4_kruskal_and_permutation_{laminar}.csv'), index=False)\n",
    "    kruskal_results_df.to_csv(os.path.join(laminar_directory, f'kruskal_results_{laminar}.csv'), index=False)\n",
    "    #perm_results_df.to_csv(os.path.join(laminar_directory, f'perm_results_{laminar}.csv'), index=False)\n",
    "    #wilcoxon_results_df.to_csv(os.path.join(laminar_directory, f'wilcoxon_results_{laminar}.csv'), index=False)\n",
    "    #bootstrap_results_df.to_csv(os.path.join(laminar_directory, f'bootstrap_results_{laminar}.csv'), index=False)\n",
    "#\n",
    "    print(f\"Completed analysis for laminar label: {laminar}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a smilar plotting and analysis approach for other analysis beyond PSTHS--- but only for FirstSpikeLatency -- you will have to change the ylim within the dynamic_plot_saving_beyond_PSTHs based on the FS and RS--- need to fix later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the box and line plots for all combinations of cell types, stim ranges, is_single_units, and stim_responsivities\n",
    "def replace_mean_stimulation_with_column_value(df, target_column):\n",
    "    \"\"\"\n",
    "    Replaces the values in the 'mean_stimulation' column with the corresponding values\n",
    "    from a specified column based on the 'Stimulation' entry.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        target_column (str): The name of the column to use for replacing 'mean_stimulation' values.\n",
    "                             This column should contain arrays with four elements corresponding to\n",
    "                             'Zero', 'Low', 'Mid', and 'Max' stimulations.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with 'mean_stimulation' values replaced.\n",
    "    \"\"\"\n",
    "    # Define a mapping from Stimulation labels to indices\n",
    "    stimulation_mapping = {'Zero': 0, 'Low': 1, 'Mid': 2, 'Max': 3}\n",
    "    \n",
    "    # Apply the replacement row by row\n",
    "    def replace_row(row):\n",
    "        stim_label = row['Stimulation']\n",
    "        if stim_label in stimulation_mapping:\n",
    "            index = stimulation_mapping[stim_label]\n",
    "            # Replace mean_stimulation with the corresponding value from the target column\n",
    "            row['mean_stimulation'] = row[target_column][index]\n",
    "        return row\n",
    "    \n",
    "    # Apply the function to each row of the DataFrame\n",
    "    df = df.apply(replace_row, axis=1)\n",
    "    \n",
    "    return df\n",
    "def dynamic_statistical_analysis_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations=None):\n",
    "    \"\"\"\n",
    "    Dynamically collects descriptive statistics across different combinations for specified cell types and parameter combinations.\n",
    "    This function is an extension of the dynamic_statistical_analysis function to handle beyond PSTH data analysis so \n",
    "    that it can be used for other data types beyond PSTHs. It used a helper to replace the mean_stimulation column with the target column data and unpacks the target column data for analysis.\n",
    "    The target columns must have four indexed within their array (zero, low, mid, and max) correspondign to the stimulations and then replaced.\n",
    "    \n",
    "    # it currently filters based on modulation_label='positive' and firstspike_latency=True so keep that in mind\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        stimulations (list of str, optional): List of stimulations to include in the analysis. If None, include all stimulations.\n",
    "        target_column (str): The target column to use for the analysis.\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'CellType': [],\n",
    "        'StimRange': [],\n",
    "        'IsSingleUnit': [],\n",
    "        'StimResponsivity': [],\n",
    "        'Stimulation': [],\n",
    "        'Group': [],\n",
    "        'N': [],\n",
    "        'Mean': [],\n",
    "        'Median': [],\n",
    "        'SD': [],\n",
    "        'SEM': [],\n",
    "        'CI Lower': [],\n",
    "        'CI Upper': [],\n",
    "        'Range': []\n",
    "    }\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "                    \n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', stim_label=None, \n",
    "                        baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                        stim_responsivity=stim_responsivity, \n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    \n",
    "                    # replace the data with the actual data of the target column\n",
    "                    df = replace_mean_stimulation_with_column_value(df, target_column)\n",
    "                    \n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Filter data for the specified stimulations\n",
    "                    if stimulations:\n",
    "                        df = df[df['Stimulation'].isin(stimulations)]\n",
    "                    \n",
    "                    \n",
    "                    # Check if the required columns are present\n",
    "                    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Loop through each stimulation and perform analysis\n",
    "                    for stim in stimulations or df['Stimulation'].unique():\n",
    "                        for group in ['CTZ', 'No_CTZ']:\n",
    "                            group_data = df[(df['Stimulation'] == stim) & (df['Group'] == group)]['mean_stimulation']\n",
    "                            \n",
    "                            if group_data.empty:\n",
    "                                print(f\"Skipping {stim} for group {group} due to lack of data\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Calculate IQR\n",
    "                            Q1 = group_data.quantile(0.25)\n",
    "                            Q3 = group_data.quantile(0.75)\n",
    "                            IQR = Q3 - Q1\n",
    "                            \n",
    "                            # Filter data within IQR\n",
    "                            filtered_data = group_data[(group_data >= Q1 - 1.5 * IQR) & (group_data <= Q3 + 1.5 * IQR)]\n",
    "                            \n",
    "                            if filtered_data.empty:\n",
    "                                print(f\"Skipping {stim} for group {group} due to all data being outside IQR\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Calculate descriptive statistics\n",
    "                            group_desc = filtered_data.describe()\n",
    "                            group_sem = sem(filtered_data, nan_policy='omit')\n",
    "                            confidence_level = 0.95\n",
    "                            degrees_freedom = len(filtered_data) - 1\n",
    "                            confidence_interval = t.interval(confidence_level, degrees_freedom, loc=group_desc['mean'], scale=group_sem)\n",
    "                            \n",
    "                            # Store results\n",
    "                            results['CellType'].append(cell_type)\n",
    "                            results['StimRange'].append(f\"{stim_range[0]}to{stim_range[1]}\")\n",
    "                            results['IsSingleUnit'].append(is_single_unit_str)\n",
    "                            results['StimResponsivity'].append(stim_responsivity_str)\n",
    "                            results['Stimulation'].append(stim)\n",
    "                            results['Group'].append(group)\n",
    "                            results['N'].append(group_desc['count'])\n",
    "                            results['Mean'].append(group_desc['mean'])\n",
    "                            results['Median'].append(filtered_data.median())\n",
    "                            results['SD'].append(group_desc['std'])\n",
    "                            results['SEM'].append(group_sem)\n",
    "                            results['CI Lower'].append(confidence_interval[0])\n",
    "                            results['CI Upper'].append(confidence_interval[1])\n",
    "                            results['Range'].append((filtered_data.min(), filtered_data.max()))\n",
    "    \n",
    "    # Convert results dictionary to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"Results DataFrame:\\n{results_df}\")\n",
    "\n",
    "    return results_df\n",
    "def dynamic_plot_saving_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    ### you might have to change the ylim values for the different cell types!!!!!! \n",
    "    \n",
    "    Dynamically saves plots across different conditions for specified cell types and parameter combinations.\n",
    "    \n",
    "    This function is an extension of the dynamic_plot_saving_beyond_PSTHs function to handle beyond PSTH data analysis so \n",
    "    that it can be used for other data types beyond PSTHs. It used a helper to replace the mean_stimulation column with the target column data and unpacks the target column data for analysis.\n",
    "    The target columns must have four indexed within their array (zero, low, mid, and max) correspondign to the stimulations and then replaced.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        base_directory (str): Base directory to save the plots.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "                    \n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', \n",
    "                        baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                        stim_responsivity=stim_responsivity, \n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    df = replace_mean_stimulation_with_column_value(df, target_column)\n",
    "                    \n",
    "                    \n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Check if the required columns are present\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Define directory for saving plots\n",
    "                    directory = os.path.join(base_directory, 'boxplots_and_line', suffix)\n",
    "                    os.makedirs(directory, exist_ok=True)\n",
    "                    print(f\"Directory created: {directory}\")\n",
    "                    \n",
    "                    \n",
    "                    #if cell_type == 'FS':\n",
    "                    #    ylim_FS = (0, 40)\n",
    "                    #if cell_type == 'RS':\n",
    "                    #    ylim = (0, 300)\n",
    "                        \n",
    "                    # Plot box and strip plots\n",
    "                    print(\"Plotting box and strip plots\")\n",
    "                    whisker_df_manager.plot_box_and_strip_with_controls(df, \n",
    "                        groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=False, show_scatter=True, \n",
    "                        directory=directory, file_name=f'{suffix}_boxplot', \n",
    "                        modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    #whisker_df_manager.plot_box_and_strip(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=True,\n",
    "                    #    directory=directory, file_name=f'{suffix}_boxplot', ylim=ylim, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "                    \n",
    "                    # Plot mean and SEM line plots\n",
    "                    #print(\"Plotting mean and SEM line plots\")\n",
    "                    #whisker_df_manager.plot_mean_and_sem_lineplot(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, directory=directory,\n",
    "                    #    file_name=f'{suffix}_lineplot', ylim=None, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "\n",
    "#all possible combinations of cell types, stim ranges, is_single_units, and stim_responsivities\n",
    "cell_types = ['FS', 'RS']\n",
    "stim_ranges =  [(0,20)]\n",
    "is_single_units = [1.0]\n",
    "stim_responsivities = [1.0]\n",
    "stimulations = ['Low', 'Mid', 'Max']\n",
    "target_column='FirstSpikeLatency'\n",
    "\n",
    "\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency'\n",
    "dynamic_plot_saving_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations = ['Low', 'Mid', 'Max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "cell_types = ['FS', 'RS']\n",
    "stim_ranges =  [(0,20)]\n",
    "is_single_units = [1.0]\n",
    "stim_responsivities = [1.0]\n",
    "stimulations = ['Low', 'Mid', 'Max']\n",
    "target_column='FirstSpikeLatency'\n",
    "\n",
    "descriptive_stats_df_FirstSpikeLatency = dynamic_statistical_analysis_beyond_PSTHs(whisker_df_manager, target_column, \n",
    "                                                                                   cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "input_df_4_kruskal_and_permutation_FirstSpikeLatency = generate_actual_data(whisker_df_manager, \n",
    "                                                              cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "kruskal_results_df_FirstSpikeLatency = perform_kruskal_wallis_on_actual_data(input_df_4_kruskal_and_permutation, stimulations)\n",
    "perm_results_df_FirstSpikeLatency  = perform_permutation_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations, n_permutations=1000)\n",
    "\n",
    "descriptive_stats_df_FirstSpikeLatency.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency/descriptive_stats_FirstSpikeLatency.csv', index=False)\n",
    "input_df_4_kruskal_and_permutation_FirstSpikeLatency.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency/input_df_4_kruskal_and_permutation_FirstSpikeLatency.csv', index=False)\n",
    "kruskal_results_df_FirstSpikeLatency.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency/kruskal_results_FirstSpikeLatency.csv', index=False)\n",
    "perm_results_df_FirstSpikeLatency.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency/perm_results_FirstSpikeLatency.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a smilar plotting and analysis approach for other analysis beyond PSTHS--- but only for FirstSpikeLatency_Reliability--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = ['FS', 'RS']\n",
    "stim_ranges =  [(0,20)]\n",
    "is_single_units = [1.0]\n",
    "stim_responsivities = [1.0]\n",
    "stimulations = ['Low', 'Mid', 'Max']\n",
    "target_column='FirstSpikeLatency_Reliability'\n",
    "\n",
    "### hard coded to change ylim for FirstSpikeLatency_Reliability !!!!! \n",
    "def dynamic_plot_saving_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    ### you might have to change the ylim values for the different cell types!!!!!! \n",
    "    \n",
    "    Dynamically saves plots across different conditions for specified cell types and parameter combinations.\n",
    "    \n",
    "    This function is an extension of the dynamic_plot_saving_beyond_PSTHs function to handle beyond PSTH data analysis so \n",
    "    that it can be used for other data types beyond PSTHs. It used a helper to replace the mean_stimulation column with the target column data and unpacks the target column data for analysis.\n",
    "    The target columns must have four indexed within their array (zero, low, mid, and max) correspondign to the stimulations and then replaced.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        base_directory (str): Base directory to save the plots.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "                    \n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', \n",
    "                        baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                        stim_responsivity=stim_responsivity, \n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    df = replace_mean_stimulation_with_column_value(df, target_column)\n",
    "                    \n",
    "                    \n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Check if the required columns are present\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Define directory for saving plots\n",
    "                    directory = os.path.join(base_directory, 'boxplots_and_line', suffix)\n",
    "                    os.makedirs(directory, exist_ok=True)\n",
    "                    print(f\"Directory created: {directory}\")\n",
    "                    \n",
    "                    \n",
    "                    #if cell_type == 'FS':\n",
    "                    #    ylim = (0.0, 0.08)\n",
    "                    #if cell_type == 'RS':\n",
    "                    #   ylim = (0.0, 0.03)\n",
    "                        \n",
    "                    #whisker_df_manager.plot_box_and_strip(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=True,\n",
    "                    #    directory=directory, file_name=f'{suffix}_boxplot', ylim=ylim, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "                    \n",
    "                    \n",
    "                    # Plot box and strip plots\n",
    "                    print(\"Plotting box and strip plots\")\n",
    "                    whisker_df_manager.plot_box_and_strip_with_controls(df, \n",
    "                        groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=False, show_scatter=True, \n",
    "                        directory=directory, file_name=f'{suffix}_boxplot', \n",
    "                        modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Plot mean and SEM line plots\n",
    "                    #print(\"Plotting mean and SEM line plots\")\n",
    "                    #whisker_df_manager.plot_mean_and_sem_lineplot(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, directory=directory,\n",
    "                    #    file_name=f'{suffix}_lineplot', ylim=None, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "\n",
    "\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency_Reliability'\n",
    "dynamic_plot_saving_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations = ['Low', 'Mid', 'Max'])\n",
    "\n",
    "\n",
    "descriptive_stats_df_FirstSpikeLatency_Reliability = dynamic_statistical_analysis_beyond_PSTHs(whisker_df_manager, target_column, \n",
    "                                                                                   cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "input_df_4_kruskal_and_permutation_FirstSpikeLatency_Reliability = generate_actual_data(whisker_df_manager, \n",
    "                                                              cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "kruskal_results_df_FirstSpikeLatency_Reliability = perform_kruskal_wallis_on_actual_data(input_df_4_kruskal_and_permutation, stimulations)\n",
    "perm_results_df_FirstSpikeLatency_Reliability  = perform_permutation_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations, n_permutations=1000)\n",
    "\n",
    "descriptive_stats_df_FirstSpikeLatency_Reliability.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency_Reliability/descriptive_stats_FirstSpikeLatency_Reliability.csv', index=False)\n",
    "input_df_4_kruskal_and_permutation_FirstSpikeLatency_Reliability.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency_Reliability/input_df_4_kruskal_and_permutation_FirstSpikeLatency_Reliability.csv', index=False)\n",
    "kruskal_results_df_FirstSpikeLatency_Reliability.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency_Reliability/kruskal_results_FirstSpikeLatency_Reliability.csv', index=False)\n",
    "perm_results_df_FirstSpikeLatency_Reliability.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency_Reliability/perm_results_FirstSpikeLatency_Reliability.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a smilar plotting and analysis approach for other analysis beyond PSTHS--- but only for FanoFactor_stim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_types = ['FS', 'RS']\n",
    "stim_ranges =  [(0, 500), (0,50), (0,20)]\n",
    "is_single_units = [1.0, 0.0]\n",
    "stim_responsivities = [1.0]\n",
    "stimulations = ['Zero', 'Low', 'Mid', 'Max']\n",
    "target_column='FanoFactor_stim'\n",
    "\n",
    "def dynamic_plot_saving_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    ### you might have to change the ylim values for the different cell types!!!!!! \n",
    "    \n",
    "    Dynamically saves plots across different conditions for specified cell types and parameter combinations.\n",
    "    \n",
    "    This function is an extension of the dynamic_plot_saving_beyond_PSTHs function to handle beyond PSTH data analysis so \n",
    "    that it can be used for other data types beyond PSTHs. It used a helper to replace the mean_stimulation column with the target column data and unpacks the target column data for analysis.\n",
    "    The target columns must have four indexed within their array (zero, low, mid, and max) correspondign to the stimulations and then replaced.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        cell_types (list of str): List of cell types (e.g., ['FS', 'RS']).\n",
    "        stim_ranges (list of tuple): List of stimulation ranges (e.g., [(0, 500), (0, 1000)]).\n",
    "        is_single_units (list of float): List of is_single_unit values (e.g., [1.0, 0.0, None]).\n",
    "        stim_responsivities (list of float): List of stim_responsivity values (e.g., [1.0, 0.0, None]).\n",
    "        base_directory (str): Base directory to save the plots.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot. If None, include all stimulations.\n",
    "    \"\"\"\n",
    "    required_columns = ['mean_stimulation', 'Stimulation', 'Group']\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        for stim_range in stim_ranges:\n",
    "            for is_single_unit in is_single_units:\n",
    "                for stim_responsivity in stim_responsivities:\n",
    "                    # Define file name suffix based on parameters\n",
    "                    is_single_unit_str = 'All' if is_single_unit is None else str(is_single_unit)\n",
    "                    stim_responsivity_str = 'All' if stim_responsivity is None else str(stim_responsivity)\n",
    "                    suffix = f\"{cell_type}_IsSingleUnit_{is_single_unit_str}_IsStimResponsive_{stim_responsivity_str}_StimRange_{stim_range[0]}to{stim_range[1]}\"\n",
    "                    \n",
    "                    # Calculate basic stats\n",
    "                    print(f\"Calculating basic stats for: {suffix}\")\n",
    "                    whisker_df_manager.calculate_basic_stats(\n",
    "                        'CTZ', 'No_CTZ', \n",
    "                        baseline_range=(-100, -1), stim_range=stim_range, \n",
    "                        cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                        stim_responsivity=stim_responsivity, \n",
    "                        smoothing_window=3, modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Prepare data for boxplot\n",
    "                    print(\"Preparing data for boxplot\")\n",
    "                    df = whisker_df_manager.prepare_for_boxplot()\n",
    "                    df = replace_mean_stimulation_with_column_value(df, target_column)\n",
    "                    \n",
    "                    \n",
    "                    print(f\"DataFrame shape after preparation: {df.shape}\")\n",
    "                    print(f\"DataFrame columns: {df.columns}\")\n",
    "                    print(f\"First few rows of DataFrame: {df.head()}\")\n",
    "\n",
    "                    # Check if the required columns are present\n",
    "                    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Skipping combination {suffix} due to missing columns: {missing_columns}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Define directory for saving plots\n",
    "                    directory = os.path.join(base_directory, 'boxplots_and_line', suffix)\n",
    "                    os.makedirs(directory, exist_ok=True)\n",
    "                    print(f\"Directory created: {directory}\")\n",
    "                    \n",
    "                    \n",
    "                    if cell_type == 'FS':\n",
    "                        ylim = (0.80, 1.02)\n",
    "                    if cell_type == 'RS':\n",
    "                       ylim = (0.95, 1.02)\n",
    "                        \n",
    "                    # Plot box and strip plots\n",
    "                    print(\"Plotting box and strip plots\")\n",
    "                    whisker_df_manager.plot_box_and_strip(df, \n",
    "                        groups=['CTZ', 'No_CTZ'], stimulations=stimulations, show_outliers=True,\n",
    "                        directory=directory, file_name=f'{suffix}_boxplot', ylim=ylim, \n",
    "                        modulation_label='positive'\n",
    "                    )\n",
    "                    \n",
    "                    # Plot mean and SEM line plots\n",
    "                    #print(\"Plotting mean and SEM line plots\")\n",
    "                    #whisker_df_manager.plot_mean_and_sem_lineplot(df, \n",
    "                    #    groups=['CTZ', 'No_CTZ'], stimulations=stimulations, directory=directory,\n",
    "                    #    file_name=f'{suffix}_lineplot', ylim=None, \n",
    "                    #    modulation_label='positive'\n",
    "                    #)\n",
    "\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FanoFactor_stim'\n",
    "\n",
    "dynamic_plot_saving_beyond_PSTHs(whisker_df_manager, target_column, cell_types, stim_ranges, is_single_units, stim_responsivities, base_directory, stimulations = ['Low', 'Mid', 'Max'])\n",
    "\n",
    "\n",
    "descriptive_stats_df_FanoFactor_stim = dynamic_statistical_analysis_beyond_PSTHs(whisker_df_manager, target_column, \n",
    "                                                                                   cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "input_df_4_kruskal_and_permutation_FanoFactor_stim = generate_actual_data(whisker_df_manager, \n",
    "                                                              cell_types, stim_ranges, is_single_units, stim_responsivities, stimulations)\n",
    "kruskal_results_df_FanoFactor_stim = perform_kruskal_wallis_on_actual_data(input_df_4_kruskal_and_permutation, stimulations)\n",
    "perm_results_df_FanoFactor_stim  = perform_permutation_test_on_actual_data(input_df_4_kruskal_and_permutation, stimulations, n_permutations=1000)\n",
    "\n",
    "\n",
    "\n",
    "descriptive_stats_df_FanoFactor_stim.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FirstSpikeLatency_Reliability/descriptive_stats_FanoFactor_stim.csv', index=False)\n",
    "input_df_4_kruskal_and_permutation_FanoFactor_stim.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FanoFactor_stim/input_df_4_kruskal_and_permutation_FanoFactor_stim.csv', index=False)\n",
    "kruskal_results_df_FanoFactor_stim.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FanoFactor_stim/kruskal_results_FanoFactor_stim.csv', index=False)\n",
    "perm_results_df_FanoFactor_stim.to_csv('/Volumes/MannySSD/figures/comparing_conditions_dynamic/all_stimulations_FanoFactor_stim/perm_results_FanoFactor_stim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot and analyze bases on location of unit on probe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert the modulation \n",
    "whisker_df_manager.revert_modulation_index_to_numeric()\n",
    "print(whisker_df_manager.dataframes['basic_metrics']['ModulationIndex'].head())\n",
    "print(whisker_df_manager.dataframes['basic_metrics'][['groupname', 'recordingname', 'cid', 'ModulationIndex_Numeric']].head())\n",
    "print(whisker_df_manager.dataframes['basic_metrics'][['groupname', 'recordingname', 'cid', 'ModulationIndex']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to plot per group, laminar profile with basefline FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size('basic_metrics', group_name='CTZ', is_single_unit=0.0, cell_type=None, stim_responsivity=None, jitter=0.9)\n",
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size('basic_metrics', group_name='No_CTZ', is_single_unit=0.0, cell_type=None, stim_responsivity=None, jitter=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size_groupcomparison('basic_metrics', \n",
    "                                                                                         is_single_unit=None,\n",
    "                                                                                         cell_type=None, \n",
    "                                                                                         stim_responsivity=None, \n",
    "                                                                                         jitter=0.5, \n",
    "                                                                                         size_multiplier=50, \n",
    "                                                                                         min_size=5, \n",
    "                                                                                         min_fr_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size_groupcomparison('basic_metrics', \n",
    "                                                                                         is_single_unit=1.0,\n",
    "                                                                                         cell_type='FS', \n",
    "                                                                                         stim_responsivity=None, \n",
    "                                                                                         jitter=0.5, \n",
    "                                                                                         size_multiplier=50, \n",
    "                                                                                         min_size=5, \n",
    "                                                                                         min_fr_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_modulation_index_with_stim_prob_color_and_size_groupcomparison(data_obj, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_prob_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Plot the modulation index as a function of electrode location for CTZ and No_CTZ groups side by side.\n",
    "    StimProb AUROC (first value in the array) is represented by both color and size of circles, with improved scaling for AUROC values.\n",
    "    The 'plasma' colormap is used for values > 0.5, and black triangles are used for values == 0.5.\n",
    "    Circles have black outlines and are slightly offset horizontally to reduce overlap. \n",
    "    Horizontal lines connect each point to x=0.\n",
    "\n",
    "    Parameters:\n",
    "    data_obj (object): Object containing the data and methods for data preparation.\n",
    "    df_name (str): Name of the DataFrame to filter and plot.\n",
    "    is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                    if None, do not filter by this criterion.\n",
    "    cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "    stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                    If None, no filtering by StimResponsivity.\n",
    "    jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "    size_multiplier (float): Multiplier for circle sizes. Default is 1.\n",
    "    min_size (float): Minimum size for circles with StimProb at the threshold. Default is 20.\n",
    "    min_prob_threshold (float): StimProb threshold for minimum size. Default is 0.5.\n",
    "    \"\"\"\n",
    "    groups = ['CTZ', 'No_CTZ']\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 10), sharey=True)\n",
    "\n",
    "    all_data = []\n",
    "    for group in groups:\n",
    "        data = data_obj.prepare_plotting_data(df_name, group, is_single_unit, cell_type, stim_responsivity)\n",
    "        if data is not None and not data.empty:\n",
    "            all_data.append(data)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Combine all data for global color scaling\n",
    "    combined_data = pd.concat(all_data)\n",
    "\n",
    "    # Extract the first value (AUROC) from each StimProb array\n",
    "    combined_data['StimProb_AUROC'] = combined_data['StimProb'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) and len(x) > 0 else x)\n",
    "    combined_data['StimProb_AUROC'] = pd.to_numeric(combined_data['StimProb_AUROC'], errors='coerce')\n",
    "\n",
    "    # Ensure StimProb_AUROC values are between 0.5 and 1\n",
    "    combined_data['StimProb_AUROC'] = combined_data['StimProb_AUROC'].clip(0.5, 1)\n",
    "\n",
    "    min_prob = combined_data['StimProb_AUROC'].min()\n",
    "    max_prob = combined_data['StimProb_AUROC'].max()\n",
    "\n",
    "    # Define size calculation function with improved scaling\n",
    "    def calculate_size(prob):\n",
    "        return min_size + (prob - min_prob_threshold) * size_multiplier * 1000\n",
    "\n",
    "    for ax, group in zip(axes, groups):\n",
    "        plotting_data = combined_data[combined_data['groupname'] == group]\n",
    "\n",
    "        if plotting_data.empty:\n",
    "            ax.text(0.5, 0.5, f'No data for {group}', ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "\n",
    "        # Add horizontal jitter\n",
    "        plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "        # Calculate sizes\n",
    "        plotting_data['sizes'] = plotting_data['StimProb_AUROC'].apply(calculate_size)\n",
    "\n",
    "        # Plot horizontal lines from x=0 to each point\n",
    "        for _, row in plotting_data.iterrows():\n",
    "            ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                    color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "        # Separate data for AUROC == 0.5 and AUROC > 0.5\n",
    "        data_05 = plotting_data[plotting_data['StimProb_AUROC'] == 0.5]\n",
    "        data_gt_05 = plotting_data[plotting_data['StimProb_AUROC'] > 0.5]\n",
    "\n",
    "        # Plot circles for AUROC > 0.5\n",
    "        scatter = ax.scatter(data_gt_05['ModulationIndex_Numeric'], data_gt_05['Jittered_ElectrodeOrder'], \n",
    "                            c=data_gt_05['StimProb_AUROC'], cmap='plasma', \n",
    "                            s=data_gt_05['sizes'], alpha=0.8, vmin=0.5, vmax=1.0, \n",
    "                            edgecolors='black', linewidths=0.5)\n",
    "\n",
    "        # Plot black triangles for AUROC == 0.5\n",
    "        ax.scatter(data_05['ModulationIndex_Numeric'], data_05['Jittered_ElectrodeOrder'], \n",
    "                   c='black', marker='^', s=data_05['sizes'], \n",
    "                   edgecolors='black', linewidths=0.5)\n",
    "\n",
    "        # Add a vertical line at x=0\n",
    "        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax.set_xlabel('Spontaneous MI')\n",
    "        ax.set_title(f'{group}\\n{cell_type or \"All\"} {\"single\" if is_single_unit == 1.0 else \"multi\" if is_single_unit == 0.0 else \"all\"} units')\n",
    "        ax.invert_yaxis()  # To match the order from the provided list\n",
    "        ax.set_xlim(-1.25, 1.25)\n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "    # Add a common y-label\n",
    "    fig.text(0.04, 0.5, 'Electrode Order', va='center', rotation='vertical')\n",
    "\n",
    "    # Add colorbar for StimProb AUROC with improved scaling\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(scatter, cax=cbar_ax, extend='max')\n",
    "    cbar.set_label('StimProb AUROC')\n",
    "    \n",
    "    # Create more ticks for the colorbar\n",
    "    tick_locator = plt.LinearLocator(numticks=6)\n",
    "    cbar.locator = tick_locator\n",
    "    cbar.update_ticks()\n",
    "\n",
    "    # Add legend for black triangles\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='^', color='w', label='AUROC = 0.5',\n",
    "                                  markerfacecolor='black', markersize=10)]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.9)  # Make room for the colorbar\n",
    "\n",
    "    # Save the plot\n",
    "    directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "    try:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "        return\n",
    "\n",
    "    file_name = f'CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\" if is_single_unit == 0.0 else \"all\"}_units_stim_prob_auroc_color_size_jittered_plasma_triangle'\n",
    "    if cell_type:\n",
    "        file_name += f'_{cell_type}'\n",
    "    if stim_responsivity is not None:\n",
    "        file_name += f'_stim_{stim_responsivity}'\n",
    "\n",
    "    file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "\n",
    "    try:\n",
    "        plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {file_path}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "     \n",
    "plot_modulation_index_with_stim_prob_color_and_size_groupcomparison(\n",
    "    whisker_df_manager, \n",
    "    'basic_metrics', \n",
    "    is_single_unit=1.0, \n",
    "    cell_type='FS', \n",
    "    stim_responsivity=None, \n",
    "    size_multiplier=2, \n",
    "    jitter=0.5, \n",
    "    min_prob_threshold=0.5, \n",
    "    min_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_modulation_index_with_stim_prob_color_and_size_groupcomparison(data_obj, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_prob_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Plot the modulation index as a function of electrode location for CTZ and No_CTZ groups side by side.\n",
    "    StimProb is represented by both color and size of circles, with customizable size controls.\n",
    "    Circles have black outlines and are slightly offset horizontally to reduce overlap. \n",
    "    Horizontal lines connect each point to x=0.\n",
    "\n",
    "    Parameters:\n",
    "    data_obj (object): Object containing the data and methods for data preparation.\n",
    "    df_name (str): Name of the DataFrame to filter and plot.\n",
    "    is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                    if None, do not filter by this criterion.\n",
    "    cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "    stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                    If None, no filtering by StimResponsivity.\n",
    "    jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "    size_multiplier (float): Multiplier for circle sizes. Default is 1.\n",
    "    min_size (float): Minimum size for circles with StimProb below the threshold. Default is 20.\n",
    "    min_prob_threshold (float): StimProb threshold below which circles will have the minimum size. Default is 0.001.\n",
    "    \"\"\"\n",
    "    groups = ['CTZ', 'No_CTZ']\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 8), sharey=True)\n",
    "\n",
    "    all_data = []\n",
    "    for group in groups:\n",
    "        data = data_obj.prepare_plotting_data(df_name, group, is_single_unit, cell_type, stim_responsivity)\n",
    "        if data is not None and not data.empty:\n",
    "            all_data.append(data)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data available for plotting.\")\n",
    "        return\n",
    "\n",
    "    # Combine all data for global color scaling\n",
    "    combined_data = pd.concat(all_data)\n",
    "\n",
    "    # Convert StimProb to scalar values\n",
    "    combined_data['StimProb'] = combined_data['StimProb'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "    combined_data['StimProb'] = pd.to_numeric(combined_data['StimProb'], errors='coerce')\n",
    "\n",
    "    max_prob = combined_data['StimProb'].max()\n",
    "\n",
    "    # Define size calculation function\n",
    "    def calculate_size(prob):\n",
    "        if prob <= min_prob_threshold:\n",
    "            return min_size\n",
    "        else:\n",
    "            return min_size + (prob - min_prob_threshold) * size_multiplier\n",
    "\n",
    "    for ax, group in zip(axes, groups):\n",
    "        plotting_data = combined_data[combined_data['groupname'] == group]\n",
    "\n",
    "        if plotting_data.empty:\n",
    "            ax.text(0.5, 0.5, f'No data for {group}', ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "\n",
    "        # Add horizontal jitter\n",
    "        plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "        # Calculate sizes\n",
    "        plotting_data['sizes'] = plotting_data['StimProb'].apply(calculate_size)\n",
    "\n",
    "        # Plot horizontal lines from x=0 to each point\n",
    "        for _, row in plotting_data.iterrows():\n",
    "            ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                    color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "        # Plot the modulation index points\n",
    "        scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                            c=plotting_data['StimProb'], cmap='hot', \n",
    "                            s=plotting_data['sizes'], alpha=0.8, vmin=0, vmax=max_prob, \n",
    "                            edgecolors='black', linewidths=0.5)\n",
    "\n",
    "        # Add a vertical line at x=0\n",
    "        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax.set_xlabel('Spontaneous MI')\n",
    "        ax.set_title(f'{group}\\n{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units')\n",
    "        ax.invert_yaxis()  # To match the order from the provided list\n",
    "        ax.set_xlim(-1.0, 1.0)\n",
    "        ax.grid(False)\n",
    "        ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "    # Add a common y-label\n",
    "    fig.text(0.04, 0.5, 'Electrode Order', va='center', rotation='vertical')\n",
    "\n",
    "    # Add colorbar for StimProb\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "    cbar.set_label('StimProb')\n",
    "    cbar.set_ticks([0, max_prob])\n",
    "    cbar.set_ticklabels(['0', f'{max_prob:.2f}'])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.9)  # Make room for the colorbar\n",
    "\n",
    "    # Save the plot\n",
    "    directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "    try:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error creating directory: {e}\")\n",
    "        return\n",
    "\n",
    "    file_name = f'CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_stim_prob_color_size_jittered_hot'\n",
    "    if cell_type:\n",
    "        file_name += f'_{cell_type}'\n",
    "    if stim_responsivity is not None:\n",
    "        file_name += f'_stim_{stim_responsivity}'\n",
    "\n",
    "    file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "\n",
    "    try:\n",
    "        plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {file_path}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "        \n",
    "\n",
    "plot_modulation_index_with_stim_prob_color_and_size_groupcomparison(whisker_df_manager, 'basic_metrics', is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_prob_threshold=0.001)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size_groupcomparison('basic_metrics', \n",
    "                                                                                         is_single_unit=0.0,\n",
    "                                                                                         cell_type=None, \n",
    "                                                                                         stim_responsivity=None, \n",
    "                                                                                         jitter=0.5, \n",
    "                                                                                         size_multiplier=50, \n",
    "                                                                                         min_size=5, \n",
    "                                                                                         min_fr_threshold=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size_groupcomparison('basic_metrics', \n",
    "                                                                                         is_single_unit=None,\n",
    "                                                                                         cell_type='RS', \n",
    "                                                                                         stim_responsivity=None, \n",
    "                                                                                         jitter=0.5, \n",
    "                                                                                         size_multiplier=100, \n",
    "                                                                                         min_size=5, \n",
    "                                                                                         min_fr_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size_groupcomparison('basic_metrics', \n",
    "                                                                                         is_single_unit=0.0,\n",
    "                                                                                         cell_type=None, \n",
    "                                                                                         stim_responsivity=None, \n",
    "                                                                                         jitter=0.5, \n",
    "                                                                                         size_multiplier=30, \n",
    "                                                                                         min_size=2, \n",
    "                                                                                         min_fr_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_modulation_index_with_firing_rate_color_and_size_groupcomparison_evoked('basic_metrics', \n",
    "                                                                                         is_single_unit=0.0,\n",
    "                                                                                         stim_index=3,  # Use max stimulation (index 4) \n",
    "                                                                                         cell_type=None, \n",
    "                                                                                         stim_responsivity=None, \n",
    "                                                                                         jitter=0.5, \n",
    "                                                                                         size_multiplier=5, \n",
    "                                                                                         min_size=2, \n",
    "                                                                                         min_fr_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "\n",
    "def plot_modulation_index_comparison(obj, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, save_dir='/Volumes/MannySSD/figures/laminaranalysis'):\n",
    "    \"\"\"\n",
    "    Plot the modulation index comparison between two groups as a function of electrode location using the object's methods.\n",
    "    Ensures the y-axis always represents 32 electrodes in the specified order and saves the plot dynamically based on input parameters.\n",
    "\n",
    "    Parameters:\n",
    "    obj (object): The object containing the prepare_plotting_data method.\n",
    "    df_name (str): Name of the DataFrame to filter and plot.\n",
    "    is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                    if None, do not filter by this criterion.\n",
    "    cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "    stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                       If None, no filtering by Stim Responsivity.\n",
    "    save_dir (str): Directory where the plot will be saved. Defaults to './plots'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the groups\n",
    "    group1 = 'CTZ'\n",
    "    group2 = 'No_CTZ'\n",
    "    \n",
    "    # Define the custom electrode order\n",
    "    electrodes_order = [14, 20, 16, 18, 1, 31, 3, 29, 5, 27, 7, 25, 9, 23, 11, 21, 13, 19, 15, 17, 12, 22, 10, 24, 8, 26, 6, 28, 4, 30, 2, 32]\n",
    "    \n",
    "    # Prepare the data for each group using the object's method\n",
    "    plotting_data1 = obj.prepare_plotting_data(df_name, group1, is_single_unit, cell_type, stim_responsivity)\n",
    "    plotting_data2 = obj.prepare_plotting_data(df_name, group2, is_single_unit, cell_type, stim_responsivity)\n",
    "\n",
    "    # Map the electrode order to the correct positions\n",
    "    electrode_mapping = {electrode: idx for idx, electrode in enumerate(electrodes_order)}\n",
    "\n",
    "    # Adjust the ElectrodeOrder column to reflect the custom order\n",
    "    plotting_data1['ElectrodeOrder'] = plotting_data1['ElectrodeOrder'].map(electrode_mapping)\n",
    "    plotting_data2['ElectrodeOrder'] = plotting_data2['ElectrodeOrder'].map(electrode_mapping)\n",
    "\n",
    "    # Create a 1x2 subplot with narrower plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(4, 6))\n",
    "\n",
    "    for ax, (plotting_data, group) in zip(axes, [(plotting_data1, group1), (plotting_data2, group2)]):\n",
    "        if plotting_data is None or plotting_data.empty:\n",
    "            ax.set_title(f'No data available for {group}')\n",
    "            continue\n",
    "\n",
    "        # Drop rows with NaN in relevant columns\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Convert 'MeanFR_baseline' from array-like to scalar values\n",
    "        plotting_data['MeanFR_baseline'] = plotting_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "        # Convert to numeric and ensure correct length\n",
    "        plotting_data['MeanFR_baseline'] = pd.to_numeric(plotting_data['MeanFR_baseline'], errors='coerce')\n",
    "        plotting_data['ModulationIndex_Numeric'] = pd.to_numeric(plotting_data['ModulationIndex_Numeric'], errors='coerce')\n",
    "        plotting_data['ElectrodeOrder'] = pd.to_numeric(plotting_data['ElectrodeOrder'], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of these columns have NaNs after conversion\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Ensure all columns have the same length\n",
    "        if not (len(plotting_data['MeanFR_baseline']) == len(plotting_data['ModulationIndex_Numeric']) == len(plotting_data['ElectrodeOrder'])):\n",
    "            print(f\"Mismatch in data sizes for group {group}. Skipping plot.\")\n",
    "            continue\n",
    "\n",
    "        # Normalize size values if necessary\n",
    "        size_values = plotting_data['MeanFR_baseline'].values\n",
    "        size_values = size_values / np.max(size_values) * 200  # Normalize to range suitable for plotting, to calculate back to the \n",
    "        \n",
    "        # Plot the modulation index\n",
    "        scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['ElectrodeOrder'], \n",
    "                             s=size_values,  # Use normalized sizes\n",
    "                             c=plotting_data['ModulationIndex_Numeric'], \n",
    "                             cmap='bwr', alpha=0.6)\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f'{group} ({cell_type} units)')\n",
    "        ax.set_xlabel('Spontaneous MI')\n",
    "        ax.set_ylabel('Electrode Order')\n",
    "        ax.invert_yaxis()  # To match the order from the provided list\n",
    "\n",
    "        # Ensure the y-axis has all 32 electrodes listed \n",
    "        ax.set_yticks(np.arange(0, 32, 1))  # Set y-ticks to represent all 32 electrodes\n",
    "        ax.set_ylim(31, 0)  # Set y-limits to cover all 32 electrodes\n",
    "        ax.set_xlim(-1.1, 1.1)  # Enforce the color bar is always from -1 to 1\n",
    "\n",
    "        # Enforce the color bar is always from -1 to 1 \n",
    "        scatter.set_clim(-1, 1)\n",
    "\n",
    "        # Add a light grey dotted vertical line at x = -0.3\n",
    "        ax.axvline(x=-0.3, color='lightgrey', linestyle='--', linewidth=1)\n",
    "        ax.axvline(x=0.3, color='lightgrey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Add a shared colorbar\n",
    "    cbar = fig.colorbar(scatter, ax=axes, orientation='vertical', label='Modulation Index')\n",
    "\n",
    "    # Create a custom legend for circle sizes using the proxy artist approach\n",
    "    legend_labels = [50, 100, 200]  # Example sizes to show in the legend\n",
    "    handles = [plt.scatter([], [], s=size, color='gray', alpha=0.6, label=f'{size:.2f}') for size in legend_labels]\n",
    "    plt.legend(handles=handles, title=\"MeanFR Baseline\", loc='upper right', frameon=True)\n",
    "\n",
    "    # Adjust layout to avoid overlap and make the plots more narrow\n",
    "    plt.subplots_adjust(wspace=0.3)  # Increase the space between the subplots\n",
    "    fig.tight_layout(pad=2)  # Adjust the overall padding around the figure\n",
    "\n",
    "    # Create a dynamic file name based on the input parameters\n",
    "    file_name = f\"{df_name}_{cell_type}_SU{is_single_unit}_SR{stim_responsivity}.svg\"\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(file_path, format='svg', transparent=True)\n",
    "    print(f\"Plot saved to {file_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#whisker_df_manager.plot_modulation_index('basic_metrics', is_single_unit=None, cell_type='RS', stim_responsivity=None)\n",
    "#whisker_df_manager.plot_modulation_index('basic_metrics', is_single_unit=None, cell_type='FS', stim_responsivity=None)\n",
    "#whisker_df_manager.plot_modulation_index('basic_metrics', is_single_unit=None, cell_type=None, stim_responsivity=None)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming whisker_df_manager is your object with prepare_plotting_data defined\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=1.0, cell_type='RS', stim_responsivity=1.0)\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=1.0, cell_type='FS', stim_responsivity=1.0)\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=1.0, cell_type='FS', stim_responsivity=None)\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=None, cell_type='FS', stim_responsivity=None)\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=None, cell_type='RS', stim_responsivity=None)\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=0.0, cell_type='FS', stim_responsivity=None)\n",
    "plot_modulation_index_comparison(whisker_df_manager, df_name='basic_metrics', is_single_unit=0.0, cell_type='RS', stim_responsivity=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import (\n",
    "    ks_2samp,\n",
    "    mannwhitneyu,\n",
    "    cramervonmises_2samp,\n",
    "    sem,\n",
    "    t\n",
    ")\n",
    "import os\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for a data series.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "def plot_modulation_index_comparison(\n",
    "    obj,\n",
    "    df_name,\n",
    "    is_single_unit=None,\n",
    "    cell_type=None,\n",
    "    stim_responsivity=None,\n",
    "    save_dir='/Volumes/MannySSD/figures/laminaranalysis'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot and analyze the modulation index comparison between two groups as a function of electrode location.\n",
    "    Performs statistical tests to determine if distributions differ significantly.\n",
    "    \n",
    "    Parameters:\n",
    "    obj (object): The object containing the prepare_plotting_data method.\n",
    "    df_name (str): Name of the DataFrame to filter and plot.\n",
    "    is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                    if None, do not filter by this criterion.\n",
    "    cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "    stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                       If None, no filtering by Stim Responsivity.\n",
    "    save_dir (str): Directory where the plot and results will be saved.\n",
    "    \"\"\"\n",
    "    # Define the groups\n",
    "    group1 = 'CTZ'\n",
    "    group2 = 'No_CTZ'\n",
    "    \n",
    "    # Prepare the data for each group using the object's method\n",
    "    data1 = obj.prepare_plotting_data(df_name, group1, is_single_unit, cell_type, stim_responsivity)\n",
    "    data2 = obj.prepare_plotting_data(df_name, group2, is_single_unit, cell_type, stim_responsivity)\n",
    "    \n",
    "    # Check if data is available\n",
    "    if data1 is None or data1.empty:\n",
    "        print(f\"No data available for group {group1} with the specified filters.\")\n",
    "        return\n",
    "    if data2 is None or data2.empty:\n",
    "        print(f\"No data available for group {group2} with the specified filters.\")\n",
    "        return\n",
    "    \n",
    "    # Check for the presence of ElectrodeOrder instead of Template_Channel\n",
    "    if 'ElectrodeOrder' not in data1.columns or 'ElectrodeOrder' not in data2.columns:\n",
    "        print(\"The column 'ElectrodeOrder' is not present in one of the datasets.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure necessary columns are present and drop NaNs\n",
    "    required_columns = ['ModulationIndex_Numeric', 'ElectrodeOrder', 'MeanFR_baseline']\n",
    "    data1 = data1.dropna(subset=required_columns)\n",
    "    data2 = data2.dropna(subset=required_columns)\n",
    "    \n",
    "    # Convert columns to appropriate types\n",
    "    for col in required_columns:\n",
    "        data1[col] = pd.to_numeric(data1[col], errors='coerce')\n",
    "        data2[col] = pd.to_numeric(data2[col], errors='coerce')\n",
    "    \n",
    "    # Descriptive Statistics\n",
    "    def get_descriptive_stats(data, group_name):\n",
    "        positions = data['ElectrodeOrder']\n",
    "        modulation_indices = data['ModulationIndex_Numeric']\n",
    "        mean_pos = positions.mean()\n",
    "        median_pos = positions.median()\n",
    "        std_pos = positions.std()\n",
    "        range_pos = positions.max() - positions.min()\n",
    "        ci_low, ci_high = calculate_confidence_interval(positions)\n",
    "        n_units = len(positions)\n",
    "        \n",
    "        stats = {\n",
    "            'Group': group_name,\n",
    "            'N_Units': n_units,\n",
    "            'Mean_Position': mean_pos,\n",
    "            'Median_Position': median_pos,\n",
    "            'Std_Position': std_pos,\n",
    "            'Range_Position': range_pos,\n",
    "            'CI_Low': ci_low,\n",
    "            'CI_High': ci_high\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    stats1 = get_descriptive_stats(data1, group1)\n",
    "    stats2 = get_descriptive_stats(data2, group2)\n",
    "    \n",
    "    descriptive_stats_df = pd.DataFrame([stats1, stats2])\n",
    "    \n",
    "    # Statistical Tests\n",
    "    # Kolmogorov-Smirnov Test\n",
    "    ks_stat, ks_p = ks_2samp(data1['ElectrodeOrder'], data2['ElectrodeOrder'])\n",
    "    \n",
    "    # Mann-Whitney U Test\n",
    "    mw_stat, mw_p = mannwhitneyu(data1['ElectrodeOrder'], data2['ElectrodeOrder'], alternative='two-sided')\n",
    "    \n",
    "    # Cramér–von Mises Test\n",
    "    cvm_result = cramervonmises_2samp(data1['ElectrodeOrder'], data2['ElectrodeOrder'])\n",
    "    \n",
    "    statistical_tests_results = {\n",
    "        'Kolmogorov-Smirnov': {'Statistic': ks_stat, 'p-value': ks_p},\n",
    "        'Mann-Whitney U': {'Statistic': mw_stat, 'p-value': mw_p},\n",
    "        'Cramér–von Mises': {'Statistic': cvm_result.statistic, 'p-value': cvm_result.pvalue}\n",
    "    }\n",
    "    \n",
    "    statistical_results_df = pd.DataFrame(statistical_tests_results).T\n",
    "    \n",
    "    # Visualization\n",
    "    sns.set(style='whitegrid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Histogram Plot\n",
    "    bins = np.arange(min(data1['ElectrodeOrder'].min(), data2['ElectrodeOrder'].min()),\n",
    "                     max(data1['ElectrodeOrder'].max(), data2['ElectrodeOrder'].max()) + 1)\n",
    "    \n",
    "    plt.hist(\n",
    "        data1['ElectrodeOrder'],\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=f'{group1} (N={stats1[\"N_Units\"]})',\n",
    "        color='skyblue',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    plt.hist(\n",
    "        data2['ElectrodeOrder'],\n",
    "        bins=bins,\n",
    "        alpha=0.5,\n",
    "        label=f'{group2} (N={stats2[\"N_Units\"]})',\n",
    "        color='salmon',\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Electrode Order')\n",
    "    plt.ylabel('Unit Count')\n",
    "    plt.title(f'Distribution of Units Across Probe\\nCell Type: {cell_type} | Single Unit: {is_single_unit} | Stim Responsivity: {stim_responsivity}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Annotate with Statistical Test Results\n",
    "    annotation_text = (\n",
    "        f\"Kolmogorov-Smirnov Test:\\n\"\n",
    "        f\"  Statistic = {ks_stat:.3f}\\n\"\n",
    "        f\"  p-value = {ks_p:.3e}\\n\"\n",
    "        f\"Mann-Whitney U Test:\\n\"\n",
    "        f\"  Statistic = {mw_stat:.3f}\\n\"\n",
    "        f\"  p-value = {mw_p:.3e}\\n\"\n",
    "        f\"Cramér–von Mises Test:\\n\"\n",
    "        f\"  Statistic = {cvm_result.statistic:.3f}\\n\"\n",
    "        f\"  p-value = {cvm_result.pvalue:.3e}\"\n",
    "    )\n",
    "    \n",
    "    plt.gcf().text(0.75, 0.5, annotation_text, bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.5))\n",
    "    \n",
    "    # Save Plot\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plot_filename = f\"Unit_Distribution_{df_name}_CellType_{cell_type}_SU_{is_single_unit}_SR_{stim_responsivity}.png\"\n",
    "    plot_path = os.path.join(save_dir, plot_filename)\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Plot saved at {plot_path}\")\n",
    "    \n",
    "    # Save Descriptive Statistics\n",
    "    desc_stats_filename = f\"Descriptive_Stats_{df_name}_CellType_{cell_type}_SU_{is_single_unit}_SR_{stim_responsivity}.csv\"\n",
    "    desc_stats_path = os.path.join(save_dir, desc_stats_filename)\n",
    "    descriptive_stats_df.to_csv(desc_stats_path, index=False)\n",
    "    \n",
    "    # Save Statistical Test Results\n",
    "    stats_results_filename = f\"Statistical_Results_{df_name}_CellType_{cell_type}_SU_{is_single_unit}_SR_{stim_responsivity}.csv\"\n",
    "    stats_results_path = os.path.join(save_dir, stats_results_filename)\n",
    "    statistical_results_df.to_csv(stats_results_path)\n",
    "    \n",
    "    print(f\"Descriptive statistics saved at {desc_stats_path}\")\n",
    "    print(f\"Statistical test results saved at {stats_results_path}\")\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nDescriptive Statistics:\")\n",
    "    print(descriptive_stats_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nStatistical Test Results:\")\n",
    "    print(statistical_results_df.to_string())\n",
    "    \n",
    "    # Example function call\n",
    "\n",
    "plot_modulation_index_comparison(\n",
    "    whisker_df_manager,  \n",
    "    df_name='basic_metrics',  # DataFrame name\n",
    "    is_single_unit=None,  # Set filter for single units\n",
    "    cell_type=None,  # Set filter for Regular-Spiking (RS) cells\n",
    "    stim_responsivity=0.0,  # Filter for responsive cells\n",
    "    save_dir='/Volumes/MannySSD/figures/laminaranalysis'  # Directory to save plots and results\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do not re-run unless you need to re process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = whisker_df_manager.dataframes['basic_metrics']\n",
    "#exposrt and save the dataframe but onluy for the columns that are needed for the plotting: groupname, recordingname, cid, Cell_Type, IsSingleUnit, StimResponsivity, ModulationIndex, ModulationIndex_Numeric, save the dataframe as a csv file in my directory/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv\n",
    "df[['groupname', 'recordingname', 'cid', 'Cell_Type', 'IsSingleUnit', 'StimResponsivity', 'ModulationIndex', 'ModulationIndex_Numeric', 'MeanFR_baseline', 'SpikeTimes_all']].to_csv('', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the AUROC data by group and within groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem, t, ranksums\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    \"\"\"Calculate statistics for the given data.\"\"\"\n",
    "    N = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sd = np.std(data)\n",
    "    median = np.median(data)\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    # Calculate the 95% confidence interval\n",
    "    ci_low, ci_high = calculate_confidence_interval(data)\n",
    "    \n",
    "    return {\n",
    "        'N': N,\n",
    "        'Mean': mean,\n",
    "        'SD': sd,\n",
    "        'Median': median,\n",
    "        'CI_Low': ci_low,\n",
    "        'CI_High': ci_high,\n",
    "        'Min': min_val,\n",
    "        'Max': max_val\n",
    "    }\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate the confidence interval for a data series.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "def compare_auroc_stats(obj, df_name, groupnames, is_single_unit=None, cell_type=None):\n",
    "    \"\"\"\n",
    "    Compare the AUROC values between responsive and non-responsive neurons within and between groups.\n",
    "    \n",
    "    Parameters:\n",
    "    obj (object): The object containing the data and methods.\n",
    "    df_name (str): The name of the DataFrame to use.\n",
    "    groupnames (list of str): List of groups to analyze (e.g., ['CTZ', 'No_CTZ']).\n",
    "    is_single_unit (float or None): Filter by single-unit activity (1.0 for SUA, 0.0 for MUA, None for both).\n",
    "    cell_type (str or None): Filter by cell type ('FS' for Fast-Spiking, 'RS' for Regular-Spiking, None for both).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the calculated statistics and comparison results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for groupname in groupnames:\n",
    "        # Retrieve data for the specified group with optional filters\n",
    "        df = obj.get_filtered_data(df_name, groupname=groupname, is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        \n",
    "        # Pool StimResponsivity = -1.0 and 1.0 as responsive\n",
    "        responsive_df = df[(df['StimResponsivity'] == -1.0) | (df['StimResponsivity'] == 1.0)]\n",
    "        non_responsive_df = df[df['StimResponsivity'] == 0.0]\n",
    "        \n",
    "        # Extract StimProb values\n",
    "        responsive_stimprob = responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        non_responsive_stimprob = non_responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        \n",
    "        # Calculate statistics for each group\n",
    "        responsive_stats = calculate_statistics(responsive_stimprob)\n",
    "        non_responsive_stats = calculate_statistics(non_responsive_stimprob)\n",
    "        \n",
    "        # Perform statistical comparison within the group\n",
    "        if len(responsive_stimprob) > 0 and len(non_responsive_stimprob) > 0:\n",
    "            test_stat, p_value = ranksums(responsive_stimprob, non_responsive_stimprob)\n",
    "        else:\n",
    "            test_stat, p_value = np.nan, np.nan  # Handle cases where one or both groups have no data\n",
    "        \n",
    "        # Store the results in the list\n",
    "        results.append({\n",
    "            'Group': groupname,\n",
    "            'Comparison': 'Responsive vs Non-Responsive',\n",
    "            'N_Responsive': responsive_stats['N'],\n",
    "            'N_Non_Responsive': non_responsive_stats['N'],\n",
    "            'Mean_Responsive': responsive_stats['Mean'],\n",
    "            'Mean_Non_Responsive': non_responsive_stats['Mean'],\n",
    "            'Median_Responsive': responsive_stats['Median'],\n",
    "            'Median_Non_Responsive': non_responsive_stats['Median'],\n",
    "            'SD_Responsive': responsive_stats['SD'],\n",
    "            'SD_Non_Responsive': non_responsive_stats['SD'],\n",
    "            'Min_Responsive': responsive_stats['Min'],\n",
    "            'Max_Responsive': responsive_stats['Max'],\n",
    "            'Min_Non_Responsive': non_responsive_stats['Min'],\n",
    "            'Max_Non_Responsive': non_responsive_stats['Max'],\n",
    "            'CI_Low_Responsive': responsive_stats['CI_Low'],\n",
    "            'CI_High_Responsive': responsive_stats['CI_High'],\n",
    "            'CI_Low_Non_Responsive': non_responsive_stats['CI_Low'],\n",
    "            'CI_High_Non_Responsive': non_responsive_stats['CI_High'],\n",
    "            'Test_Statistic': test_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    # Between-group comparison for responsive neurons only\n",
    "    if len(groupnames) == 2:\n",
    "        group1_df = obj.get_filtered_data(df_name, groupname=groupnames[0], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        group2_df = obj.get_filtered_data(df_name, groupname=groupnames[1], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        \n",
    "        group1_responsive = group1_df[(group1_df['StimResponsivity'] == -1.0) | (group1_df['StimResponsivity'] == 1.0)]['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        group2_responsive = group2_df[(group2_df['StimResponsivity'] == -1.0) | (group2_df['StimResponsivity'] == 1.0)]['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        \n",
    "        if len(group1_responsive) > 0 and len(group2_responsive) > 0:\n",
    "            test_stat, p_value = ranksums(group1_responsive, group2_responsive)\n",
    "        else:\n",
    "            test_stat, p_value = np.nan, np.nan  # Handle cases where one or both groups have no data\n",
    "        \n",
    "        # Calculate statistics for each responsive group\n",
    "        group1_stats = calculate_statistics(group1_responsive)\n",
    "        group2_stats = calculate_statistics(group2_responsive)\n",
    "        \n",
    "        results.append({\n",
    "            'Group': f'{groupnames[0]} vs {groupnames[1]}',\n",
    "            'Comparison': 'Responsive Neurons Between Groups',\n",
    "            'N_Responsive_Group1': group1_stats['N'],\n",
    "            'N_Responsive_Group2': group2_stats['N'],\n",
    "            'Mean_Responsive_Group1': group1_stats['Mean'],\n",
    "            'Mean_Responsive_Group2': group2_stats['Mean'],\n",
    "            'Median_Responsive_Group1': group1_stats['Median'],\n",
    "            'Median_Responsive_Group2': group2_stats['Median'],\n",
    "            'SD_Responsive_Group1': group1_stats['SD'],\n",
    "            'SD_Responsive_Group2': group2_stats['SD'],\n",
    "            'Min_Responsive_Group1': group1_stats['Min'],\n",
    "            'Max_Responsive_Group1': group1_stats['Max'],\n",
    "            'Min_Responsive_Group2': group2_stats['Min'],\n",
    "            'Max_Responsive_Group2': group2_stats['Max'],\n",
    "            'CI_Low_Responsive_Group1': group1_stats['CI_Low'],\n",
    "            'CI_High_Responsive_Group1': group1_stats['CI_High'],\n",
    "            'CI_Low_Responsive_Group2': group2_stats['CI_Low'],\n",
    "            'CI_High_Responsive_Group2': group2_stats['CI_High'],\n",
    "            'Test_Statistic': test_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Perform the analysis for FS and RS neurons separately\n",
    "cell_types = ['FS', 'RS']\n",
    "results_dfs = []\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    # Run the comparison analysis for each cell type\n",
    "    results_df = compare_auroc_stats(\n",
    "        whisker_df_manager,\n",
    "        df_name='basic_metrics',\n",
    "        groupnames=['CTZ', 'No_CTZ'],\n",
    "        is_single_unit=None,  # No filter on single units for this analysis\n",
    "        cell_type=cell_type  # Filter by cell type (FS or RS)\n",
    "    )\n",
    "    # Add a column for cell type in the results\n",
    "    results_df['Cell_Type'] = cell_type\n",
    "    results_dfs.append(results_df)\n",
    "\n",
    "# Combine the results into a single DataFrame\n",
    "combined_results_df = pd.concat(results_dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined results\n",
    "combined_results_df.head(), combined_results_df.describe()\n",
    "\n",
    "# Extract the p-values from the combined results DataFrame\n",
    "p_values = combined_results_df['p_value']\n",
    "\n",
    "# Check the p-values to confirm significance\n",
    "p_values_significant = p_values < 0.05\n",
    "\n",
    "# Display p-values along with their significance\n",
    "p_values, p_values_significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin the PSTH shit show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the mean PSTH for all recording easily to visualize in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctz_df1, noctz_df2 = whisker_df_manager.compare_groups('CTZ', 'No_CTZ', 'Max', 'RS', is_single_unit=1.0, stim_responsivity=None)\n",
    "\n",
    "def process_data(df, time_range):\n",
    "    \n",
    "    time_array = whisker_df_manager.eed.relative_time_ms['relative_time_ms']\n",
    "    \n",
    "    if time_range:\n",
    "        time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "        time_array = time_array[time_mask]\n",
    "    else:\n",
    "        time_mask = slice(None)\n",
    "    \n",
    "    grouped = df.groupby('recordingname')\n",
    "    grouped_data = grouped['PSTHs_raw'].apply(lambda x: np.mean([np.array(i)[time_mask] for i in x], axis=0))\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "ctz_df1_grouped = process_data(ctz_df1, time_range=(0, 50))\n",
    "noctz_df2_grouped = process_data(noctz_df2, time_range=(0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Create a color map\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(ctz_df1_grouped)))\n",
    "\n",
    "# Create a 1xN subplot\n",
    "fig, axs = plt.subplots(1, len(ctz_df1_grouped), figsize=(15, 5))\n",
    "\n",
    "# Iterate over the grouped data\n",
    "for ax, data, color, name in zip(axs, ctz_df1_grouped, colors, ctz_df1_grouped.index):\n",
    "    ax.plot(data, color=color)\n",
    "    ax.set_title(name)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color map\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(noctz_df2_grouped)))\n",
    "\n",
    "# Create a 1xN subplot\n",
    "fig, axs = plt.subplots(1, len(noctz_df2_grouped), figsize=(15, 5))\n",
    "\n",
    "# Iterate over the grouped data\n",
    "for ax, data, color, name in zip(axs, noctz_df2_grouped, colors, noctz_df2_grouped.index):\n",
    "    ax.plot(data, color=color)\n",
    "    ax.set_title(name)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the AUROC distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the distriution for ALL singlue units (FS and RS combined) for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.inspect_raw_data('basic_metrics', is_single_unit=1.0, cell_type=None, groupname=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the distibution for ALL FS and RS single units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.inspect_categorized_data('basic_metrics', is_single_unit=1.0, cell_type=None, groupname=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.check_stim_responsivity_distribution_before_after('basic_metrics', is_single_unit=1.0, cell_type=None, groupname=None)\n",
    "whisker_df_manager.plot_raw_auroc_distribution('basic_metrics', is_single_unit=1.0, cell_type=None, groupname=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem, t, ranksums\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    \"\"\"Calculate statistics for the given data.\"\"\"\n",
    "    N = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sd = np.std(data)\n",
    "    median = np.median(data)\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    # Calculate the 95% confidence interval\n",
    "    ci_low, ci_high = calculate_confidence_interval(data)\n",
    "    \n",
    "    return {\n",
    "        'N': N,\n",
    "        'Mean': mean,\n",
    "        'SD': sd,\n",
    "        'Median': median,\n",
    "        'CI_Low': ci_low,\n",
    "        'CI_High': ci_high,\n",
    "        'Min': min_val,\n",
    "        'Max': max_val\n",
    "    }\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate the confidence interval for a data series.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "def analyze_stimprob_by_responsiveness(obj, df_name, groupname, is_single_unit=None, cell_type=None):\n",
    "    \"\"\"\n",
    "    Analyze and compare the StimProb values for responsive and non-responsive neurons in a specified group.\n",
    "    Allows filtering by IsSingleUnit and Cell_Type.\n",
    "    \n",
    "    Parameters:\n",
    "    obj (object): The object containing the data and methods.\n",
    "    df_name (str): The name of the DataFrame to use.\n",
    "    groupname (str): The treatment group to analyze (e.g., 'CTZ' or 'No_CTZ').\n",
    "    is_single_unit (float or None): Filter by single-unit activity (1.0 for SUA, 0.0 for MUA, None for both).\n",
    "    cell_type (str or None): Filter by cell type ('FS' for Fast-Spiking, 'RS' for Regular-Spiking, None for both).\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary containing the statistics for responsive and non-responsive groups.\n",
    "    \"\"\"\n",
    "    # Retrieve data for the specified group with optional filters\n",
    "    df = obj.get_filtered_data(df_name, groupname=groupname, is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "    \n",
    "    # Pool StimResponsivity = -1.0 and 1.0 as responsive\n",
    "    responsive_df = df[(df['StimResponsivity'] == -1.0) | (df['StimResponsivity'] == 1.0)]\n",
    "    non_responsive_df = df[df['StimResponsivity'] == 0.0]\n",
    "    \n",
    "    # Extract StimProb values\n",
    "    responsive_stimprob = responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "    non_responsive_stimprob = non_responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "    \n",
    "    # Calculate statistics for each group\n",
    "    responsive_stats = calculate_statistics(responsive_stimprob)\n",
    "    non_responsive_stats = calculate_statistics(non_responsive_stimprob)\n",
    "    \n",
    "    # Return the results\n",
    "    return {\n",
    "        'Responsive': responsive_stats,\n",
    "        'Non_Responsive': non_responsive_stats\n",
    "    }\n",
    "\n",
    "# Example usage: Analyze for CTZ group, filtering for SUA (IsSingleUnit=1.0) and FS cells\n",
    "ctz_stats_sua_fs = analyze_stimprob_by_responsiveness(\n",
    "    whisker_df_manager, \n",
    "    df_name='basic_metrics', \n",
    "    groupname='CTZ', \n",
    "    is_single_unit=1.0,  # Filter for single-unit activity (SUA)\n",
    "    cell_type='FS'  # Filter for Fast-Spiking cells (FS)\n",
    ")\n",
    "\n",
    "# Example usage: Analyze for No_CTZ group, without filtering by SUA or cell type\n",
    "no_ctz_stats = analyze_stimprob_by_responsiveness(\n",
    "    whisker_df_manager, \n",
    "    df_name='basic_metrics', \n",
    "    groupname='No_CTZ'\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"CTZ Group Analysis (SUA, FS):\")\n",
    "print(pd.DataFrame(ctz_stats_sua_fs))\n",
    "\n",
    "print(\"\\nNo_CTZ Group Analysis (No Filters):\")\n",
    "print(pd.DataFrame(no_ctz_stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem, t, ranksums\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    \"\"\"Calculate statistics for the given data.\"\"\"\n",
    "    N = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sd = np.std(data)\n",
    "    median = np.median(data)\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    # Calculate the 95% confidence interval\n",
    "    ci_low, ci_high = calculate_confidence_interval(data)\n",
    "    \n",
    "    return {\n",
    "        'N': N,\n",
    "        'Mean': mean,\n",
    "        'SD': sd,\n",
    "        'Median': median,\n",
    "        'CI_Low': ci_low,\n",
    "        'CI_High': ci_high,\n",
    "        'Min': min_val,\n",
    "        'Max': max_val\n",
    "    }\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate the confidence interval for a data series.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "def compare_auroc_stats(obj, df_name, groupnames, is_single_unit=None, cell_type=None):\n",
    "    \"\"\"\n",
    "    Compare the AUROC values between responsive and non-responsive neurons within and between groups.\n",
    "    \n",
    "    Parameters:\n",
    "    obj (object): The object containing the data and methods.\n",
    "    df_name (str): The name of the DataFrame to use.\n",
    "    groupnames (list of str): List of groups to analyze (e.g., ['CTZ', 'No_CTZ']).\n",
    "    is_single_unit (float or None): Filter by single-unit activity (1.0 for SUA, 0.0 for MUA, None for both).\n",
    "    cell_type (str or None): Filter by cell type ('FS' for Fast-Spiking, 'RS' for Regular-Spiking, None for both).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the calculated statistics and comparison results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for groupname in groupnames:\n",
    "        # Retrieve data for the specified group with optional filters\n",
    "        df = obj.get_filtered_data(df_name, groupname=groupname, is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        \n",
    "        # Pool StimResponsivity = -1.0 and 1.0 as responsive\n",
    "        responsive_df = df[(df['StimResponsivity'] == -1.0) | (df['StimResponsivity'] == 1.0)]\n",
    "        non_responsive_df = df[df['StimResponsivity'] == 0.0]\n",
    "        \n",
    "        # Extract StimProb values\n",
    "        responsive_stimprob = responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        non_responsive_stimprob = non_responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        \n",
    "        # Calculate statistics for each group\n",
    "        responsive_stats = calculate_statistics(responsive_stimprob)\n",
    "        non_responsive_stats = calculate_statistics(non_responsive_stimprob)\n",
    "        \n",
    "        # Perform statistical comparison within the group\n",
    "        if len(responsive_stimprob) > 0 and len(non_responsive_stimprob) > 0:\n",
    "            test_stat, p_value = ranksums(responsive_stimprob, non_responsive_stimprob)\n",
    "        else:\n",
    "            test_stat, p_value = np.nan, np.nan  # Handle cases where one or both groups have no data\n",
    "        \n",
    "        # Store the results in the list\n",
    "        results.append({\n",
    "            'Group': groupname,\n",
    "            'Comparison': 'Responsive vs Non-Responsive',\n",
    "            'N_Responsive': responsive_stats['N'],\n",
    "            'N_Non_Responsive': non_responsive_stats['N'],\n",
    "            'Mean_Responsive': responsive_stats['Mean'],\n",
    "            'Mean_Non_Responsive': non_responsive_stats['Mean'],\n",
    "            'Median_Responsive': responsive_stats['Median'],\n",
    "            'Median_Non_Responsive': non_responsive_stats['Median'],\n",
    "            'SD_Responsive': responsive_stats['SD'],\n",
    "            'SD_Non_Responsive': non_responsive_stats['SD'],\n",
    "            'Min_Responsive': responsive_stats['Min'],\n",
    "            'Max_Responsive': responsive_stats['Max'],\n",
    "            'Min_Non_Responsive': non_responsive_stats['Min'],\n",
    "            'Max_Non_Responsive': non_responsive_stats['Max'],\n",
    "            'CI_Low_Responsive': responsive_stats['CI_Low'],\n",
    "            'CI_High_Responsive': responsive_stats['CI_High'],\n",
    "            'CI_Low_Non_Responsive': non_responsive_stats['CI_Low'],\n",
    "            'CI_High_Non_Responsive': non_responsive_stats['CI_High'],\n",
    "            'Test_Statistic': test_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    # Between-group comparison for responsive neurons only\n",
    "    if len(groupnames) == 2:\n",
    "        group1_df = obj.get_filtered_data(df_name, groupname=groupnames[0], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        group2_df = obj.get_filtered_data(df_name, groupname=groupnames[1], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        \n",
    "        group1_responsive = group1_df[(group1_df['StimResponsivity'] == -1.0) | (group1_df['StimResponsivity'] == 1.0)]['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        group2_responsive = group2_df[(group2_df['StimResponsivity'] == -1.0) | (group2_df['StimResponsivity'] == 1.0)]['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        \n",
    "        if len(group1_responsive) > 0 and len(group2_responsive) > 0:\n",
    "            test_stat, p_value = ranksums(group1_responsive, group2_responsive)\n",
    "        else:\n",
    "            test_stat, p_value = np.nan, np.nan  # Handle cases where one or both groups have no data\n",
    "        \n",
    "        # Calculate statistics for each responsive group\n",
    "        group1_stats = calculate_statistics(group1_responsive)\n",
    "        group2_stats = calculate_statistics(group2_responsive)\n",
    "        \n",
    "        results.append({\n",
    "            'Group': f'{groupnames[0]} vs {groupnames[1]}',\n",
    "            'Comparison': 'Responsive Neurons Between Groups',\n",
    "            'N_Responsive_Group1': group1_stats['N'],\n",
    "            'N_Responsive_Group2': group2_stats['N'],\n",
    "            'Mean_Responsive_Group1': group1_stats['Mean'],\n",
    "            'Mean_Responsive_Group2': group2_stats['Mean'],\n",
    "            'Median_Responsive_Group1': group1_stats['Median'],\n",
    "            'Median_Responsive_Group2': group2_stats['Median'],\n",
    "            'SD_Responsive_Group1': group1_stats['SD'],\n",
    "            'SD_Responsive_Group2': group2_stats['SD'],\n",
    "            'Min_Responsive_Group1': group1_stats['Min'],\n",
    "            'Max_Responsive_Group1': group1_stats['Max'],\n",
    "            'Min_Responsive_Group2': group2_stats['Min'],\n",
    "            'Max_Responsive_Group2': group2_stats['Max'],\n",
    "            'CI_Low_Responsive_Group1': group1_stats['CI_Low'],\n",
    "            'CI_High_Responsive_Group1': group1_stats['CI_High'],\n",
    "            'CI_Low_Responsive_Group2': group2_stats['CI_Low'],\n",
    "            'CI_High_Responsive_Group2': group2_stats['CI_High'],\n",
    "            'Test_Statistic': test_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "groupnames = ['CTZ', 'No_CTZ']\n",
    "results_df = compare_auroc_stats(\n",
    "    whisker_df_manager, \n",
    "    df_name='basic_metrics', \n",
    "    groupnames=groupnames, \n",
    "    is_single_unit=1.0,  # Example filter for single-unit activity (SUA)\n",
    "    cell_type='FS'  # Example filter for Fast-Spiking cells (FS)\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import sem, t, ranksums\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    \"\"\"Calculate statistics for the given data.\"\"\"\n",
    "    N = len(data)\n",
    "    mean = np.mean(data)\n",
    "    sd = np.std(data)\n",
    "    median = np.median(data)\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    \n",
    "    # Calculate the 95% confidence interval\n",
    "    ci_low, ci_high = calculate_confidence_interval(data)\n",
    "    \n",
    "    return {\n",
    "        'N': N,\n",
    "        'Mean': mean,\n",
    "        'SD': sd,\n",
    "        'Median': median,\n",
    "        'CI_Low': ci_low,\n",
    "        'CI_High': ci_high,\n",
    "        'Min': min_val,\n",
    "        'Max': max_val\n",
    "    }\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate the confidence interval for a data series.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "def compare_auroc_stats(obj, df_name, groupnames, is_single_unit=None, cell_type=None):\n",
    "    \"\"\"\n",
    "    Compare the AUROC values between responsive and non-responsive neurons within and between groups.\n",
    "    \n",
    "    Parameters:\n",
    "    obj (object): The object containing the data and methods.\n",
    "    df_name (str): The name of the DataFrame to use.\n",
    "    groupnames (list of str): List of groups to analyze (e.g., ['CTZ', 'No_CTZ']).\n",
    "    is_single_unit (float or None): Filter by single-unit activity (1.0 for SUA, 0.0 for MUA, None for both).\n",
    "    cell_type (str or None): Filter by cell type ('FS' for Fast-Spiking, 'RS' for Regular-Spiking, None for both).\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the calculated statistics and comparison results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for groupname in groupnames:\n",
    "        # Retrieve data for the specified group with optional filters\n",
    "        df = obj.get_filtered_data(df_name, groupname=groupname, is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        \n",
    "        # Pool StimResponsivity = -1.0 and 1.0 as responsive\n",
    "        responsive_df = df[(df['StimResponsivity'] == -1.0) | (df['StimResponsivity'] == 1.0)]\n",
    "        non_responsive_df = df[df['StimResponsivity'] == 0.0]\n",
    "        \n",
    "        # Extract StimProb values\n",
    "        responsive_stimprob = responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        non_responsive_stimprob = non_responsive_df['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        \n",
    "        # Calculate statistics for each group\n",
    "        responsive_stats = calculate_statistics(responsive_stimprob)\n",
    "        non_responsive_stats = calculate_statistics(non_responsive_stimprob)\n",
    "        \n",
    "        # Perform statistical comparison within the group\n",
    "        if len(responsive_stimprob) > 0 and len(non_responsive_stimprob) > 0:\n",
    "            test_stat, p_value = ranksums(responsive_stimprob, non_responsive_stimprob)\n",
    "        else:\n",
    "            test_stat, p_value = np.nan, np.nan  # Handle cases where one or both groups have no data\n",
    "        \n",
    "        # Store the results in the list\n",
    "        results.append({\n",
    "            'Group': groupname,\n",
    "            'Comparison': 'Responsive vs Non-Responsive',\n",
    "            'N_Responsive': responsive_stats['N'],\n",
    "            'N_Non_Responsive': non_responsive_stats['N'],\n",
    "            'Mean_Responsive': responsive_stats['Mean'],\n",
    "            'Mean_Non_Responsive': non_responsive_stats['Mean'],\n",
    "            'Median_Responsive': responsive_stats['Median'],\n",
    "            'Median_Non_Responsive': non_responsive_stats['Median'],\n",
    "            'SD_Responsive': responsive_stats['SD'],\n",
    "            'SD_Non_Responsive': non_responsive_stats['SD'],\n",
    "            'Min_Responsive': responsive_stats['Min'],\n",
    "            'Max_Responsive': responsive_stats['Max'],\n",
    "            'Min_Non_Responsive': non_responsive_stats['Min'],\n",
    "            'Max_Non_Responsive': non_responsive_stats['Max'],\n",
    "            'CI_Low_Responsive': responsive_stats['CI_Low'],\n",
    "            'CI_High_Responsive': responsive_stats['CI_High'],\n",
    "            'CI_Low_Non_Responsive': non_responsive_stats['CI_Low'],\n",
    "            'CI_High_Non_Responsive': non_responsive_stats['CI_High'],\n",
    "            'Test_Statistic': test_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    # Between-group comparison for responsive neurons only\n",
    "    if len(groupnames) == 2:\n",
    "        group1_df = obj.get_filtered_data(df_name, groupname=groupnames[0], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        group2_df = obj.get_filtered_data(df_name, groupname=groupnames[1], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=None)\n",
    "        \n",
    "        group1_responsive = group1_df[(group1_df['StimResponsivity'] == -1.0) | (group1_df['StimResponsivity'] == 1.0)]['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        group2_responsive = group2_df[(group2_df['StimResponsivity'] == -1.0) | (group2_df['StimResponsivity'] == 1.0)]['StimProb'].apply(lambda x: x[0]).dropna().values\n",
    "        \n",
    "        if len(group1_responsive) > 0 and len(group2_responsive) > 0:\n",
    "            test_stat, p_value = ranksums(group1_responsive, group2_responsive)\n",
    "        else:\n",
    "            test_stat, p_value = np.nan, np.nan  # Handle cases where one or both groups have no data\n",
    "        \n",
    "        # Calculate statistics for each responsive group\n",
    "        group1_stats = calculate_statistics(group1_responsive)\n",
    "        group2_stats = calculate_statistics(group2_responsive)\n",
    "        \n",
    "        results.append({\n",
    "            'Group': f'{groupnames[0]} vs {groupnames[1]}',\n",
    "            'Comparison': 'Responsive Neurons Between Groups',\n",
    "            'N_Responsive_Group1': group1_stats['N'],\n",
    "            'N_Responsive_Group2': group2_stats['N'],\n",
    "            'Mean_Responsive_Group1': group1_stats['Mean'],\n",
    "            'Mean_Responsive_Group2': group2_stats['Mean'],\n",
    "            'Median_Responsive_Group1': group1_stats['Median'],\n",
    "            'Median_Responsive_Group2': group2_stats['Median'],\n",
    "            'SD_Responsive_Group1': group1_stats['SD'],\n",
    "            'SD_Responsive_Group2': group2_stats['SD'],\n",
    "            'Min_Responsive_Group1': group1_stats['Min'],\n",
    "            'Max_Responsive_Group1': group1_stats['Max'],\n",
    "            'Min_Responsive_Group2': group2_stats['Min'],\n",
    "            'Max_Responsive_Group2': group2_stats['Max'],\n",
    "            'CI_Low_Responsive_Group1': group1_stats['CI_Low'],\n",
    "            'CI_High_Responsive_Group1': group1_stats['CI_High'],\n",
    "            'CI_Low_Responsive_Group2': group2_stats['CI_Low'],\n",
    "            'CI_High_Responsive_Group2': group2_stats['CI_High'],\n",
    "            'Test_Statistic': test_stat,\n",
    "            'p_value': p_value\n",
    "        })\n",
    "    \n",
    "    # Convert the results list into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Perform the analysis for FS and RS neurons separately\n",
    "cell_types = ['FS', 'RS']\n",
    "results_dfs = []\n",
    "\n",
    "for cell_type in cell_types:\n",
    "    # Run the comparison analysis for each cell type\n",
    "    results_df = compare_auroc_stats(\n",
    "        whisker_df_manager,\n",
    "        df_name='basic_metrics',\n",
    "        groupnames=['CTZ', 'No_CTZ'],\n",
    "        is_single_unit=1.0,  # No filter on single units for this analysis\n",
    "        cell_type=cell_type  # Filter by cell type (FS or RS)\n",
    "    )\n",
    "    # Add a column for cell type in the results\n",
    "    results_df['Cell_Type'] = cell_type\n",
    "    results_dfs.append(results_df)\n",
    "\n",
    "# Combine the results into a single DataFrame\n",
    "combined_results_df = pd.concat(results_dfs, ignore_index=True)\n",
    "\n",
    "# Display the combined results\n",
    "combined_results_df.head(), combined_results_df.describe()\n",
    "combined_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_raw_auroc_distribution('basic_metrics', is_single_unit=None, cell_type='RS', groupname='CTZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.check_stim_responsivity_distribution_before_after('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname='CTZ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.inspect_categorized_data('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname='CTZ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname=None, include_no_change=True, xlim_range=(0.48, 0.57), plot_stacked_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the distributio for RS or FS cells per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname='CTZ', include_no_change=False, xlim_range=(0.45, 0.7), plot_stacked_bar=True)\n",
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname='No_CTZ', include_no_change=False, xlim_range=(0.45, 0.7), plot_stacked_bar=True)\n",
    "\n",
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='FS', groupname='CTZ', include_no_change=True, xlim_range=(0.45, 0.7), plot_stacked_bar=True)\n",
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='FS', groupname='No_CTZ', include_no_change=True, xlim_range=(0.45, 0.7), plot_stacked_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='FS', groupname='CTZ')\n",
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='FS', groupname='No_CTZ')\n",
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname='CTZ')\n",
    "whisker_df_manager.plot_stim_prob_distribution('basic_metrics', is_single_unit=1.0, cell_type='RS', groupname='No_CTZ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot PSTHs based on modulation index, per group as mean, traces, or per recordings, using the older PSTHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the modulation index to the dataframe psth_dataframe\n",
    "whisker_df_manager.add_modulation_label_column('psth_dataframe')\n",
    "whisker_df_manager.add_modulation_label_column('psth_dataframe_Zero')\n",
    "whisker_df_manager.add_modulation_label_column('psth_dataframe_Low')\n",
    "whisker_df_manager.add_modulation_label_column('psth_dataframe_Mid')\n",
    "whisker_df_manager.add_modulation_label_column('psth_dataframe_Max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL FIGURES Below is all possible combinations of FS and RS MUA PSTHs -- Final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try to scale up for easier plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_plot_generator(\n",
    "    stimulations, \n",
    "    cell_types=['FS', 'RS'], \n",
    "    is_single_unit_options=[1.0, 0.0, None], \n",
    "    stim_responsivity_options=[1.0, 0.0, None], \n",
    "    groupnames=['CTZ', 'No_CTZ'], \n",
    "    modulation_label='positive', \n",
    "    time_ranges={\n",
    "        'plot_group_comparison_meansem': [-500, 1000], \n",
    "        'plot_all_stimulations_with_stim': [(-15, 50), (-500, 1000)]\n",
    "    }, \n",
    "    smoothing_window=1, \n",
    "    base_directory='/Volumes/MannySSD/figures/thesis_final_figs/PSTHs/'\n",
    "):\n",
    "    # Helper function to translate the options to meaningful folder names\n",
    "    def get_folder_name(option, option_type):\n",
    "        if option_type == 'is_single_unit':\n",
    "            return 'SUA' if option == 1.0 else 'MUA' if option == 0.0 else 'ALL'\n",
    "        elif option_type == 'stim_responsivity':\n",
    "            return 'sensory_responsive' if option == 1.0 else 'non_responsive' if option == 0.0 else 'ALL'\n",
    "    \n",
    "    # Loop through each combination of cell_type, is_single_unit, and stim_responsivity\n",
    "    for cell_type in cell_types:\n",
    "        for is_single_unit in is_single_unit_options:\n",
    "            for stim_responsivity in stim_responsivity_options:\n",
    "                \n",
    "                # Create the folder structure with meaningful names\n",
    "                su_folder = get_folder_name(is_single_unit, 'is_single_unit')\n",
    "                sr_folder = get_folder_name(stim_responsivity, 'stim_responsivity')\n",
    "                \n",
    "                cell_type_folder = os.path.join(base_directory, cell_type)\n",
    "                is_single_unit_folder = os.path.join(cell_type_folder, su_folder)\n",
    "                stim_responsivity_folder = os.path.join(is_single_unit_folder, sr_folder)\n",
    "                os.makedirs(stim_responsivity_folder, exist_ok=True)\n",
    "\n",
    "                # File name base with meaningful names\n",
    "                base_file_name = f\"{cell_type}_{su_folder}_{sr_folder}\"\n",
    "                \n",
    "                # Debug: Display the current filtering parameters\n",
    "                print(f\"Attempting plot with {cell_type}, SU: {su_folder}, SR: {sr_folder}\")\n",
    "                \n",
    "                # Plot group comparison meansem\n",
    "                file_name_meansem = f\"{base_file_name}_meansem\"\n",
    "\n",
    "                try:\n",
    "                    whisker_df_manager.plot_group_comparison_meansem(\n",
    "                        stimulations,  # Passed as a positional argument\n",
    "                        cell_type=cell_type,\n",
    "                        is_single_unit=is_single_unit,\n",
    "                        stim_responsivity=stim_responsivity,\n",
    "                        groupnames=groupnames,\n",
    "                        modulation_label=modulation_label,\n",
    "                        time_range=time_ranges['plot_group_comparison_meansem'],\n",
    "                        smoothing_window=smoothing_window,\n",
    "                        directory=stim_responsivity_folder,\n",
    "                        file_name=file_name_meansem\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error plotting meansem for {cell_type}, SU: {su_folder}, SR: {sr_folder}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                # Plot all stimulations with stim (traces and sem) for both time ranges\n",
    "                for time_range in time_ranges['plot_all_stimulations_with_stim']:\n",
    "                    time_label = f\"{time_range[0]}to{time_range[1]}\"\n",
    "\n",
    "                    file_name_traces = f\"{base_file_name}_traces_{time_label}\"\n",
    "                    try:\n",
    "                        whisker_df_manager.plot_all_stimulations_with_stim(\n",
    "                            groupnames[0],\n",
    "                            groupnames[1],\n",
    "                            cell_type=cell_type,\n",
    "                            is_single_unit=is_single_unit,\n",
    "                            stim_responsivity=stim_responsivity,\n",
    "                            time_range=time_range,\n",
    "                            plot_mode='traces',\n",
    "                            smoothing_window=smoothing_window,\n",
    "                            directory=stim_responsivity_folder,\n",
    "                            file_name=file_name_traces\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting traces for {cell_type}, SU: {su_folder}, SR: {sr_folder}, Time: {time_label}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                    file_name_sem = f\"{base_file_name}_sem_{time_label}\"\n",
    "                    try:\n",
    "                        whisker_df_manager.plot_all_stimulations_with_stim(\n",
    "                            groupnames[0],\n",
    "                            groupnames[1],\n",
    "                            cell_type=cell_type,\n",
    "                            is_single_unit=is_single_unit,\n",
    "                            stim_responsivity=stim_responsivity,\n",
    "                            time_range=time_range,\n",
    "                            plot_mode='sem',\n",
    "                            smoothing_window=smoothing_window,\n",
    "                            directory=stim_responsivity_folder,\n",
    "                            file_name=file_name_sem\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting SEM for {cell_type}, SU: {su_folder}, SR: {sr_folder}, Time: {time_label}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "# Example call\n",
    "dynamic_plot_generator(\n",
    "    stimulations=['Zero', 'Low', 'Mid', 'Max'],\n",
    "    time_ranges={\n",
    "        'plot_group_comparison_meansem': [-500, 1000], \n",
    "        'plot_all_stimulations_with_stim': [(-15, 50), (-500, 1000)]\n",
    "    },\n",
    "    base_directory='/Volumes/MannySSD/figures/thesis_final_figs/PSTHs/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT TEMPORAL SPIKING METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.dataframes['basic_metrics'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Filter the DataFrame based on user inputs\n",
    "filtered_df = whisker_df_manager.dataframes['basic_metrics'][\n",
    "        (whisker_df_manager.dataframes['basic_metrics']['IsSingleUnit'] == 1.0) & \n",
    "        (whisker_df_manager.dataframes['basic_metrics']['Cell_Type'] == 'FS')\n",
    "    ]\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filtered_metrics(dataframes_obj, column_to_plot, stimulations=None, is_single_unit=1.0, cell_type='RS', stim_responsivity=1.0, directory=None, file_name=None, ylim=None):\n",
    "    \"\"\"\n",
    "    Plots the specified metric with detailed filtering based on user inputs.\n",
    "\n",
    "    Args:\n",
    "        dataframes_obj (object): The object containing the dataframes, accessed as dataframes_obj.dataframes['basic_metrics'].\n",
    "        column_to_plot (str): The column name to plot (like 'FirstSpikeLatency' or 'FirstSpikeLatency_Reliability').\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "        is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0).\n",
    "        cell_type (str, optional): Filter for specific cell types ('FS' or 'RS').\n",
    "        stim_responsivity (float, optional): Filter by stimulus responsivity (1.0, 0.0, or -1.0).\n",
    "        modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "        directory (str, optional): Directory to save the plot.\n",
    "        file_name (str, optional): File name to save the plot.\n",
    "        ylim (tuple, optional): Limits for the y-axis.\n",
    "    \"\"\"\n",
    "    # Extract the relevant DataFrame\n",
    "    df = dataframes_obj.dataframes['basic_metrics']\n",
    "\n",
    "    # Filter the DataFrame based on user inputs\n",
    "    filtered_df = df[\n",
    "        (df['IsSingleUnit'] == is_single_unit) & \n",
    "        (df['Cell_Type'] == cell_type) & \n",
    "        (df['StimResponsivity'] == stim_responsivity)\n",
    "    ]\n",
    "    \n",
    "    # If no data is found, print a message and return\n",
    "    if filtered_df.empty:\n",
    "        print(\"No data found for the specified filtering criteria.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize a list to hold data for plotting\n",
    "    plot_data = []\n",
    "\n",
    "    # Loop through each row in the filtered DataFrame\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        for i, stim in enumerate(stimulations):\n",
    "            plot_data.append({\n",
    "                'Group': row['groupname'],\n",
    "                'Stimulation': stim,\n",
    "                'Value': row[column_to_plot][i]  # Extract the value for each stimulation\n",
    "            })\n",
    "\n",
    "    # Convert the list to a DataFrame for easy plotting\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=plot_df, x='Stimulation', y='Value', hue='Group')\n",
    "    sns.stripplot(data=plot_df, x='Stimulation', y='Value', hue='Group', dodge=True, jitter=True, color='black')\n",
    "\n",
    "    # Control the upper and lower limits of the y-axis\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    # Enhance the plot\n",
    "    plt.title(f'Comparison of {column_to_plot} Across Stimulations for {cell_type} Units')\n",
    "    plt.ylabel(column_to_plot)\n",
    "    plt.xlabel('Stimulation Type')\n",
    "    plt.legend(title='Group')\n",
    "\n",
    "    # Save the figure as an SVG file in the specified directory\n",
    "    if directory and file_name:\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_filtered_metrics(\n",
    "    dataframes_obj=whisker_df_manager,  # Your data object\n",
    "    column_to_plot='FirstSpikeLatency',  # The column you want to plot\n",
    "    stimulations=['Zero', 'Low', 'Mid', 'Max'],  # List of stimulations\n",
    "    is_single_unit=1.0,  # Filter for single units (1.0) or multi-units (0.0)\n",
    "    cell_type='FS',  # Filter for specific cell types ('FS' or 'RS')\n",
    "    stim_responsivity=1.0,  # Filter by stimulus responsivity\n",
    "    directory='/Volumes/MannySSD/figures/temporal_spiking_metrics/FirstSpikeLatency', \n",
    "    file_name='boxplot_RS_FirstSpikeLatency', \n",
    "    ylim=None # Adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS WILL BE CONSIDERED THE FINAL FOR FirstSpikeLatency AND  FOR RS SUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.dataframes['basic_metrics'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Max'], show_outliers=True, dataframe_key='temporal_spiking_metrics', \n",
    "                                              list_column='FirstSpikeLatency', cell_type_filter='RS', is_single_unit=1.0, stim_responsivity_filter=1.0, \n",
    "                                              modulation_label='positive', remove_outliers_option=False, \n",
    "                                              directory='/Volumes/MannySSD/figures/temporal_spiking_metrics/FirstSpikeLatency', \n",
    "                                              file_name='boxplot_RS_FirstSpikeLatency')\n",
    "#now do FS cells\n",
    "whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Max'], show_outliers=True, dataframe_key='temporal_spiking_metrics',\n",
    "                                              list_column='FirstSpikeLatency', cell_type_filter='FS', is_single_unit=1.0, stim_responsivity_filter=1.0,\n",
    "                                                modulation_label='positive', remove_outliers_option=False, \n",
    "                                                directory='/Volumes/MannySSD/figures/temporal_spiking_metrics/FirstSpikeLatency',\n",
    "                                                file_name='boxplot_FS_FirstSpikeLatency')\n",
    "\n",
    "\n",
    "whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Max'], show_outliers=True, dataframe_key='temporal_spiking_metrics', \n",
    "                                              list_column='FirstSpikeLatency_Reliability', cell_type_filter='RS', is_single_unit=1.0, stim_responsivity_filter=1.0, \n",
    "                                              modulation_label='positive', remove_outliers_option=False, \n",
    "                                              directory='/Volumes/MannySSD/figures/temporal_spiking_metrics/FirstSpikeLatency_Reliability', \n",
    "                                              file_name='boxplot_RS_FirstSpikeLatency_Reliability')\n",
    "\n",
    "### now do FS cells\n",
    "whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low','Mid', 'Max'], show_outliers=True, dataframe_key='temporal_spiking_metrics', \n",
    "                                              list_column='FirstSpikeLatency_Reliability', cell_type_filter='FS', is_single_unit=1.0, stim_responsivity_filter=1.0, \n",
    "                                              modulation_label='positive', remove_outliers_option=False, \n",
    "                                              directory='/Volumes/MannySSD/figures/temporal_spiking_metrics/FirstSpikeLatency_Reliability', \n",
    "                                              file_name='boxplot_FS_FirstSpikeLatency_Reliability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the raster only or the raster with the PSTH for a specific cell id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whisker_df_manager.plot_rasters_for_cid('CTZ', 'ctz_2303_1', 'cid530', time_window=(-50,50))\n",
    "#whisker_df_manager.plot_combined_psth_and_raster('CTZ', 'ctz_2303_1', 'cid530', time_window=(-50,50))\n",
    "whisker_df_manager.plot_combined_psth_and_raster_normalized('CTZ', 'ctz_2303_1', 'cid530', time_window=(-50,50), smoothing_window=3, normalize=False, show=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_combined_psth_and_raster_normalized('No_CTZ', 'veh_3062_2', 'cid109', time_window=(-50,50), smoothing_window=3, normalize=False, show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whisker_df_manager.plot_rasters_for_cid_changebinsize('CTZ', 'ctz_2303_1', 'cid365', time_window=(-50,50), bin_size=1, filter_empty_trials=True)\n",
    "#whisker_df_manager.plot_rasters_for_cid_changebinsize('CTZ', 'ctz_2303_1', 'cid530', time_window=(-50,50), bin_size=1, filter_empty_trials=True)\n",
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('CTZ', 'ctz_2303_2', 'cid208', time_window=(-50,50), bin_size=1, filter_empty_trials=True, recording_dir='/Volumes/MannySSD/figures' )\n",
    "#whisker_df_manager.plot_rasters_for_cid_changebinsize('CTZ', 'ctz_3689_2', 'cid108', time_window=(-50,50), bin_size=1, filter_empty_trials=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('No_CTZ', 'veh_3062_2', 'cid109', time_window=(-50,50), bin_size=1, filter_empty_trials=False, recording_dir='/Volumes/MannySSD/figures')\n",
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('No_CTZ', 'veh_3142_2', 'cid456', time_window=(-50,50), bin_size=1, filter_empty_trials=False, recording_dir='/Volumes/MannySSD/figures')\n",
    "whisker_df_manager.plot_rasters_for_cid_changebinsize('No_CTZ', 'veh_2836_2', 'cid72', time_window=(-50,50), bin_size=1, filter_empty_trials=False, recording_dir='/Volumes/MannySSD/figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now do rbp4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more whisker rbp4 plotting and data manager creatr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager = DataFrameManager(rbp4_whisker)\n",
    "rbp4_whisker_df_manager.eed\n",
    "rbp4_whisker_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'StimResponsivity', 'MeanFR_baseline', 'MeanFR_stim','LaminarLabel', 'PeakEvokedFR', 'PeakEvokedFR_Latency', 'FanoFactor_baseline', 'FanoFactor_stim', 'SpikeTimes_all', 'MeanFR_inst_stim' ], 'basic_metrics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=['Low'], show_outliers=True, directory='/Volumes/MannySSD/figures', file_name='boxplot_RS_baseline_stim_rbp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type='FS', is_single_unit=1.0, stim_responsivity=1.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=None, show_outliers=True, directory='/Volumes/MannySSD/figures', file_name='boxplot_FS_baseline_stim_rbp4')\n",
    "\n",
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=None, show_outliers=True, directory='/Volumes/MannySSD/figures', file_name='boxplot_RS_baseline_stim_rbp4')\n",
    "\n",
    "rbp4_whisker_df_manager.create_psth_dataframe()\n",
    "rbp4_whisker_df_manager.calculate_basic_stats('CTZ', 'No_CTZ', stim_label=None, baseline_range=(-100, -1), stim_range=(10,50), cell_type=None, is_single_unit=None, stim_responsivity=0.0, smoothing_window=5)\n",
    "rbp4_whisker_df_manager.prepare_for_boxplot()\n",
    "rbp4_whisker_df_manager.plot_box_and_strip(groups=['CTZ', 'No_CTZ'], stimulations=None, show_outliers=True, directory='/Volumes/MannySSD/figures', file_name='boxplot_MUA_baseline_stim_rbp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager.plot_all_stimulations_with_stim('CTZ', 'No_CTZ', cell_type='RS', is_single_unit=None, stim_responsivity=None, time_range=(-10,200), plot_mode='sem', smoothing_window=5)\n",
    "rbp4_whisker_df_manager.plot_all_stimulations_with_stim('CTZ', 'No_CTZ', cell_type='FS', is_single_unit=None, stim_responsivity=None, time_range=(-10,200), plot_mode='sem', smoothing_window=5)\n",
    "\n",
    "rbp4_whisker_df_manager.plot_all_stimulations_with_stim('CTZ', 'No_CTZ', cell_type='RS', is_single_unit=None, stim_responsivity=None, time_range=(-10,50), plot_mode='sem', smoothing_window=5)\n",
    "rbp4_whisker_df_manager.plot_all_stimulations_with_stim('CTZ', 'No_CTZ', cell_type='FS', is_single_unit=None, stim_responsivity=None, time_range=(-10,50), plot_mode='sem', smoothing_window=5)\n",
    "\n",
    "rbp4_whisker_df_manager.plot_all_stimulations('CTZ', 'No_CTZ', cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-10,600), plot_mode='sem', smoothing_window=8)\n",
    "rbp4_whisker_df_manager.plot_all_stimulations('CTZ', 'No_CTZ', cell_type='FS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-10,600), plot_mode='sem', smoothing_window=8)\n",
    "rbp4_whisker_df_manager.plot_all_stimulations('CTZ', 'No_CTZ', cell_type='RS', is_single_unit=0.0, stim_responsivity=1.0, time_range=(-10,600), plot_mode='sem', smoothing_window=8)\n",
    "rbp4_whisker_df_manager.plot_all_stimulations('CTZ', 'No_CTZ', cell_type='FS', is_single_unit=0.0, stim_responsivity=1.0, time_range=(-10,600), plot_mode='sem', smoothing_window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager.plot_all_stimulations('CTZ', 'No_CTZ', cell_type='RS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-10,100), plot_mode='traces', smoothing_window=3)\n",
    "rbp4_whisker_df_manager.plot_all_stimulations('CTZ', 'No_CTZ', cell_type='FS', is_single_unit=1.0, stim_responsivity=1.0, time_range=(-10,100), plot_mode='traces', smoothing_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp4_whisker_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'StimResponsivity', 'SpikeTimes_all', 'FirstSpikeLatency', 'FirstSpikeLatency_Reliability', 'ISI_baseline_CV', 'ISI_baseline_vec', 'ISI_pdf_peak_xy', 'ISI_pdf_x', 'ISI_pdf_y'  ], 'temporal_spiking_metrics')\n",
    "\n",
    "rbp4_whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Mid', 'Max'], show_outliers=False, dataframe_key='temporal_spiking_metrics', list_column='FirstSpikeLatency', cell_type_filter='RS', is_single_unit=True, stim_responsivity_filter=1, remove_outliers_option=False)\n",
    "rbp4_whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Mid', 'Max'], show_outliers=False, dataframe_key='temporal_spiking_metrics', list_column='FirstSpikeLatency_Reliability', cell_type_filter='RS', is_single_unit=True, stim_responsivity_filter=1, remove_outliers_option=False)\n",
    "rbp4_whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Mid', 'Max'], show_outliers=False, dataframe_key='temporal_spiking_metrics', list_column='FirstSpikeLatency', cell_type_filter='FS', is_single_unit=True, stim_responsivity_filter=1, remove_outliers_option=False)\n",
    "rbp4_whisker_df_manager.plot_box_and_strip_default(groups=['CTZ', 'No_CTZ'], stimulations=['Low', 'Mid', 'Max'], show_outliers=False, dataframe_key='temporal_spiking_metrics', list_column='FirstSpikeLatency_Reliability', cell_type_filter='FS', is_single_unit=True, stim_responsivity_filter=1, remove_outliers_option=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## begin stats analysis (bonus laminar analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whisker_df_manager.add_modulation_label_column('basic_metrics') \n",
    "#whisker_df_manager.dataframes['basic_metrics'].head()\n",
    "#print(whisker_df_manager.dataframes['basic_metrics'].shape)\n",
    "\n",
    "#mean_df, detailed_df, _ = whisker_df_manager.calculate_all_psths(is_single_unit=True, cell_type='RS', stim_responsivity=0.0, time_window=(25,50), normalize=True, filter_empty_trials=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process and run the stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mean_df_by_modulation_label(mean_df, basic_metrics_df):\n",
    "    \"\"\"\n",
    "    Filters mean_df to retain only entries with 'positive' ModulationLabel in basic_metrics_df.\n",
    "    \n",
    "    Args:\n",
    "    mean_df (pd.DataFrame): DataFrame containing mean stimulation data.\n",
    "    basic_metrics_df (pd.DataFrame): DataFrame containing modulation label and other metrics.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered mean_df with only 'positive' ModulationLabel entries.\n",
    "    \"\"\"\n",
    "    # Merge mean_df with basic_metrics_df on the shared columns\n",
    "    merged_df = mean_df.merge(basic_metrics_df, how='inner', \n",
    "                              left_on=['cid', 'LaminarLabel', 'recordingname', 'Group'], \n",
    "                              right_on=['cid', 'LaminarLabel', 'recordingname', 'groupname'])\n",
    "    \n",
    "    # Filter to retain only entries where ModulationLabel is 'positive'\n",
    "    filtered_df = merged_df[merged_df['ModulationLabel'] == 'positive']\n",
    "    \n",
    "    # Drop columns from basic_metrics_df that were added during the merge\n",
    "    filtered_df = filtered_df[mean_df.columns]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Example usage\n",
    "mean_df_filtered = filter_mean_df_by_modulation_label(mean_df, whisker_df_manager.dataframes['basic_metrics'])\n",
    "mean_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maake plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_stats_df, _= whisker_df_manager.plot_mean_stimulation_box_and_strip2(mean_df_filtered, groups=['CTZ', 'No_CTZ'], stimulations=['Zero','Max'], remove_outliers_option=False, laminar_labels=['SG'], ylim=None, directory='/Volumes/MannySSD/figures', file_name='mean_stimulations_boxplot_SG_MUA')\n",
    "L4_stats_df, _= whisker_df_manager.plot_mean_stimulation_box_and_strip2(mean_df_filtered, groups=['CTZ', 'No_CTZ'], stimulations=['Zero','Max'],remove_outliers_option=False, laminar_labels=['L4'], ylim=None, directory='/Volumes/MannySSD/figures', file_name='mean_stimulations_boxplot_L4_MUA')\n",
    "IG_stats_df, _= whisker_df_manager.plot_mean_stimulation_box_and_strip2(mean_df_filtered, groups=['CTZ', 'No_CTZ'], stimulations=['Zero','Max'],remove_outliers_option=False, laminar_labels=['IG'], ylim=None, directory='/Volumes/MannySSD/figures', file_name='mean_stimulations_boxplot_IG_MUA')\n",
    "all_layers_df, _ = whisker_df_manager.plot_mean_stimulation_box_and_strip2(mean_df_filtered, groups=['CTZ', 'No_CTZ'], stimulations=['Zero','Low', 'Mid', 'Max'],remove_outliers_option=False, laminar_labels=None, ylim=None, directory='/Volumes/MannySSD/figures', file_name='mean_stimulations_boxplot_all_layers_MUA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SG_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L4_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IG_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random stuff to be sorted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### do not delete this cell neeed to work on the code below fopr custom box plot\n",
    "def run_group_comparisons(df, group_column='Group', value_column='mean_stimulation', stim_column='Stimulation'):\n",
    "    \"\"\"\n",
    "    Performs statistical comparisons between groups for each type of stimulation, checks for normality,\n",
    "    uses the appropriate non-parametric tests, and includes detailed descriptive statistics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for stim in df[stim_column].unique():\n",
    "        sub_df = df[df[stim_column] == stim]\n",
    "        groups = sub_df[group_column].unique()\n",
    "\n",
    "        if len(groups) < 2:\n",
    "            continue  # Skip if not enough groups for comparison\n",
    "\n",
    "        group_data = [sub_df[sub_df[group_column] == g][value_column].dropna() for g in groups]\n",
    "        normality_results = [shapiro(data) for data in group_data]\n",
    "        normality_p_values = [result.pvalue for result in normality_results]\n",
    "\n",
    "        # Choose the appropriate statistical test based on group count\n",
    "        if len(groups) == 2:\n",
    "            stat, p_value = mannwhitneyu(*group_data)\n",
    "            test_used = 'Mann-Whitney U'\n",
    "        elif len(groups) > 2:\n",
    "            stat, p_value = kruskal(*group_data)\n",
    "            test_used = 'Kruskal-Wallis'\n",
    "            # Note: If using Kruskal-Wallis, consider post-hoc tests for detailed group comparisons\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        descriptive_stats = [{\n",
    "            'mean': data.mean(),\n",
    "            'SD': data.std(),\n",
    "            'median': data.median(),\n",
    "            'range_min': data.min(),\n",
    "            'range_max': data.max(),\n",
    "        } for data in group_data]\n",
    "\n",
    "        result_entry = {\n",
    "            'Stimulation': stim,\n",
    "            'Test Used': test_used,\n",
    "            'Test Statistic': stat,\n",
    "            'p-value': p_value,\n",
    "            **{f'N Group{i+1}': len(data) for i, data in enumerate(group_data)},\n",
    "            **{f'Normality p-value Group{i+1}': p for i, p in enumerate(normality_p_values)},\n",
    "            **{f'{stat_key} Group{i+1}': stat_val for i, stats in enumerate(descriptive_stats) for stat_key, stat_val in stats.items()}\n",
    "        }\n",
    "\n",
    "        results.append(result_entry)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage in your data processing workflow\n",
    "cleaned_df = whisker_df_manager.remove_outliers_by_stimulation(mean_df)  # Assuming outliers have been removed\n",
    "stats_results = run_group_comparisons(cleaned_df)\n",
    "\n",
    "stats_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.plot_group_cell_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Function to pad arrays to the maximum length found in any array\n",
    "def pad_array_to_max_length(arr, max_length):\n",
    "    padding = max_length - len(arr)\n",
    "    if padding > 0:\n",
    "        return np.pad(arr, (0, padding), 'constant', constant_values=(0,))\n",
    "    return arr\n",
    "\n",
    "# Function to plot the heatmaps with time bins on the x-axis and adjustable colormap range\n",
    "def plot_group_stimulation_heatmaps(df, vmin=None, vmax=None):\n",
    "    groupnames = df['groupname'].unique()\n",
    "    stimulations = ['Zero', 'Low', 'Mid', 'Max']\n",
    "    num_groups = len(groupnames)\n",
    "    \n",
    "    # Create a subplot grid with 2 rows (or more if more groups) and 4 columns\n",
    "    fig, axes = plt.subplots(num_groups, 4, figsize=(20, 5 * num_groups), sharex='col', sharey='row')\n",
    "    \n",
    "    # If no specific vmin and vmax are provided, determine them from the data\n",
    "    if vmin is None or vmax is None:\n",
    "        vmin = df['counts'].apply(np.min).min()  # Minimum value across all counts\n",
    "        vmax = df['counts'].apply(np.max).max()  # Maximum value\n",
    "\n",
    "    # Iterate over each group and stimulation to create each subplot\n",
    "    for i, group in enumerate(groupnames):\n",
    "        for j, stim in enumerate(stimulations):\n",
    "            ax = axes[i, j] if num_groups > 1 else axes[j]\n",
    "            \n",
    "            # Filter DataFrame for the current group and stimulation\n",
    "            stim_df = df[(df['groupname'] == group) & (df['Stimulation'] == stim)]\n",
    "\n",
    "            if not stim_df.empty:\n",
    "                # Find the maximum length of arrays in the 'counts' column for current group and stimulation\n",
    "                max_length = stim_df['counts'].map(len).max()\n",
    "\n",
    "                # Pad all 'counts' arrays to this maximum length\n",
    "                padded_counts = stim_df['counts'].apply(lambda x: pad_array_to_max_length(x, max_length))\n",
    "\n",
    "                # Stack the padded counts to create a matrix\n",
    "                counts_matrix = np.stack(padded_counts.values)\n",
    "                \n",
    "                # Extract the time_array from the first row of the filtered DataFrame\n",
    "                time_bins = stim_df.iloc[0]['time_array']\n",
    "                num_ticks = min(len(time_bins), 10)\n",
    "                tick_positions = np.linspace(0, len(time_bins) - 1, num_ticks, dtype=int)\n",
    "                tick_labels = [f\"{time_bins[pos]:.0f}\" for pos in tick_positions]\n",
    "\n",
    "                # Plot the heatmap with user-defined or data-driven color range\n",
    "                sns.heatmap(counts_matrix, ax=ax, cmap=\"viridis\", vmin=vmin, vmax=vmax, cbar=(j == len(stimulations) - 1))\n",
    "                \n",
    "                # Set x-ticks to align with time bins\n",
    "                ax.set_xticks(tick_positions)\n",
    "                ax.set_xticklabels(tick_labels, rotation=45, ha='right')\n",
    "                \n",
    "                ax.set_title(f'{group} - {stim}')\n",
    "                ax.set_xlabel('Time (ms)')\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel('Cells')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_mean_df, unfiltered_detailed_df = whisker_df_manager.calculate_all_psths(is_single_unit=None, cell_type=None, stim_responsivity=None, time_window=(0, 100), normalize=True, filter_empty_trials=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.dataframes['psth_dataframe_Zero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, detailed_df_entirepsth= whisker_df_manager.calculate_all_psths(is_single_unit=1.0, cell_type='RS', stim_responsivity=1.0, time_window=(0, 100), normalize=False, filter_empty_trials=False)\n",
    "plot_group_stimulation_heatmaps(detailed_df_entirepsth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, detailed_df_entirepsth= whisker_df_manager.calculate_all_psths(is_single_unit=1.0, cell_type='FS', stim_responsivity=1.0, time_window=(0, 100), normalize=True, filter_empty_trials=True)\n",
    "plot_group_stimulation_heatmaps(detailed_df_entirepsth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing correlations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.dataframes['psth_dataframe_Zero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dataframe' is your DataFrame\n",
    "def find_recording_with_most_single_units(data):\n",
    "    # Filter data for single units\n",
    "    single_unit_data = data[data['IsSingleUnit'] == 1.0]\n",
    "\n",
    "    # Group by recording name and count entries\n",
    "    counts = single_unit_data.groupby('recordingname').size()\n",
    "\n",
    "    # Find the recording with the maximum count\n",
    "    max_single_units = counts.idxmax()\n",
    "    max_count = counts.max()\n",
    "\n",
    "    return max_single_units, max_count\n",
    "\n",
    "max_recording, max_count = find_recording_with_most_single_units(whisker_df_manager.dataframes['psth_dataframe_Zero'])\n",
    "print(f\"The recording with the most single units is {max_recording} with {max_count} single units.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisker_df_manager.extract_spike_trains('veh_3141_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# waveform plots and analysis -- plot dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_plot_saving_waveforms(whisker_df_manager, base_directory):\n",
    "    \"\"\"\n",
    "    Dynamically generates and saves plots across different conditions for waveforms and trough-to-peak histograms.\n",
    "\n",
    "    Args:\n",
    "        whisker_df_manager: The data manager object with methods to calculate stats and plot.\n",
    "        base_directory (str): Base directory to save the plots.\n",
    "    \"\"\"\n",
    "    groupnames = [None, 'CTZ', 'No_CTZ']\n",
    "    is_single_units = [None, 1.0, 0.0]\n",
    "    stim_responsivities = [1.0, 0.0, -1.0, None]\n",
    "    \n",
    "    for groupname in groupnames:\n",
    "        # Determine the top-level folder based on groupname\n",
    "        if groupname is None:\n",
    "            top_folder = 'All'\n",
    "        else:\n",
    "            top_folder = groupname\n",
    "        \n",
    "        for is_single_unit in is_single_units:\n",
    "            # Determine the subfolder based on is_single_unit\n",
    "            if is_single_unit is None:\n",
    "                subfolder = 'All_Units'\n",
    "            elif is_single_unit == 1.0:\n",
    "                subfolder = 'SUA'\n",
    "            else:\n",
    "                subfolder = 'MUA'\n",
    "            \n",
    "            #print the current status of the loop\n",
    "            print(f\"Generating plots for: {top_folder} - {subfolder}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            for stim_responsivity in stim_responsivities:\n",
    "                # Define the directory for saving plots\n",
    "                directory = os.path.join(base_directory, top_folder, subfolder, f\"StimResponsivity_{stim_responsivity}\")\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "                \n",
    "                # Create a suffix for filenames\n",
    "                suffix = f\"{top_folder}_{subfolder}_StimResponsivity_{stim_responsivity}\"\n",
    "                \n",
    "                # Generate and save the trough-to-peak histogram\n",
    "                print(f\"Generating trough-to-peak histogram for: {suffix}\")\n",
    "                whisker_df_manager.plot_trough_to_peak_histogram(\n",
    "                    groupname=groupname, is_single_unit=is_single_unit, \n",
    "                    stim_responsivity=stim_responsivity, modulation_label='positive'\n",
    "                )\n",
    "                plt.savefig(os.path.join(directory, f\"{suffix}_trough_to_peak_histogram.svg\"), transparent=True)\n",
    "                plt.close()  # Ensure the plot is closed and saved correctly\n",
    "                \n",
    "                # Generate and save the combined waveforms (mean+SEM)\n",
    "                print(f\"Generating combined waveforms (mean+SEM) for: {suffix}\")\n",
    "                whisker_df_manager.plot_combined_waveforms_meansem(\n",
    "                    groupname=groupname, modulation_label='positive', \n",
    "                     stim_responsivity=stim_responsivity, is_single_unit=is_single_unit\n",
    "                )\n",
    "                plt.savefig(os.path.join(directory, f\"{suffix}_combined_waveforms_meansem.svg\"), transparent=True)\n",
    "                plt.close()  # Ensure the plot is closed and saved correctly\n",
    "                \n",
    "                # Generate and save the combined waveforms\n",
    "                print(f\"Generating combined waveforms for: {suffix}\")\n",
    "                whisker_df_manager.plot_combined_waveforms(\n",
    "                    groupname=groupname, modulation_label='positive', \n",
    "                    stim_responsivity=stim_responsivity, is_single_unit=is_single_unit\n",
    "                )\n",
    "                plt.savefig(os.path.join(directory, f\"{suffix}_combined_waveforms.svg\"), transparent=True)\n",
    "                plt.close()  # Ensure the plot is closed and saved correctly\n",
    "                \n",
    "                #print the current status of the loop\n",
    "                print(f\"Plots saved for: {top_folder} - {subfolder} - StimResponsivity_{stim_responsivity}\")\n",
    "\n",
    "    print(\"All plots have been generated and saved.\")\n",
    "    \n",
    "    \n",
    "# Set the base directory\n",
    "base_directory = '/Volumes/MannySSD/figures/comparing_waveforms_dynamic'\n",
    "\n",
    "# Call the function to generate and save the plots\n",
    "dynamic_plot_saving_waveforms(whisker_df_manager, base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets try to use plotly all of these worked except for the last one -- still need to modify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "\n",
    "def plot_3d_waveform_metrics_with_stimulation(obj, x_metric='TroughToPeak_duration', y_metric='MeanFR_baseline', z_metric='SpikeHalfWidth', is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "    \"\"\"\n",
    "    Creates an interactive 3D scatter plot of specified waveform metrics for FS and RS cells,\n",
    "    with dot size based on mean_stimulation and color-coded by Cell_Type.\n",
    "\n",
    "    Parameters:\n",
    "    obj: The object containing the `calculate_basic_stats` and `prepare_for_boxplot` methods.\n",
    "    x_metric (str): The metric to plot on the x-axis.\n",
    "    y_metric (str): The metric to plot on the y-axis.\n",
    "    z_metric (str): The metric to plot on the z-axis.\n",
    "    is_single_unit (float or None): Filter by single units (1.0), non-single units (0.0), or None for no filtering.\n",
    "    stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "    modulation_label (str or None): Filter by ModulationLabel ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "    firstspike_latency (bool or None): Whether to include first spike latency in the calculations.\n",
    "\n",
    "    Returns:\n",
    "    None: The function generates an interactive 3D plot in a new browser window and returns the combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the necessary calculations\n",
    "    obj.calculate_basic_stats(\n",
    "        'CTZ', 'No_CTZ', stim_label='Max', \n",
    "        baseline_range=(-100, -1), stim_range=(0, 500), \n",
    "        cell_type=None, is_single_unit=is_single_unit, \n",
    "        stim_responsivity=stim_responsivity, \n",
    "        smoothing_window=3, modulation_label=modulation_label\n",
    "    )\n",
    "    \n",
    "    boxplot_df = obj.prepare_for_boxplot()\n",
    "\n",
    "    # Combine FS and RS into a single DataFrame, adding a column to distinguish them\n",
    "    fs_df = boxplot_df[boxplot_df['Cell_Type'] == 'FS']\n",
    "    rs_df = boxplot_df[boxplot_df['Cell_Type'] == 'RS']\n",
    "    combined_df = pd.concat([fs_df, rs_df])\n",
    "\n",
    "    # Drop NaN values for the selected metrics and ensure `mean_stimulation` is included\n",
    "    combined_df = combined_df[[x_metric, y_metric, z_metric, 'mean_stimulation', 'Cell_Type', 'Group', 'recordingname', 'cid', 'StimResponsivity']].dropna()\n",
    "\n",
    "    # Create a 3D scatter plot with size based on `mean_stimulation` and color by `Cell_Type`\n",
    "    fig = px.scatter_3d(combined_df, \n",
    "                        x=x_metric, \n",
    "                        y=y_metric, \n",
    "                        z=z_metric, \n",
    "                        color='Cell_Type',  # Color by Cell_Type (FS or RS)\n",
    "                        size='mean_stimulation',  # Size by mean_stimulation\n",
    "                        size_max=20,  # Max size for the dots\n",
    "                        labels={\n",
    "                            x_metric: x_metric.replace('_', ' ').title(),\n",
    "                            y_metric: y_metric.replace('_', ' ').title(),\n",
    "                            z_metric: z_metric.replace('_', ' ').title(),\n",
    "                        },\n",
    "                        hover_data={\n",
    "                            'Group': True,  # Corrected the column name from 'groupname' to 'Group'\n",
    "                            'recordingname': True,\n",
    "                            'cid': True,\n",
    "                            'StimResponsivity': True,\n",
    "                            'mean_stimulation': True,\n",
    "                        },\n",
    "                        title=f'3D Scatter Plot with Mean Stimulation Intensity of {x_metric}, {y_metric}, and {z_metric}')\n",
    "\n",
    "    # Update the layout to make the figure larger\n",
    "    fig.update_layout(\n",
    "        width=1000,  # Adjust width as needed\n",
    "        height=800,  # Adjust height as needed\n",
    "    )\n",
    "    \n",
    "    # Show the figure in a standalone browser window\n",
    "    #pyo.plot(fig, auto_open=True)\n",
    "    \n",
    "    #show the figure in the notebook\n",
    "    fig.show()\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "combined_df = plot_3d_waveform_metrics_with_stimulation(\n",
    "    whisker_df_manager,\n",
    "    x_metric='Peak2ToTrough_ratio', \n",
    "    y_metric='TroughToPeak_duration', \n",
    "    z_metric='Peak1ToTrough_ratio', \n",
    "    is_single_unit=None,\n",
    "    stim_responsivity=None,\n",
    "    modulation_label=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_3d_waveform_metrics_with_stimulation(obj, x_metric='TroughToPeak_duration', y_metric='MeanFR_baseline', z_metric='SpikeHalfWidth', is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "    \"\"\"\n",
    "    Creates an interactive 3D scatter plot of specified waveform metrics for FS and RS cells,\n",
    "    with dot size based on mean_stimulation and color-coded by Cell_Type.\n",
    "\n",
    "    Parameters:\n",
    "    obj: The object containing the `calculate_basic_stats` and `prepare_for_boxplot` methods.\n",
    "    x_metric (str): The metric to plot on the x-axis.\n",
    "    y_metric (str): The metric to plot on the y-axis.\n",
    "    z_metric (str): The metric to plot on the z-axis.\n",
    "    is_single_unit (float or None): Filter by single units (1.0), non-single units (0.0), or None for no filtering.\n",
    "    stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "    modulation_label (str or None): Filter by ModulationLabel ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "\n",
    "    Returns:\n",
    "    None: The function generates an interactive 3D plot in a new browser window and returns the combined DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the necessary calculations\n",
    "    obj.calculate_basic_stats(\n",
    "        'CTZ', 'No_CTZ', stim_label='Max', \n",
    "        baseline_range=(-100, -1), stim_range=(0, 500), \n",
    "        cell_type=None, is_single_unit=is_single_unit, \n",
    "        stim_responsivity=stim_responsivity, \n",
    "        smoothing_window=3, modulation_label=modulation_label\n",
    "    )\n",
    "    \n",
    "    boxplot_df = obj.prepare_for_boxplot()\n",
    "\n",
    "    # Combine FS and RS into a single DataFrame\n",
    "    fs_df = boxplot_df[boxplot_df['Cell_Type'] == 'FS']\n",
    "    rs_df = boxplot_df[boxplot_df['Cell_Type'] == 'RS']\n",
    "    combined_df = pd.concat([fs_df, rs_df])\n",
    "\n",
    "    # Drop NaN values for the selected metrics and ensure `mean_stimulation` is included\n",
    "    combined_df = combined_df[[x_metric, y_metric, z_metric, 'mean_stimulation', 'Cell_Type', 'Group', 'recordingname', 'cid', 'StimResponsivity']].dropna()\n",
    "\n",
    "    # Create traces for FS and RS\n",
    "    trace_fs = go.Scatter3d(\n",
    "        x=fs_df[x_metric],\n",
    "        y=fs_df[y_metric],\n",
    "        z=fs_df[z_metric],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=fs_df['mean_stimulation'] / 5,  # Adjust size scaling as needed\n",
    "            color='darkgoldenrod',  # FS color\n",
    "            opacity=0.8,\n",
    "        ),\n",
    "        name='FS',\n",
    "        text=[f\"Group: {g}<br>Recording: {r}<br>cid: {c}<br>StimResponsivity: {s}<br>Mean Stimulation: {m:.2f}\" \n",
    "              for g, r, c, s, m in zip(fs_df['Group'], fs_df['recordingname'], fs_df['cid'], fs_df['StimResponsivity'], fs_df['mean_stimulation'])],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    trace_rs = go.Scatter3d(\n",
    "        x=rs_df[x_metric],\n",
    "        y=rs_df[y_metric],\n",
    "        z=rs_df[z_metric],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=rs_df['mean_stimulation'] / 5,  # Adjust size scaling as needed\n",
    "            color='sienna',  # RS color\n",
    "            opacity=0.8,\n",
    "        ),\n",
    "        name='RS',\n",
    "        text=[f\"Group: {g}<br>Recording: {r}<br>cid: {c}<br>StimResponsivity: {s}<br>Mean Stimulation: {m:.2f}\" \n",
    "              for g, r, c, s, m in zip(rs_df['Group'], rs_df['recordingname'], rs_df['cid'], rs_df['StimResponsivity'], rs_df['mean_stimulation'])],\n",
    "        hoverinfo='text'\n",
    "    )\n",
    "\n",
    "    # Combine traces\n",
    "    fig = go.Figure(data=[trace_fs, trace_rs])\n",
    "\n",
    "    # Update the layout\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=x_metric.replace('_', ' ').title(),\n",
    "            yaxis_title=y_metric.replace('_', ' ').title(),\n",
    "            zaxis_title=z_metric.replace('_', ' ').title(),\n",
    "        ),\n",
    "        width=1000,  # Adjust width as needed\n",
    "        height=800,  # Adjust height as needed\n",
    "        title=f'3D Scatter Plot with Mean Stimulation Intensity of {x_metric}, {y_metric}, and {z_metric}',\n",
    "    )\n",
    "\n",
    "    # Show the figure in the notebook\n",
    "    fig.show()\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Example usage:\n",
    "combined_df = plot_3d_waveform_metrics_with_stimulation(\n",
    "    whisker_df_manager,\n",
    "    x_metric='Peak2ToTrough_ratio', \n",
    "    y_metric='TroughToPeak_duration', \n",
    "    z_metric='Peak1ToTrough_ratio', \n",
    "    is_single_unit=None,\n",
    "    stim_responsivity=None,\n",
    "    modulation_label=None\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager # Context manager to suppress output but just for below function\n",
    "def suppress_output():\n",
    "    \"\"\"\n",
    "    Suppress all output (stdout and stderr) within the context.\n",
    "    \"\"\"\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = devnull\n",
    "        sys.stderr = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_static_3d_waveform_metrics_with_stimulation(obj, x_metric='TroughToPeak_duration', \n",
    "                                                     y_metric='MeanFR_baseline', z_metric='SpikeHalfWidth', \n",
    "                                                     is_single_unit=None, stim_responsivity=None, \n",
    "                                                     modulation_label=None, \n",
    "                                                     output_path='3d_plot.svg', elev=30, azim=30):\n",
    "    \"\"\"\n",
    "    Creates a static 3D scatter plot of specified waveform metrics for FS and RS cells,\n",
    "    with dot size based on mean_stimulation and color-coded by Cell_Type, and saves as an SVG.\n",
    "\n",
    "    Parameters:\n",
    "    obj: The object containing the `calculate_basic_stats` and `prepare_for_boxplot` methods.\n",
    "    x_metric (str): The metric to plot on the x-axis.\n",
    "    y_metric (str): The metric to plot on the y-axis.\n",
    "    z_metric (str): The metric to plot on the z-axis.\n",
    "    is_single_unit (float or None): Filter by single units (1.0), non-single units (0.0), or None for no filtering.\n",
    "    stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "    modulation_label (str or None): Filter by ModulationLabel ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "    output_path (str): Path to save the output SVG file.\n",
    "    elev (int or float): Elevation angle for the 3D plot (default is 30).\n",
    "    azim (int or float): Azimuth angle for the 3D plot (default is 30).\n",
    "\n",
    "    Returns:\n",
    "    None: The function generates a static 3D plot and saves it as an SVG.\n",
    "    \"\"\"\n",
    "\n",
    "    # Suppress output during calculations and data preparation\n",
    "    with suppress_output():\n",
    "        # Run the necessary calculations\n",
    "        obj.calculate_basic_stats(\n",
    "            'CTZ', 'No_CTZ', stim_label='Max', \n",
    "            baseline_range=(-100, -1), stim_range=(0, 500), \n",
    "            cell_type=None, is_single_unit=is_single_unit, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            smoothing_window=3, modulation_label=modulation_label\n",
    "        )\n",
    "    \n",
    "        boxplot_df = obj.prepare_for_boxplot()\n",
    "\n",
    "    # Combine FS and RS into a single DataFrame\n",
    "    fs_df = boxplot_df[boxplot_df['Cell_Type'] == 'FS']\n",
    "    rs_df = boxplot_df[boxplot_df['Cell_Type'] == 'RS']\n",
    "    combined_df = pd.concat([fs_df, rs_df])\n",
    "\n",
    "    # Drop NaN values for the selected metrics and ensure `mean_stimulation` is included\n",
    "    combined_df = combined_df[[x_metric, y_metric, z_metric, 'mean_stimulation', 'Cell_Type', 'Group', 'recordingname', 'cid', 'StimResponsivity']].dropna()\n",
    "\n",
    "    # Create a 3D scatter plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot FS cells\n",
    "    fs = combined_df[combined_df['Cell_Type'] == 'FS']\n",
    "    scatter_fs = ax.scatter(\n",
    "        fs[x_metric], fs[y_metric], fs[z_metric],\n",
    "        s=fs['mean_stimulation'] * 2,  # Scale marker size\n",
    "        c='darkgoldenrod', label='FS', alpha=0.8\n",
    "    )\n",
    "\n",
    "    # Plot RS cells\n",
    "    rs = combined_df[combined_df['Cell_Type'] == 'RS']\n",
    "    scatter_rs = ax.scatter(\n",
    "        rs[x_metric], rs[y_metric], rs[z_metric],\n",
    "        s=rs['mean_stimulation'] * 2,  # Scale marker size\n",
    "        c='sienna', label='RS', alpha=0.8\n",
    "    )\n",
    "\n",
    "    # Set axis labels\n",
    "    ax.set_xlabel(x_metric.replace('_', ' ').title())\n",
    "    ax.set_ylabel(y_metric.replace('_', ' ').title())\n",
    "    ax.set_zlabel(z_metric.replace('_', ' ').title())\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(f'3D Scatter Plot with Mean Stimulation Intensity of {x_metric}, {y_metric}, and {z_metric}')\n",
    "\n",
    "    # Set viewing angle\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "    # Add a legend for FS and RS\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    # Add a custom legend for dot sizes\n",
    "    handles, labels = scatter_fs.legend_elements(prop=\"sizes\", alpha=0.6, num=4)\n",
    "    size_legend = plt.legend(handles, labels, title=\"Mean Stimulation\", loc=\"lower right\", bbox_to_anchor=(1.1, 0.5))\n",
    "\n",
    "    # Add the custom legend to the plot\n",
    "    ax.add_artist(size_legend)\n",
    "    \n",
    "    # Save the figure as an SVG file\n",
    "    plt.savefig(output_path, format='svg', bbox_inches='tight', transparent=True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_static_3d_waveform_metrics_with_stimulation(\n",
    "    whisker_df_manager,\n",
    "    x_metric='Peak2ToTrough_ratio', \n",
    "    y_metric='TroughToPeak_duration', \n",
    "    z_metric='Peak1ToTrough_ratio', \n",
    "    is_single_unit=1.0,\n",
    "    stim_responsivity=None,\n",
    "    modulation_label=None,\n",
    "    output_path='/Volumes/MannySSD/figures/3dwaveformplots_SUA.svg',\n",
    "    elev=30,  # Adjust elevation angle\n",
    "    azim=5  # Adjust azimuth angle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import os\n",
    "\n",
    "### this used to work but need to fix \n",
    "\n",
    "def plot_box_and_strip_with_plotly(obj, groups=None, stimulations=None, show_outliers=True, hue_order=None, \n",
    "                                   directory=None, file_name=None, ylim=None, \n",
    "                                   modulation_label=None, cell_type=None, \n",
    "                                   is_single_unit=None, stim_responsivity=None):\n",
    "    \"\"\"\n",
    "    Plots interactive boxplots and stripplots for specified groups and stimulations using Plotly, with metadata accessible via hover.\n",
    "\n",
    "    Args:\n",
    "        obj: The object containing the `prepare_for_boxplot` method.\n",
    "        groups (list of str, optional): List of groups to include in the plot.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "        show_outliers (bool, optional): Whether to show outliers.\n",
    "        hue_order (list, optional): Order of the hue levels.\n",
    "        directory (str, optional): Directory to save the plot.\n",
    "        file_name (str, optional): File name for saving the plot.\n",
    "        ylim (tuple, optional): Limits for the y-axis.\n",
    "        modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        cell_type (str, optional): Filter for specific cell types ('FS', 'RS', etc.).\n",
    "        is_single_unit (float, optional): Filter for single units (1.0), multi-units (0.0), or all (None).\n",
    "        stim_responsivity (float, optional): Filter by stimulus responsivity (1.0, 0.0, -1.0).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame used for the plot.\n",
    "    \"\"\"\n",
    "    # Prepare data for boxplot\n",
    "    boxplot_df = obj.prepare_for_boxplot()\n",
    "\n",
    "    # Apply filtering as needed\n",
    "    if groups:\n",
    "        boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "    if stimulations:\n",
    "        boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "    if modulation_label:\n",
    "        boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "    if cell_type:\n",
    "        boxplot_df = boxplot_df[boxplot_df['Cell_Type'] == cell_type]\n",
    "    if is_single_unit is not None:\n",
    "        boxplot_df = boxplot_df[boxplot_df['IsSingleUnit'] == is_single_unit]\n",
    "    if stim_responsivity is not None:\n",
    "        boxplot_df = boxplot_df[boxplot_df['StimResponsivity'] == stim_responsivity]\n",
    "    \n",
    "    # Create a boxplot with stripplot overlay using Plotly\n",
    "    fig = px.box(boxplot_df, \n",
    "                 x='Stimulation', \n",
    "                 y='mean_stimulation', \n",
    "                 color='Group', \n",
    "                 points='all' if show_outliers else 'outliers',\n",
    "                 hover_data=['recordingname', 'ModulationLabel', 'cid', 'Cell_Type', 'Template_Channel', 'LaminarLabel', 'IsSingleUnit', 'TroughToPeak_duration', \n",
    "                             'PeakToPeak_ratio', 'peak1_normalized_amplitude', 'Peak1ToTrough_ratio', 'Peak2ToTrough_ratio', 'SpikeHalfWidth', 'MeanFR_baseline', 'StimResponsivity'],\n",
    "                 color_discrete_map={\n",
    "                     'No_CTZ': '#797979',\n",
    "                     'CTZ': '#5a00c2'\n",
    "                 })\n",
    "    \n",
    "    # Adjust y-axis limits if provided\n",
    "    if ylim:\n",
    "        fig.update_yaxes(range=ylim)\n",
    "\n",
    "    # Add title and labels\n",
    "\n",
    "\n",
    "    # Save the figure if directory and file name are provided\n",
    "    if directory and file_name:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        file_path = os.path.join(directory, f'{file_name}.html')\n",
    "        fig.write_html(file_path)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Comparison of Mean Stimulation Across Groups and Stimulations',\n",
    "        xaxis_title='Stimulation Type',\n",
    "        yaxis_title='Mean Stimulation',\n",
    "        width=1000,  # Adjust width as needed\n",
    "        height=800,  # Adjust height as needed\n",
    "    )\n",
    "    \n",
    "        # Update the layout to make the figure larger\n",
    "\n",
    "    \n",
    "    # Show the figure in a standalone browser window\n",
    "    #pyo.plot(fig, auto_open=True)\n",
    "    #plot the figure in the notebook\n",
    "    fig.show()\n",
    "    \n",
    "# Call the function with your object and desired filtering options\n",
    "boxplot_df = plot_box_and_strip_with_plotly(\n",
    "    whisker_df_manager,  # Your object containing the data and methods\n",
    "    groups=['CTZ', 'No_CTZ'],  # Filter by groups\n",
    "    stimulations=['8Hz'],  # Filter by stimulations\n",
    "    show_outliers=True,  # Show or hide outliers\n",
    "    directory='/Volumes/MannySSD/figures/comparing_conditions_dynamic_plotly',  # Directory to save the plot\n",
    "    file_name='plot_name',  # File name for saving the plot\n",
    "    ylim=None,  # Set y-axis limits\n",
    "    modulation_label='positive',  # Filter by modulation label\n",
    "    cell_type='FS',  # Filter by cell type ('FS', 'RS')\n",
    "    is_single_unit=1.0,  # Filter for single units\n",
    "    stim_responsivity=1.0  # Filter by stimulus responsivity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opto Emx data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import the opto ephys data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for opto data\n",
    "opto = ExtractEphysData('/Volumes/MannySSD/emx_led/','all_data.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intialize the opto dataframe object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opto_df_manager = DataFrameManager(opto)\n",
    "opto_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'StimResponsivity', 'MeanFR_baseline', 'MeanFR_stim', \n",
    "                                  'PeakEvokedFR', 'PeakEvokedFR_Latency', 'FanoFactor_baseline', 'FanoFactor_stim', 'SpikeTimes_all', \n",
    "                                  'MeanFR_inst_stim', 'Template_Channel', 'ModulationIndex', 'Normalized_Template_Waveform', 'TroughToPeak_duration', \n",
    "                                  'UnNormalized_Template_Waveform', 'ISI_violations_percent', 'Recording_Duration', 'Sampling_Frequency', 'SpikeTimes_all' ], 'basic_metrics')\n",
    "opto_df_manager.create_psth_dataframe_opto()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "opto_df_manager.plot_optogenetic_psth_comparison_grid('CTZ', 'No_CTZ', '8 Hz LED',\n",
    "                                      cell_types=['RS', 'FS'], is_single_unit=None, \n",
    "                                      stim_responsivity=1.0, modulation_label='positive',\n",
    "                                      time_range=(-200, 800), plot_mode='sem', smoothing_window=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_data_laminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "fig = opto_df_manager.plot_interactive_psth_comparison_grid('CTZ', 'No_CTZ', '8 Hz LED',\n",
    "                                                      cell_types=['RS', 'FS'], is_single_unit=1.0,\n",
    "                                                      stim_responsivity=1.0, modulation_label='positive',\n",
    "                                                      time_range=(-200, 800), smoothing_window=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opto_df_manager.plot_optogenetic_psth_comparison_subplot_with_led('No_CTZ', 'CTZ', '8 Hz LED', \n",
    "                                      cell_type='FS', \n",
    "                                      is_single_unit=1.0, \n",
    "                                      stim_responsivity=1.0, \n",
    "                                      time_range=None, \n",
    "                                      plot_mode='sem', \n",
    "                                      smoothing_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_custom_boxplot_grid(opto_df_manager, column_to_analyze, \n",
    "                             single_unit_only=None, stim_responsivity=1.0, \n",
    "                             show_outliers=False, show_scatter=True, \n",
    "                             modulation_index='positive', jitter=True, jitter_amount=0.1):\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 16), sharex='col')\n",
    "    color_palette = {'CTZ': 'purple', 'No_CTZ': 'grey'}\n",
    "    \n",
    "    def plot_for_cell_type(ax, cell_type, column):\n",
    "        # Filter data\n",
    "        data = opto_df_manager.dataframes['basic_metrics'].copy()\n",
    "        \n",
    "        # Convert IsSingleUnit to boolean\n",
    "        data['IsSingleUnit'] = data['IsSingleUnit'].apply(lambda x: 1.0 in x or 1 in x if isinstance(x, (np.ndarray, list)) else x == 1.0 or x == 1)\n",
    "        \n",
    "        # Extract numeric values from the specified column\n",
    "        data[f'{column}_numeric'] = data[column].apply(lambda x: float(x) if not pd.isna(x) else np.nan)\n",
    "        \n",
    "        if single_unit_only:\n",
    "            data = data[data['IsSingleUnit']]\n",
    "        \n",
    "        if stim_responsivity is not None:\n",
    "            data = data[data['StimResponsivity'] == stim_responsivity]\n",
    "        \n",
    "        if modulation_index is not None:\n",
    "            data = data[data['ModulationIndex'] == modulation_index]\n",
    "        \n",
    "        # Remove any NaN or infinite values\n",
    "        data = data.dropna(subset=[f'{column}_numeric'])\n",
    "        data = data[~np.isinf(data[f'{column}_numeric'])]\n",
    "        \n",
    "        data = data[data['Cell_Type'] == cell_type]\n",
    "        \n",
    "        # Create box plot\n",
    "        sns.boxplot(x='groupname', y=f'{column}_numeric', data=data, \n",
    "                    palette=color_palette, showfliers=show_outliers, ax=ax)\n",
    "        \n",
    "        # Add scatter plot if requested\n",
    "        if show_scatter:\n",
    "            positions = {group: i for i, group in enumerate(data['groupname'].unique())}\n",
    "            \n",
    "            for group in data['groupname'].unique():\n",
    "                group_data = data[data['groupname'] == group]\n",
    "                \n",
    "                if show_outliers:\n",
    "                    x = [positions[group]] * len(group_data)\n",
    "                    y = group_data[f'{column}_numeric']\n",
    "                else:\n",
    "                    q1, q3 = group_data[f'{column}_numeric'].quantile([0.25, 0.75])\n",
    "                    iqr = q3 - q1\n",
    "                    lower_bound, upper_bound = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "                    non_outliers = group_data[(group_data[f'{column}_numeric'] >= lower_bound) & \n",
    "                                              (group_data[f'{column}_numeric'] <= upper_bound)]\n",
    "                    x = [positions[group]] * len(non_outliers)\n",
    "                    y = non_outliers[f'{column}_numeric']\n",
    "                \n",
    "                if jitter:\n",
    "                    x = np.array(x) + np.random.uniform(-jitter_amount, jitter_amount, len(x))\n",
    "                \n",
    "                ax.scatter(x, y, color=color_palette[group], alpha=0.5, edgecolor='none', zorder=3)\n",
    "        \n",
    "        # Perform Mann-Whitney U test\n",
    "        group1 = data[data['groupname'] == 'CTZ'][f'{column}_numeric']\n",
    "        group2 = data[data['groupname'] == 'No_CTZ'][f'{column}_numeric']\n",
    "        _, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "        \n",
    "        # Add statistical annotation\n",
    "        ax.text(0.5, 0.95, f'p = {p_value:.4f}', \n",
    "                horizontalalignment='center', verticalalignment='top',\n",
    "                transform=ax.transAxes, fontsize=10)\n",
    "        \n",
    "        # Customize the plot\n",
    "        ax.set_title(f\"{cell_type} cells - {column}\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel('Group')\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(column)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        \n",
    "        return ax.get_ylim()  # Return y-axis limits for synchronization\n",
    "    \n",
    "    # Plot for RS cells - MeanFR_baseline\n",
    "    ylim1 = plot_for_cell_type(ax1, 'RS', 'MeanFR_baseline')\n",
    "    \n",
    "    # Plot for FS cells - MeanFR_baseline\n",
    "    ylim2 = plot_for_cell_type(ax2, 'FS', 'MeanFR_baseline')\n",
    "    \n",
    "    # Synchronize y-axis for top row\n",
    "    ymin_top = min(ylim1[0], ylim2[0])\n",
    "    ymax_top = max(ylim1[1], ylim2[1])\n",
    "    ax1.set_ylim(ymin_top, ymax_top)\n",
    "    ax2.set_ylim(ymin_top, ymax_top)\n",
    "    \n",
    "    # Plot for RS cells - user specified column\n",
    "    ylim3 = plot_for_cell_type(ax3, 'RS', column_to_analyze)\n",
    "    \n",
    "    # Plot for FS cells - user specified column\n",
    "    ylim4 = plot_for_cell_type(ax4, 'FS', column_to_analyze)\n",
    "    \n",
    "    # Synchronize y-axis for bottom row\n",
    "    ymin_bottom = min(ylim3[0], ylim4[0])\n",
    "    ymax_bottom = max(ylim3[1], ylim4[1])\n",
    "    ax3.set_ylim(ymin_bottom, ymax_bottom)\n",
    "    ax4.set_ylim(ymin_bottom, ymax_bottom)\n",
    "    \n",
    "    # Set overall title\n",
    "    fig.suptitle(f\"Comparison of MeanFR_baseline and {column_to_analyze} for RS and FS cells\", fontsize=16)\n",
    "    \n",
    "    # Adjust layout and show plot\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #save the plot in the normal directory \n",
    "    plt.savefig(f'/Volumes/MannySSD/figures/opto_psths_comparison/BoxplotsforPSTHsCTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_evoked_FR.svg', format='svg', dpi=300, bbox_inches='tight', transparent=True)\n",
    "\n",
    "    plt.show()\n",
    "# Usage\n",
    "columname = 'MeanFR_stim'  # This can be changed to any other column name\n",
    "plot_custom_boxplot_grid(opto_df_manager, column_to_analyze=columname, \n",
    "                         single_unit_only=None, stim_responsivity=1.0, \n",
    "                         show_outliers=False, show_scatter=True, \n",
    "                         modulation_index='positive', jitter=True, jitter_amount=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def plot_custom_boxplot_grid_plotly(opto_df_manager, column_to_analyze, \n",
    "                                    single_unit_only=None, stim_responsivity=1.0, \n",
    "                                    show_outliers=False, show_scatter=True, \n",
    "                                    modulation_index='positive', jitter=True, jitter_amount=0.1):\n",
    "\n",
    "    fig = make_subplots(rows=2, cols=2, \n",
    "                        subplot_titles=(\"RS cells - MeanFR_baseline\", \"FS cells - MeanFR_baseline\",\n",
    "                                        f\"RS cells - {column_to_analyze}\", f\"FS cells - {column_to_analyze}\"),\n",
    "                        shared_xaxes='columns')\n",
    "\n",
    "    color_palette = {'CTZ': 'purple', 'No_CTZ': 'grey'}\n",
    "\n",
    "    def plot_for_cell_type(row, col, cell_type, column):\n",
    "        # Filter data\n",
    "        data = opto_df_manager.dataframes['basic_metrics'].copy()\n",
    "\n",
    "        # Convert IsSingleUnit to boolean\n",
    "        data['IsSingleUnit'] = data['IsSingleUnit'].apply(lambda x: 1.0 in x or 1 in x if isinstance(x, (np.ndarray, list)) else x == 1.0 or x == 1)\n",
    "\n",
    "        # Extract numeric values from the specified column\n",
    "        column_numeric = f\"{column}_numeric\"\n",
    "        data[column_numeric] = pd.to_numeric(data[column], errors='coerce')\n",
    "\n",
    "        if single_unit_only:\n",
    "            data = data[data['IsSingleUnit']]\n",
    "\n",
    "        if stim_responsivity is not None:\n",
    "            data = data[data['StimResponsivity'] == stim_responsivity]\n",
    "\n",
    "        if modulation_index is not None:\n",
    "            data = data[data['ModulationIndex'] == modulation_index]\n",
    "\n",
    "        # Remove any NaN or infinite values\n",
    "        data = data.dropna(subset=[column_numeric])\n",
    "        data = data[~data[column_numeric].isin([np.inf, -np.inf])]\n",
    "\n",
    "        data = data[data['Cell_Type'] == cell_type]\n",
    "\n",
    "        for group_index, group in enumerate(['CTZ', 'No_CTZ']):\n",
    "            group_data = data[data['groupname'] == group]\n",
    "            \n",
    "            # Create box plot\n",
    "            fig.add_trace(go.Box(\n",
    "                y=group_data[column_numeric],\n",
    "                name=group,\n",
    "                boxpoints=False,  # Hide individual points in boxplot\n",
    "                marker_color=color_palette[group],\n",
    "                showlegend=False,\n",
    "                x0=group_index,\n",
    "                opacity=0.6,  # Make boxplot slightly transparent\n",
    "                line=dict(width=2),  # Increase line width for visibility\n",
    "                zorder=1  # Set lower z-order for boxplot\n",
    "            ), row=row, col=col)\n",
    "\n",
    "            # Add scatter plot if requested\n",
    "            if show_scatter:\n",
    "                x = [group_index] * len(group_data)\n",
    "                y = group_data[column_numeric]\n",
    "\n",
    "                if jitter:\n",
    "                    x = [xi + np.random.uniform(-jitter_amount, jitter_amount) for xi in x]\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    mode='markers',\n",
    "                    marker=dict(color=color_palette[group], size=5, opacity=0.7),\n",
    "                    name=group,\n",
    "                    showlegend=False,\n",
    "                    hovertemplate=f\"Group: {group}<br>Value: %{{y}}<br>Cell ID: %{{customdata}}\",\n",
    "                    customdata=group_data['cid'],\n",
    "                    zorder=2  # Set higher z-order for scatter plot\n",
    "                ), row=row, col=col)\n",
    "\n",
    "        # Perform Mann-Whitney U test\n",
    "        group1 = data[data['groupname'] == 'CTZ'][column_numeric].dropna().astype(float)\n",
    "        group2 = data[data['groupname'] == 'No_CTZ'][column_numeric].dropna().astype(float)\n",
    "        \n",
    "        if len(group1) > 0 and len(group2) > 0:\n",
    "            _, p_value = stats.mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "            \n",
    "            # Add statistical annotation\n",
    "            fig.add_annotation(\n",
    "                xref=\"x domain\", yref=\"y domain\",\n",
    "                x=0.5, y=1,\n",
    "                text=f'p = {p_value:.4f}',\n",
    "                showarrow=False,\n",
    "                row=row, col=col\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: Not enough data to perform Mann-Whitney U test for {cell_type} cells, {column}\")\n",
    "\n",
    "        # Update x-axis\n",
    "        fig.update_xaxes(tickvals=[0, 1], ticktext=['CTZ', 'No_CTZ'], row=row, col=col)\n",
    "\n",
    "    # Plot for each cell type and column\n",
    "    plot_for_cell_type(1, 1, 'RS', 'MeanFR_baseline')\n",
    "    plot_for_cell_type(1, 2, 'FS', 'MeanFR_baseline')\n",
    "    plot_for_cell_type(2, 1, 'RS', column_to_analyze)\n",
    "    plot_for_cell_type(2, 2, 'FS', column_to_analyze)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Comparison of MeanFR_baseline and {column_to_analyze} for RS and FS cells\",\n",
    "        height=800,\n",
    "        width=1200,\n",
    "        boxmode='group'\n",
    "    )\n",
    "\n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Group\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Group\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"MeanFR_baseline\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=column_to_analyze, row=2, col=1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Usage\n",
    "columname = 'MeanFR_stim'  # This can be changed to any other column name\n",
    "fig = plot_custom_boxplot_grid_plotly(opto_df_manager, column_to_analyze=columname, \n",
    "                                      single_unit_only=None, stim_responsivity=1.0, \n",
    "                                      show_outliers=False, show_scatter=True, \n",
    "                                      modulation_index='positive', jitter=True, jitter_amount=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opto_df_manager.dataframes['basic_metrics'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optogenetic_psth_per_recording(self, group1, group2, opto_freq, \n",
    "                                        cell_type=None, is_single_unit=None, \n",
    "                                        stim_responsivity=None, modulation_label=None,\n",
    "                                        time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "    \"\"\"\n",
    "    Plots PSTHs for optogenetic experiments, filtered at the recording level for each group,\n",
    "    and saves the results in a specified directory structure.\n",
    "\n",
    "    Args:\n",
    "        group1 (str): First group name.\n",
    "        group2 (str): Second group name.\n",
    "        opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "        cell_type (str, optional): Cell type to filter.\n",
    "        is_single_unit (float, optional): Single unit filter.\n",
    "        stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "        modulation_label (str, optional): Modulation label filter.\n",
    "        time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "        plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "        smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "    \"\"\"\n",
    "    # Define colors\n",
    "    group_colors = {\n",
    "        'No_CTZ': '#797979',\n",
    "        'CTZ': '#5a00c2'\n",
    "    }\n",
    "\n",
    "    # Fetch data\n",
    "    df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "    if df1.empty and df2.empty:\n",
    "        print(\"Both groups have no data after filtering.\")\n",
    "        return\n",
    "    \n",
    "    # Get the time array and adjust for the specified time range\n",
    "    time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "    if time_range:\n",
    "        time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "        time_array = time_array[time_mask]\n",
    "    else:\n",
    "        time_mask = slice(None)\n",
    "\n",
    "    # Extract LED signal\n",
    "    extracted_led_signal = self.extract_stim_signals_opto()\n",
    "    led_array = extracted_led_signal[0][opto_freq]\n",
    "    led_array = led_array[time_mask]  # Apply the same time mask to LED signal\n",
    "\n",
    "    # Base directory for saving figures\n",
    "    base_dir = \"/Volumes/MannySSD/figures/opto_psths_perrecording\"\n",
    "\n",
    "    # Process each group\n",
    "    for group, df in zip([group1, group2], [df1, df2]):\n",
    "        if df.empty:\n",
    "            print(f\"No data for group {group} after filtering.\")\n",
    "            continue\n",
    "\n",
    "        # Create group directory\n",
    "        group_dir = os.path.join(base_dir, group)\n",
    "        os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "        # Process each unique recording\n",
    "        for recording in df['recording'].unique():\n",
    "            recording_df = df[df['recording'] == recording]\n",
    "            \n",
    "            # Create recording directory\n",
    "            recording_dir = os.path.join(group_dir, recording)\n",
    "            os.makedirs(recording_dir, exist_ok=True)\n",
    "\n",
    "            # Create a new figure for this recording\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "            # Plot PSTH data\n",
    "            data = recording_df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)\n",
    "\n",
    "            elif plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])\n",
    "\n",
    "            # Plot LED signal\n",
    "            ax_led = ax.twinx()\n",
    "            ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "\n",
    "            # Set plot attributes\n",
    "            ax.set_title(f'{group} - {opto_freq} - Recording: {recording}')\n",
    "            ax.set_xlabel('Time (ms)')\n",
    "            ax.set_ylabel('Average Spike Rate')\n",
    "            ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "            ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "            # Add legends\n",
    "            ax.legend(loc='upper left')\n",
    "            ax_led.legend(loc='upper right')\n",
    "\n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            fig_path = os.path.join(recording_dir, f'PSTH_{opto_freq}.png')\n",
    "            plt.savefig(fig_path)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Saved figure for {group} - {recording} at {fig_path}\")\n",
    "\n",
    "    print(\"Finished processing all recordings.\")\n",
    "\n",
    "opto_df_manager.plot_optogenetic_psth_per_recording('No_CTZ', 'CTZ', '8 Hz LED', \n",
    "                                      cell_type='RS', \n",
    "                                      is_single_unit=None, \n",
    "                                      stim_responsivity=None, \n",
    "                                      time_range=None, \n",
    "                                      plot_mode='sem', \n",
    "                                      smoothing_window=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the waveforms for the opto LED df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to do "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the distriubtion of units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import os\n",
    "\n",
    "### this used to work but need to fix \n",
    "\n",
    "def plot_box_and_strip_with_plotly(obj, groups=None, stimulations=None, show_outliers=True, hue_order=None, \n",
    "                                   directory=None, file_name=None, ylim=None, \n",
    "                                   modulation_label=None, cell_type=None, \n",
    "                                   is_single_unit=None, stim_responsivity=None):\n",
    "    \"\"\"\n",
    "    Plots interactive boxplots and stripplots for specified groups and stimulations using Plotly, with metadata accessible via hover.\n",
    "\n",
    "    Args:\n",
    "        obj: The object containing the `prepare_for_boxplot` method.\n",
    "        groups (list of str, optional): List of groups to include in the plot.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "        show_outliers (bool, optional): Whether to show outliers.\n",
    "        hue_order (list, optional): Order of the hue levels.\n",
    "        directory (str, optional): Directory to save the plot.\n",
    "        file_name (str, optional): File name for saving the plot.\n",
    "        ylim (tuple, optional): Limits for the y-axis.\n",
    "        modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        cell_type (str, optional): Filter for specific cell types ('FS', 'RS', etc.).\n",
    "        is_single_unit (float, optional): Filter for single units (1.0), multi-units (0.0), or all (None).\n",
    "        stim_responsivity (float, optional): Filter by stimulus responsivity (1.0, 0.0, -1.0).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame used for the plot.\n",
    "    \"\"\"\n",
    "    # Prepare data for boxplot\n",
    "    boxplot_df = obj.prepare_for_boxplot()\n",
    "\n",
    "    # Apply filtering as needed\n",
    "    if groups:\n",
    "        boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "    if stimulations:\n",
    "        boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "    if modulation_label:\n",
    "        boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "    if cell_type:\n",
    "        boxplot_df = boxplot_df[boxplot_df['Cell_Type'] == cell_type]\n",
    "    if is_single_unit is not None:\n",
    "        boxplot_df = boxplot_df[boxplot_df['IsSingleUnit'] == is_single_unit]\n",
    "    if stim_responsivity is not None:\n",
    "        boxplot_df = boxplot_df[boxplot_df['StimResponsivity'] == stim_responsivity]\n",
    "    \n",
    "    # Create a boxplot with stripplot overlay using Plotly\n",
    "    fig = px.box(boxplot_df, \n",
    "                 x='Stimulation', \n",
    "                 y='mean_stimulation', \n",
    "                 color='Group', \n",
    "                 points='all' if show_outliers else 'outliers',\n",
    "                 hover_data=['recordingname', 'ModulationLabel', 'cid', 'Cell_Type', 'Template_Channel', 'LaminarLabel', 'IsSingleUnit', 'TroughToPeak_duration', \n",
    "                             'PeakToPeak_ratio', 'peak1_normalized_amplitude', 'Peak1ToTrough_ratio', 'Peak2ToTrough_ratio', 'SpikeHalfWidth', 'MeanFR_baseline', 'StimResponsivity'],\n",
    "                 color_discrete_map={\n",
    "                     'No_CTZ': '#797979',\n",
    "                     'CTZ': '#5a00c2'\n",
    "                 })\n",
    "    \n",
    "    # Adjust y-axis limits if provided\n",
    "    if ylim:\n",
    "        fig.update_yaxes(range=ylim)\n",
    "\n",
    "    # Add title and labels\n",
    "\n",
    "\n",
    "    # Save the figure if directory and file name are provided\n",
    "    if directory and file_name:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        file_path = os.path.join(directory, f'{file_name}.html')\n",
    "        fig.write_html(file_path)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Comparison of Mean Stimulation Across Groups and Stimulations',\n",
    "        xaxis_title='Stimulation Type',\n",
    "        yaxis_title='Mean Stimulation',\n",
    "        width=1000,  # Adjust width as needed\n",
    "        height=800,  # Adjust height as needed\n",
    "    )\n",
    "    \n",
    "        # Update the layout to make the figure larger\n",
    "\n",
    "    \n",
    "    # Show the figure in a standalone browser window\n",
    "    #pyo.plot(fig, auto_open=True)\n",
    "    #plot the figure in the notebook\n",
    "    fig.show()\n",
    "    \n",
    "# Call the function with your object and desired filtering options\n",
    "boxplot_df = plot_box_and_strip_with_plotly(\n",
    "    whisker_df_manager,  # Your object containing the data and methods\n",
    "    groups=['CTZ', 'No_CTZ'],  # Filter by groups\n",
    "    stimulations=['8Hz'],  # Filter by stimulations\n",
    "    show_outliers=True,  # Show or hide outliers\n",
    "    directory='/Volumes/MannySSD/figures/comparing_conditions_dynamic_plotly',  # Directory to save the plot\n",
    "    file_name='plot_name',  # File name for saving the plot\n",
    "    ylim=None,  # Set y-axis limits\n",
    "    modulation_label='positive',  # Filter by modulation label\n",
    "    cell_type='FS',  # Filter by cell type ('FS', 'RS')\n",
    "    is_single_unit=1.0,  # Filter for single units\n",
    "    stim_responsivity=1.0  # Filter by stimulus responsivity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see distribution of FS and RS for opto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opto_df_manager.plot_trough_to_peak_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at each indidual recording "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctz_df1, noctz_df2 = opto_df_manager.compare_groups('CTZ', 'No_CTZ', '8 Hz LED', 'RS', is_single_unit=1.0, stim_responsivity=1.0)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def process_data(df, time_range):\n",
    "    \n",
    "    time_array = whisker_df_manager.eed.relative_time_ms['relative_time_ms']\n",
    "    \n",
    "    if time_range:\n",
    "        time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "        time_array = time_array[time_mask]\n",
    "    else:\n",
    "        time_mask = slice(None)\n",
    "    \n",
    "    grouped = df.groupby('recordingname')\n",
    "    grouped_data = grouped['PSTHs_raw'].apply(lambda x: np.mean([np.array(i)[time_mask] for i in x], axis=0))\n",
    "    \n",
    "    return grouped_data\n",
    "\n",
    "ctz_df1_grouped = process_data(ctz_df1, time_range=(-10, 600))\n",
    "noctz_df2_grouped = process_data(noctz_df2, time_range=(-10, 600))\n",
    "\n",
    "\n",
    "# Create a color map\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(ctz_df1_grouped)))\n",
    "\n",
    "# Create a 1xN subplot\n",
    "fig, axs = plt.subplots(1, len(ctz_df1_grouped), figsize=(15, 5))\n",
    "\n",
    "# Iterate over the grouped data\n",
    "for ax, data, color, name in zip(axs, ctz_df1_grouped, colors, ctz_df1_grouped.index):\n",
    "    ax.plot(data, color=color)\n",
    "    ax.set_title(name)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color map\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(noctz_df2_grouped)))\n",
    "\n",
    "# Create a 1xN subplot\n",
    "fig, axs = plt.subplots(1, len(noctz_df2_grouped), figsize=(15, 5))\n",
    "\n",
    "# Iterate over the grouped data\n",
    "for ax, data, color, name in zip(axs, noctz_df2_grouped, colors, noctz_df2_grouped.index):\n",
    "    ax.plot(data, color=color)\n",
    "    ax.set_title(name)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot and save rasters and Wavefornms \n",
    "RS_SUA_df = opto_df_manager.get_filtered_data('basic_metrics', is_single_unit=1.0, cell_type='RS', stim_responsivity=None)\n",
    "\n",
    "def iterate_and_plot(df, plot_combined_psth_and_raster, output_dir, folder_name, time_window=None, smoothing_window=10, show=False):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create the folder within the output directory\n",
    "    save_dir = os.path.join(output_dir, folder_name)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    unique_combinations = df[['groupname', 'recordingname', 'cid']].drop_duplicates()\n",
    "\n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        groupname = row['groupname']\n",
    "        recordingname = row['recordingname']\n",
    "        cid = row['cid']\n",
    "\n",
    "        fig = plot_combined_psth_and_raster(\n",
    "            groupname, recordingname, cid, time_window=time_window, smoothing_window=smoothing_window, show=show\n",
    "        )\n",
    "\n",
    "        if not show:\n",
    "            save_path = os.path.join(save_dir, f'plot_{groupname}_{recordingname}_{cid}.png')\n",
    "            fig.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "iterate_and_plot(RS_SUA_df, whisker_df_manager.plot_combined_psth_and_raster, '/Volumes/MannySSD/figures', 'RSSUA_opto', time_window=None, smoothing_window=10, show=False)\n",
    "\n",
    "def iterate_and_plot_normalizedwaveforms(df, output_dir, folder_name, show=False):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create the folder within the output directory\n",
    "    save_dir = os.path.join(output_dir, folder_name)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    unique_combinations = df[['groupname', 'recordingname', 'cid']].drop_duplicates()\n",
    "\n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        groupname = row['groupname']\n",
    "        recordingname = row['recordingname']\n",
    "        cid = row['cid']\n",
    "\n",
    "        condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "        waveforms = df.loc[condition, 'Normalized_Template_Waveform'].values\n",
    "\n",
    "        if waveforms.size > 0:\n",
    "            waveform = waveforms[0]\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.plot(waveform)\n",
    "            ax.set_title(f'Normalized Template Waveform for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                save_path = os.path.join(save_dir, f'normalized_waveform_{groupname}_{recordingname}_{cid}.svg')\n",
    "                fig.savefig(save_path)\n",
    "            plt.close(fig)          \n",
    "def iterate_and_plot_unnormalizedwaveforms(df, output_dir, folder_name, show=False):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create the folder within the output directory\n",
    "    save_dir = os.path.join(output_dir, folder_name)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    unique_combinations = df[['groupname', 'recordingname', 'cid']].drop_duplicates()\n",
    "\n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        groupname = row['groupname']\n",
    "        recordingname = row['recordingname']\n",
    "        cid = row['cid']\n",
    "\n",
    "        condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "        waveforms = df.loc[condition, 'UnNormalized_Template_Waveform'].values\n",
    "\n",
    "        if waveforms.size > 0:\n",
    "            waveform = waveforms[0]\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.plot(waveform)\n",
    "            ax.set_title(f'UnNormalized Template Waveform for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "            else:\n",
    "                save_path = os.path.join(save_dir, f'unnormalized_waveform_{groupname}_{recordingname}_{cid}.svg')\n",
    "                fig.savefig(save_path)\n",
    "            plt.close(fig)\n",
    "\n",
    "iterate_and_plot_normalizedwaveforms(RS_SUA_df, '/Volumes/MannySSD/figures', 'RSSUA_opto_early_normalized_waveforms', show=False)\n",
    "iterate_and_plot_unnormalizedwaveforms(RS_SUA_df, '/Volumes/MannySSD/figures', 'RSSUA_opto_early_unnormalized_waveforms', show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distribuition fun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the KS Test:\n",
    "KS Statistic: Measures the maximum distance between the empirical cumulative distribution function (CDF) of the data and the CDF of the fitted distribution. A lower KS statistic means the distribution fits the data better.\n",
    "KS p-value: The p-value in the KS test tells you the probability that the observed data could have come from the fitted distribution. A higher p-value indicates that the difference between the observed and expected distribution is small, meaning the fit is good.\n",
    "Why Use the Highest KS p-value?\n",
    "Higher p-value = Better Fit: A higher p-value suggests that the observed data is not significantly different from the expected distribution, implying that the fitted distribution is a good match for the data.\n",
    "Lower p-value = Poorer Fit: A lower p-value suggests that there is a significant difference between the observed and expected distributions, meaning the fit is not good.\n",
    "Example:\n",
    "If the KS p-value is 0.8, it suggests that there is an 80% chance that the data could come from the fitted distribution (a good fit).\n",
    "If the KS p-value is 0.05, it suggests that there is only a 5% chance that the data could come from the fitted distribution (a poor fit).\n",
    "Summary:\n",
    "Select the Distribution with the Highest KS p-value: This approach ensures that the distribution that best fits the data is chosen.\n",
    "Avoid Selecting the Lowest KS p-value: A low p-value indicates that the distribution is not a good fit for the dat\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/scipy/scipy/issues/11806 \n",
    "\n",
    "https://stackoverflow.com/questions/37487830/how-to-find-probability-distribution-and-parameters-for-real-data-python-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "def evaluate_distributions(data):\n",
    "    dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n",
    "    results = []\n",
    "    \n",
    "    num_bins = 20\n",
    "    x_range = np.linspace(-1, 1, num_bins + 1)\n",
    "    \n",
    "    for dist_name in dist_names:\n",
    "        dist = getattr(st, dist_name)\n",
    "        params = dist.fit(data)\n",
    "        \n",
    "        bin_counts, bin_edges = np.histogram(data, bins=x_range, density=True)\n",
    "        pdf_fitted = dist.pdf((bin_edges[:-1] + bin_edges[1:]) / 2, *params[:-2], loc=params[-2], scale=params[-1])\n",
    "        \n",
    "        sse = np.sum((pdf_fitted - bin_counts) ** 2)\n",
    "        log_likelihood = np.sum(np.log(dist.pdf(data, *params)))\n",
    "        aic = 2 * len(params) - 2 * log_likelihood\n",
    "        bic = len(params) * np.log(len(data)) - 2 * log_likelihood\n",
    "        \n",
    "        ks_statistic, ks_pvalue = st.kstest(data, dist_name, args=params)\n",
    "        \n",
    "        results.append({\n",
    "            \"distribution\": dist_name,\n",
    "            \"sumsquare_error\": sse,\n",
    "            \"aic\": aic,\n",
    "            \"bic\": bic,\n",
    "            \"ks_statistic\": ks_statistic,\n",
    "            \"ks_pvalue\": ks_pvalue,\n",
    "            \"params\": params\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"ks_pvalue\", ascending=False)\n",
    "    return results_df\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "\n",
    "# Initialize a list to store the results\n",
    "best_fits = []\n",
    "\n",
    "# Iterate over each combination of groupname, Cell_Type, and IsSingleUnit\n",
    "for (group, cell_type, is_sua), group_data in df.groupby(['groupname', 'Cell_Type', 'IsSingleUnit']):\n",
    "    \n",
    "    # Extract the Modulation Index data\n",
    "    data = group_data['ModulationIndex_Numeric'].dropna().values\n",
    "    \n",
    "    # Skip if the data is too small for analysis\n",
    "    if len(data) < 5:\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the best distribution\n",
    "    result = evaluate_distributions(data)\n",
    "    best_fit = result.iloc[0]  # Get the best fit (highest p-value)\n",
    "    \n",
    "    # Store the result with metadata\n",
    "    best_fits.append({\n",
    "        \"groupname\": group,\n",
    "        \"Cell_Type\": cell_type,\n",
    "        \"IsSingleUnit\": is_sua,\n",
    "        \"best_distribution\": best_fit['distribution'],\n",
    "        \"ks_pvalue\": best_fit['ks_pvalue'],\n",
    "        \"aic\": best_fit['aic'],\n",
    "        \"bic\": best_fit['bic'],\n",
    "        \"params\": best_fit['params'],\n",
    "        \"ModulationIndex_Numeric\": list(data)  # Store the data used for fitting\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "best_fits_df = pd.DataFrame(best_fits)\n",
    "\n",
    "# Save the best fits to a CSV file\n",
    "best_fits_df.to_csv('/Volumes/MannySSD/figures/laminaranalysis/bestfit_output.csv', index=False)\n",
    "\n",
    "# Load the CSV file (if you need to reload)\n",
    "best_fits_df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/bestfit_output.csv')\n",
    "\n",
    "# Determine the most frequent best distribution across all combinations\n",
    "best_overall_distribution = best_fits_df['best_distribution'].mode()[0]\n",
    "print(f\"The best overall distribution is: {best_overall_distribution}\")\n",
    "\n",
    "# Aggregate all data and fit the best overall distribution\n",
    "all_data = np.concatenate(best_fits_df['ModulationIndex_Numeric'].apply(lambda x: eval(x)).values)\n",
    "\n",
    "# Fit the best overall distribution to the aggregated data\n",
    "dist = getattr(st, best_overall_distribution)\n",
    "params = dist.fit(all_data)\n",
    "\n",
    "# Generate synthetic data using the best overall distribution\n",
    "synthetic_data = dist.rvs(*params, size=1000)\n",
    "print(synthetic_data[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "# Load the best fits DataFrame (if not already loaded)\n",
    "best_fits_df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/bestfit_output.csv')\n",
    "\n",
    "# Determine the most frequent best distribution across all combinations\n",
    "best_overall_distribution = best_fits_df['best_distribution'].mode()[0]\n",
    "print(f\"The best overall distribution is: {best_overall_distribution}\")\n",
    "\n",
    "# Aggregate all ModulationIndex_Numeric data\n",
    "all_data = np.concatenate(best_fits_df['ModulationIndex_Numeric'].apply(lambda x: eval(x)).values)\n",
    "\n",
    "# Fit the best overall distribution to the aggregated data\n",
    "dist = getattr(st, best_overall_distribution)\n",
    "params = dist.fit(all_data)\n",
    "\n",
    "# Print the parameters of the best overall distribution\n",
    "print(f\"Parameters for {best_overall_distribution}: {params}\")\n",
    "\n",
    "# Generate synthetic data using the best overall distribution\n",
    "synthetic_data = dist.rvs(*params, size=1000)\n",
    "\n",
    "# Show the first few values of the synthetic data\n",
    "print(\"Synthetic data (first 10 values):\", synthetic_data[:10])\n",
    "\n",
    "# Optional: Save the synthetic data and parameters for further use\n",
    "pd.DataFrame(synthetic_data, columns=['synthetic_data']).to_csv('/Volumes/MannySSD/figures/laminaranalysis/synthetic_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "# Assuming best_overall_distribution = 'weibull_max' and params are already determined\n",
    "dist = getattr(st, best_overall_distribution)\n",
    "params = dist.fit(all_data)\n",
    "\n",
    "# Print the parameters of the distribution\n",
    "print(f\"Parameters for {best_overall_distribution}: {params}\")\n",
    "\n",
    "# Define the range of values over which to calculate the PDF\n",
    "x_values = np.linspace(-1, 1, 1000)\n",
    "\n",
    "# Calculate the PDF using the fitted parameters\n",
    "pdf_values = dist.pdf(x_values, *params[:-2], loc=params[-2], scale=params[-1])\n",
    "\n",
    "# Plot only the histogram first to check the data distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "plt.title(f'Empirical Data - Modulation Index')\n",
    "plt.xlabel('Modulation Index')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Now plot the PDF over the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "plt.plot(x_values, pdf_values, label=f'{best_overall_distribution} PDF', color='blue')\n",
    "plt.title(f'Empirical Data and PDF - {best_overall_distribution}')\n",
    "plt.xlabel('Modulation Index')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Try fitting a different distribution for comparison\n",
    "alternative_dist = 'norm'  # Example: change this to any other distribution you want to test\n",
    "alt_dist = getattr(st, alternative_dist)\n",
    "alt_params = alt_dist.fit(all_data)\n",
    "alt_pdf_values = alt_dist.pdf(x_values, *alt_params[:-2], loc=alt_params[-2], scale=alt_params[-1])\n",
    "\n",
    "# Plot the alternative PDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "plt.plot(x_values, alt_pdf_values, label=f'{alternative_dist} PDF', color='red')\n",
    "plt.title(f'Empirical Data and PDF - {alternative_dist}')\n",
    "plt.xlabel('Modulation Index')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Distributions to Try:\n",
    "Gamma Distribution: Often used to model skewed data, could capture some of the asymmetry.\n",
    "Exponential Weibull (exponweib): A flexible distribution that can model different types of data with varying skewness.\n",
    "Lognormal (lognorm): Might capture the multiplicative effects in data and handle skewness better.\n",
    "Conclusion:\n",
    "Iterative Testing: By testing multiple distributions, you can identify which one provides the best visual and statistical fit for your data.\n",
    "Parameter Analysis: Ensure that the parameters make sense given the context of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "def evaluate_distributions(data):\n",
    "    dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n",
    "    results = []\n",
    "    \n",
    "    num_bins = 20\n",
    "    x_range = np.linspace(-1, 1, num_bins + 1)\n",
    "    \n",
    "    for dist_name in dist_names:\n",
    "        dist = getattr(st, dist_name)\n",
    "        params = dist.fit(data)\n",
    "        \n",
    "        bin_counts, bin_edges = np.histogram(data, bins=x_range, density=True)\n",
    "        pdf_fitted = dist.pdf((bin_edges[:-1] + bin_edges[1:]) / 2, *params[:-2], loc=params[-2], scale=params[-1])\n",
    "        \n",
    "        sse = np.sum((pdf_fitted - bin_counts) ** 2)\n",
    "        log_likelihood = np.sum(np.log(dist.pdf(data, *params)))\n",
    "        aic = 2 * len(params) - 2 * log_likelihood\n",
    "        bic = len(params) * np.log(len(data)) - 2 * log_likelihood\n",
    "        \n",
    "        ks_statistic, ks_pvalue = st.kstest(data, dist_name, args=params)\n",
    "        \n",
    "        results.append({\n",
    "            \"distribution\": dist_name,\n",
    "            \"sumsquare_error\": sse,\n",
    "            \"aic\": aic,\n",
    "            \"bic\": bic,\n",
    "            \"ks_statistic\": ks_statistic,\n",
    "            \"ks_pvalue\": ks_pvalue,\n",
    "            \"params\": params\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values(by=\"ks_pvalue\", ascending=False)\n",
    "    return results_df\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "\n",
    "# Initialize a list to store the results\n",
    "best_fits = []\n",
    "\n",
    "# Iterate over each combination of groupname, Cell_Type, and IsSingleUnit\n",
    "for (group, cell_type, is_sua), group_data in df.groupby(['groupname', 'Cell_Type', 'IsSingleUnit']):\n",
    "    \n",
    "    # Extract the Modulation Index data\n",
    "    data = group_data['ModulationIndex_Numeric'].dropna().values\n",
    "    \n",
    "    # Skip if the data is too small for analysis\n",
    "    if len(data) < 5:\n",
    "        continue\n",
    "    \n",
    "    # Evaluate the best distribution\n",
    "    result = evaluate_distributions(data)\n",
    "    best_fit = result.iloc[0]  # Get the best fit (highest p-value)\n",
    "    \n",
    "    # Store the result with metadata\n",
    "    best_fits.append({\n",
    "        \"groupname\": group,\n",
    "        \"Cell_Type\": cell_type,\n",
    "        \"IsSingleUnit\": is_sua,\n",
    "        \"best_distribution\": best_fit['distribution'],\n",
    "        \"ks_pvalue\": best_fit['ks_pvalue'],\n",
    "        \"aic\": best_fit['aic'],\n",
    "        \"bic\": best_fit['bic'],\n",
    "        \"params\": best_fit['params'],\n",
    "        \"ModulationIndex_Numeric\": list(data)  # Store the data used for fitting\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "best_fits_df = pd.DataFrame(best_fits)\n",
    "\n",
    "# Save the best fits to a CSV file\n",
    "best_fits_df.to_csv('/Volumes/MannySSD/figures/laminaranalysis/bestfit_output.csv', index=False)\n",
    "\n",
    "# Load the CSV file (if you need to reload)\n",
    "best_fits_df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/bestfit_output.csv')\n",
    "\n",
    "# Determine the most frequent best distribution across all combinations\n",
    "best_overall_distribution = best_fits_df['best_distribution'].mode()[0]\n",
    "print(f\"The best overall distribution is: {best_overall_distribution}\")\n",
    "\n",
    "# Aggregate all data and fit the best overall distribution\n",
    "all_data = np.concatenate(best_fits_df['ModulationIndex_Numeric'].apply(lambda x: eval(x)).values)\n",
    "\n",
    "# Fit the best overall distribution to the aggregated data\n",
    "dist = getattr(st, best_overall_distribution)\n",
    "params = dist.fit(all_data)\n",
    "\n",
    "# Generate synthetic data using the best overall distribution\n",
    "synthetic_data = dist.rvs(*params, size=1000)\n",
    "print(synthetic_data[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test and plot additional distributions\n",
    "def plot_distribution(dist_name, all_data, x_values):\n",
    "    dist = getattr(st, dist_name)\n",
    "    params = dist.fit(all_data)\n",
    "    \n",
    "    # Print the parameters of the distribution\n",
    "    print(f\"Parameters for {dist_name}: {params}\")\n",
    "    \n",
    "    # Calculate the PDF using the fitted parameters\n",
    "    pdf_values = dist.pdf(x_values, *params[:-2], loc=params[-2], scale=params[-1])\n",
    "    \n",
    "    # Plot the PDF over the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "    plt.plot(x_values, pdf_values, label=f'{dist_name} PDF', color='blue')\n",
    "    plt.title(f'Empirical Data and PDF - {dist_name}')\n",
    "    plt.xlabel('Modulation Index')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example of trying different distributions\n",
    "distributions_to_try = ['gamma', 'exponweib', 'lognorm']  # Add more if needed\n",
    "\n",
    "for dist_name in distributions_to_try:\n",
    "    plot_distribution(dist_name, all_data, x_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Gamma Distribution:\n",
    "The Gamma distribution provides a moderately better fit than the previous attempts (like Weibull Max), but it still fails to capture the peaks at the extremes (around -1 and 1).\n",
    "The PDF has a smooth curve but doesn’t align well with the highly skewed nature of your data at the extremes.\n",
    "2. Lognormal Distribution:\n",
    "Similar to the Gamma distribution, the Lognormal distribution also fails to capture the extreme peaks.\n",
    "The PDF is more spread out and has a flatter curve, which suggests that it’s not matching the data's density at the extreme ends (-1 and 1).\n",
    "3. Exponential Weibull (exponweib) Distribution:\n",
    "The exponweib distribution shows an extreme spike at -1, which suggests that the parameters might be leading to a very poor fit.\n",
    "The density value is incredibly high (in the order of 10^10), which is not realistic and indicates that the distribution is overfitting to the extreme data points.\n",
    "Key Observations:\n",
    "Extremes Are Poorly Fitted: None of these distributions, including gamma, lognorm, and exponweib, are capturing the behavior of the data well, especially at the extremes of the modulation index (-1 and 1).\n",
    "Highly Skewed Data: Your data is heavily skewed, with significant peaks at both ends, which makes it challenging to model with standard distributions.\n",
    "Next Steps:\n",
    "Consider a Mixture Model:\n",
    "Since your data might have two distinct modes (bimodal distribution), a mixture model that combines two or more distributions (e.g., two normal distributions) might be more appropriate.\n",
    "You can use Gaussian Mixture Models (GMMs) to fit this data.\n",
    "Manual Binning/Analysis:\n",
    "Consider breaking down the data into smaller bins and analyzing each segment separately. This might help in fitting different distributions to different sections of the data.\n",
    "Try a Custom Distribution:\n",
    "If none of the standard distributions work, you might need to create a custom distribution that can better capture the specific characteristics of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Fit a Gaussian Mixture Model with 2 components\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(all_data.reshape(-1, 1))\n",
    "\n",
    "# Generate the GMM PDF\n",
    "x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "pdf_values = np.exp(gmm.score_samples(x_values))\n",
    "\n",
    "# Plot the GMM PDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "plt.plot(x_values, pdf_values, label='GMM PDF', color='green')\n",
    "plt.title('Empirical Data and GMM PDF')\n",
    "plt.xlabel('Modulation Index')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " seems that the Gaussian Mixture Model (GMM) is also struggling to fit the data well, especially around the extremes. The GMM PDF shows a very high density at -1, but it doesn't capture the rest of the distribution effectively.\n",
    "\n",
    "Observations:\n",
    "Overfitting to Extremes: The GMM appears to be overfitting to the extreme values (particularly -1), leading to a sharp spike that doesn’t reflect the true spread of the data.\n",
    "Flat Regions: The GMM fails to capture the distribution in the middle range of the modulation index values, leading to a near-zero density across most of the plot.\n",
    "Possible Reasons:\n",
    "Highly Skewed Data: Your data's extreme skewness and the presence of strong peaks at both ends (-1 and 1) make it difficult for typical parametric models, including GMMs, to fit the data well.\n",
    "Bimodal or Multimodal Nature: If your data is truly bimodal or even multimodal, the GMM might need more than two components, or it might require a more complex mixture model.\n",
    "Next Steps:\n",
    "Increase the Number of Components in GMM:\n",
    "Try fitting the GMM with more components (e.g., 3 or 4) to see if it can better capture the multimodal nature of your data.\n",
    "Non-Parametric Methods:\n",
    "Consider using non-parametric methods like Kernel Density Estimation (KDE) to estimate the PDF. KDE does not assume any specific parametric form, making it more flexible for complex data distributions.\n",
    "Manual Segmentation:\n",
    "Manually segment the data into different regions and fit different distributions to each segment. This might help in better understanding and modeling the data.\n",
    "Example: Fitting GMM with More Components\n",
    "Here’s how you can try fitting a GMM with more components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Fit a Gaussian Mixture Model with more components (e.g., 3 components)\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(all_data.reshape(-1, 1))\n",
    "\n",
    "# Generate the GMM PDF\n",
    "x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "pdf_values = np.exp(gmm.score_samples(x_values))\n",
    "\n",
    "# Plot the GMM PDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "plt.plot(x_values, pdf_values, label='GMM PDF', color='green')\n",
    "plt.title('Empirical Data and GMM PDF (3 Components)')\n",
    "plt.xlabel('Modulation Index')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Fit Kernel Density Estimation (KDE)\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(all_data.reshape(-1, 1))\n",
    "\n",
    "# Generate the KDE PDF\n",
    "pdf_values = np.exp(kde.score_samples(x_values))\n",
    "\n",
    "# Plot the KDE PDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_data, bins=20, density=True, alpha=0.5, label='Empirical Data', color='gray')\n",
    "plt.plot(x_values, pdf_values, label='KDE PDF', color='purple')\n",
    "plt.title('Empirical Data and KDE PDF')\n",
    "plt.xlabel('Modulation Index')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "GMM with 3 Components:\n",
    "The Gaussian Mixture Model (GMM) with 3 components still struggles to capture the distribution effectively. The density is extremely high at -1, but it fails to model the rest of the data properly.\n",
    "This suggests that the data's structure might be too complex or too heavily skewed for a simple GMM to capture, even with additional components.\n",
    "Kernel Density Estimation (KDE):\n",
    "The KDE approach provides a much better fit to the data. The KDE curve captures the bimodal nature of the data, with peaks around -1 and 1, and it follows the distribution more closely across the entire range.\n",
    "This method is more flexible as it does not assume a specific parametric form, which allows it to adapt to the data's shape more effectively.\n",
    "Key Insights:\n",
    "GMM Limitations: The GMM, even with additional components, is not flexible enough to capture the heavy skewness and extreme peaks present in your data.\n",
    "KDE Strength: The KDE method shows clear advantages in this case, as it is non-parametric and can adapt to the data's unique characteristics without being constrained by a predefined distribution form.\n",
    "Next Steps:\n",
    "Consider Using KDE for Your Analysis:\n",
    "Given the results, KDE might be the best method for estimating the Probability Density Function (PDF) of your data.\n",
    "This approach is particularly useful when the data does not follow a common distribution or when it exhibits complex multimodal behavior.\n",
    "Fine-Tune KDE:\n",
    "You can adjust the bandwidth parameter in KDE to see how different levels of smoothing affect the PDF.\n",
    "A smaller bandwidth might capture more detailed features, while a larger bandwidth will provide a smoother curve.\n",
    "Apply KDE to Subsets:\n",
    "If your data contains different subgroups (e.g., by treatment or cell type), you might apply KDE to each subgroup to understand the differences in distribution more clearly.\n",
    "Use KDE for Further Modeling:\n",
    "You can use the KDE-derived PDF in subsequent analyses or simulations, especially if you need to generate synthetic data or compute probabilities based on your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o proceed with the 2x2 comparison as before, we'll follow these steps:\n",
    "\n",
    "Steps:\n",
    "Apply KDE to Each Subgroup:\n",
    "We'll apply Kernel Density Estimation (KDE) to each combination of Cell_Type (FS or RS) and Treatment (CTZ or No_CTZ).\n",
    "Plot the Distributions:\n",
    "We'll create a 2x2 grid of plots where each subplot corresponds to one combination of Cell_Type and Treatment.\n",
    "The KDE curve for each subgroup will be plotted along with the histogram of the data.\n",
    "Overlay the Distributions:\n",
    "For easier comparison, we'll overlay the KDE distributions of CTZ vs. No_CTZ within the same plot for each Cell_Type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "# Define the groups\n",
    "cell_types = ['FS', 'RS']\n",
    "treatments = ['CTZ', 'No_CTZ']\n",
    "\n",
    "# Prepare the 2x2 plot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Iterate over each combination of cell type and treatment\n",
    "for i, cell_type in enumerate(cell_types):\n",
    "    for j, treatment in enumerate(treatments):\n",
    "        \n",
    "        # Filter the data for the current combination\n",
    "        data = df[(df['Cell_Type'] == cell_type) & (df['groupname'] == treatment)]['ModulationIndex_Numeric'].dropna().values\n",
    "        \n",
    "        # Fit KDE\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(data.reshape(-1, 1))\n",
    "        x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "        pdf_values = np.exp(kde.score_samples(x_values))\n",
    "        \n",
    "        # Plot the histogram and KDE for this group\n",
    "        axs[i, j].hist(data, bins=20, density=True, alpha=0.5, label=f'{treatment} Data', color='gray')\n",
    "        axs[i, j].plot(x_values, pdf_values, label=f'{treatment} KDE', color='purple')\n",
    "        axs[i, j].set_title(f'{cell_type} Cells - {treatment}')\n",
    "        axs[i, j].set_xlabel('Modulation Index')\n",
    "        axs[i, j].set_ylabel('Density')\n",
    "        axs[i, j].legend()\n",
    "        axs[i, j].grid(True)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "# Define the groups\n",
    "cell_types = ['FS', 'RS']\n",
    "is_single_unit_types = [1.0, 0.0]  # 1.0 for SUA, 0.0 for MUA\n",
    "treatments = ['CTZ', 'No_CTZ']\n",
    "\n",
    "# Define colors for each treatment\n",
    "colors = {'CTZ': 'purple', 'No_CTZ': 'gray'}\n",
    "\n",
    "# Prepare the 2x2 plot with shared y-axis\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharey=True)\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n",
    "# Iterate over each combination of cell type and unit type (SUA or MUA)\n",
    "for i, cell_type in enumerate(cell_types):\n",
    "    for j, is_sua in enumerate(is_single_unit_types):\n",
    "        ax = axs[i, j]\n",
    "        \n",
    "        # Iterate over each treatment to overlay them on the same plot\n",
    "        for treatment in treatments:\n",
    "            # Filter the data for the current combination\n",
    "            data = df[(df['Cell_Type'] == cell_type) & (df['IsSingleUnit'] == is_sua) & (df['groupname'] == treatment)]['ModulationIndex_Numeric'].dropna().values\n",
    "            \n",
    "            # Fit KDE\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(data.reshape(-1, 1))\n",
    "            x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "            pdf_values = np.exp(kde.score_samples(x_values))\n",
    "            \n",
    "            # Plot the histogram and KDE for this group\n",
    "            ax.hist(data, bins=20, density=True, alpha=0.7, label=f'{treatment} (N={len(data)})', color=colors[treatment], edgecolor='black')\n",
    "            ax.plot(x_values, pdf_values, color=colors[treatment], linewidth=2)\n",
    "        \n",
    "        # Set title and labels\n",
    "        cell_label = 'SUA' if is_sua == 1.0 else 'MUA'\n",
    "        ax.set_title(f'{cell_type} {cell_label} Cells - Normalized Distribution (PDF)', fontsize=12)\n",
    "        ax.set_xlabel('Modulation Index', fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_ylim(0, 6)  # Adjust this value to fit your data range\n",
    "\n",
    "# Adjust the layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Iterate over each combination of cell type and unit type (SUA or MUA)\n",
    "results = []\n",
    "for cell_type in cell_types:\n",
    "    for is_sua in is_single_unit_types:\n",
    "        # Filter the data for the two treatments\n",
    "        data_ctz = df[(df['Cell_Type'] == cell_type) & (df['IsSingleUnit'] == is_sua) & (df['groupname'] == 'CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        data_no_ctz = df[(df['Cell_Type'] == cell_type) & (df['IsSingleUnit'] == is_sua) & (df['groupname'] == 'No_CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        \n",
    "        # Perform KS test\n",
    "        ks_statistic, p_value = ks_2samp(data_ctz, data_no_ctz)\n",
    "        \n",
    "        # Store results\n",
    "        cell_label = 'SUA' if is_sua == 1.0 else 'MUA'\n",
    "        results.append({\n",
    "            \"Cell_Type\": cell_type,\n",
    "            \"Unit_Type\": cell_label,\n",
    "            \"KS_Statistic\": ks_statistic,\n",
    "            \"P_Value\": p_value\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for easier viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nterpretation of the KS Test Results:\n",
    "FS SUA (Fast-Spiking Single Unit Activity)\n",
    "KS Statistic: 0.318182\n",
    "P-Value: 0.428730\n",
    "Interpretation: There is no statistically significant difference in the distribution of the Modulation Index between the CTZ and No_CTZ groups for FS SUA cells. The p-value is well above 0.05, indicating that any observed differences could be due to random chance.\n",
    "FS MUA (Fast-Spiking Multi-Unit Activity)\n",
    "KS Statistic: 0.223077\n",
    "P-Value: 0.672240\n",
    "Interpretation: Similar to FS SUA, there is no statistically significant difference in the Modulation Index distributions for FS MUA cells between the CTZ and No_CTZ groups. The high p-value suggests no meaningful difference between the two distributions.\n",
    "RS SUA (Regular-Spiking Single Unit Activity)\n",
    "KS Statistic: 0.244061\n",
    "P-Value: 0.026209\n",
    "Interpretation: There is a statistically significant difference in the Modulation Index distributions for RS SUA cells between the CTZ and No_CTZ groups. The p-value is below 0.05, indicating that the distributions are likely different, and this difference is unlikely to be due to random variation.\n",
    "RS MUA (Regular-Spiking Multi-Unit Activity)\n",
    "KS Statistic: 0.207071\n",
    "P-Value: 0.486006\n",
    "Interpretation: No statistically significant difference is observed between the CTZ and No_CTZ groups for RS MUA cells. The p-value is well above 0.05, suggesting no meaningful difference in the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Filtering the Data:\n",
    "The DataFrame filtered_df is created by filtering df to include only sensory-responsive cells (StimResponsivity == 1) and Modulation Index values greater than or equal to ±0.3 (ModulationIndex_Numeric.abs() >= 0.3).\n",
    "Running the KS Test:\n",
    "After filtering the data, the script performs the KS test for each combination of Cell_Type, IsSingleUnit, and groupname (CTZ vs. No_CTZ). The results are stored and then printed.\n",
    "Plotting:\n",
    "The histograms and KDE plots are generated for each subgroup, similar to the previous approach, but now only include the filtered data.\n",
    "The histograms and KDE plots are generated for each subgroup, similar to the previous approach, but now only include the filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this assumes you aready have ran this \n",
    "\n",
    "df = whisker_df_manager.dataframes['basic_metrics']\n",
    "#exposrt and save the dataframe but onluy for the columns that are needed for the plotting: groupname, recordingname, cid, Cell_Type, IsSingleUnit, StimResponsivity, ModulationIndex, ModulationIndex_Numeric, save the dataframe as a csv file in my directory/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv\n",
    "df[['groupname', 'recordingname', 'cid', 'Cell_Type', 'IsSingleUnit', 'StimResponsivity', 'ModulationIndex', 'ModulationIndex_Numeric', 'MeanFR_baseline', 'SpikeTimes_all']].to_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "# Define the groups\n",
    "cell_types = ['FS', 'RS']\n",
    "is_single_unit_types = [1.0, 0.0]  # 1.0 for SUA, 0.0 for MUA\n",
    "treatments = ['CTZ', 'No_CTZ']\n",
    "\n",
    "# Apply filter for sensory responsive cells and modulation index greater than or equal to ±0.3\n",
    "filtered_df = df[(df['StimResponsivity'] == 1) & (df['ModulationIndex_Numeric'].abs() >= 0.3)]\n",
    "\n",
    "# Prepare to store KS test results\n",
    "results = []\n",
    "\n",
    "# Prepare the 2x2 plot with shared y-axis\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharey=True)\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n",
    "# Iterate over each combination of cell type and unit type (SUA or MUA)\n",
    "for i, cell_type in enumerate(cell_types):\n",
    "    for j, is_sua in enumerate(is_single_unit_types):\n",
    "        ax = axs[i, j]\n",
    "        \n",
    "        # Iterate over each treatment to overlay them on the same plot\n",
    "        for treatment in treatments:\n",
    "            # Filter the data for the current combination\n",
    "            data = filtered_df[(filtered_df['Cell_Type'] == cell_type) & (filtered_df['IsSingleUnit'] == is_sua) & (filtered_df['groupname'] == treatment)]['ModulationIndex_Numeric'].dropna().values\n",
    "            \n",
    "            # Fit KDE\n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(data.reshape(-1, 1))\n",
    "            x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "            pdf_values = np.exp(kde.score_samples(x_values))\n",
    "            \n",
    "            # Plot the histogram and KDE for this group\n",
    "            ax.hist(data, bins=20, density=True, alpha=0.7, label=f'{treatment} (N={len(data)})', color=colors[treatment], edgecolor='black')\n",
    "            ax.plot(x_values, pdf_values, color=colors[treatment], linewidth=2)\n",
    "        \n",
    "        # Set title and labels\n",
    "        cell_label = 'SUA' if is_sua == 1.0 else 'MUA'\n",
    "        ax.set_title(f'{cell_type} {cell_label} Cells - Normalized Distribution (PDF)', fontsize=12)\n",
    "        ax.set_xlabel('Modulation Index', fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_ylim(0, 6)  # Adjust this value to fit your data range\n",
    "        \n",
    "        # Perform KS test between CTZ and No_CTZ for this group\n",
    "        data_ctz = filtered_df[(filtered_df['Cell_Type'] == cell_type) & (filtered_df['IsSingleUnit'] == is_sua) & (filtered_df['groupname'] == 'CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        data_no_ctz = filtered_df[(filtered_df['Cell_Type'] == cell_type) & (filtered_df['IsSingleUnit'] == is_sua) & (filtered_df['groupname'] == 'No_CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        \n",
    "        ks_statistic, p_value = ks_2samp(data_ctz, data_no_ctz)\n",
    "        \n",
    "        results.append({\n",
    "            \"Cell_Type\": cell_type,\n",
    "            \"Unit_Type\": cell_label,\n",
    "            \"KS_Statistic\": ks_statistic,\n",
    "            \"P_Value\": p_value\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for easier viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Adjust the layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "# Define the groups and criteria\n",
    "cell_types = ['FS', 'RS']\n",
    "is_single_unit_types = [1.0, 0.0]  # 1.0 for SUA, 0.0 for MUA\n",
    "treatments = ['CTZ', 'No_CTZ']\n",
    "\n",
    "# Apply filter for sensory responsive cells and modulation index greater than or equal to ±0.3\n",
    "filtered_df = df[(df['StimResponsivity'] == 1) & (df['ModulationIndex_Numeric'].abs() >= 0.3)]\n",
    "\n",
    "# Initialize results storage\n",
    "best_fits = []\n",
    "ks_results = []\n",
    "\n",
    "# Iterate over each combination of cell type and unit type (SUA or MUA)\n",
    "for cell_type in cell_types:\n",
    "    for is_sua in is_single_unit_types:\n",
    "        \n",
    "        # Filter the data for the current combination\n",
    "        data_ctz = filtered_df[(filtered_df['Cell_Type'] == cell_type) & (filtered_df['IsSingleUnit'] == is_sua) & (filtered_df['groupname'] == 'CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        data_no_ctz = filtered_df[(filtered_df['Cell_Type'] == cell_type) & (filtered_df['IsSingleUnit'] == is_sua) & (filtered_df['groupname'] == 'No_CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        \n",
    "        # Skip if data is too small for analysis\n",
    "        if len(data_ctz) < 5 or len(data_no_ctz) < 5:\n",
    "            continue\n",
    "        \n",
    "        # Perform distribution fitting and evaluation\n",
    "        for data, treatment in zip([data_ctz, data_no_ctz], treatments):\n",
    "            dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n",
    "            results = []\n",
    "\n",
    "            for dist_name in dist_names:\n",
    "                dist = getattr(st, dist_name)\n",
    "                params = dist.fit(data)\n",
    "                \n",
    "                # Histogram bins\n",
    "                num_bins = 10\n",
    "                x_values = np.linspace(-1, 1, num_bins + 1)\n",
    "                \n",
    "                # Calculate PDF\n",
    "                bin_counts, bin_edges = np.histogram(data, bins=x_values, density=True)\n",
    "                pdf_fitted = dist.pdf((bin_edges[:-1] + bin_edges[1:]) / 2, *params[:-2], loc=params[-2], scale=params[-1])\n",
    "                \n",
    "                # Calculate goodness-of-fit metrics\n",
    "                sse = np.sum((pdf_fitted - bin_counts) ** 2)\n",
    "                log_likelihood = np.sum(np.log(dist.pdf(data, *params)))\n",
    "                aic = 2 * len(params) - 2 * log_likelihood\n",
    "                bic = len(params) * np.log(len(data)) - 2 * log_likelihood\n",
    "                ks_statistic, ks_pvalue = st.kstest(data, dist_name, args=params)\n",
    "                \n",
    "                results.append({\n",
    "                    \"distribution\": dist_name,\n",
    "                    \"sumsquare_error\": sse,\n",
    "                    \"aic\": aic,\n",
    "                    \"bic\": bic,\n",
    "                    \"ks_statistic\": ks_statistic,\n",
    "                    \"ks_pvalue\": ks_pvalue,\n",
    "                    \"params\": params\n",
    "                })\n",
    "            \n",
    "            results_df = pd.DataFrame(results).sort_values(by=\"ks_pvalue\", ascending=False)\n",
    "            best_fit = results_df.iloc[0]  # Get the best fit (highest p-value)\n",
    "            \n",
    "            # Store best fit results\n",
    "            cell_label = 'SUA' if is_sua == 1.0 else 'MUA'\n",
    "            best_fits.append({\n",
    "                \"groupname\": treatment,\n",
    "                \"Cell_Type\": cell_type,\n",
    "                \"IsSingleUnit\": cell_label,\n",
    "                \"best_distribution\": best_fit['distribution'],\n",
    "                \"ks_pvalue\": best_fit['ks_pvalue'],\n",
    "                \"aic\": best_fit['aic'],\n",
    "                \"bic\": best_fit['bic'],\n",
    "                \"params\": best_fit['params']\n",
    "            })\n",
    "        \n",
    "        # Perform KS test\n",
    "        ks_statistic, p_value = ks_2samp(data_ctz, data_no_ctz)\n",
    "        ks_results.append({\n",
    "            \"Cell_Type\": cell_type,\n",
    "            \"Unit_Type\": cell_label,\n",
    "            \"KS_Statistic\": ks_statistic,\n",
    "            \"P_Value\": p_value\n",
    "        })\n",
    "\n",
    "# Convert the results to DataFrames\n",
    "best_fits_df = pd.DataFrame(best_fits)\n",
    "ks_results_df = pd.DataFrame(ks_results)\n",
    "\n",
    "# Save the best fits and KS results to CSV files\n",
    "best_fits_df.to_csv('/Volumes/MannySSD/figures/laminaranalysis/bestfit_output_filtered.csv', index=False)\n",
    "ks_results_df.to_csv('/Volumes/MannySSD/figures/laminaranalysis/ks_results_filtered.csv', index=False)\n",
    "\n",
    "# Plot the 2x2 grid\n",
    "colors = {'CTZ': 'purple', 'No_CTZ': 'gray'}\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharey=True)\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.2)\n",
    "\n",
    "# Iterate over each combination for plotting\n",
    "for i, cell_type in enumerate(cell_types):\n",
    "    for j, is_sua in enumerate(is_single_unit_types):\n",
    "        ax = axs[i, j]\n",
    "        \n",
    "        for treatment in treatments:\n",
    "            data = filtered_df[(filtered_df['Cell_Type'] == cell_type) & (filtered_df['IsSingleUnit'] == is_sua) & (filtered_df['groupname'] == treatment)]['ModulationIndex_Numeric'].dropna().values\n",
    "            \n",
    "            kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(data.reshape(-1, 1))\n",
    "            x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "            pdf_values = np.exp(kde.score_samples(x_values))\n",
    "            \n",
    "            ax.hist(data, bins=20, density=True, alpha=0.7, label=f'{treatment} (N={len(data)})', color=colors[treatment], edgecolor='black')\n",
    "            ax.plot(x_values, pdf_values, color=colors[treatment], linewidth=2)\n",
    "        \n",
    "        cell_label = 'SUA' if is_sua == 1.0 else 'MUA'\n",
    "        ax.set_title(f'{cell_type} {cell_label} Cells - Normalized Distribution (PDF)', fontsize=12)\n",
    "        ax.set_xlabel('Modulation Index', fontsize=10)\n",
    "        ax.set_ylabel('Density', fontsize=10)\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_ylim(0, 6)  # Adjust to fit data range\n",
    "\n",
    "# Final adjustments and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o run the statistical comparisons between the CTZ and No_CTZ groups for each combination of Cell_Type and IsSingleUnit under the current filtering criteria (sensory responsive cells with a Modulation Index of ±0.3 or greater), I will use the Mann-Whitney U test, which is a non-parametric test suitable for comparing two independent samples.\n",
    "\n",
    "how the comparisons will be conducted:\n",
    "\n",
    "FS SUA Cells: Compare CTZ vs No_CTZ.\n",
    "FS MUA Cells: Compare CTZ vs No_CTZ.\n",
    "RS SUA Cells: Compare CTZ vs No_CTZ.\n",
    "RS MUA Cells: Compare CTZ vs No_CTZ.\n",
    "\n",
    "\n",
    "\n",
    "1. FS SUA Cells: CTZ vs No_CTZ\n",
    "Null Hypothesis: The distributions of Modulation Index values for FS SUA cells in the CTZ and No_CTZ groups are the same.\n",
    "Alternative Hypothesis: The distributions are different.\n",
    "2. FS MUA Cells: CTZ vs No_CTZ\n",
    "Null Hypothesis: The distributions of Modulation Index values for FS MUA cells in the CTZ and No_CTZ groups are the same.\n",
    "Alternative Hypothesis: The distributions are different.\n",
    "3. RS SUA Cells: CTZ vs No_CTZ\n",
    "Null Hypothesis: The distributions of Modulation Index values for RS SUA cells in the CTZ and No_CTZ groups are the same.\n",
    "Alternative Hypothesis: The distributions are different.\n",
    "4. RS MUA Cells: CTZ vs No_CTZ\n",
    "Null Hypothesis: The distributions of Modulation Index values for RS MUA cells in the CTZ and No_CTZ groups are the same.\n",
    "Alternative Hypothesis: The distributions are different.\n",
    "I will execute the Mann-Whitney U tests for these comparisons now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Load the filtered data\n",
    "df_filtered = pd.read_csv('/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline_andspikes.csv')\n",
    "\n",
    "\n",
    "# Apply the additional filter for StimResponsivity = 1.0 and ModulationIndex_Numeric >= 0.3\n",
    "df_filtered = df_filtered[(df_filtered['StimResponsivity'] == 1.0) & \n",
    "                          (df_filtered['ModulationIndex_Numeric'].abs() >= 0.3)]\n",
    "\n",
    "# Set a shared y-axis limit based on the maximum density value observed in the data\n",
    "y_max = max([np.max(np.histogram(df_filtered[(df_filtered['Cell_Type'] == cell_type) & \n",
    "                                             (df_filtered['groupname'] == treatment)]\n",
    "                                             ['ModulationIndex_Numeric'].dropna().values, bins=20, density=True)[0])\n",
    "             for cell_type in cell_types for treatment in treatments])\n",
    "\n",
    "# Define the groups\n",
    "cell_types = ['FS', 'RS']\n",
    "treatments = ['CTZ', 'No_CTZ']\n",
    "\n",
    "# Re-plot with overlaid data and color-coded matching\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Colors for the two groups\n",
    "colors = {'CTZ': 'purple', 'No_CTZ': 'gray'}\n",
    "\n",
    "# Iterate over each combination of cell type and treatment\n",
    "for i, cell_type in enumerate(cell_types):\n",
    "    for j, treatment in enumerate(treatments):\n",
    "        \n",
    "        # Filter the data for the current combination\n",
    "        data_ctz = df_filtered[(df_filtered['Cell_Type'] == cell_type) & \n",
    "                               (df_filtered['groupname'] == 'CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        \n",
    "        data_no_ctz = df_filtered[(df_filtered['Cell_Type'] == cell_type) & \n",
    "                                  (df_filtered['groupname'] == 'No_CTZ')]['ModulationIndex_Numeric'].dropna().values\n",
    "        \n",
    "        # Fit KDE for both groups\n",
    "        if len(data_ctz) > 0 and len(data_no_ctz) > 0:\n",
    "            kde_ctz = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(data_ctz.reshape(-1, 1))\n",
    "            kde_no_ctz = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(data_no_ctz.reshape(-1, 1))\n",
    "            \n",
    "            x_values = np.linspace(-1, 1, 1000).reshape(-1, 1)\n",
    "            pdf_values_ctz = np.exp(kde_ctz.score_samples(x_values))\n",
    "            pdf_values_no_ctz = np.exp(kde_no_ctz.score_samples(x_values))\n",
    "            \n",
    "            # Plot the histograms and KDEs for both groups\n",
    "            axs[i, j].hist(data_ctz, bins=20, density=True, alpha=0.5, label=f'CTZ Data (N={len(data_ctz)})', color=colors['CTZ'])\n",
    "            axs[i, j].hist(data_no_ctz, bins=20, density=True, alpha=0.5, label=f'No_CTZ Data (N={len(data_no_ctz)})', color=colors['No_CTZ'])\n",
    "            axs[i, j].plot(x_values, pdf_values_ctz, label='CTZ KDE', color=colors['CTZ'])\n",
    "            axs[i, j].plot(x_values, pdf_values_no_ctz, label='No_CTZ KDE', color=colors['No_CTZ'])\n",
    "            axs[i, j].set_title(f'{cell_type} Cells')\n",
    "            axs[i, j].set_xlabel('Modulation Index')\n",
    "            axs[i, j].set_ylabel('Density')\n",
    "            axs[i, j].set_ylim(0, y_max)  # Set the shared y-axis limit\n",
    "            axs[i, j].legend()\n",
    "            axs[i, j].grid(True)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot cell density and cell distribution on probe with stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot and runs stats on IHC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    se = sem(data)\n",
    "    h = se * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h\n",
    "\n",
    "def plot_and_save_all_combinations(data, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Regions and cell types to iterate over\n",
    "    regions = ['PFC', 'BC']\n",
    "    cell_types = ['SST', 'PV']\n",
    "    \n",
    "    # Prepare to store statistics\n",
    "    stats_data = []\n",
    "\n",
    "    for region in regions:\n",
    "        for cell_type in cell_types:\n",
    "            # Filter the data\n",
    "            filtered_data = data[(data['Region'] == region) & (data['Cell'] == cell_type)]\n",
    "            \n",
    "            if filtered_data.empty:\n",
    "                continue\n",
    "\n",
    "            # Separate CTZ and VEH data\n",
    "            ctz_data = filtered_data[filtered_data['Group'] == 'CTZ']\n",
    "            veh_data = filtered_data[filtered_data['Group'] == 'VEH']\n",
    "\n",
    "            # Calculate statistics for both groups\n",
    "            ctz_summary = ctz_data['Count'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "            veh_summary = veh_data['Count'].agg(['mean', 'median', 'std', 'min', 'max', 'count'])\n",
    "            \n",
    "            # Calculate confidence intervals\n",
    "            ctz_ci = calculate_confidence_interval(ctz_data['Count'])\n",
    "            veh_ci = calculate_confidence_interval(veh_data['Count'])\n",
    "            \n",
    "            # Perform a Mann-Whitney U test\n",
    "            stat, p_value = mannwhitneyu(ctz_data['Count'], veh_data['Count'])\n",
    "            \n",
    "            # Save statistics\n",
    "            stats_data.extend([\n",
    "                {'Region': region, 'Cell_Type': cell_type, 'Group': 'CTZ', 'N': ctz_summary['count'], 'Mean': ctz_summary['mean'],\n",
    "                 'Median': ctz_summary['median'], 'CI_Lower': ctz_ci[1], 'CI_Upper': ctz_ci[2], 'Min': ctz_summary['min'],\n",
    "                 'Max': ctz_summary['max'], 'P_Value': p_value},\n",
    "                \n",
    "                {'Region': region, 'Cell_Type': cell_type, 'Group': 'VEH', 'N': veh_summary['count'], 'Mean': veh_summary['mean'],\n",
    "                 'Median': veh_summary['median'], 'CI_Lower': veh_ci[1], 'CI_Upper': veh_ci[2], 'Min': veh_summary['min'],\n",
    "                 'Max': veh_summary['max'], 'P_Value': p_value}\n",
    "            ])\n",
    "            \n",
    "            # Plotting\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            \n",
    "            # Bar plot for CTZ\n",
    "            plt.bar(x=0, height=ctz_summary['mean'], yerr=ctz_summary['std']/np.sqrt(ctz_summary['count']),\n",
    "                    color='purple', alpha=0.7, capsize=5, width=0.4)\n",
    "            \n",
    "            # Scatter plot for CTZ\n",
    "            plt.scatter(np.zeros(len(ctz_data)) - 0.2, ctz_data['Count'], color='purple', edgecolor='black', s=50, alpha=0.8)\n",
    "            \n",
    "            # Bar plot for VEH\n",
    "            plt.bar(x=1, height=veh_summary['mean'], yerr=veh_summary['std']/np.sqrt(veh_summary['count']),\n",
    "                    color='grey', alpha=0.7, capsize=5, width=0.4)\n",
    "            \n",
    "            # Scatter plot for VEH\n",
    "            plt.scatter(np.ones(len(veh_data)) + 0.2, veh_data['Count'], color='grey', edgecolor='black', s=50, alpha=0.8)\n",
    "            \n",
    "            # Adding labels and title\n",
    "            plt.xticks([0, 1], ['CTZ', 'VEH'])\n",
    "            plt.ylabel('Cell Count')\n",
    "            plt.title(f'{cell_type} Cells in {region}')\n",
    "            \n",
    "            # Setting y-axis limits and ticks\n",
    "            plt.ylim(0, 1500)\n",
    "            plt.yticks([0, 500, 1000, 1500])\n",
    "            \n",
    "            # Save plot\n",
    "            plot_filename = f\"{cell_type}_{region}_Cell_Count.svg\"\n",
    "            plot_path = os.path.join(save_dir, plot_filename)\n",
    "            plt.savefig(plot_path, format='svg')\n",
    "            plt.close()\n",
    "\n",
    "    # Convert stats data to DataFrame and save as CSV\n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    stats_filename = os.path.join(save_dir, 'cell_count_statistics.csv')\n",
    "    stats_df.to_csv(stats_filename, index=False)\n",
    "\n",
    "# Example usage\n",
    "data_path = '/Volumes/MannySSD/figures/thesis_final_figs/CellCountData_Final.csv'\n",
    "save_directory = '/Volumes/MannySSD/figures/thesis_final_figs/cell_count_analysis'\n",
    "cell_count_data = pd.read_csv(data_path)\n",
    "\n",
    "plot_and_save_all_combinations(cell_count_data, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots and analyzes the cell counts but with ephys related stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu, sem, t\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for a data series.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2., n-1)\n",
    "    return mean - h, mean + h\n",
    "\n",
    "def analyze_and_plot_data(data_path, save_directory):\n",
    "    # Load the data\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Explore the basic structure and some key statistics\n",
    "    print(df.info())\n",
    "    print(df.describe())\n",
    "\n",
    "    # Check unique values in important columns\n",
    "    unique_values = {\n",
    "        'groupname': df['groupname'].unique(),\n",
    "        'Cell_Type': df['Cell_Type'].unique(),\n",
    "        'IsSingleUnit': df['IsSingleUnit'].unique(),\n",
    "        'StimResponsivity': df['StimResponsivity'].unique()\n",
    "    }\n",
    "    print(f\"Unique values in important columns: {unique_values}\")\n",
    "    \n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    \n",
    "    # Plotting the proportion of FS vs RS in CTZ and No_CTZ groups\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='groupname', hue='Cell_Type')\n",
    "    plt.title('Proportion of FS vs RS in CTZ and No_CTZ Groups')\n",
    "    plt.ylabel('Cell Count')\n",
    "    plt.xlabel('Group')\n",
    "    plt.legend(title='Cell Type')\n",
    "    plt.savefig(os.path.join(save_directory, 'fs_vs_rs_proportion.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plotting the proportion of Single-Unit vs Multi-Unit in CTZ and No_CTZ groups\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x='groupname', hue='IsSingleUnit')\n",
    "    plt.title('Proportion of Single-Unit vs Multi-Unit in CTZ and No_CTZ Groups')\n",
    "    plt.ylabel('Unit Count')\n",
    "    plt.xlabel('Group')\n",
    "    plt.legend(title='Is Single Unit', labels=['Multi-Unit', 'Single-Unit'])\n",
    "    plt.savefig(os.path.join(save_directory, 'su_vs_mu_proportion.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plotting the distribution of Modulation Index by Cell Type and Group\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.violinplot(data=df, x='groupname', y='ModulationIndex_Numeric', hue='Cell_Type', split=True)\n",
    "    plt.title('Distribution of Modulation Index by Cell Type and Group')\n",
    "    plt.ylabel('Modulation Index (Numeric)')\n",
    "    plt.xlabel('Group')\n",
    "    plt.legend(title='Cell Type')\n",
    "    plt.savefig(os.path.join(save_directory, 'modulation_index_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Descriptive statistics and statistical analysis\n",
    "    results = []\n",
    "\n",
    "    for group in df['groupname'].unique():\n",
    "        for cell_type in df['Cell_Type'].unique():\n",
    "            subset = df[(df['groupname'] == group) & (df['Cell_Type'] == cell_type)]['ModulationIndex_Numeric'].dropna()\n",
    "            if len(subset) > 1:\n",
    "                mean = subset.mean()\n",
    "                median = subset.median()\n",
    "                std_dev = subset.std()\n",
    "                min_val = subset.min()\n",
    "                max_val = subset.max()\n",
    "                range_val = max_val - min_val\n",
    "                ci_low, ci_high = calculate_confidence_interval(subset)\n",
    "\n",
    "                results.append({\n",
    "                    'Group': group,\n",
    "                    'Cell_Type': cell_type,\n",
    "                    'N': len(subset),\n",
    "                    'Mean': mean,\n",
    "                    'Median': median,\n",
    "                    'CI_Low': ci_low,\n",
    "                    'CI_High': ci_high,\n",
    "                    'Range': range_val,\n",
    "                    'SD': std_dev\n",
    "                })\n",
    "\n",
    "    # Converting descriptive stats to DataFrame\n",
    "    descriptive_stats_df = pd.DataFrame(results)\n",
    "\n",
    "    # Chi-square tests\n",
    "    fs_rs_contingency = pd.crosstab(df['groupname'], df['Cell_Type'])\n",
    "    su_mu_contingency = pd.crosstab(df['groupname'], df['IsSingleUnit'])\n",
    "\n",
    "    chi2_fs_rs, p_fs_rs, dof_fs_rs, ex_fs_rs = chi2_contingency(fs_rs_contingency)\n",
    "    chi2_su_mu, p_su_mu, dof_su_mu, ex_su_mu = chi2_contingency(su_mu_contingency)\n",
    "\n",
    "    # T-tests for Modulation Index (FS vs RS)\n",
    "    fs_modulation_ctz = df[(df['groupname'] == 'CTZ') & (df['Cell_Type'] == 'FS')]['ModulationIndex_Numeric'].dropna()\n",
    "    rs_modulation_ctz = df[(df['groupname'] == 'CTZ') & (df['Cell_Type'] == 'RS')]['ModulationIndex_Numeric'].dropna()\n",
    "\n",
    "    fs_modulation_noctz = df[(df['groupname'] == 'No_CTZ') & (df['Cell_Type'] == 'FS')]['ModulationIndex_Numeric'].dropna()\n",
    "    rs_modulation_noctz = df[(df['groupname'] == 'No_CTZ') & (df['Cell_Type'] == 'RS')]['ModulationIndex_Numeric'].dropna()\n",
    "\n",
    "    t_test_ctz = ttest_ind(fs_modulation_ctz, rs_modulation_ctz, nan_policy='omit')\n",
    "    t_test_noctz = ttest_ind(fs_modulation_noctz, rs_modulation_noctz, nan_policy='omit')\n",
    "\n",
    "    # Mann-Whitney U tests (non-parametric alternative)\n",
    "    mw_test_ctz = mannwhitneyu(fs_modulation_ctz, rs_modulation_ctz)\n",
    "    mw_test_noctz = mannwhitneyu(fs_modulation_noctz, rs_modulation_noctz)\n",
    "\n",
    "    # Adding statistical test results to descriptive statistics\n",
    "    statistical_results = {\n",
    "        \"Chi2_FS_RS\": {\"statistic\": chi2_fs_rs, \"p_value\": p_fs_rs},\n",
    "        \"Chi2_SingleUnit_MultiUnit\": {\"statistic\": chi2_su_mu, \"p_value\": p_su_mu},\n",
    "        \"TTest_ModulationIndex_CTz\": {\"statistic\": t_test_ctz.statistic, \"p_value\": t_test_ctz.pvalue},\n",
    "        \"TTest_ModulationIndex_NoCTZ\": {\"statistic\": t_test_noctz.statistic, \"p_value\": t_test_noctz.pvalue},\n",
    "        \"MW_Test_ModulationIndex_CTz\": {\"statistic\": mw_test_ctz.statistic, \"p_value\": mw_test_ctz.pvalue},\n",
    "        \"MW_Test_ModulationIndex_NoCTZ\": {\"statistic\": mw_test_noctz.statistic, \"p_value\": mw_test_noctz.pvalue}\n",
    "    }\n",
    "\n",
    "    stats_df = pd.DataFrame(statistical_results).T\n",
    "\n",
    "    # Save descriptive stats and statistical results\n",
    "    descriptive_stats_df.to_csv(os.path.join(save_directory, 'descriptive_statistics.csv'), index=False)\n",
    "    stats_df.to_csv(os.path.join(save_directory, 'statistical_results.csv'))\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    print(descriptive_stats_df)\n",
    "    print(\"\\nStatistical Results:\")\n",
    "    print(stats_df)\n",
    "\n",
    "# Call the function with your specific paths\n",
    "data_path = '/Volumes/MannySSD/figures/laminaranalysis/basic_metrics_for_plotting_baseline.csv'\n",
    "save_directory = '/Volumes/MannySSD/figures/laminaranalysis'\n",
    "\n",
    "analyze_and_plot_data(data_path, save_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiketurnpike_postanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
