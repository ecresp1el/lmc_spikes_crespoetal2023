{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spiketurnpike_postanalysis.Extract_ephys_from_struct import * #the * imports all functions from the file\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.stats import mannwhitneyu, kruskal, shapiro, ttest_ind\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, permutation_test\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class and methods set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameManager:\n",
    "    def __init__(self, eed_object):\n",
    "        self.eed = eed_object\n",
    "        self.dataframes = {}  # Dictionary to hold multiple DataFrames\n",
    "        self.detailed_dataframes = {}\n",
    "        \n",
    "    def create_dataframe(self, columns, df_name):\n",
    "        data = []\n",
    "        for groupname, recordings in self.eed.all_data.items():\n",
    "            for recordingname, cells in recordings.items():\n",
    "                for cid, metrics in cells.items():\n",
    "                    \n",
    "                    row = {'groupname': groupname, 'recordingname': recordingname, 'cid': cid}\n",
    "                    for column in columns:\n",
    "                        value = metrics.get(column, None)\n",
    "                        \n",
    "                        # Reassign 'IsSingleUnit' based on 'ISI_violations_percent' if necessary\n",
    "                        if column == 'ISI_violations_percent' and value is not None and value < 0.01:\n",
    "                            print(f\"Changing IsSingleUnit to 1.0 for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                            row['IsSingleUnit'] = 1.0\n",
    "                            continue  # Skip to the next column\n",
    "                        \n",
    "                        # Reassign 'Cell_Type' based on 'TroughToPeak_duration' if necessary\n",
    "                        if column == 'TroughToPeak_duration' and value is not None and value <= 0.4:\n",
    "                            print(f\"Changing Cell_Type to FS for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                            row['Cell_Type'] = 'FS'\n",
    "                            # Now add the 'TroughToPeak_duration' value to the row\n",
    "                            row['TroughToPeak_duration'] = value\n",
    "                            continue  # Skip to the next column\n",
    "                        \n",
    "                        # Reassign 'ModulationIndex' based on its value if necessary\n",
    "                        if column == 'ModulationIndex' and value is not None:\n",
    "                            if value >= 0.3:\n",
    "                                print(f\"Changing ModulationIndex {value} to positive for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                                row['ModulationIndex'] = 'positive'\n",
    "                            elif value <= -0.3:\n",
    "                                print(f\"Changing ModulationIndex {value} to negative for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                                row['ModulationIndex'] = 'negative'\n",
    "                            else:\n",
    "                                print(f\"Changing ModulationIndex {value} to none for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                                row['ModulationIndex'] = 'none'\n",
    "                            continue  # Skip to the next column\n",
    "\n",
    "                        # Add the original value if no reassignment happened\n",
    "                        row[column] = value\n",
    "                    \n",
    "                    data.append(row)\n",
    "        \n",
    "        new_df = pd.DataFrame(data)\n",
    "        if df_name in self.dataframes:\n",
    "            self.dataframes[df_name] = pd.concat([self.dataframes[df_name], new_df], ignore_index=True)\n",
    "        else:\n",
    "            self.dataframes[df_name] = new_df\n",
    "\n",
    "    def revert_modulation_index_to_numeric(self):\n",
    "        \"\"\"\n",
    "        Revert the 'ModulationIndex' labels ('positive', 'negative', 'none') \n",
    "        back to the original numeric values and store them in a new column \n",
    "        named 'ModulationIndex_Numeric'.\n",
    "        \"\"\"\n",
    "        for df_name, df in self.dataframes.items():\n",
    "            if 'ModulationIndex' in df.columns:\n",
    "                for index, row in df.iterrows():\n",
    "                    groupname = row['groupname']\n",
    "                    recordingname = row['recordingname']\n",
    "                    cid = row['cid']\n",
    "                    \n",
    "                    # Retrieve the original ModulationIndex value from the self.eed.all_data structure\n",
    "                    original_value = self.eed.all_data[groupname][recordingname][cid].get('ModulationIndex', None)\n",
    "                    \n",
    "                    if original_value is not None:\n",
    "                        print(f\"Reverting ModulationIndex from {row['ModulationIndex']} to {original_value} for group: {groupname}, recording: {recordingname}, cid: {cid}\")\n",
    "                        df.at[index, 'ModulationIndex_Numeric'] = original_value  # Store the numeric value in a new column\n",
    "\n",
    "                # Ensure the DataFrame is updated in the self.dataframes dictionary\n",
    "                self.dataframes[df_name] = df\n",
    "\n",
    "\n",
    "    def append_data(self, df_name, new_data):\n",
    "        if df_name in self.dataframes:\n",
    "            self.dataframes[df_name] = pd.concat([self.dataframes[df_name], new_data], ignore_index=True)\n",
    "        else:\n",
    "            self.dataframes[df_name] = new_data    \n",
    "    \n",
    "                \n",
    "    def get_filtered_data(self, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, groupname=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Retrieve filtered data from the specified DataFrame based on IsSingleUnit, Cell_Type, \n",
    "        StimResponsivity, groupname, and ModulationIndex with the option to not filter on any of these by passing None.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        modulation_label (str or None): Filter for 'positive', 'negative', or 'none' in the ModulationIndex column.\n",
    "                                        If None, no filtering by ModulationIndex.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The filtered DataFrame.\n",
    "        \"\"\"\n",
    "        \n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        # Start with the full DataFrame\n",
    "        filtered_df = self.dataframes[df_name]\n",
    "\n",
    "        # Filter by cell type if provided\n",
    "        if cell_type:\n",
    "            filtered_df = filtered_df[filtered_df['Cell_Type'] == cell_type]\n",
    "\n",
    "        # Filter by IsSingleUnit if not None\n",
    "        if is_single_unit is not None:\n",
    "            filtered_df = filtered_df[filtered_df['IsSingleUnit'] == is_single_unit]\n",
    "\n",
    "        # Filter by StimResponsivity if not None\n",
    "        if stim_responsivity is not None:\n",
    "            filtered_df = filtered_df[filtered_df['StimResponsivity'] == stim_responsivity]\n",
    "\n",
    "        # Filter by groupname if provided\n",
    "        if groupname is not None:\n",
    "            filtered_df = filtered_df[filtered_df['groupname'] == groupname]\n",
    "\n",
    "        # Filter by ModulationIndex if provided\n",
    "        if modulation_label is not None:\n",
    "            filtered_df = filtered_df[filtered_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        return filtered_df\n",
    "   \n",
    "   \n",
    "    def create_psth_dataframe(self):\n",
    "        \"\"\"\n",
    "        Creates and stores a DataFrame for each stimulation type using the 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns from the base PSTH DataFrame. \n",
    "        Each DataFrame is stored as an attribute of the DataFrameManager under a name that corresponds to the stimulation type.\n",
    "\n",
    "        Details:\n",
    "            'SpikeTrains_for_PSTHs' is expected to be a pandas Series where each entry is a list of arrays.\n",
    "            Each array in the list corresponds to spike train data for one of the four distinct stimulations, with dimensions (n_trials, n_time_points), \n",
    "            where 'n_trials' varies per stimulation and 'n_time_points' is consistent (usually the length of the trial in ms).\n",
    "\n",
    "            'PSTHs_conv' is expected to be a pandas Series where each entry is a numpy ndarray with dimensions (4, n_time_points), \n",
    "            where the first dimension corresponds to the four stimulation types and 'n_time_points' matches the second dimension of the arrays in 'SpikeTrains_for_PSTHs'.\n",
    "\n",
    "        Processes:\n",
    "            - A base DataFrame is created with necessary columns.\n",
    "            - For each stimulation label (e.g., 'Zero', 'Low', 'Mid', 'Max'), a new DataFrame is created.\n",
    "            - Each new DataFrame includes adjusted 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns to isolate the data corresponding to the respective stimulation type.\n",
    "            - Each DataFrame is stored in the class dictionary, keyed by the name 'psth_dataframe_' followed by the stimulation label.\n",
    "        \"\"\"\n",
    "        # Create the base dataframe for PSTH analysis\n",
    "        self.create_dataframe(['Cell_Type', 'LaminarLabel','IsSingleUnit', 'StimResponsivity', 'SpikeTrains_for_PSTHs', 'PSTHs_conv', 'ModulationIndex'], 'psth_dataframe')\n",
    "\n",
    "        # Extracting trial tags\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        # Process each label\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            # Retrieve spike trains and PSTHs for each label and create a new DataFrame\n",
    "            df_name = f'psth_dataframe_{label}'\n",
    "            self.dataframes[df_name] = self.dataframes['psth_dataframe'].copy()\n",
    "            self.dataframes[df_name]['SpikeTrains_for_PSTHs'] = self.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'].apply(lambda x: x[index])\n",
    "            self.dataframes[df_name]['PSTHs_conv'] = self.dataframes['psth_dataframe']['PSTHs_conv'].apply(lambda x: x[index])\n",
    "\n",
    "    def create_psth_dataframe_opto(self, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Creates and stores a DataFrame for optogenetic stimulation using the 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns from the base PSTH DataFrame. \n",
    "        The DataFrame is stored as an attribute of the DataFrameManager under a name that corresponds to the optogenetic stimulation type.\n",
    "\n",
    "        Details:\n",
    "            'SpikeTrains_for_PSTHs' is expected to be a pandas Series where each entry is an array of spike train data for the optogenetic stimulation,\n",
    "            with dimensions (n_trials, n_time_points), where 'n_trials' is the number of trials and 'n_time_points' is consistent (usually the length of the trial in ms).\n",
    "\n",
    "            'PSTHs_conv' is expected to be a pandas Series where each entry is a numpy ndarray with dimensions (n_time_points),\n",
    "            where 'n_time_points' is consistent and should be 1500, corresponding to the time points of the optogenetic stimulation.\n",
    "\n",
    "        Processes:\n",
    "            - A base DataFrame is created with necessary columns.\n",
    "            - Retrieve trial tags specific for optogenetic stimulation.\n",
    "            - For each label (expected to be one for optogenetic stimulation), a new DataFrame is created.\n",
    "            - Adjusted 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns are created to isolate the data corresponding to the optogenetic stimulation.\n",
    "            - The DataFrame is stored in the class dictionary, keyed by the name 'psth_dataframe_' followed by the stimulation label.\n",
    "        \"\"\"\n",
    "        # Create the base dataframe for PSTH analysis\n",
    "        \n",
    "          # Create the base dataframe for PSTH analysis with modulation filtering\n",
    "        self.create_dataframe(['Cell_Type', \n",
    "            'LaminarLabel', \n",
    "            'IsSingleUnit', \n",
    "            'StimResponsivity', \n",
    "            'SpikeTrains_for_PSTHs', \n",
    "            'PSTHs_conv', \n",
    "            'ModulationIndex',\n",
    "            'FirstSpikeLatency', \n",
    "            'FirstSpikeLatency_Reliability', \n",
    "            'TroughToPeak_duration', \n",
    "            'SpikeHalfWidth', \n",
    "            'PeakToPeak_ratio', \n",
    "            'peak1_normalized_amplitude', \n",
    "            'Peak1ToTrough_ratio', \n",
    "            'Peak2ToTrough_ratio', \n",
    "            'Template_Channel', \n",
    "            'MeanFR_baseline', \n",
    "            'Normalized_Template_Waveform', \n",
    "            'UnNormalized_Template_Waveform',\n",
    "            'SpikeTimes_all', \n",
    "            'ISI_baseline_CV', \n",
    "            'ISI_baseline_vec', \n",
    "            'ISI_pdf_peak_xy', \n",
    "            'ISI_pdf_x', \n",
    "            'ISI_pdf_y',\n",
    "            'StimProb', \n",
    "            'MeanFR_stim', \n",
    "            'PeakEvokedFR', \n",
    "            'PeakEvokedFR_Latency', \n",
    "            'FanoFactor_baseline', \n",
    "            'FanoFactor_stim', \n",
    "            'MeanFR_inst_stim', \n",
    "            'ISI_violations_percent', \n",
    "            'Recording_Duration', \n",
    "            'Sampling_Frequency'], 'psth_dataframe')\n",
    "        \n",
    "       \n",
    "        # Extracting trial tags\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # This should only contain the optogenetic stimulation label\n",
    "\n",
    "        # Process each label (typically only one for optogenetic stimulation)\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            # Create a new DataFrame for optogenetic stimulation\n",
    "            df_name = f'psth_dataframe_{label}'\n",
    "            self.dataframes[df_name] = self.dataframes['psth_dataframe'].copy()\n",
    "            self.dataframes[df_name]['SpikeTrains_for_PSTHs'] = self.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'].apply(lambda x: x[index])\n",
    "            # Keep the same number of time points (1500) for the PSTH data\n",
    "            self.dataframes[df_name]['PSTHs_conv'] = self.dataframes['psth_dataframe']['PSTHs_conv'].apply(lambda x: x[:, index] if x.ndim > 1 else x)\n",
    "            \n",
    "    def plot_psth(self, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Plots the PSTH for a given stimulation type, with optional filtering on cell type, single unit status, and stimulus responsivity.\n",
    "        Uses the 'relative_time_ms' from the ExtractEphysData object to correctly label the time axis.\n",
    "\n",
    "        Args:\n",
    "            stim_label (str): The label of the stimulation type to plot (e.g., 'Zero', 'Low', 'Mid', 'Max').\n",
    "            cell_type (str, optional): Filter for specific cell types (e.g., 'FS', 'RS'). Default is None, which means no filtering by cell type.\n",
    "            is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0). None means no filtering.\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity (1.0, 0.0, -1.0). None means no filtering.\n",
    "\n",
    "        Processes:\n",
    "            - Retrieves the corresponding DataFrame for the specified stimulation.\n",
    "            - Applies additional filtering based on the provided arguments.\n",
    "            - Averages the PSTH data across all remaining units and plots the result using the relative time axis.\n",
    "        \"\"\"\n",
    "        # Retrieve the DataFrame for the specified stimulation\n",
    "        df_name = f'psth_dataframe_{stim_label}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the stimulation: {stim_label}\")\n",
    "            return\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "\n",
    "        # Apply filtering based on the optional parameters\n",
    "        if cell_type is not None:\n",
    "            df = df[df['Cell_Type'] == cell_type]\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "        if groupname is not None:\n",
    "            df = df[df['groupname'] == groupname]\n",
    "\n",
    "        # Check if there is any data left after filtering\n",
    "        if df.empty:\n",
    "            print(\"No data matches the specified filters.\")\n",
    "            return\n",
    "\n",
    "        # Get relative time array for x-axis\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Aggregate the PSTH data\n",
    "        aggregated_psth = df['PSTHs_conv'].apply(pd.Series).mean(axis=0)\n",
    "\n",
    "        # Plotting the aggregated PSTH\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(time_array, aggregated_psth, label=f'PSTH for {stim_label}')\n",
    "        plt.title(f'PSTH for {stim_label} - {cell_type or \"All Types\"}, SingleUnit: {is_single_unit}, Responsivity: {stim_responsivity}')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Average Spike Rate')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_time_mask(self, time_array, time_range):\n",
    "        \"\"\"\n",
    "        Creates a mask for the time array based on the specified time range.\n",
    "        \n",
    "        Args:\n",
    "            time_array (np.array): The array of time points.\n",
    "            time_range (tuple): The start and end time for the mask (in ms).\n",
    "            \n",
    "        Returns:\n",
    "            time_mask (np.array): Boolean array where True indicates the time points within the specified range.\n",
    "        \"\"\"\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "        return time_mask\n",
    "\n",
    "    def filter_data(self, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, groupname=None, modulation_label=None, recordingname=None):\n",
    "        df_name = f'psth_dataframe_{stim_label}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the stimulation: {stim_label}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "        \n",
    "        # Apply filtering based on the optional parameters\n",
    "        if cell_type is not None:\n",
    "            df = df[df['Cell_Type'] == cell_type]\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "        if groupname is not None:\n",
    "            df = df[df['groupname'] == groupname]\n",
    "        if recordingname is not None:\n",
    "            df = df[df['recordingname'] == recordingname]\n",
    "        \n",
    "        if modulation_label is not None:\n",
    "            # Validate the modulation label input\n",
    "            if modulation_label not in ['positive', 'negative', 'none']:\n",
    "                raise ValueError(\"Modulation label must be one of 'positive', 'negative', or 'none'.\")\n",
    "            \n",
    "            # Apply modulation label filtering\n",
    "            if modulation_label == 'positive' or modulation_label == 'negative':\n",
    "                df = df[df['ModulationIndex'] == modulation_label]\n",
    "            elif modulation_label == 'none':\n",
    "                df = df[df['ModulationIndex'].isnull()]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def plot_individual_psths(self, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, groupname=None, modulation_label=None, time_range=None):\n",
    "        df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label)\n",
    "        \n",
    "        # Print the number of units that match the filter\n",
    "        print(f\"Number of units that match the filter: {df.shape[0]}\")\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"No data matches the specified filters.\")\n",
    "            return\n",
    "\n",
    "        # Get relative time array for x-axis\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        \n",
    "        # Create time mask and adjust time array\n",
    "        time_mask = self.create_time_mask(time_array, time_range)\n",
    "        time_array = time_array[time_mask]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            individual_psth = row['PSTHs_conv']\n",
    "            # Apply time mask to individual PSTH\n",
    "            individual_psth = np.array(individual_psth)[time_mask]\n",
    "            # Apply a smoothing window\n",
    "            window = np.ones(3) / 3  # 3ms window of smoothing\n",
    "\n",
    "\n",
    "            \n",
    "            individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "            plt.plot(time_array, individual_psth, label=f'Unit {index}')\n",
    "\n",
    "        plt.title(f'Individual PSTHs for {stim_label} - {cell_type or \"All Types\"}, SingleUnit: {is_single_unit}, Responsivity: {stim_responsivity}, Modulation: {modulation_label}')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Spike Rate')\n",
    "        plt.legend().set_visible(False)\n",
    "        plt.show()\n",
    "        \n",
    "    def compare_groups(self, group1, group2, stim_label, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Compares PSTH data between two specified groups for a given stimulation.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): The first group name for comparison.\n",
    "            group2 (str): The second group name for comparison.\n",
    "            stim_label (str): The label of the stimulation type (e.g., 'Zero', 'Low', 'Mid', 'Max').\n",
    "            cell_type (str, optional): Specific cell type to filter by.\n",
    "            is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0).\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "            modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two pandas DataFrames representing the filtered data for each group.\n",
    "        \"\"\"\n",
    "        df_name = f'psth_dataframe_{stim_label}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the stimulation: {stim_label}\")\n",
    "            return None, None\n",
    "\n",
    "        base_df = self.dataframes[df_name]\n",
    "        \n",
    "        # Print the shape of the pre-filtered DataFrame\n",
    "        print(f\"Pre-filtered DataFrame shape: {base_df.shape}\")\n",
    "\n",
    "        # Define a helper function to create a mask for a given group\n",
    "        def create_mask(group):\n",
    "            mask = (base_df['groupname'] == group)\n",
    "            if cell_type is not None:\n",
    "                mask &= (base_df['Cell_Type'] == cell_type)\n",
    "            if is_single_unit is not None:\n",
    "                mask &= (base_df['IsSingleUnit'] == is_single_unit)\n",
    "            if stim_responsivity is not None:\n",
    "                mask &= (base_df['StimResponsivity'] == stim_responsivity)\n",
    "            if modulation_label is not None:\n",
    "                if modulation_label not in ['positive', 'negative', 'none']:\n",
    "                    raise ValueError(\"Modulation label must be one of 'positive', 'negative', or 'none'.\")\n",
    "                mask &= (base_df['ModulationIndex'] == modulation_label)\n",
    "            return mask\n",
    "\n",
    "        # Create masks for each group\n",
    "        mask1 = create_mask(group1)\n",
    "        mask2 = create_mask(group2)\n",
    "\n",
    "        # Filter the DataFrame for each group\n",
    "        channel1 = base_df[mask1]\n",
    "        channel2 = base_df[mask2]\n",
    "\n",
    "        # Print the shape of the filtered DataFrames\n",
    "        print(f\"Filtered df1 (group {group1}) shape: {channel1.shape}\")\n",
    "        print(f\"Filtered df2 (group {group2}) shape: {channel2.shape}\")\n",
    "\n",
    "        return channel1, channel2\n",
    "\n",
    "        \n",
    "    def plot_all_stimulations(self, group1, group2, cell_type=None, is_single_unit=None, stim_responsivity=None, time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a 1x4 subplot of PSTH comparisons for all stimulation types.\n",
    "        1. Grabs the trial tags labels from the ExtractEphysData object to determine the stimulation types\n",
    "        and create labels for the subplots.\n",
    "        2. Calls the plot_psth_comparison method for each stimulation type and plots them in a single figure.\n",
    "        \n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            cell_type (str): 'FS' or 'RS'\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "\n",
    "        for i, stim_label in enumerate(stim_labels):\n",
    "            self.plot_psth_comparison(group1, group2, stim_label, axs[i], cell_type=cell_type, is_single_unit=is_single_unit, stim_responsivity=stim_responsivity, time_range=time_range, plot_mode=plot_mode, smoothing_window=smoothing_window)\n",
    "            axs[i].set_title(stim_label)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_psth_comparison(self, group1, group2, stim_label, \n",
    "                             ax=None, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_label=None,\n",
    "                             time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        called within plot_all_stimulations which is called within the main function plot_all_stimulations\n",
    "        \n",
    "        uses the compare_groups method to get the data for the two groups and the specified stimulation type \n",
    "        and fetches the two DataFrames for the groups\n",
    "        \n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided.\n",
    "        Optionally overlays the stimulator signal as a thin black line.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            stim_label (str): Stimulation label.\n",
    "            ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "            groups_to_plot (str, optional): Specifies which group(s) to plot ('both', 'group1', 'group2').\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'}\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_groups(group1, group2, stim_label, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "        \n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        \n",
    "        # DataFrame to store all traces ###new addition to the code\n",
    "        all_traces_df = pd.DataFrame()\n",
    "        \n",
    "        # Process and plot data\n",
    "        for df, group in zip([df1, df2], [group1, group2]):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "                \n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "            \n",
    "            # Store each trace in the DataFrame ## new addition to the code \n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = stim_label\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            \n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            \n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "        # Set plot attributes\n",
    "        ax.set_title(f'PSTH Comparison of {stim_label} between {group1} and {group2}')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Average Spike Rate')\n",
    "        ax.legend()\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "    \n",
    "    def plot_psth_comparison_grouprecordings(self, group1, group2, stim_label, \n",
    "                                            ax=None, cell_type=None, is_single_unit=None, stim_responsivity=None, \n",
    "                                            time_range=None, plot_mode='mean', smoothing_window=None, group_by_recordings=False):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided.\n",
    "        Optionally overlays the stimulator signal as a thin black line.\n",
    "        Optionally groups data by recordings before calculating mean PSTHs.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            stim_label (str): Stimulation label.\n",
    "            ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "            group_by_recordings (bool, optional): If True, calculates the mean PSTHs at the recording level.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "        \n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_groups(group1, group2, stim_label, cell_type, is_single_unit, stim_responsivity)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # Helper function to process data\n",
    "        def process_data(df, group):\n",
    "            if group_by_recordings:\n",
    "                grouped = df.groupby('recordingname')\n",
    "                grouped_data = grouped['PSTHs_conv'].apply(lambda x: np.mean([np.array(i)[time_mask] for i in x], axis=0))\n",
    "            else:\n",
    "                grouped_data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                grouped_data = grouped_data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "            \n",
    "            return grouped_data\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group in zip([df1, df2], [group1, group2]):\n",
    "            data = process_data(df, group)\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "            \n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            \n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "        # Set plot attributes\n",
    "        ax.set_title(f'PSTH Comparison of {stim_label} between {group1} and {group2}')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Average Spike Rate')\n",
    "        ax.legend()\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        return data \n",
    "\n",
    "    def extract_stim_signals(self):\n",
    "        \"\"\"\n",
    "        Extracts and formats the stimulation signals for each relevant stimulation type and synthesizes a flat line for the 'Zero' stimulation.\n",
    "        \"\"\"\n",
    "        stim_voltages = self.eed.StimVoltageTraces_ms['StimVoltageTraces_ms']\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        aligned_signals = {}\n",
    "        full_length = len(self.eed.relative_time_ms['relative_time_ms'])\n",
    "        stim_start_index = 500\n",
    "        stim_end_index = 1000\n",
    "\n",
    "        # Determine the global maximum voltage to set a unified y-axis for the stimulation signals\n",
    "        max_voltage = np.max(stim_voltages)\n",
    "\n",
    "        # Extract and pad signals\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            if label in ['Low', 'Mid', 'Max']:\n",
    "                signal_column_index = ['Low', 'Mid', 'Max'].index(label)\n",
    "                signal = stim_voltages[:, signal_column_index]\n",
    "\n",
    "                pre_padding = np.zeros(stim_start_index)\n",
    "                post_padding = np.zeros(full_length - stim_end_index)\n",
    "                padded_signal = np.concatenate((pre_padding, signal, post_padding))\n",
    "\n",
    "                aligned_signals[label] = padded_signal[:1500]\n",
    "\n",
    "            elif label == 'Zero':\n",
    "                # Create a flat line using the minimum voltage from 'Low'\n",
    "                min_voltage = np.min(stim_voltages[:, 0])\n",
    "                flat_signal = np.full(full_length, min_voltage)\n",
    "                aligned_signals[label] = flat_signal[:1500]\n",
    "\n",
    "        return aligned_signals, max_voltage\n",
    "    \n",
    "    def extract_stim_signals_opto(self):\n",
    "        \"\"\"\n",
    "        Extracts and formats the stimulation signals for the optogenetic stimulation type. \n",
    "        Since there is only one stimulation type and the array is 1D, the method simplifies handling of the array.\n",
    "        \"\"\"\n",
    "        stim_voltages = self.eed.StimVoltageTraces_ms['StimVoltageTrace_ms'] #Trace not Traces for opto\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # This should contain only the optogenetic stimulation label.\n",
    "\n",
    "        aligned_signals = {}\n",
    "        full_length = len(self.eed.relative_time_ms['relative_time_ms'])\n",
    "        stim_start_index = 500\n",
    "        stim_end_index = 1000\n",
    "\n",
    "        # Determine the global maximum voltage to set a unified y-axis for the stimulation signals\n",
    "        max_voltage = np.max(stim_voltages)\n",
    "\n",
    "        # Extract and pad the signal for the optogenetic stimulation\n",
    "        for label in stim_labels:\n",
    "            signal = stim_voltages  # Directly use the 1D array as there's only one type of stimulation\n",
    "\n",
    "            pre_padding = np.zeros(stim_start_index)\n",
    "            post_padding = np.zeros(full_length - stim_end_index)\n",
    "            padded_signal = np.concatenate((pre_padding, signal, post_padding))\n",
    "\n",
    "            aligned_signals[label] = padded_signal[:1500]\n",
    "\n",
    "        return aligned_signals, max_voltage\n",
    "\n",
    "    def plot_psth_with_stim(self, group1, group2, stim_label, ax=None, max_voltage=None, cell_type=None, is_single_unit=None, stim_responsivity=None, \n",
    "                            time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided,\n",
    "        and overlays the stimulation signal on a secondary y-axis.\n",
    "\n",
    "        The other parameters function as in the original plot_psth_comparison method.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add debug statement to check input parameters and data\n",
    "        print(f\"Plotting PSTH for groups: {group1}, {group2} with stimulation: {stim_label}\")\n",
    "        \n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            created_fig = True\n",
    "        else:\n",
    "            created_fig = False\n",
    "\n",
    "        # Plot PSTH data as before\n",
    "        traces_df = self.plot_psth_comparison(group1, group2, stim_label, ax=ax, cell_type=cell_type, is_single_unit=is_single_unit, \n",
    "                                              stim_responsivity=stim_responsivity, time_range=time_range, plot_mode=plot_mode, \n",
    "                                              smoothing_window=smoothing_window)\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        stim_signals, _ = self.extract_stim_signals()\n",
    "        if stim_label in stim_signals:\n",
    "            stim_signal = stim_signals[stim_label]  # Assuming full signal is handled correctly\n",
    "            stim_signal = stim_signal[:1500]\n",
    "            if time_range:\n",
    "                stim_signal = stim_signal[time_mask]  # Apply time mask if time range is specified\n",
    "            ax2.plot(time_array, stim_signal, 'r-', label='Stim Signal', alpha=0.5)\n",
    "            ax2.set_ylabel('Stimulation Voltage (uV)', color='r')\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylim(0, max_voltage)  # Set consistent y-axis scale\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if created_fig:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return traces_df\n",
    "    \n",
    "    def plot_psth_with_stim_opto(self, group1, group2, stim_label, ax=None, max_voltage=None, cell_type=None, is_single_unit=None, stim_responsivity=None, time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison on the provided axes object or creates a new figure if not provided,\n",
    "        and overlays the stimulation signal on a secondary y-axis.\n",
    "\n",
    "        The other parameters function as in the original plot_psth_comparison method.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            created_fig = True\n",
    "        else:\n",
    "            created_fig = False\n",
    "\n",
    "        # Plot PSTH data as before\n",
    "        self.plot_psth_comparison(group1, group2, stim_label, ax=ax, \n",
    "                                  cell_type=cell_type, is_single_unit=is_single_unit, stim_responsivity=stim_responsivity, \n",
    "                                  time_range=time_range, plot_mode=plot_mode, smoothing_window=smoothing_window)\n",
    "        ax2 = ax.twinx()\n",
    "        stim_signals, _ = self.extract_stim_signals_opto()\n",
    "        if stim_label in stim_signals:\n",
    "            stim_signal = stim_signals[stim_label]  # Assuming full signal is handled correctly\n",
    "            stim_signal = stim_signal[:1500]\n",
    "            if time_range:\n",
    "                stim_signal = stim_signal[time_mask]  # Apply time mask if time range is specified\n",
    "            ax2.plot(time_array, stim_signal, 'r-', label='Stim Signal', alpha=0.5)\n",
    "            ax2.set_ylabel('LED Stimulation Voltage (uV)', color='r')\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylim(0, max_voltage)  # Set consistent y-axis scale\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if created_fig:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    def plot_all_stimulations_with_stim(self, group1, group2, cell_type=None, is_single_unit=None, stim_responsivity=None, time_range=None, plot_mode='mean', smoothing_window=None, directory=None, file_name=None, firstspike_latency=None):\n",
    "        \n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        fig, axs = plt.subplots(1, len(stim_labels), figsize=(20, 5), sharey=True)\n",
    "\n",
    "        # Retrieve stimulation signals to determine the maximum voltage\n",
    "        stim_signals, max_voltage = self.extract_stim_signals()\n",
    "\n",
    "        all_traces_df = pd.DataFrame()\n",
    "        \n",
    "        for i, stim_label in enumerate(stim_labels):\n",
    "            traces_df = self.plot_psth_with_stim(group1, group2, stim_label, axs[i], max_voltage=max_voltage, \n",
    "                                                 cell_type=cell_type, is_single_unit=is_single_unit, stim_responsivity=stim_responsivity, \n",
    "                                                 time_range=time_range, plot_mode=plot_mode, \n",
    "                                                 smoothing_window=smoothing_window, firstspike_latency=firstspike_latency)\n",
    "            \n",
    "            traces_df['Stimulation'] = stim_label  # Ensure the correct stim label is assigned\n",
    "            \n",
    "            all_traces_df = pd.concat([all_traces_df, traces_df], ignore_index=True)\n",
    "            axs[i].set_title(stim_label)\n",
    "            \n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        fig.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return all_traces_df\n",
    "    \n",
    "    def calculate_basic_stats(self, group1, group2, stim_label=None, baseline_range=(-100, 1), stim_range=(0, 50), cell_type=None, is_single_unit=None, stim_responsivity=None, smoothing_window=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Calculates basic statistics for baseline and stimulation windows for two groups across all or a specific stimulation type and stores the detailed data used for these calculations.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            stim_label (str or None): Specific stimulation label to analyze, or None to analyze all.\n",
    "            baseline_range (tuple): Time range for baseline window.\n",
    "            stim_range (tuple): Time range for stimulation window.\n",
    "            cell_type (str, optional): Filter for specific cell types.\n",
    "            is_single_unit (float, optional): Filter for single units.\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "            smoothing_window (int, optional): Size of the smoothing window.\n",
    "            modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing means and standard deviations for the specified windows.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        if stim_label:\n",
    "            stim_labels = [stim_label]  # If specific stim_label is provided, use that\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for stim in stim_labels:\n",
    "            df1, df2 = self.compare_groups(group1, group2, stim, cell_type, is_single_unit, \n",
    "                                           stim_responsivity, modulation_label)\n",
    "\n",
    "            # Function to calculate stats and add to DataFrame\n",
    "            def calculate_and_store_stats(df, time_range, window_label):\n",
    "                if df is None or df.empty:\n",
    "                    return df\n",
    "\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "                \n",
    "                # Apply the time mask and store the result in a new column safely\n",
    "                df.loc[:, 'masked_data'] = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "\n",
    "                if smoothing_window:\n",
    "                    window = np.ones(smoothing_window) / smoothing_window\n",
    "                    df.loc[:, 'smoothed_data'] = df['masked_data'].apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "                else:\n",
    "                    df.loc[:, 'smoothed_data'] = df['masked_data']\n",
    "\n",
    "                # Calculate mean and standard deviation and store them in new columns\n",
    "                df.loc[:, f'mean_{window_label}'] = df['smoothed_data'].apply(np.max)\n",
    "                df.loc[:, f'std_{window_label}'] = df['smoothed_data'].apply(np.std)\n",
    "\n",
    "                return df\n",
    "\n",
    "\n",
    "            # Calculate and store stats in the DataFrames\n",
    "            df1 = calculate_and_store_stats(df1, baseline_range, 'baseline')\n",
    "            df1 = calculate_and_store_stats(df1, stim_range, 'stimulation')\n",
    "            df2 = calculate_and_store_stats(df2, baseline_range, 'baseline')\n",
    "            df2 = calculate_and_store_stats(df2, stim_range, 'stimulation')\n",
    "\n",
    "            self.detailed_dataframes[(group1, stim)] = df1\n",
    "            self.detailed_dataframes[(group2, stim)] = df2\n",
    "\n",
    "            # Extract group-level results to return\n",
    "            baseline_stats_group1 = {'mean': df1['mean_baseline'].max(), 'std': df1['std_baseline'].max()}\n",
    "            stim_stats_group1 = {'mean': df1['mean_stimulation'].max(), 'std': df1['std_stimulation'].max()}\n",
    "            baseline_stats_group2 = {'mean': df2['mean_baseline'].max(), 'std': df2['std_baseline'].max()}\n",
    "            stim_stats_group2 = {'mean': df2['mean_stimulation'].max(), 'std': df2['std_stimulation'].max()}\n",
    "\n",
    "            results.extend([\n",
    "                {'Group': group1, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group1['mean'], 'Std': baseline_stats_group1['std']},\n",
    "                {'Group': group1, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group1['mean'], 'Std': stim_stats_group1['std']},\n",
    "                {'Group': group2, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group2['mean'], 'Std': baseline_stats_group2['std']},\n",
    "                {'Group': group2, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group2['mean'], 'Std': stim_stats_group2['std']}\n",
    "            ])\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def calculate_basic_stats_laminar(self, group1, group2, stim_label=None, baseline_range=(-100, 1), stim_range=(0, 50), cell_type=None, is_single_unit=None, stim_responsivity=None, smoothing_window=None, modulation_label=None):\n",
    "            \"\"\"\n",
    "            Calculates basic statistics for baseline and stimulation windows for two groups across all or a specific stimulation type and stores the detailed data used for these calculations.\n",
    "\n",
    "            Args:\n",
    "                group1 (str): First group name.\n",
    "                group2 (str): Second group name.\n",
    "                stim_label (str or None): Specific stimulation label to analyze, or None to analyze all.\n",
    "                baseline_range (tuple): Time range for baseline window.\n",
    "                stim_range (tuple): Time range for stimulation window.\n",
    "                cell_type (str, optional): Filter for specific cell types.\n",
    "                is_single_unit (float, optional): Filter for single units.\n",
    "                stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "                smoothing_window (int, optional): Size of the smoothing window.\n",
    "                modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "            Returns:\n",
    "                pandas.DataFrame: A DataFrame containing means and standard deviations for the specified windows.\n",
    "            \"\"\"\n",
    "            stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "            if stim_label:\n",
    "                stim_labels = [stim_label]  # If specific stim_label is provided, use that\n",
    "\n",
    "            results = []\n",
    "\n",
    "            for stim in stim_labels:\n",
    "                df1, df2 = self.compare_groups(group1, group2, stim, cell_type, is_single_unit, \n",
    "                                            stim_responsivity, modulation_label)\n",
    "\n",
    "                # Function to calculate stats and add to DataFrame\n",
    "                def calculate_and_store_stats(df, time_range, window_label):\n",
    "                    if df is None or df.empty:\n",
    "                        return df\n",
    "\n",
    "                    time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                    time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "                    \n",
    "                    # Apply the time mask and store the result in a new column safely\n",
    "                    df.loc[:, 'masked_data'] = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "\n",
    "                    if smoothing_window:\n",
    "                        window = np.ones(smoothing_window) / smoothing_window\n",
    "                        df.loc[:, 'smoothed_data'] = df['masked_data'].apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "                    else:\n",
    "                        df.loc[:, 'smoothed_data'] = df['masked_data']\n",
    "\n",
    "                    # Calculate mean and standard deviation and store them in new columns\n",
    "                    df.loc[:, f'mean_{window_label}'] = df['smoothed_data'].apply(np.max)\n",
    "                    df.loc[:, f'std_{window_label}'] = df['smoothed_data'].apply(np.std)\n",
    "\n",
    "                    return df\n",
    "\n",
    "\n",
    "                # Calculate and store stats in the DataFrames\n",
    "                df1 = calculate_and_store_stats(df1, baseline_range, 'baseline')\n",
    "                df1 = calculate_and_store_stats(df1, stim_range, 'stimulation')\n",
    "                df2 = calculate_and_store_stats(df2, baseline_range, 'baseline')\n",
    "                df2 = calculate_and_store_stats(df2, stim_range, 'stimulation')\n",
    "\n",
    "                self.detailed_dataframes[(group1, stim)] = df1\n",
    "                self.detailed_dataframes[(group2, stim)] = df2\n",
    "\n",
    "                # Extract group-level results to return\n",
    "                baseline_stats_group1 = {'mean': df1['mean_baseline'].max(), 'std': df1['std_baseline'].max()}\n",
    "                stim_stats_group1 = {'mean': df1['mean_stimulation'].max(), 'std': df1['std_stimulation'].max()}\n",
    "                baseline_stats_group2 = {'mean': df2['mean_baseline'].max(), 'std': df2['std_baseline'].max()}\n",
    "                stim_stats_group2 = {'mean': df2['mean_stimulation'].max(), 'std': df2['std_stimulation'].max()}\n",
    "\n",
    "                results.extend([\n",
    "                    {'Group': group1, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group1['mean'], 'Std': baseline_stats_group1['std']},\n",
    "                    {'Group': group1, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group1['mean'], 'Std': stim_stats_group1['std']},\n",
    "                    {'Group': group2, 'Stimulation': stim, 'Window': 'Baseline', 'Mean': baseline_stats_group2['mean'], 'Std': baseline_stats_group2['std']},\n",
    "                    {'Group': group2, 'Stimulation': stim, 'Window': 'Stimulation', 'Mean': stim_stats_group2['mean'], 'Std': stim_stats_group2['std']}\n",
    "                ])\n",
    "\n",
    "            return pd.DataFrame(results)    \n",
    "    \n",
    "    \n",
    "    def prepare_for_boxplot(self):\n",
    "        \"\"\"\n",
    "        Organizes data into a DataFrame suitable for plotting boxplots. It extracts the 'mean_stimulation'\n",
    "        values from detailed DataFrames, including labels for stimulation type and group.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with columns for 'mean_stimulation', 'Stimulation', and 'Group'.\n",
    "        \"\"\"\n",
    "        boxplot_data = []\n",
    "\n",
    "        # Iterate over each stored DataFrame key (group and stimulation)\n",
    "        for (group, stim), df in self.detailed_dataframes.items():\n",
    "            if not df.empty:  # Corrected check for an empty DataFrame\n",
    "                # Extract 'mean_stimulation' and corresponding labels\n",
    "                for index, row in df.iterrows():\n",
    "                    boxplot_data.append({\n",
    "                        'mean_stimulation': row['mean_stimulation'],\n",
    "                        'Stimulation': stim,\n",
    "                        'Group': group, \n",
    "                        'recordingname': row['recordingname'],\n",
    "                        'ModulationIndex': row['ModulationIndex'], \n",
    "                        'cid': row['cid'], \n",
    "                        'Cell_Type': row['Cell_Type'], \n",
    "                        'Template_Channel': row['Template_Channel'],\n",
    "                        'LaminarLabel': row['LaminarLabel'],\n",
    "                        'IsSingleUnit': row['IsSingleUnit'],\n",
    "                        'TroughToPeak_duration': row['TroughToPeak_duration'],\n",
    "                        'PeakToPeak_ratio': row['PeakToPeak_ratio'],\n",
    "                        'peak1_normalized_amplitude': row['peak1_normalized_amplitude'],\n",
    "                        'Peak1ToTrough_ratio': row['Peak1ToTrough_ratio'],\n",
    "                        'Peak2ToTrough_ratio': row['Peak2ToTrough_ratio'],\n",
    "                        'SpikeHalfWidth': row['SpikeHalfWidth'],\n",
    "                        'MeanFR_baseline': row['MeanFR_baseline'],\n",
    "                        'StimResponsivity': row['StimResponsivity'],\n",
    "                        'UnNormalized_Template_Waveform': row['UnNormalized_Template_Waveform'],\n",
    "                        'Normalized_Template_Waveform': row['Normalized_Template_Waveform'],\n",
    "                        'FirstSpikeLatency': row['FirstSpikeLatency'],\n",
    "                        'FirstSpikeLatency_Reliability': row['FirstSpikeLatency_Reliability'],\n",
    "                        'FanoFactor_baseline': row['FanoFactor_baseline'],\n",
    "                        'FanoFactor_stim': row['FanoFactor_stim']\n",
    "                    })\n",
    "\n",
    "        # Convert list of data to DataFrame\n",
    "        boxplot_df = pd.DataFrame(boxplot_data)\n",
    "\n",
    "        return boxplot_df\n",
    "    \n",
    "    def plot_box_and_strip(self, df, groups=None, stimulations=None, show_outliers=True, hue_order=None, directory=None, file_name=None, ylim=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations, with color adjustments made directly in the plotting calls.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame containing the data to plot made with prepare_for_boxplot.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "     \n",
    "        # Filter by specified groups and stimulations\n",
    "        boxplot_df = df\n",
    "        # Filter by specified groups and stimulations\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "        if modulation_label:\n",
    "            boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='Stimulation', y='mean_stimulation', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=boxplot_df, x='Stimulation', y='mean_stimulation', hue='Group', **stripplot_kwargs)\n",
    "        \n",
    "        #control the upper and lower limits of the y-axis\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        return boxplot_df # Return the DataFrame for further analysis\n",
    "\n",
    "    def plot_box_and_strip_with_controls(self, df, groups=None, stimulations=None, show_outliers=True, show_scatter=True, hue_order=None, directory=None, file_name=None, ylim=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and optional stripplots for specified groups and stimulations, with control over outlier and scatter display.\n",
    "        \n",
    "        Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the data to plot made with prepare_for_boxplot.\n",
    "        groups (list of str, optional): List of groups to include in the plot.\n",
    "        stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "        show_outliers (bool, optional): Whether to show outliers in both boxplot and stripplot. Default is True.\n",
    "        show_scatter (bool, optional): Whether to show the scatter plot (stripplot). Default is True.\n",
    "        hue_order (list, optional): Order of the hue levels.\n",
    "        directory (str, optional): Directory to save the plot.\n",
    "        file_name (str, optional): File name to save the plot.\n",
    "        ylim (tuple, optional): Y-axis limits.\n",
    "        modulation_label (str, optional): Label for modulation index filtering.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Filter by specified groups and stimulations\n",
    "        boxplot_df = df\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "        if modulation_label:\n",
    "            boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        # Calculate outliers\n",
    "        Q1 = boxplot_df.groupby(['Stimulation', 'Group'])['mean_stimulation'].transform('quantile', 0.25)\n",
    "        Q3 = boxplot_df.groupby(['Stimulation', 'Group'])['mean_stimulation'].transform('quantile', 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (boxplot_df['mean_stimulation'] < Q1 - 1.5 * IQR) | (boxplot_df['mean_stimulation'] > Q3 + 1.5 * IQR)\n",
    "        \n",
    "        # Separate outliers and non-outliers\n",
    "        outliers = boxplot_df[outlier_mask]\n",
    "        non_outliers = boxplot_df[~outlier_mask]\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='Stimulation', y='mean_stimulation', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot if show_scatter is True\n",
    "        if show_scatter:\n",
    "            # Add stripplot for non-outliers\n",
    "            sns.stripplot(data=non_outliers, x='Stimulation', y='mean_stimulation', hue='Group', **stripplot_kwargs)\n",
    "\n",
    "            # Add stripplot for outliers if show_outliers is True\n",
    "            if show_outliers:\n",
    "                sns.stripplot(data=outliers, x='Stimulation', y='mean_stimulation', hue='Group', \n",
    "                            **{**stripplot_kwargs, 'marker': 'D', 'size': 8})\n",
    "\n",
    "        # Control the upper and lower limits of the y-axis\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "        plt.show()\n",
    "\n",
    "        return boxplot_df  # Return the DataFrame for further analysis\n",
    "\n",
    "    def plot_mean_and_sem_lineplot(self, df, groups=None, stimulations=None, show_outliers=True, hue_order=None, directory=None, file_name=None, ylim=None, modulation_label=None, file_format='svg'):\n",
    "        \"\"\"\n",
    "        Plots mean and SEM bars for specified groups and stimulations, with color adjustments.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame containing the data to plot made with prepare_for_boxplot \n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "            directory (str, optional): Directory to save the plot.\n",
    "            file_name (str, optional): File name to save the plot.\n",
    "            ylim (tuple, optional): Y-axis limits for the plot.\n",
    "            modulation_label (str, optional): Label to filter modulation.\n",
    "            firstspike_latency (bool, optional): Whether to filter by first spike latency.\n",
    "            file_format (str, optional): Format to save the plot ('svg' or 'png'). Default is 'svg'.\n",
    "        \"\"\"\n",
    "        # Validate the file format\n",
    "        if file_format not in ['svg', 'png']:\n",
    "            raise ValueError(\"file_format must be either 'svg' or 'png'\")\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Define the order of stimulations\n",
    "        stim_order = ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        boxplot_df = df\n",
    "\n",
    "        # Filter by specified groups and stimulations\n",
    "        #if groups:\n",
    "        #    boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        #if stimulations:\n",
    "        #    boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "        #if modulation_label:\n",
    "        #    boxplot_df = boxplot_df[boxplot_df['ModulationIndex'] == modulation_label]\n",
    "\n",
    "        # Calculate mean and SEM\n",
    "        summary_df = boxplot_df.groupby(['Stimulation', 'Group']).agg(\n",
    "            mean_stimulation=('mean_stimulation', 'mean'),\n",
    "            sem_stimulation=('mean_stimulation', 'sem')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Ensure the stimulations are ordered correctly\n",
    "        summary_df['Stimulation'] = pd.Categorical(summary_df['Stimulation'], categories=stim_order, ordered=True)\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        ax = sns.lineplot(data=summary_df, x='Stimulation', y='mean_stimulation', hue='Group', palette=group_colors, hue_order=hue_order, markers=True, style='Group', markersize=10, linewidth=2.5)\n",
    "\n",
    "        # Add error bars\n",
    "        for group in summary_df['Group'].unique():\n",
    "            group_data = summary_df[summary_df['Group'] == group]\n",
    "            plt.errorbar(group_data['Stimulation'], group_data['mean_stimulation'], yerr=group_data['sem_stimulation'], fmt='o', c=group_colors[group], capsize=5, elinewidth=2, markersize=10)\n",
    "\n",
    "        # Control the upper and lower limits of the y-axis\n",
    "        if ylim is not None:\n",
    "            plt.ylim(ylim)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG or PNG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.{file_format}')\n",
    "        plt.savefig(file_path, format=file_format, transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def plot_rasters_for_cid(self, groupname, recordingname, cid, time_window=None):\n",
    "        \"\"\"\n",
    "        Plots raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                           Default is None, which uses the full range.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']  # Full time array from -500 to 999 ms\n",
    "\n",
    "        # Determine the indices for slicing time_array based on the specified or default time window\n",
    "        if time_window is not None:\n",
    "            start_index = np.searchsorted(time_array, time_window[0])\n",
    "            end_index = np.searchsorted(time_array, time_window[1], side='right')\n",
    "        else:\n",
    "            start_index, end_index = 0, len(time_array)  # Use full range if no window is specified\n",
    "        \n",
    "        # Prepare the figure\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "        fig.suptitle(f'Raster plots for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        # Iterate through each stimulation type\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            ax = axes[i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                # Filter data for specific cid\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values #a numpy.ndarry of shape(1,) and size 1 which contains a list of arrays\n",
    "                \n",
    "                # Check if there is any data to plot\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # extract the binary spike trains with numpy.ndarray of shape (n_trials, n_time_points)\n",
    "                    # Plot each trial's spikes within the time window\n",
    "                    for trial_index, trial in enumerate(spike_trains): #enumerate over the trials which are the rows of the spike_trains. trial is a numpy.ndarray of shape (n_time_points,) and trial_index is the index of the trial\n",
    "                        spikes = np.where(trial == 1)[0]  # Get indices where spikes occur\n",
    "                        spikes = spikes[(spikes >= start_index) & (spikes < end_index)]  # Filter spikes by time window\n",
    "                        spike_times = time_array[spikes]  # Convert indices to times\n",
    "                        ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, colors='black')\n",
    "\n",
    "                    ax.set_xlim(time_window[0], time_window[1]) if time_window else ax.set_xlim(time_array[start_index], time_array[end_index-1])\n",
    "                    ax.set_title(f'Stimulation: {stim}')\n",
    "                    ax.set_xlabel('Time (ms)')\n",
    "                    if i == 0:\n",
    "                        ax.set_ylabel('Trial')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rasters_for_cid_2msbins(self, groupname, recordingname, cid, time_window=None):\n",
    "        \"\"\"\n",
    "        Plots raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']  # Full time array from -500 to 999 ms\n",
    "\n",
    "        # Create a new time array with 2ms bins\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, 2)\n",
    "\n",
    "        # Determine the indices for slicing time_array based on the specified or default time window\n",
    "        if time_window is not None:\n",
    "            start_index = np.searchsorted(new_time_array, time_window[0])\n",
    "            end_index = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "        else:\n",
    "            start_index, end_index = 0, len(new_time_array)  # Use full range if no window is specified\n",
    "        \n",
    "        # Prepare the figure\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "        fig.suptitle(f'Raster plots for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        # Iterate through each stimulation type\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            ax = axes[i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                # Filter data for specific cid\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values #a numpy.ndarry of shape(1,) and size 1 which contains a list of arrays\n",
    "                \n",
    "                # Check if there is any data to plot\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # extract the binary spike trains with numpy.ndarray of shape (n_trials, n_time_points)\n",
    "                    \n",
    "                    # Bin the spike trains into 2ms intervals\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], 2), axis=1)\n",
    "                    \n",
    "                    # Plot each trial's spikes within the time window\n",
    "                    for trial_index, trial in enumerate(binned_spike_trains): #enumerate over the trials which are the rows of the spike_trains. trial is a numpy.ndarray of shape (n_time_points,) and trial_index is the index of the trial\n",
    "                        spikes = np.where(trial > 0)[0]  # Get indices where spikes occur (considering 2ms bins)\n",
    "                        spikes = spikes[(spikes >= start_index) & (spikes < end_index)]  # Filter spikes by time window\n",
    "                        spike_times = new_time_array[spikes]  # Convert indices to times\n",
    "                        ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, colors='black')\n",
    "\n",
    "                    ax.set_xlim(time_window[0], time_window[1]) if time_window else ax.set_xlim(new_time_array[start_index], new_time_array[end_index-1])\n",
    "                    ax.set_title(f'Stimulation: {stim}')\n",
    "                    ax.set_xlabel('Time (ms)')\n",
    "                    if i == 0:\n",
    "                        ax.set_ylabel('Trial')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_rasters_for_cid_changebinsize(self, groupname, recordingname, cid, time_window=None, bin_size=1, filter_empty_trials=False, recording_dir=None):\n",
    "        \"\"\"\n",
    "        Plots raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds. Default is 1ms.\n",
    "            filter_empty_trials (bool, optional): If True, only plots trials with at least one spike. Default is False.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']  # Full time array from -500 to 999 ms\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Determine the indices for slicing time_array based on the specified or default time window\n",
    "        if time_window is not None:\n",
    "            start_index = np.searchsorted(new_time_array, time_window[0])\n",
    "            end_index = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "        else:\n",
    "            start_index, end_index = 0, len(new_time_array)  # Use full range if no window is specified\n",
    "        \n",
    "        # Prepare the figure\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "        fig.suptitle(f'Raster plots for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        # Iterate through each stimulation type\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            ax = axes[i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                # Filter data for specific cid\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "                \n",
    "                # Check if there is any data to plot\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # extract the binary spike trains\n",
    "                    \n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "                    \n",
    "                    # Optionally filter out trials without any spikes\n",
    "                    if filter_empty_trials:\n",
    "                        trial_spike_counts = binned_spike_trains.sum(axis=1)\n",
    "                        binned_spike_trains = binned_spike_trains[trial_spike_counts > 0]\n",
    "                    \n",
    "                    # Plot each trial's spikes within the time window\n",
    "                    for trial_index, trial in enumerate(binned_spike_trains):\n",
    "                        spikes = np.where(trial > 0)[0]  # Get indices where spikes occur (considering bin size)\n",
    "                        spikes = spikes[(spikes >= start_index) & (spikes < end_index)]  # Filter spikes by time window\n",
    "                        spike_times = new_time_array[spikes]  # Convert indices to times\n",
    "                        ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, colors='black')\n",
    "\n",
    "                    ax.set_xlim(time_window[0], time_window[1]) if time_window else ax.set_xlim(new_time_array[start_index], new_time_array[end_index-1])\n",
    "                    ax.set_title(f'Stimulation: {stim}')\n",
    "                    ax.set_xlabel('Time (ms)')\n",
    "                    if i == 0:\n",
    "                        ax.set_ylabel('Trial')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        save_path = os.path.join(recording_dir, f'raster_{groupname}_{recordingname}_{cid}.svg')\n",
    "        fig.savefig(save_path, format='svg', transparent=True)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Raster plots have been saved successfully.\")\n",
    "\n",
    "\n",
    "    def smooth_data(self, data, window_size=5):\n",
    "        \"\"\"Smooths data using a moving average filter with a specified window size.\"\"\"\n",
    "        window = np.ones(int(window_size)) / float(window_size)\n",
    "        return np.convolve(data, window, 'same')\n",
    "    \n",
    "    def plot_combined_psth_and_raster(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, show=False):\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial == 1)[0] for trial in spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(time_array), range=(0, len(time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    # Plot Smoothed PSTH\n",
    "                    smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=1, align='edge', color='skyblue')\n",
    "                    smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                    smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                    # Plot Raw PSTH\n",
    "                    raw_psth_ax.bar(displayed_time_array, displayed_counts, width=1, align='edge', color='gray')\n",
    "                    raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                    raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                    # Plot Raster\n",
    "                    for trial_index, trial in enumerate(spike_trains):\n",
    "                        spike_times = time_array[np.where(trial == 1)]\n",
    "                        raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1)\n",
    "                    raster_ax.set_title(f'{stim} Raster')\n",
    "                    raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        return fig\n",
    "\n",
    "    def plot_combined_psth_and_raster_changebinsize(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, bin_size=1, show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window, smoothing window, and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to plot the PSTH and raster plots.\n",
    "            time_window (tuple, optional): The window of time to plot, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        max_smoothed_counts = 0\n",
    "        max_raw_counts = 0\n",
    "\n",
    "        all_smoothed_counts = []\n",
    "        all_raw_counts = []\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    all_smoothed_counts.append(smoothed_counts)\n",
    "                    all_raw_counts.append(displayed_counts)\n",
    "\n",
    "                    max_smoothed_counts = max(max_smoothed_counts, np.max(smoothed_counts))\n",
    "                    max_raw_counts = max(max_raw_counts, np.max(displayed_counts))\n",
    "\n",
    "        # Set y-limits based on the maximum counts across all stimulations\n",
    "        ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "        ylimit_raw = 1.1 * max_raw_counts\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    # Plot Smoothed PSTH\n",
    "                    smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=bin_size, align='edge', color='skyblue')\n",
    "                    smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                    smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "                    smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "                    # Plot Raw PSTH\n",
    "                    raw_psth_ax.bar(displayed_time_array, displayed_counts, width=bin_size, align='edge', color='gray')\n",
    "                    raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                    raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "                    raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "                    # Plot Raster\n",
    "                    for trial_index, trial in enumerate(binned_spike_trains):\n",
    "                        spike_times = new_time_array[np.where(trial > 0)]\n",
    "                        spike_times = spike_times[(spike_times >= displayed_time_array[0]) & (spike_times < displayed_time_array[-1])]\n",
    "                        raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "                    raster_ax.set_title(f'{stim} Raster')\n",
    "                    raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "    def extract_psth_and_raster_data(self, groupname, recordingname, cid, time_window=None, bin_size=1, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Extracts the PSTH and raster data for a specific cid within a specific group and recording across all stimulations.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to extract the PSTH and raster data.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the extracted data. The structure of the dictionary is as follows:\n",
    "            \n",
    "            {\n",
    "                'groupname': str,  # The name of the group\n",
    "                'recordingname': str,  # The name of the recording\n",
    "                'cid': str,  # The cell ID\n",
    "                'stim_labels': list of str,  # List of stimulation labels\n",
    "                'time_array': numpy.ndarray,  # Array of time points based on the specified bin size (1D array, length depends on bin size)\n",
    "                'data': {  # Nested dictionary containing data for each stimulation\n",
    "                    'stim_label_1': {  # Replace 'stim_label_1' with actual stimulation labels\n",
    "                        'spike_trains': numpy.ndarray,  # Binned spike trains (2D array, shape: [number of trials, length of new_time_array])\n",
    "                        'raw_counts': numpy.ndarray,  # Raw spike counts for each time bin (1D array, length depends on time window and bin size)\n",
    "                        'smoothed_counts': numpy.ndarray,  # Smoothed spike counts for each time bin (1D array, same length as raw_counts)\n",
    "                        'time_array': numpy.ndarray  # Array of time points for the specified time window (1D array, length depends on time window and bin size)\n",
    "                    },\n",
    "                    'stim_label_2': { ... },  # Repeat for each stimulation label\n",
    "                    ...\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        extracted_data = {\n",
    "            'groupname': groupname,\n",
    "            'recordingname': recordingname,\n",
    "            'cid': cid,\n",
    "            'stim_labels': stim_labels,\n",
    "            'time_array': new_time_array,\n",
    "            'data': {}\n",
    "        }\n",
    "\n",
    "        for stim in stim_labels:\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "                    \n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    extracted_data['data'][stim] = {\n",
    "                        'spike_trains': binned_spike_trains,\n",
    "                        'raw_counts': displayed_counts,\n",
    "                        'smoothed_counts': smoothed_counts,\n",
    "                        'time_array': displayed_time_array\n",
    "                    }\n",
    "\n",
    "        return extracted_data\n",
    "\n",
    "    def plot_psth_and_raster(self, extracted_data, show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots using the extracted data.\n",
    "\n",
    "        Args:\n",
    "            extracted_data (dict): The data extracted by the extract_psth_and_raster_data method. \n",
    "                                The structure of the dictionary should be:\n",
    "                                {\n",
    "                                    'groupname': str,\n",
    "                                    'recordingname': str,\n",
    "                                    'cid': str,\n",
    "                                    'stim_labels': list of str,\n",
    "                                    'time_array': numpy.ndarray,\n",
    "                                    'data': {\n",
    "                                        'stim_label_1': {\n",
    "                                            'spike_trains': numpy.ndarray,\n",
    "                                            'raw_counts': numpy.ndarray,\n",
    "                                            'smoothed_counts': numpy.ndarray,\n",
    "                                            'time_array': numpy.ndarray\n",
    "                                        },\n",
    "                                        ...\n",
    "                                    }\n",
    "                                }\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {extracted_data[\"cid\"]}, '\n",
    "                    f'Group: {extracted_data[\"groupname\"]}, Recording: {extracted_data[\"recordingname\"]}')\n",
    "\n",
    "        max_smoothed_counts = 0\n",
    "        max_raw_counts = 0\n",
    "\n",
    "        for stim, data in extracted_data['data'].items():\n",
    "            max_smoothed_counts = max(max_smoothed_counts, np.max(data['smoothed_counts']))\n",
    "            max_raw_counts = max(max_raw_counts, np.max(data['raw_counts']))\n",
    "\n",
    "        ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "        ylimit_raw = 1.1 * max_raw_counts\n",
    "\n",
    "        for i, stim in enumerate(extracted_data['stim_labels']):\n",
    "            if stim in extracted_data['data']:\n",
    "                smoothed_psth_ax = axes[0, i]\n",
    "                raw_psth_ax = axes[1, i]\n",
    "                raster_ax = axes[2, i]\n",
    "                data = extracted_data['data'][stim]\n",
    "\n",
    "                # Plot Smoothed PSTH\n",
    "                smoothed_psth_ax.bar(data['time_array'], data['smoothed_counts'], width=1, align='edge', color='skyblue')\n",
    "                smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                smoothed_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "                # Plot Raw PSTH\n",
    "                raw_psth_ax.bar(data['time_array'], data['raw_counts'], width=1, align='edge', color='gray')\n",
    "                raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                raw_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "                # Plot Raster\n",
    "                for trial_index, trial in enumerate(data['spike_trains']):\n",
    "                    spike_times = extracted_data['time_array'][np.where(trial > 0)]\n",
    "                    spike_times = spike_times[(spike_times >= data['time_array'][0]) & (spike_times < data['time_array'][-1])]\n",
    "                    raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "                raster_ax.set_title(f'{stim} Raster')\n",
    "                raster_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot if requested\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def process_and_store_psth_raster_data(self, is_single_unit=None, cell_type=None, stim_responsivity=None, \n",
    "                                        time_window=None, bin_size=1, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid', extracts the PSTH and raster data,\n",
    "        and stores the results in a structured format.\n",
    "\n",
    "        Args:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary containing the extracted data for each group, recording, and cid.\n",
    "                The structure of the dictionary is as follows:\n",
    "                {\n",
    "                    'groupname_1': {\n",
    "                        'recordingname_1': {\n",
    "                            'cid_1': extracted_data,\n",
    "                            'cid_2': extracted_data,\n",
    "                            ...\n",
    "                        },\n",
    "                        'recordingname_2': { ... },\n",
    "                        ...\n",
    "                    },\n",
    "                    'groupname_2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        stored_data = {}\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "                is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "\n",
    "            # Extract the PSTH and raster data\n",
    "            extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                            time_window=time_window, bin_size=bin_size, smoothing_window=smoothing_window)\n",
    "\n",
    "            # Add the parameters used to the extracted data\n",
    "            extracted_data['bin_size'] = bin_size\n",
    "            extracted_data['smoothing_window'] = smoothing_window\n",
    "            extracted_data['time_window'] = time_window\n",
    "\n",
    "            # Ensure the nested dictionary structure is created\n",
    "            if groupname not in stored_data:\n",
    "                stored_data[groupname] = {}\n",
    "            if recordingname not in stored_data[groupname]:\n",
    "                stored_data[groupname][recordingname] = {}\n",
    "            \n",
    "            stored_data[groupname][recordingname][cid] = extracted_data\n",
    "\n",
    "        return stored_data\n",
    "\n",
    "    def plot_all_psth_and_raster(self, stored_data, output_dir='/Volumes/MannySSD/figures', folder_name='PSTH_and_Rasters_RS_SUA',file_format='svg', show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots using the stored data for each unique cid per recording per group.\n",
    "\n",
    "        Args:\n",
    "            stored_data (dict): The nested dictionary containing the extracted data.\n",
    "                                The structure of the dictionary should be:\n",
    "                                {\n",
    "                                    'groupname_1': {\n",
    "                                        'recordingname_1': {\n",
    "                                            'cid_1': {\n",
    "                                                'data': extracted_data,\n",
    "                                                'bin_size': int,\n",
    "                                                'smoothing_window': int,\n",
    "                                                'time_window': tuple,\n",
    "                                            },\n",
    "                                            'cid_2': { ... },\n",
    "                                            ...\n",
    "                                        },\n",
    "                                        'recordingname_2': { ... },\n",
    "                                        ...\n",
    "                                    },\n",
    "                                    'groupname_2': { ... },\n",
    "                                    ...\n",
    "                                }\n",
    "            output_dir (str): The root directory where the figures will be saved.\n",
    "            folder_name (str): The folder name to be created within the output directory.\n",
    "            file_format (str, optional): The format in which to save the figures. Default is 'svg'.\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        # Validate the file format\n",
    "        if file_format not in ['svg', 'png']:\n",
    "            raise ValueError(\"file_format must be either 'svg' or 'png'\")\n",
    "\n",
    "        # Create the output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Create the folder within the output directory\n",
    "        save_dir = os.path.join(output_dir, folder_name)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        for groupname, recordings in stored_data.items():\n",
    "            group_dir = os.path.join(save_dir, groupname)\n",
    "            if not os.path.exists(group_dir):\n",
    "                os.makedirs(group_dir)\n",
    "\n",
    "            for recordingname, cids in recordings.items():\n",
    "                recording_dir = os.path.join(group_dir, recordingname)\n",
    "                if not os.path.exists(recording_dir):\n",
    "                    os.makedirs(recording_dir)\n",
    "\n",
    "                for cid, extracted_data in cids.items():\n",
    "                    fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "                    fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, '\n",
    "                                f'Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "                    max_smoothed_counts = 0\n",
    "                    max_raw_counts = 0\n",
    "\n",
    "                    for stim, data in extracted_data['data'].items():\n",
    "                        max_smoothed_counts = max(max_smoothed_counts, np.max(data['smoothed_counts']))\n",
    "                        max_raw_counts = max(max_raw_counts, np.max(data['raw_counts']))\n",
    "\n",
    "                    ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "                    ylimit_raw = 1.1 * max_raw_counts\n",
    "                    bin_size = extracted_data['bin_size']\n",
    "\n",
    "                    for i, stim in enumerate(extracted_data['stim_labels']):\n",
    "                        if stim in extracted_data['data']:\n",
    "                            smoothed_psth_ax = axes[0, i]\n",
    "                            raw_psth_ax = axes[1, i]\n",
    "                            raster_ax = axes[2, i]\n",
    "                            data = extracted_data['data'][stim]\n",
    "\n",
    "                            # Plot Smoothed PSTH\n",
    "                            smoothed_psth_ax.bar(data['time_array'], data['smoothed_counts'], width=bin_size, align='edge', color='skyblue')\n",
    "                            smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "                            smoothed_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                            smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "                            # Plot Raw PSTH\n",
    "                            raw_psth_ax.bar(data['time_array'], data['raw_counts'], width=bin_size, align='edge', color='gray')\n",
    "                            raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "                            raw_psth_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "                            raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "                            # Plot Raster\n",
    "                            for trial_index, trial in enumerate(data['spike_trains']):\n",
    "                                spike_times = extracted_data['time_array'][np.where(trial > 0)]\n",
    "                                spike_times = spike_times[(spike_times >= data['time_array'][0]) & (spike_times < data['time_array'][-1])]\n",
    "                                raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "                            raster_ax.set_title(f'{stim} Raster')\n",
    "                            raster_ax.set_xlim(data['time_array'][0], data['time_array'][-1])\n",
    "\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    if show:\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        save_path = os.path.join(recording_dir, f'psth_raster_{cid}.{file_format}')\n",
    "                        fig.savefig(save_path, transparent=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_baseline_and_response(self, groupname, recordingname, cid, baseline_window=(-500, 0), response_window=(0, 50)):\n",
    "        \"\"\"\n",
    "        Calculates the mean baseline firing rate, mean response magnitude, and z-scored magnitude for each stimulation.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID.\n",
    "            baseline_window (tuple): The time window for calculating the baseline firing rate. Default is (-500, 0).\n",
    "            response_window (tuple): The time window for calculating the response magnitude. Default is (0, 50).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the calculated values for each stimulation.\n",
    "                {\n",
    "                    'stim1': {\n",
    "                        'mean_baseline_firing_rate': float,\n",
    "                        'mean_response_magnitude': float,\n",
    "                        'z_scored_magnitude': float\n",
    "                    },\n",
    "                    'stim2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        # Get the full time array from the eed object\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        \n",
    "        # Set a fixed bin size of 1 ms for accurate baseline and response calculation\n",
    "        bin_size = 1\n",
    "        \n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Create masks for baseline and response windows based on the new time array\n",
    "        baseline_indices = (new_time_array >= baseline_window[0]) & (new_time_array < baseline_window[1])\n",
    "        response_indices = (new_time_array >= response_window[0]) & (new_time_array < response_window[1])\n",
    "\n",
    "        # Dictionary to store results for each stimulation\n",
    "        stim_results = {}\n",
    "\n",
    "        # Extract PSTH and raster data with the fixed bin size and no smoothing\n",
    "        extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                        time_window=None, bin_size=bin_size, smoothing_window=1)\n",
    "\n",
    "        # Iterate over each stimulation label in the extracted data\n",
    "        for stim, data in extracted_data['data'].items():\n",
    "            # Get the spike trains for the current stimulation\n",
    "            spike_trains = data['spike_trains']\n",
    "            \n",
    "            # Calculate the total number of spikes in the baseline and response windows for each trial\n",
    "            baseline_spikes = spike_trains[:, baseline_indices].sum(axis=1)\n",
    "            response_spikes = spike_trains[:, response_indices].sum(axis=1)\n",
    "            \n",
    "            # Calculate the mean number of spikes during the baseline and response windows\n",
    "            mean_baseline_firing_rate = baseline_spikes.mean()\n",
    "            mean_response_magnitude = response_spikes.mean() - mean_baseline_firing_rate\n",
    "            \n",
    "            # Calculate the standard deviation of the baseline firing rates across trials\n",
    "            baseline_sd = np.std(baseline_spikes)\n",
    "            \n",
    "            # Calculate the z-scored magnitude by dividing the mean response magnitude by the baseline standard deviation\n",
    "            z_scored_magnitude = mean_response_magnitude / baseline_sd if baseline_sd > 0 else 0\n",
    "\n",
    "            # Store the results for the current stimulation in the dictionary\n",
    "            stim_results[stim] = {\n",
    "                'mean_baseline_firing_rate': mean_baseline_firing_rate,\n",
    "                'mean_response_magnitude': mean_response_magnitude,\n",
    "                'z_scored_magnitude': z_scored_magnitude\n",
    "            }\n",
    "\n",
    "        # Return the dictionary containing results for all stimulations\n",
    "        return stim_results\n",
    "\n",
    "    def calculate_population_psth(self, groupname, recordingname, cid, baseline_window=(-500, 0), bin_size=1):\n",
    "        \"\"\"\n",
    "        Calculates the population PSTH by subtracting the overall prestimulus baseline spike rate from every bin value.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID.\n",
    "            baseline_window (tuple): The time window for calculating the baseline firing rate. Default is (-500, 0).\n",
    "            bin_size (int): The size of the bins in milliseconds for the PSTH. Default is 1ms.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the adjusted PSTH values for each stimulation.\n",
    "                {\n",
    "                    'stim1': adjusted_psth_array,\n",
    "                    'stim2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        # Get the full time array from the eed object\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        \n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Create a mask for the baseline window based on the new time array\n",
    "        baseline_indices = (new_time_array >= baseline_window[0]) & (new_time_array < baseline_window[1])\n",
    "\n",
    "        # Dictionary to store adjusted PSTH results for each stimulation\n",
    "        stim_results = {}\n",
    "\n",
    "        # Extract PSTH and raster data with the specified bin size and no smoothing\n",
    "        extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                        time_window=None, bin_size=bin_size, smoothing_window=1)\n",
    "\n",
    "        # Iterate over each stimulation label in the extracted data\n",
    "        for stim, data in extracted_data['data'].items():\n",
    "            # Get the spike trains for the current stimulation\n",
    "            spike_trains = data['spike_trains']\n",
    "            \n",
    "            # Calculate the total number of spikes in the baseline window for each trial\n",
    "            baseline_spikes = spike_trains[:, baseline_indices].sum(axis=1)\n",
    "            \n",
    "            # Calculate the mean baseline firing rate\n",
    "            mean_baseline_firing_rate = baseline_spikes.mean()\n",
    "\n",
    "            # Calculate the PSTH by summing spike counts across trials for each time bin\n",
    "            psth_counts = np.sum(spike_trains, axis=0)\n",
    "            \n",
    "            # Subtract the mean baseline firing rate from each bin value in the PSTH\n",
    "            adjusted_psth = psth_counts - mean_baseline_firing_rate\n",
    "            \n",
    "            # Store the adjusted PSTH values for the current stimulation\n",
    "            stim_results[stim] = adjusted_psth\n",
    "\n",
    "        return stim_results\n",
    "\n",
    "    def process_and_store_psth_raster_data_comprehensive(self, is_single_unit=None, cell_type=None, stim_responsivity=None, \n",
    "                                                        time_window=None, bin_size=1, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid', extracts the PSTH and raster data,\n",
    "        calculates the baseline and response magnitudes, and stores the results in a structured format.\n",
    "\n",
    "        Args:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary containing the extracted data for each group, recording, and cid.\n",
    "                The structure of the dictionary is as follows:\n",
    "                {\n",
    "                    'groupname_1': {\n",
    "                        'recordingname_1': {\n",
    "                            'cid_1': {\n",
    "                                'data': extracted_data,\n",
    "                                'bin_size': int,\n",
    "                                'smoothing_window': int,\n",
    "                                'time_window': tuple,\n",
    "                                'stimulation_results': dict,\n",
    "                                'population_psth': dict\n",
    "                            },\n",
    "                            'cid_2': { ... },\n",
    "                            ...\n",
    "                        },\n",
    "                        'recordingname_2': { ... },\n",
    "                        ...\n",
    "                    },\n",
    "                    'groupname_2': { ... },\n",
    "                    ...\n",
    "                }\n",
    "        \"\"\"\n",
    "        stored_data = {}\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "                is_single_unit=is_single_unit, \n",
    "                cell_type=cell_type, \n",
    "                stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "\n",
    "            # Extract the PSTH and raster data\n",
    "            extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                            time_window=time_window, bin_size=bin_size, smoothing_window=smoothing_window)\n",
    "\n",
    "            \n",
    "            # Calculate baseline and response magnitudes using consistent binning and smoothing parameters store the \n",
    "            # mean_baseline_firing_rate, mean_response_magnitude, z_scored_magnitude per stimulation recoreded at each cid \n",
    "            # for 1ms bins and smoothing window of 1 \n",
    "            # Calculate baseline and response magnitudes for each stimulation\n",
    "            stim_results = self.calculate_baseline_and_response(groupname=groupname, recordingname=recordingname, cid=cid)\n",
    "\n",
    "            # Calculate population PSTH for each stimulation\n",
    "            population_psth = self.calculate_population_psth(groupname=groupname, recordingname=recordingname, cid=cid)\n",
    "\n",
    "            # Add the parameters and calculated values to the extracted data\n",
    "            extracted_data['bin_size'] = bin_size\n",
    "            extracted_data['smoothing_window'] = smoothing_window\n",
    "            extracted_data['time_window'] = time_window\n",
    "            extracted_data['stimulation_results'] = stim_results\n",
    "            extracted_data['population_psth'] = population_psth\n",
    "\n",
    "            # Ensure the nested dictionary structure is created\n",
    "            if groupname not in stored_data:\n",
    "                stored_data[groupname] = {}\n",
    "            if recordingname not in stored_data[groupname]:\n",
    "                stored_data[groupname][recordingname] = {}\n",
    "            \n",
    "            stored_data[groupname][recordingname][cid] = extracted_data\n",
    "\n",
    "        return stored_data\n",
    "\n",
    "    def plot_population_psth(self, stored_data, groupname, time_window=(-500, 999), bin_size=1):\n",
    "        \"\"\"\n",
    "        Plots the mean and SEM population PSTHs using the formatted data for a specified group.\n",
    "\n",
    "        Args:\n",
    "            stored_data (dict): The nested dictionary containing the extracted data.\n",
    "            groupname (str): The name of the group.\n",
    "            time_window (tuple): The window of time to plot, within the range -500 to 999 ms. Default is (-500, 999).\n",
    "            bin_size (int): The size of the bins in milliseconds for the PSTH. Default is 1ms.\n",
    "        \"\"\"\n",
    "        # Initialize a dictionary to store population PSTHs for each stimulation\n",
    "        population_psths = {}\n",
    "\n",
    "        # Iterate through recordings and cell IDs within the specified group\n",
    "        for recordingname, cids in stored_data[groupname].items():\n",
    "            for cid, extracted_data in cids.items():\n",
    "                for stim, psth in extracted_data['population_psth'].items():\n",
    "                    if stim not in population_psths:\n",
    "                        population_psths[stim] = []\n",
    "                    population_psths[stim].append(psth)\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        # Create a mask for the time window to plot\n",
    "        time_mask = (new_time_array >= time_window[0]) & (new_time_array <= time_window[1])\n",
    "        plot_time_array = new_time_array[time_mask]\n",
    "\n",
    "        # Initialize variables to track the global y-axis limits\n",
    "        global_min, global_max = np.inf, -np.inf\n",
    "\n",
    "        # Calculate the mean and SEM for each stimulation to find global y-axis limits\n",
    "        mean_sems = {}\n",
    "        for stim, psth_list in population_psths.items():\n",
    "            if len(psth_list) > 0:\n",
    "                psth_array = np.array(psth_list)\n",
    "                mean_psth = np.mean(psth_array, axis=0)\n",
    "                sem_psth = np.std(psth_array, axis=0) / np.sqrt(psth_array.shape[0])\n",
    "                mean_sems[stim] = (mean_psth, sem_psth)\n",
    "                global_min = min(global_min, (mean_psth - sem_psth).min())\n",
    "                global_max = max(global_max, (mean_psth + sem_psth).max())\n",
    "\n",
    "        # Plot the mean and SEM for each stimulation in a 1x4 grid\n",
    "        fig, axes = plt.subplots(1, len(population_psths), figsize=(20, 5), sharey=True)\n",
    "        if len(population_psths) == 1:\n",
    "            axes = [axes]  # Ensure axes is always iterable\n",
    "\n",
    "        for i, (stim, (mean_psth, sem_psth)) in enumerate(mean_sems.items()):\n",
    "            axes[i].plot(plot_time_array, mean_psth[time_mask], label=f'{stim} Mean PSTH')\n",
    "            axes[i].fill_between(plot_time_array, mean_psth[time_mask] - sem_psth[time_mask], \n",
    "                                mean_psth[time_mask] + sem_psth[time_mask], alpha=0.5, label=f'{stim} SEM')\n",
    "            axes[i].set_title(f'Mean and SEM Population PSTH for {stim}')\n",
    "            axes[i].set_xlabel('Time (ms)')\n",
    "            axes[i].set_ylabel('Firing Rate (spikes/bin)')\n",
    "            axes[i].set_ylim(global_min, global_max)\n",
    "            axes[i].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_psth_and_raster_data(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, bin_size=1):\n",
    "        \"\"\"\n",
    "        Computes PSTH and raster data for a specific cid within a specific group and recording across all stimulations,\n",
    "        with an optional custom time window, smoothing window, and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The cell ID for which to compute the PSTH and raster data.\n",
    "            time_window (tuple, optional): The window of time to consider, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the group name, recording name, cid, and the smoothed PSTH counts, raw PSTH counts, \n",
    "                raster spike times, and time arrays for each stimulation.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        results = {\n",
    "            'groupname': groupname,\n",
    "            'recordingname': recordingname,\n",
    "            'cid': cid,\n",
    "            'data': {\n",
    "                stim: {\n",
    "                    'smoothed_counts': None,\n",
    "                    'raw_counts': None,\n",
    "                    'raster_spike_times': [],\n",
    "                    'time_array': None\n",
    "                }\n",
    "                for stim in stim_labels\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]\n",
    "\n",
    "                    # Bin the spike trains into the specified bin size\n",
    "                    binned_spike_trains = np.add.reduceat(spike_trains, np.arange(0, spike_trains.shape[1], bin_size), axis=1)\n",
    "\n",
    "                    # Calculate PSTH\n",
    "                    all_spikes = np.concatenate([np.where(trial > 0)[0] for trial in binned_spike_trains])\n",
    "                    counts, _ = np.histogram(all_spikes, bins=len(new_time_array), range=(0, len(new_time_array)))\n",
    "\n",
    "                    # Apply time window if specified\n",
    "                    if time_window:\n",
    "                        start_idx = np.searchsorted(new_time_array, time_window[0])\n",
    "                        end_idx = np.searchsorted(new_time_array, time_window[1], side='right')\n",
    "                        displayed_time_array = new_time_array[start_idx:end_idx]\n",
    "                        displayed_counts = counts[start_idx:end_idx]\n",
    "                    else:\n",
    "                        displayed_time_array = new_time_array\n",
    "                        displayed_counts = counts\n",
    "\n",
    "                    # Smooth the data for the smoothed PSTH\n",
    "                    smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "\n",
    "                    # Collect raster spike times\n",
    "                    raster_spike_times = []\n",
    "                    for trial in binned_spike_trains:\n",
    "                        spike_times = new_time_array[np.where(trial > 0)]\n",
    "                        spike_times = spike_times[(spike_times >= displayed_time_array[0]) & (spike_times < displayed_time_array[-1])]\n",
    "                        raster_spike_times.append(spike_times)\n",
    "\n",
    "                    # Store results\n",
    "                    results['data'][stim]['smoothed_counts'] = smoothed_counts\n",
    "                    results['data'][stim]['raw_counts'] = displayed_counts\n",
    "                    results['data'][stim]['raster_spike_times'] = raster_spike_times\n",
    "                    results['data'][stim]['time_array'] = displayed_time_array\n",
    "\n",
    "        return results\n",
    "\n",
    "    def plot_from_computed_data(self, computed_data, bin_size=1, show=False):\n",
    "        \"\"\"\n",
    "        Plots combined PSTH and raster plots using precomputed data.\n",
    "\n",
    "        Args:\n",
    "            computed_data (dict): The precomputed data containing the group name, recording name, cid, \n",
    "                                smoothed PSTH counts, raw PSTH counts, raster spike times, and time arrays for each stimulation.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            show (bool, optional): Whether to display the plot. Default is False.\n",
    "        \"\"\"\n",
    "        groupname = computed_data['groupname']\n",
    "        recordingname = computed_data['recordingname']\n",
    "        cid = computed_data['cid']\n",
    "        data = computed_data['data']\n",
    "\n",
    "        stim_labels = list(data.keys())\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))  # Adjusted for 3 rows of plots\n",
    "        fig.suptitle(f'Comprehensive Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        max_smoothed_counts = max(np.max(data[stim]['smoothed_counts']) for stim in stim_labels)\n",
    "        max_raw_counts = max(np.max(data[stim]['raw_counts']) for stim in stim_labels)\n",
    "\n",
    "        ylimit_smoothed = 1.1 * max_smoothed_counts\n",
    "        ylimit_raw = 1.1 * max_raw_counts\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "\n",
    "            smoothed_counts = data[stim]['smoothed_counts']\n",
    "            raw_counts = data[stim]['raw_counts']\n",
    "            raster_spike_times = data[stim]['raster_spike_times']\n",
    "            displayed_time_array = data[stim]['time_array']\n",
    "\n",
    "            # Plot Smoothed PSTH\n",
    "            smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=bin_size, align='edge', color='skyblue')\n",
    "            smoothed_psth_ax.set_title(f'{stim} Smoothed PSTH')\n",
    "            smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "            smoothed_psth_ax.set_ylim(0, ylimit_smoothed)\n",
    "\n",
    "            # Plot Raw PSTH\n",
    "            raw_psth_ax.bar(displayed_time_array, raw_counts, width=bin_size, align='edge', color='gray')\n",
    "            raw_psth_ax.set_title(f'{stim} Raw PSTH')\n",
    "            raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "            raw_psth_ax.set_ylim(0, ylimit_raw)\n",
    "\n",
    "            # Plot Raster\n",
    "            for trial_index, spike_times in enumerate(raster_spike_times):\n",
    "                raster_ax.eventplot(spike_times, lineoffsets=trial_index + 1, linelengths=1, color='black')\n",
    "            raster_ax.set_title(f'{stim} Raster')\n",
    "            raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    def compute_population_psth_and_raster_data(self, groupname, cell_type, is_single_unit=1.0, time_window=None, smoothing_window=5, bin_size=1):\n",
    "        \"\"\"\n",
    "        Computes population-level PSTH data for a specific cell type within a specific group across all recordings,\n",
    "        with an optional custom time window, smoothing window, and bin size.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            cell_type (str): The cell type for which to compute the population PSTH data.\n",
    "            is_single_unit (float, optional): Filter for single units. Default is 1.0.\n",
    "            time_window (tuple, optional): The window of time to consider, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH. Default is 1ms.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the group name, cell type, and the smoothed PSTH counts, raw PSTH counts, \n",
    "                and time arrays for each stimulation.\n",
    "        \"\"\"\n",
    "        # Filter data to get the relevant cells\n",
    "        channel = self.get_filtered_data('basic_metrics', is_single_unit=is_single_unit, cell_type=cell_type, groupname=groupname)\n",
    "\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        # Create a new time array with the specified bin size\n",
    "        new_time_array = np.arange(time_array[0], time_array[-1] + 1, bin_size)\n",
    "\n",
    "        results = {\n",
    "            'groupname': groupname,\n",
    "            'cell_type': cell_type,\n",
    "            'data': {\n",
    "                stim: {\n",
    "                    'smoothed_counts': np.zeros(len(new_time_array)),\n",
    "                    'raw_counts': np.zeros(len(new_time_array)),\n",
    "                    'time_array': new_time_array\n",
    "                }\n",
    "                for stim in stim_labels\n",
    "            }\n",
    "        }\n",
    "\n",
    "        num_cells = 0\n",
    "\n",
    "        # Loop through each cell and recording\n",
    "        for _, row in channel.iterrows():\n",
    "            recordingname = row['recordingname']\n",
    "            cid = row['cid']\n",
    "            cell_data = self.compute_psth_and_raster_data(groupname, recordingname, cid, time_window, smoothing_window, bin_size)\n",
    "            cell_data = cell_data['data']\n",
    "\n",
    "            for stim in stim_labels:\n",
    "                if cell_data[stim]['smoothed_counts'] is not None:\n",
    "                    results['data'][stim]['smoothed_counts'] += cell_data[stim]['smoothed_counts']\n",
    "                if cell_data[stim]['raw_counts'] is not None:\n",
    "                    results['data'][stim]['raw_counts'] += cell_data[stim]['raw_counts']\n",
    "\n",
    "            num_cells += 1\n",
    "\n",
    "        # Compute the mean by dividing by the number of cells\n",
    "        if num_cells > 0:\n",
    "            for stim in stim_labels:\n",
    "                results['data'][stim]['smoothed_counts'] /= num_cells\n",
    "                results['data'][stim]['raw_counts'] /= num_cells\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    def plot_combined_psth_and_raster_normalized(self, groupname, recordingname, cid, time_window=None, smoothing_window=5, normalize=True, show=False):\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(24, 15))\n",
    "        fig.suptitle(f'Normalized Neural Response Analysis for CID: {cid}, Group: {groupname}, Recording: {recordingname}')\n",
    "\n",
    "        for i, stim in enumerate(stim_labels):\n",
    "            smoothed_psth_ax = axes[0, i]\n",
    "            raw_psth_ax = axes[1, i]\n",
    "            raster_ax = axes[2, i]\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid)\n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "\n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = [train for train in spike_trains[0] if np.any(train == 1)]  # Filter to include only trials with spikes\n",
    "\n",
    "                    if not spike_trains:\n",
    "                        for ax in axes[:, i]:  # Iterate over each subplot in the column\n",
    "                            ax.text(0.5, 0.5, 'No spikes detected', horizontalalignment='center', verticalalignment='center', transform=ax.transAxes)\n",
    "                            ax.set_axis_off()  # Optionally turn off the axis if no spikes are detected\n",
    "\n",
    "                    else:\n",
    "                        all_spikes = np.concatenate([np.where(train == 1)[0] for train in spike_trains])\n",
    "                        counts, _ = np.histogram(all_spikes, bins=len(time_array), range=(0, len(time_array)))\n",
    "                        number_of_trials = len(spike_trains)\n",
    "\n",
    "                        # Apply normalization if enabled\n",
    "                        normalized_counts = counts / number_of_trials if normalize else counts\n",
    "\n",
    "                        if time_window:\n",
    "                            start_idx = np.searchsorted(time_array, time_window[0])\n",
    "                            end_idx = np.searchsorted(time_array, time_window[1], side='right')\n",
    "                            displayed_time_array = time_array[start_idx:end_idx]\n",
    "                            displayed_counts = normalized_counts[start_idx:end_idx]\n",
    "                        else:\n",
    "                            displayed_time_array = time_array\n",
    "                            displayed_counts = normalized_counts\n",
    "\n",
    "                        # Smoothed PSTH\n",
    "                        smoothed_counts = self.smooth_data(displayed_counts, smoothing_window)\n",
    "                        smoothed_psth_ax.bar(displayed_time_array, smoothed_counts, width=1, align='edge', color='skyblue')\n",
    "                        smoothed_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                        # Raw PSTH\n",
    "                        raw_psth_ax.bar(displayed_time_array, displayed_counts, width=1, align='edge', color='gray')\n",
    "                        raw_psth_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "                        # Raster Plot\n",
    "                        for trial_index, trial in enumerate(spike_trains):\n",
    "                            spike_times = time_array[np.where(trial == 1)]\n",
    "                            raster_ax.eventplot(spike_times, lineoffsets=trial_index + 0.5, linelengths=1)\n",
    "                        raster_ax.set_xlim(displayed_time_array[0], displayed_time_array[-1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if show: \n",
    "            plt.show()\n",
    "        return fig \n",
    "\n",
    "\n",
    "\n",
    "    def save_plots_for_all_units(self):\n",
    "        df = self.get_filtered_data('basic_metrics', is_single_unit=None, cell_type=None, stim_responsivity=None)\n",
    "        base_dir = '/Volumes/MannySSD/output_data_for_emx/saved_rasterplots'\n",
    "        os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in df.groupby(['groupname', 'recordingname', 'cid']):\n",
    "            cid_dir = os.path.join(base_dir, groupname, recordingname, str(cid))\n",
    "            os.makedirs(cid_dir, exist_ok=True)\n",
    "\n",
    "            # Generate and save the standard plot\n",
    "            fig_standard = self.plot_combined_psth_and_raster(groupname, recordingname, cid, time_window=None, smoothing_window=8)\n",
    "            if fig_standard:  # Check if a figure was returned\n",
    "                fig_standard.savefig(os.path.join(cid_dir, 'standard_plot.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "                plt.close(fig_standard)\n",
    "\n",
    "            # Generate and save the normalized plot\n",
    "            fig_normalized = self.plot_combined_psth_and_raster_normalized(groupname, recordingname, cid, time_window=None, smoothing_window=8, normalize=True)\n",
    "            if fig_normalized:  # Check if a figure was returned\n",
    "                fig_normalized.savefig(os.path.join(cid_dir, 'normalized_plot.png'), dpi=300, format='png', bbox_inches='tight')\n",
    "                plt.close(fig_normalized)\n",
    "\n",
    "    def calculate_psth_data(self, groupname, recordingname, cid, time_window=None, normalize=True, filter_empty_trials=True):\n",
    "        \"\"\"\n",
    "        Overview\n",
    "\n",
    "        The calculate_psth_data method is part of the DataFrameManager class.\n",
    "        It calculates peristimulus time histograms (PSTH) for specified neural recordings and cells. \n",
    "        The method handles multiple stimuli and can be configured to include all trials or only those with spikes, based on user preference. \n",
    "        This method does not plot the data but returns a structured dictionary containing counts and time arrays for further analysis or visualization.\n",
    "\n",
    "        Parameters\n",
    "\n",
    "        groupname (str): The name of the experimental group. This parameter specifies which group the data should be retrieved from.\n",
    "        recordingname (str): The name of the recording session. It determines from which recording to fetch the spike data.\n",
    "        cid (str): Cell identifier. This specifies the neuron for which the PSTH will be calculated.\n",
    "        time_window (tuple of int, optional): A tuple representing the start and end of the time window in milliseconds within which to calculate the PSTH. If None, the entire span of time_array will be used. Default is None.\n",
    "        normalize (bool): Determines whether the spike counts should be normalized. If True, the counts are divided by the number of trials used in the calculation. The exact denominator depends on the filter_empty_trials parameter. Default is True.\n",
    "        filter_empty_trials (bool): Controls whether trials without any spikes should be included in the calculation. If True, only trials with spikes are considered. This affects both the counts and the normalization process. Default is True.\n",
    "        \n",
    "        Returns\n",
    "\n",
    "        psth_results (dict): A dictionary where keys are stimulus labels and values are dictionaries containing:\n",
    "        'counts' (numpy.array): An array of spike counts per time bin. This array is normalized if normalize is set to True.\n",
    "        'time_array' (numpy.array): The time points corresponding to the bins in 'counts'.\n",
    "        \n",
    "        Detailed Description\n",
    "\n",
    "        The method begins by accessing stimulus labels and relative time arrays from the trialTagsLabels and relative_time_ms attributes of the eed (Electrophysiology Extraction Data) instance, respectively. \n",
    "        It then iterates over each stimulus, fetching spike train data from the appropriate DataFrame. The spike trains are filtered based on the filter_empty_trials setting.\n",
    "\n",
    "        Spike trains are processed into histograms using numpy.histogram, with bin edges aligned to time_array. \n",
    "        If a time_window is provided, the method restricts the calculation to the specified window, adjusting both the counts and time arrays accordingly.\n",
    "\n",
    "        Normalization, when enabled, divides the counts by the number of trials that were used in the histogram calculation. \n",
    "        The choice of denominator is influenced by the filter_empty_trials flag: if True, it uses the count of trials with spikes; if False, it uses the total number of trials.\n",
    "        \"\"\"\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        psth_results = {}\n",
    "\n",
    "        for stim in stim_labels:\n",
    "            df_name = f'psth_dataframe_{stim}'\n",
    "            if df_name in self.dataframes:\n",
    "                df = self.dataframes[df_name]\n",
    "                condition = (df['groupname'] == groupname) & (df['recordingname'] == recordingname) & (df['cid'] == cid) \n",
    "                spike_trains = df.loc[condition, 'SpikeTrains_for_PSTHs'].values\n",
    "                \n",
    "                # Get the laminar label for this cid\n",
    "                if condition.sum() == 0:\n",
    "                    print(f\"No data found for Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "                    return None\n",
    "                \n",
    "                #get the laminar label for this cid\n",
    "                label = df.loc[condition, 'LaminarLabel'].values[0]\n",
    "                \n",
    "                \n",
    "                \n",
    "                if spike_trains.size > 0:\n",
    "                    spike_trains = spike_trains[0]  # Get the array of spike trains\n",
    "                    if filter_empty_trials:\n",
    "                        spike_trains = [train for train in spike_trains if np.any(train == 1)]  # Only include trials with spikes\n",
    "\n",
    "                    if len(spike_trains) == 0:  # Checking if the list is empty\n",
    "                        psth_results[stim] = {'counts': [], 'time_array': [], 'LaminarLabel': label}\n",
    "                    else:\n",
    "                        all_spikes = np.concatenate([np.where(train == 1)[0] for train in spike_trains])\n",
    "                        counts, _ = np.histogram(all_spikes, bins=len(time_array), range=(0, len(time_array)))\n",
    "                        \n",
    "                        if normalize:\n",
    "                            number_of_trials = len(spike_trains) if filter_empty_trials else len(spike_trains[0])\n",
    "                            normalized_counts = counts / number_of_trials\n",
    "                        else:\n",
    "                            normalized_counts = counts\n",
    "\n",
    "                        if time_window:\n",
    "                            start_idx = np.searchsorted(time_array, time_window[0])\n",
    "                            end_idx = np.searchsorted(time_array, time_window[1], side='right')\n",
    "                            displayed_time_array = time_array[start_idx:end_idx]\n",
    "                            displayed_counts = normalized_counts[start_idx:end_idx]\n",
    "                        else:\n",
    "                            displayed_time_array = time_array\n",
    "                            displayed_counts = normalized_counts\n",
    "\n",
    "                        psth_results[stim] = {'counts': displayed_counts, 'time_array': displayed_time_array, 'LaminarLabel': label}\n",
    "\n",
    "        return psth_results\n",
    "\n",
    "    def iterate_by_group_recording_cid(self, is_single_unit=None, cell_type=None, stim_responsivity=None):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid' after applying specified filters.\n",
    "        \n",
    "        This generator function fetches data using predefined criteria, applies additional filters, and yields\n",
    "        each subset of the data grouped by 'groupname', 'recordingname', and 'cid'. It is designed to facilitate\n",
    "        the processing of large datasets by providing chunks of data one group at a time, which can be particularly\n",
    "        useful for processing steps that do not require the complete dataset to be held in memory.\n",
    "\n",
    "        Parameters:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "        \n",
    "        Yields:\n",
    "            tuple: A tuple containing (groupname, recordingname, cid) as a tuple and the corresponding group DataFrame. \n",
    "                This allows for further processing of the data specific to each group.\n",
    "\n",
    "        Example:\n",
    "            for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(cell_type='pyramidal'):\n",
    "                # Process each group DataFrame here\n",
    "                print(groupname, recordingname, cid, len(group_df))\n",
    "        \"\"\"\n",
    "        # Fetch the data with potential filters applied\n",
    "        df = self.get_filtered_data('basic_metrics', is_single_unit=is_single_unit, \n",
    "                                    cell_type=cell_type, stim_responsivity=stim_responsivity)\n",
    "        \n",
    "        # Group the data by 'groupname', 'recordingname', and 'cid'\n",
    "        for (groupname, recordingname, cid), group_df in df.groupby(['groupname', 'recordingname', 'cid']):\n",
    "            yield (groupname, recordingname, cid), group_df\n",
    "\n",
    "    def calculate_all_psths(self, is_single_unit=None, cell_type=None, stim_responsivity=None, time_window=None, normalize=True, filter_empty_trials=True):\n",
    "        \"\"\"\n",
    "        Calculates PSTH for all cells across specified groups and recordings, aggregates the results into two DataFrames.\n",
    "        \n",
    "        One DataFrame contains the means of 'counts' over the entire duration for each stimulation. The other DataFrame contains detailed PSTH data for \n",
    "        each stimulation including both 'counts' and 'time_array', which can be selectively extracted based on a 'time_window'.\n",
    "\n",
    "        Parameters:\n",
    "            is_single_unit (bool, optional): Filter based on whether cells are considered single units.\n",
    "            cell_type (str, optional): Filter based on cell type.\n",
    "            stim_responsivity (bool, optional): Filter based on cell responsiveness to stimuli.\n",
    "            time_window (tuple of int, optional): Specific time window to calculate PSTH data over. If None, the full range is used.\n",
    "            normalize (bool): Whether to normalize the spike counts.\n",
    "            filter_empty_trials (bool): Whether to include only trials with spikes.\n",
    "\n",
    "        Returns:\n",
    "            tuple of DataFrames: (mean_df, detailed_df)\n",
    "                mean_df: DataFrame with columns 'mean_stimulation', 'Stimulation', 'Group', 'cid', 'recordingname'.\n",
    "                detailed_df: DataFrame with columns 'Stimulation', 'cid', 'groupname', 'recordingname', 'counts', 'time_array'.\n",
    "        \"\"\"\n",
    "        mean_results = []\n",
    "        detailed_results = []\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # Get stimulation labels from the eed object\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "            is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "            \n",
    "            psth_results = self.calculate_psth_data(groupname, recordingname, cid, time_window=time_window, \n",
    "                                                    normalize=normalize, filter_empty_trials=filter_empty_trials)\n",
    "            if psth_results is None:\n",
    "                continue\n",
    "            \n",
    "            # Process each stimulation's PSTH results\n",
    "            for stim_label in stim_labels:\n",
    "                \n",
    "                if stim_label in psth_results:\n",
    "                    counts = psth_results[stim_label]['counts']\n",
    "                    time_array = psth_results[stim_label]['time_array']\n",
    "                    \n",
    "                     # Get 'LaminarLabel' with a default value if it's not found\n",
    "                    laminar_label = psth_results[stim_label].get('LaminarLabel', 'DefaultLabel') #added 'LaminarLabel' if it's not found, it will be assigned 'DefaultLabel'\n",
    "                    #print(stim_label, laminar_label) #use print to check if the laminar label is being correctly assigned to the stim_label\n",
    "\n",
    "                    # Ensure counts is an array and has elements before calculating mean\n",
    "                    if isinstance(counts, np.ndarray) and counts.size > 0:\n",
    "                        mean_stimulation = np.mean(counts)\n",
    "                        #instead find the max value of the counts array and store it in mean_stimulation\n",
    "                        #mean_stimulation = np.max(counts)\n",
    "                    else:\n",
    "                        mean_stimulation = np.nan\n",
    "\n",
    "                    # Append to results for mean DataFrame\n",
    "                    mean_results.append({\n",
    "                        'Group': groupname,\n",
    "                        'Stimulation': stim_label,\n",
    "                        'mean_stimulation': mean_stimulation,\n",
    "                        'cid': cid,\n",
    "                        'LaminarLabel': laminar_label, #added 'LaminarLabel\n",
    "                        'recordingname': recordingname,\n",
    "                 \n",
    "                    })\n",
    "\n",
    "                    # Append to results for detailed DataFrame\n",
    "                    detailed_results.append({\n",
    "                        'Stimulation': stim_label,\n",
    "                        'cid': cid,\n",
    "                        'LaminarLabel': laminar_label, #added 'LaminarLabel\n",
    "                        'groupname': groupname,\n",
    "                        'recordingname': recordingname,\n",
    "                        'counts': counts,\n",
    "                        'time_array': time_array, \n",
    "                    })\n",
    "\n",
    "        # Create DataFrames from the aggregated results\n",
    "        mean_df = pd.DataFrame(mean_results)\n",
    "        detailed_df = pd.DataFrame(detailed_results)\n",
    "        return mean_df, detailed_df, psth_results\n",
    "    \n",
    "    def plot_mean_stimulation_box_and_strip(self, mean_df, groups=None, stimulations=None, show_outliers=True, hue_order=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations using the mean stimulation data.\n",
    "\n",
    "        Args:\n",
    "            mean_df (DataFrame): The DataFrame containing 'mean_stimulation' along with 'Group' and 'Stimulation' columns.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers in the stripplot.\n",
    "            hue_order (list, optional): Order of the hue levels, specifying how groups should be ordered in the plot.\n",
    "\n",
    "        Usage:\n",
    "            mean_df, _ = whisker_df_manager.calculate_all_psths(...)\n",
    "            whisker_df_manager.plot_mean_stimulation_box_and_strip(mean_df, ...)\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Filter by specified groups and stimulations if provided\n",
    "        if groups:\n",
    "            mean_df = mean_df[mean_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            mean_df = mean_df[mean_df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', **stripplot_kwargs)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        plt.show()\n",
    "               \n",
    "    def remove_outliers_by_stimulation(self, df, value_column='mean_stimulation'):\n",
    "        \"\"\"\n",
    "        Removes outliers within each stimulation group based on the interquartile range (IQR).\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the data, expected to have a 'Stimulation' column.\n",
    "            value_column (str): The name of the column from which outliers will be removed.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: DataFrame with outliers removed within each stimulation group.\n",
    "        \"\"\"\n",
    "        # Create an empty DataFrame to store results after removing outliers\n",
    "        channel = pd.DataFrame()\n",
    "        \n",
    "        # Process each stimulation group separately\n",
    "        for stim in df['Stimulation'].unique():\n",
    "            sub_df = df[df['Stimulation'] == stim]\n",
    "            Q1 = sub_df[value_column].quantile(0.25)\n",
    "            Q3 = sub_df[value_column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Filter out outliers and append the results to the filtered DataFrame\n",
    "            result_df = sub_df[(sub_df[value_column] >= lower_bound) & (sub_df[value_column] <= upper_bound)]\n",
    "            filtered_df = pd.concat([filtered_df, result_df], ignore_index=True)\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_mean_stimulation_box_and_strip2(self, mean_df, groups=None, stimulations=None, hue_order=None, remove_outliers_option=False, ylim=None, laminar_labels=None, directory=None, file_name=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations using the mean stimulation data, with an option to exclude outliers.\n",
    "        Args:\n",
    "            mean_df (DataFrame): The DataFrame containing 'mean_stimulation' along with 'Group' and 'Stimulation' columns.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            hue_order (list, optional): Order of the hue levels, specifying how groups should be ordered in the plot.\n",
    "            remove_outliers_option (bool, optional): If True, outliers will be removed before plotting.\n",
    "            laminar_labels (list of str, optional): List of laminar labels to include in the plot. '['IG, 'SG','L4']\n",
    "        \"\"\"\n",
    "        # Filter the DataFrame by groups and stimulations if provided\n",
    "        print(\"Initial mean_df shape:\", mean_df.shape)\n",
    "        print(\"mean_df columns:\", mean_df.columns)\n",
    "        \n",
    "        if groups:\n",
    "            mean_df = mean_df[mean_df['Group'].isin(groups)]\n",
    "            print(\"After filtering by groups, mean_df shape:\", mean_df.shape)\n",
    "        if stimulations:\n",
    "            mean_df = mean_df[mean_df['Stimulation'].isin(stimulations)]\n",
    "            print(\"After filtering by stimulations, mean_df shape:\", mean_df.shape)\n",
    "        if laminar_labels:\n",
    "            mean_df = mean_df[mean_df['LaminarLabel'].isin(laminar_labels)]\n",
    "            print(\"After filtering by laminar labels, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            mean_df = self.remove_outliers_by_stimulation(mean_df, 'mean_stimulation')\n",
    "            print(\"After removing outliers, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if mean_df.empty:\n",
    "            raise ValueError(\"The DataFrame is empty after filtering. Check the input parameters and data.\")\n",
    "        \n",
    "        stats_df = self.run_group_comparisons(mean_df, group_column='Group', value_column='mean_stimulation', stim_column='Stimulation')\n",
    "        \n",
    "        # Convert the mean_stimulation column to Hz by multiplying by 1000\n",
    "        mean_df['mean_stimulation'] = mean_df['mean_stimulation']*1000\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', \n",
    "                        boxprops={'edgecolor': 'k', 'linewidth': 2},\n",
    "                        medianprops={'color': 'k', 'linewidth': 2},\n",
    "                        whiskerprops={'color': 'k', 'linewidth': 2},\n",
    "                        capprops={'linewidth': 0},  # Hide the caps\n",
    "                        showfliers=not remove_outliers_option,\n",
    "                        palette={'No_CTZ': '#797979', 'CTZ': '#5a00c2'},\n",
    "                        hue_order=hue_order,\n",
    "                        width=0.75)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=mean_df, x='Stimulation', y='mean_stimulation', hue='Group', \n",
    "                    linewidth=0.6, size=6, alpha=0.7, jitter=True, dodge=True, marker='o',\n",
    "                    palette={'No_CTZ': '#79797933', 'CTZ': '#5a00c233'},  # Lighter colors\n",
    "                    hue_order=hue_order)\n",
    "        \n",
    "        # Adjust opacity of the box fill\n",
    "        for patch in ax.artists:\n",
    "            r, g, b, a = patch.get_facecolor()\n",
    "            patch.set_facecolor((r, g, b, .1))  # Set the fill alpha to 10%\n",
    "        \n",
    "        # Removing other components of the boxplot\n",
    "        for line in ax.lines:\n",
    "            # Only keep the median and the whiskers visible:\n",
    "            # The boxplot consists of 6 lines per group: 0 and 1 are the box, 2 is the median,\n",
    "            # 3 and 4 are the whiskers, 5 is the cap. This might slightly vary depending on Seaborn's implementation.\n",
    "            if line.get_linestyle() != '-':  # Only preserve the median line\n",
    "                line.set_linewidth(0)\n",
    "        \n",
    "        # Set the y-axis limits if specified\n",
    "        if ylim:\n",
    "            plt.ylim(ylim) #ylim is a tuple (min, max)\n",
    "        \n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return stats_df, mean_df\n",
    "\n",
    "    def plot_mean_sem_line2(self, mean_df, groups=None, stimulations=None, hue_order=None, remove_outliers_option=False, ylim=None, laminar_labels=None, directory=None, file_name=None):\n",
    "        \"\"\"\n",
    "        Plots mean and SEM line plots for specified groups and stimulations using the mean stimulation data, with an option to exclude outliers.\n",
    "\n",
    "        Args:\n",
    "            mean_df (DataFrame): The DataFrame containing 'mean_stimulation' along with 'Group' and 'Stimulation' columns.\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            hue_order (list, optional): Order of the hue levels, specifying how groups should be ordered in the plot.\n",
    "            remove_outliers_option (bool, optional): If True, outliers will be removed before plotting.\n",
    "            ylim (tuple, optional): Tuple specifying the y-axis limits (min, max).\n",
    "            laminar_labels (list of str, optional): List of laminar labels to include in the plot. '['IG, 'SG','L4']\n",
    "        \"\"\"\n",
    "        # Filter the DataFrame by groups and stimulations if provided\n",
    "        print(\"Initial mean_df shape:\", mean_df.shape)\n",
    "        print(\"mean_df columns:\", mean_df.columns)\n",
    "        \n",
    "        if groups:\n",
    "            mean_df = mean_df[mean_df['Group'].isin(groups)]\n",
    "            print(\"After filtering by groups, mean_df shape:\", mean_df.shape)\n",
    "        if stimulations:\n",
    "            mean_df = mean_df[mean_df['Stimulation'].isin(stimulations)]\n",
    "            print(\"After filtering by stimulations, mean_df shape:\", mean_df.shape)\n",
    "        if laminar_labels:\n",
    "            mean_df = mean_df[mean_df['LaminarLabel'].isin(laminar_labels)]\n",
    "            print(\"After filtering by laminar labels, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            mean_df = self.remove_outliers_by_stimulation(mean_df, 'mean_stimulation')\n",
    "            print(\"After removing outliers, mean_df shape:\", mean_df.shape)\n",
    "        \n",
    "        # Check if DataFrame is empty\n",
    "        if mean_df.empty:\n",
    "            raise ValueError(\"The DataFrame is empty after filtering. Check the input parameters and data.\")\n",
    "        \n",
    "        # Compute means and SEMs\n",
    "        summary_df = mean_df.groupby(['Group', 'Stimulation']).agg(\n",
    "            mean_value=('mean_stimulation', 'mean'),\n",
    "            sem_value=('mean_stimulation', 'sem')\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Convert the mean_stimulation column to Hz by multiplying by 1000\n",
    "        summary_df['mean_value'] = summary_df['mean_value'] * 1000\n",
    "        summary_df['sem_value'] = summary_df['sem_value'] * 1000\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        for group in groups:\n",
    "            group_data = summary_df[summary_df['Group'] == group]\n",
    "            plt.errorbar(group_data['Stimulation'], group_data['mean_value'], yerr=group_data['sem_value'],\n",
    "                        label=group, color=group_colors[group], capsize=5, marker='o', linestyle='-')\n",
    "        \n",
    "        # Set the y-axis limits if specified\n",
    "        if ylim:\n",
    "            plt.ylim(ylim)\n",
    "        \n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel('Mean Stimulation (Hz)')\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        plt.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        return summary_df, mean_df\n",
    "\n",
    "\n",
    "    def plot_group_cell_distribution(self):\n",
    "        # Dictionary to store the counts for each group and recording\n",
    "        group_counts = {}\n",
    "        recording_count_per_group = {}  # To store the number of recordings per group\n",
    "        stim_responsivity_counts = {}   # To store StimResponsivity counts for SUA and MUA separately\n",
    "        combined_stim_responsivity_counts = {}  # To store combined StimResponsivity counts\n",
    "\n",
    "        # Iterate over each cell grouping\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid():\n",
    "            if groupname not in group_counts:\n",
    "                group_counts[groupname] = {'SUA': {'FS': 0, 'RS': 0}, 'MUA': {'FS': 0, 'RS': 0}}\n",
    "                stim_responsivity_counts[groupname] = {'SUA': {'-1.0': 0, '0.0': 0, '1.0': 0}, 'MUA': {'-1.0': 0, '0.0': 0, '1.0': 0}}\n",
    "                combined_stim_responsivity_counts[groupname] = {'-1.0': 0, '0.0': 0, '1.0': 0}\n",
    "                recording_count_per_group[groupname] = set()\n",
    "\n",
    "            recording_count_per_group[groupname].add(recordingname)\n",
    "\n",
    "            # Summarize the data across recordings\n",
    "            for index, row in group_df.iterrows():\n",
    "                unit_type = 'SUA' if row['IsSingleUnit'] == 1.0 else 'MUA'\n",
    "                cell_type = row['Cell_Type']\n",
    "                stim_responsivity = str(row['StimResponsivity'])\n",
    "                if cell_type in ['FS', 'RS']:\n",
    "                    group_counts[groupname][unit_type][cell_type] += 1\n",
    "                if stim_responsivity in ['-1.0', '0.0', '1.0']:\n",
    "                    stim_responsivity_counts[groupname][unit_type][stim_responsivity] += 1\n",
    "                    combined_stim_responsivity_counts[groupname][stim_responsivity] += 1\n",
    "\n",
    "        # Plotting\n",
    "        plt.rcParams.update({'font.size': 14})  # Increase the base font size\n",
    "        for groupname in group_counts:\n",
    "            fig, axs = plt.subplots(1, 5, figsize=(24, 5))  # Adjust subplot for 5 charts\n",
    "            total_recordings = len(recording_count_per_group[groupname])\n",
    "            total_units = sum(sum(sub_counts.values()) for sub_counts in group_counts[groupname].values())\n",
    "            sua_total = sum(group_counts[groupname]['SUA'].values())\n",
    "            mua_total = sum(group_counts[groupname]['MUA'].values())\n",
    "\n",
    "            # Titles and SUA/MUA Distribution\n",
    "            fig.suptitle(f'Group: {groupname} | Mice: {total_recordings} | Total Units: {total_units} (SUA: {sua_total}, MUA: {mua_total})',\n",
    "                        fontsize=16)\n",
    "\n",
    "            for i, (unit_type, sub_counts) in enumerate(group_counts[groupname].items()):\n",
    "                labels = [f'{ct} ({n})' for ct, n in sub_counts.items()]\n",
    "                sizes = sub_counts.values()\n",
    "                if any(sizes):\n",
    "                    axs[i].pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "                            startangle=90)\n",
    "                    axs[i].set_title(f'{unit_type} Distribution', fontsize=14)\n",
    "\n",
    "            # StimResponsivity Distribution for SUA and MUA\n",
    "            for i, unit_type in enumerate(['SUA', 'MUA']):\n",
    "                labels = [f'{resp} ({count})' for resp, count in stim_responsivity_counts[groupname][unit_type].items()]\n",
    "                sizes = stim_responsivity_counts[groupname][unit_type].values()\n",
    "                if any(sizes):\n",
    "                    axs[i+2].pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "                                startangle=90)\n",
    "                    axs[i+2].set_title(f'{unit_type} Stim Responsivity', fontsize=14)\n",
    "\n",
    "            # Combined StimResponsivity Distribution\n",
    "            labels = [f'{resp} ({count})' for resp, count in combined_stim_responsivity_counts[groupname].items()]\n",
    "            sizes = combined_stim_responsivity_counts[groupname].values()\n",
    "            if any(sizes):\n",
    "                axs[4].pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p) if p > 0 else '',\n",
    "                        startangle=90)\n",
    "                axs[4].set_title('Combined Stim Responsivity', fontsize=14)\n",
    "\n",
    "            plt.tight_layout()  # Adjust layout\n",
    "            plt.show()\n",
    "\n",
    "    def run_group_comparisons(self, df, group_column='Group', value_column='mean_stimulation', stim_column='Stimulation'):\n",
    "        \"\"\"\n",
    "        Performs statistical comparisons between groups for each type of stimulation, checks for normality,\n",
    "        uses the appropriate non-parametric tests, and includes detailed descriptive statistics.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        for stim in df[stim_column].unique():\n",
    "            sub_df = df[df[stim_column] == stim]\n",
    "            groups = sub_df[group_column].unique()\n",
    "\n",
    "            if len(groups) < 2:\n",
    "                continue  # Skip if not enough groups for comparison\n",
    "\n",
    "            group_data = [sub_df[sub_df[group_column] == g][value_column].dropna() for g in groups]\n",
    "\n",
    "            # Skip normality test if any group has less than 3 data points\n",
    "            normality_results = []\n",
    "            normality_p_values = []\n",
    "            for data in group_data:\n",
    "                if len(data) < 3:\n",
    "                    normality_results.append(None)\n",
    "                    normality_p_values.append(None)\n",
    "                else:\n",
    "                    result = shapiro(data)\n",
    "                    normality_results.append(result)\n",
    "                    normality_p_values.append(result.pvalue)\n",
    "\n",
    "            # Choose the appropriate statistical test based on group count\n",
    "            if len(groups) == 2:\n",
    "                stat, p_value = mannwhitneyu(*group_data)\n",
    "                test_used = 'Mann-Whitney U'\n",
    "            elif len(groups) > 2:\n",
    "                stat, p_value = kruskal(*group_data)\n",
    "                test_used = 'Kruskal-Wallis'\n",
    "                # Note: If using Kruskal-Wallis, consider post-hoc tests for detailed group comparisons\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            descriptive_stats = [{\n",
    "                'mean': data.mean(),\n",
    "                'SD': data.std(),\n",
    "                'median': data.median(),\n",
    "                'range_min': data.min(),\n",
    "                'range_max': data.max(),\n",
    "            } for data in group_data]\n",
    "\n",
    "            result_entry = {\n",
    "                'Stimulation': stim,\n",
    "                'Test Used': test_used,\n",
    "                'Test Statistic': stat,\n",
    "                'p-value': p_value,\n",
    "                **{f'N Group{i+1}': len(data) for i, data in enumerate(group_data)},\n",
    "                **{f'Normality p-value Group{i+1}': p for i, p in enumerate(normality_p_values) if p is not None},\n",
    "                **{f'{stat_key} Group{i+1}': stat_val for i, stats in enumerate(descriptive_stats) for stat_key, stat_val in stats.items()}\n",
    "            }\n",
    "\n",
    "            results.append(result_entry)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def prepare_for_boxplot_default(self, dataframe_key, value_column, group_label='groupname'):\n",
    "        \"\"\"\n",
    "        Organizes data into a DataFrame suitable for plotting boxplots by extracting values\n",
    "        from a specified column in detailed DataFrames, including labels for group.\n",
    "\n",
    "        Parameters:\n",
    "            dataframe_key (str): Key to access the specific DataFrame.\n",
    "            value_column (str): Column name from which to extract the value for plotting.\n",
    "            group_label (str): Column name to use as label for grouping in the plot. For example, 'groupname'\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with columns for the specified 'value_column', and 'Group'.\n",
    "        \"\"\"\n",
    "        # Access the specified DataFrame\n",
    "        df = self.dataframes[dataframe_key]\n",
    "\n",
    "        boxplot_data = []\n",
    "\n",
    "        # Iterate over each stored DataFrame key (group)\n",
    "        for group in df[group_label].unique():\n",
    "            group_df = df[df[group_label] == group]\n",
    "\n",
    "            # Check for empty DataFrame\n",
    "            if not group_df.empty:\n",
    "                # Extract specified value column and corresponding group label\n",
    "                for index, row in group_df.iterrows():\n",
    "                    boxplot_data.append({\n",
    "                        value_column: row[value_column],\n",
    "                        'Group': group\n",
    "                    })\n",
    "\n",
    "        # Convert list of data to DataFrame\n",
    "        boxplot_df = pd.DataFrame(boxplot_data)\n",
    "\n",
    "        return boxplot_df\n",
    "    \n",
    "    def unpack_based_on_stim(self, dataframe_key, list_column, cell_type_filter=None, is_single_unit=True, \n",
    "                             stim_responsivity_filter=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Unpacks values from a specified list column in the DataFrame if they are four elements long,\n",
    "        and labels each value according to provided trial tags stored in eed. Includes group and additional filtering options.\n",
    "\n",
    "        Parameters:\n",
    "            dataframe_key (str): Key to access the specific DataFrame.\n",
    "            list_column (str): Column name from which to extract numpy.ndarray for unpacking.\n",
    "            cell_type_filter (str, optional): Filters rows based on 'Cell_Type' (e.g., 'FS' or 'RS').\n",
    "            is_single_unit (bool, optional): Filters rows where 'IsSingleUnit' is 1.0 (True) or 0.0 (False).\n",
    "            stim_responsivity_filter (int, optional): Filters rows based on 'StimResponsivity' (-1, 0, 1).\n",
    "            modulation_label (stf, optional): Column name to use, its a string: 'positive', 'negative' or 'none'\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with unpacked values labeled by stimulation, including additional columns.\n",
    "        \"\"\"\n",
    "        df = self.dataframes[dataframe_key]\n",
    "        \n",
    "        trial_tags_labels = self.eed.trialTagsLabels['trialTagsLabels']\n",
    "\n",
    "        # Convert and filter data as per user requirements\n",
    "        df['IsSingleUnit'] = df['IsSingleUnit'].astype(bool)\n",
    "        df['StimResponsivity'] = df['StimResponsivity'].astype(int)\n",
    "\n",
    "        if cell_type_filter is not None:\n",
    "            df = df[df['Cell_Type'] == cell_type_filter]\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "        if stim_responsivity_filter is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity_filter]\n",
    "        if modulation_label is not None:\n",
    "            df = df[df['ModulationIndex'] == modulation_label]\n",
    "        \n",
    "        unpacked_data = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            values = row[list_column]\n",
    "            if isinstance(values, np.ndarray) and len(values) == 4:\n",
    "                for i, value in enumerate(values):\n",
    "                    unpacked_data.append({\n",
    "                        'Group': row['groupname'],\n",
    "                        'Stimulation': trial_tags_labels[i],\n",
    "                        list_column: value,\n",
    "                        'Cell_Type': row['Cell_Type'],\n",
    "                        'IsSingleUnit': row['IsSingleUnit'],\n",
    "                        'StimResponsivity': row['StimResponsivity'],\n",
    "                        'recordingname': row['recordingname'],\n",
    "                        'cid': row['cid'],\n",
    "                        'ModulationIndex': row['ModulationIndex']\n",
    "                    })\n",
    "\n",
    "        unpacked_df = pd.DataFrame(unpacked_data)\n",
    "\n",
    "        return unpacked_df\n",
    "    \n",
    "    def plot_box_and_strip_default(self, groups=None, stimulations=None, show_outliers=True, hue_order=None, dataframe_key=None, list_column=None, \n",
    "                                   cell_type_filter=None, is_single_unit=None, stim_responsivity_filter=None, \n",
    "                                   remove_outliers_option=False, directory=None, file_name=None, \n",
    "                                   modulation_label=None, firstspike_latency=True):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and stimulations from the unpacked DataFrame,\n",
    "        with color adjustments made directly in the plotting calls. Designed to work with unpacked DataFrame structure.\n",
    "\n",
    "        Args:\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "            modululation_label (str, optional): Filter for 'positive', 'negative' or 'none' modulation\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Prepare data for boxplot (already prepared by a previous method)\n",
    "        boxplot_df = self.unpack_based_on_stim(dataframe_key=dataframe_key, list_column=list_column, cell_type_filter=cell_type_filter, \n",
    "                                               is_single_unit=is_single_unit, stim_responsivity_filter=stim_responsivity_filter, \n",
    "                                               modulation_label=modulation_label, firstspike_latency=firstspike_latency)\n",
    "        # Filter by specified groups and stimulations\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            boxplot_df = self.remove_outliers_by_stimulation_default(boxplot_df, value_column=list_column)\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='Stimulation', y=list_column, hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(stimulations)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=boxplot_df, x='Stimulation', y=list_column, hue='Group', **stripplot_kwargs)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Mean Stimulation Across Groups and Stimulations')\n",
    "        plt.ylabel(list_column)\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        ax.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def plot_mean_sem_line_default(self, groups=None, stimulations=None, show_outliers=True, hue_order=None, dataframe_key=None, list_column=None, cell_type_filter=None, is_single_unit=None, stim_responsivity_filter=None, remove_outliers_option=False, directory=None, file_name=None):\n",
    "        \"\"\"\n",
    "        Plots mean and SEM line plots for specified groups and stimulations from the unpacked DataFrame.\n",
    "\n",
    "        Args:\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            stimulations (list of str, optional): List of stimulations to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "            dataframe_key (str, optional): Key for accessing the correct DataFrame.\n",
    "            list_column (str, optional): Column name for the values to be plotted.\n",
    "            cell_type_filter (str, optional): Filter for cell type.\n",
    "            is_single_unit (bool, optional): Filter for single units.\n",
    "            stim_responsivity_filter (str, optional): Filter for stimulus responsivity.\n",
    "            remove_outliers_option (bool, optional): Whether to remove outliers.\n",
    "            directory (str, optional): Directory to save the plot.\n",
    "            file_name (str, optional): File name to save the plot.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Prepare data for plotting (already prepared by a previous method)\n",
    "        plot_df = self.unpack_based_on_stim(dataframe_key=dataframe_key, list_column=list_column, cell_type_filter=cell_type_filter, is_single_unit=is_single_unit, stim_responsivity_filter=stim_responsivity_filter)\n",
    "\n",
    "        # Filter by specified groups and stimulations\n",
    "        if groups:\n",
    "            plot_df = plot_df[plot_df['Group'].isin(groups)]\n",
    "        if stimulations:\n",
    "            plot_df = plot_df[plot_df['Stimulation'].isin(stimulations)]\n",
    "\n",
    "        # Remove outliers if the option is set to True\n",
    "        if remove_outliers_option:\n",
    "            plot_df = self.remove_outliers_by_stimulation_default(plot_df, value_column=list_column)\n",
    "\n",
    "        # Compute means and SEMs\n",
    "        summary_df = plot_df.groupby(['Group', 'Stimulation']).agg(\n",
    "            mean_value=(list_column, 'mean'),\n",
    "            sem_value=(list_column, 'sem')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for group in groups:\n",
    "            group_data = summary_df[summary_df['Group'] == group]\n",
    "            plt.errorbar(group_data['Stimulation'], group_data['mean_value'], yerr=group_data['sem_value'],\n",
    "                        label=group, color=group_colors[group], capsize=5, marker='o', linestyle='-')\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Mean and SEM of Stimulation Responses Across Groups')\n",
    "        plt.ylabel(list_column)\n",
    "        plt.xlabel('Stimulation Type')\n",
    "        plt.legend(title='Group')\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def remove_outliers_by_stimulation_default(self, df, value_column=None):\n",
    "        \"\"\"\n",
    "        Removes outliers within each stimulation group based on the interquartile range (IQR).\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame containing the data, expected to have a 'Stimulation' column.\n",
    "            value_column (str): The name of the column from which outliers will be removed.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: DataFrame with outliers removed within each stimulation group.\n",
    "        \"\"\"\n",
    "        # Create an empty DataFrame to store results after removing outliers\n",
    "        filtered_df = pd.DataFrame()\n",
    "        \n",
    "        # Process each stimulation group separately\n",
    "        for stim in df['Stimulation'].unique():\n",
    "            sub_df = df[df['Stimulation'] == stim]\n",
    "            Q1 = sub_df[value_column].quantile(0.25)\n",
    "            Q3 = sub_df[value_column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Filter out outliers and append the results to the filtered DataFrame\n",
    "            result_df = sub_df[(sub_df[value_column] >= lower_bound) & (sub_df[value_column] <= upper_bound)]\n",
    "            filtered_df = pd.concat([filtered_df, result_df], ignore_index=True)\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def extract_spike_trains(self, recording_name):\n",
    "        # Filter the data for a specific recording and only single units\n",
    "        filtered_data = self.data[(self.data['recordingname'] == recording_name) & (self.data['IsSingleUnit'] == 1.0)]\n",
    "        \n",
    "        # Extract the SpikeTrains_for_PSTHs for these filtered entries\n",
    "        spike_trains = filtered_data['SpikeTrains_for_PSTHs'].tolist()\n",
    "        \n",
    "        # Optional: Convert any additional processing on spike trains here\n",
    "        # For example, converting to numpy array, etc.\n",
    "        \n",
    "        return spike_trains\n",
    "    \n",
    "    def simplify_and_filter_dataframe(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Simplify and filter the DataFrame based on IsSingleUnit, Cell_Type, and StimResponsivity.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                           If None, no filtering by StimResponsivity.\n",
    "        modulation_type (str or None): Filter for 'positive', 'negative', or 'none'. \n",
    "                                            If None, no filtering by modulation type.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The simplified and filtered DataFrame.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure ModulationIndex column is present\n",
    "        self.label_modulation_index(df_name)\n",
    "\n",
    "        # Start with the full DataFrame\n",
    "        df = self.dataframes[df_name]\n",
    "\n",
    "        # Print the first few entries and data types for debugging\n",
    "        #print(\"Original DataFrame:\")\n",
    "        #print(df.head())\n",
    "        #print(\"\\nData types of the columns:\")\n",
    "        #print(df.dtypes)\n",
    "        \n",
    "\n",
    "        # Convert numpy arrays with single values to scalar values\n",
    "        def extract_single_value(x):\n",
    "            if isinstance(x, np.ndarray) and x.ndim > 0:\n",
    "                return x[0]\n",
    "            return x\n",
    "\n",
    "        for col in ['IsSingleUnit', 'StimResponsivity', 'ModulationIndex', 'Template_Channel']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(extract_single_value)\n",
    "\n",
    "        # Print the simplified DataFrame for debugging\n",
    "        #print(\"\\nSimplified DataFrame:\")\n",
    "        #print(df.head())\n",
    "        #print(\"\\nData types of the columns after simplification:\")\n",
    "        #print(df.dtypes)\n",
    "\n",
    "        # Filter by cell type if provided\n",
    "        if cell_type:\n",
    "            df = df[df['Cell_Type'] == cell_type]\n",
    "            #print(f\"\\nDataFrame after filtering by Cell_Type='{cell_type}':\")\n",
    "            #print(df)\n",
    "\n",
    "        # Filter by IsSingleUnit if not None\n",
    "        if is_single_unit is not None:\n",
    "            df = df[df['IsSingleUnit'] == is_single_unit]\n",
    "            #print(f\"\\nDataFrame after filtering by IsSingleUnit={is_single_unit}:\")\n",
    "            #print(df)\n",
    "\n",
    "        # Filter by StimResponsivity if not None\n",
    "        if stim_responsivity is not None:\n",
    "            df = df[df['StimResponsivity'] == stim_responsivity]\n",
    "            #print(f\"\\nDataFrame after filtering by StimResponsivity={stim_responsivity}:\")\n",
    "            #print(df)\n",
    "        \n",
    "        # Filter by group name if provided\n",
    "        if group_name:\n",
    "            df = df[df['groupname'] == group_name]\n",
    "            #print(f\"\\nDataFrame after filtering by groupname='{group_name}':\")\n",
    "            #print(df)\n",
    "        \n",
    "        # Filter by modulation type if provided\n",
    "        if modulation_type:\n",
    "            df = df[df['ModulationIndex'] == modulation_type]\n",
    "            print(f\"Filtered by modulation type='{modulation_type}': {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_plotting_data(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Prepare data for plotting the modulation index as a function of electrode location.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and prepare.\n",
    "        group_name (str or None): Name of the group to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        modulation_type (str or None): Filter for 'positive', 'negative', or 'none'. If None, no filtering by modulation type.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame ready for plotting.\n",
    "        \"\"\"\n",
    "        # Ensure the original numeric ModulationIndex column is present and labeled correctly\n",
    "        df = self.simplify_and_filter_dataframe(df_name, group_name, is_single_unit, cell_type, stim_responsivity, modulation_type)\n",
    "        if df is None or df.empty:\n",
    "            print(\"No data to prepare for plotting.\")\n",
    "            return None\n",
    "\n",
    "        # Define the electrode order\n",
    "        electrodes_order = [14, 20, 16, 18, 1, 31, 3, 29, 5, 27, 7, 25, 9, 23, 11, 21, 13, 19, 15, 17, 12, 22, 10, 24, 8, 26, 6, 28, 4, 30, 2, 32]\n",
    "\n",
    "        # Create a mapping from Template_Channel to electrode order index\n",
    "        channel_mapping = {electrode: idx for idx, electrode in enumerate(electrodes_order)}\n",
    "\n",
    "        # Map Template_Channel to electrode order index\n",
    "        df['ElectrodeOrder'] = df['Template_Channel'].map(channel_mapping)\n",
    "\n",
    "        # Drop rows where the mapping resulted in NaN values\n",
    "        df = df.dropna(subset=['ElectrodeOrder'])\n",
    "\n",
    "        # Print the prepared DataFrame for debugging\n",
    "        print(\"\\nPrepared DataFrame for plotting:\")\n",
    "        #print(df[['ElectrodeOrder', 'ModulationIndex_Numeric', 'MeanFR_baseline', 'groupname', 'StimProb', 'MeanFR_stim'  ]].head())  # Use the new numeric column\n",
    "\n",
    "        return df[['ElectrodeOrder', 'ModulationIndex_Numeric', 'groupname', 'MeanFR_baseline', 'StimProb', 'MeanFR_stim' ]].sort_values('ElectrodeOrder')\n",
    "\n",
    "\n",
    "    def plot_modulation_index(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        group_name (str or None): Name of the group to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        \"\"\"\n",
    "        # Prepare the data for plotting\n",
    "        plotting_data = self.prepare_plotting_data(df_name, group_name, is_single_unit, cell_type, stim_responsivity)\n",
    "        if plotting_data is None or plotting_data.empty:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Drop rows with NaN in relevant columns\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Convert 'MeanFR_baseline' from array-like to scalar values\n",
    "        plotting_data['MeanFR_baseline'] = plotting_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "        # Convert to numeric and ensure correct length\n",
    "        plotting_data['MeanFR_baseline'] = pd.to_numeric(plotting_data['MeanFR_baseline'], errors='coerce')\n",
    "        plotting_data['ModulationIndex_Numeric'] = pd.to_numeric(plotting_data['ModulationIndex_Numeric'], errors='coerce')\n",
    "        plotting_data['ElectrodeOrder'] = pd.to_numeric(plotting_data['ElectrodeOrder'], errors='coerce')\n",
    "\n",
    "        # Drop rows where any of these columns have NaNs after conversion\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Ensure all columns have the same length\n",
    "        if not (len(plotting_data['MeanFR_baseline']) == len(plotting_data['ModulationIndex_Numeric']) == len(plotting_data['ElectrodeOrder'])):\n",
    "            print(\"Mismatch in data sizes after cleaning.\")\n",
    "            return\n",
    "\n",
    "        # Normalize size values if necessary (optional step)\n",
    "        size_values = plotting_data['MeanFR_baseline'].values\n",
    "        size_values = size_values / np.max(size_values) * 200  # Normalize to range suitable for plotting\n",
    "\n",
    "        # Plot the modulation index\n",
    "        plt.figure(figsize=(2, 6))\n",
    "        scatter = plt.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['ElectrodeOrder'], \n",
    "                            s=size_values,  # Use normalized sizes\n",
    "                            c=plotting_data['ModulationIndex_Numeric'], \n",
    "                            cmap='bwr', alpha=0.6)\n",
    "        \n",
    "        \n",
    "        # Enforce the color bar is always from -1 to 1 \n",
    "        plt.clim(-1, 1)\n",
    "        plt.colorbar(scatter, label='Modulation Index')\n",
    "        plt.xlabel('Spontaneous MI')\n",
    "        plt.ylabel('Electrode Order')\n",
    "        plt.title(f'{cell_type} multi-units from {group_name}')\n",
    "        plt.gca().invert_yaxis()  # To match the order from the provided list\n",
    "\n",
    "        \n",
    "        \n",
    "        # Ensure the y-axis has all the electrodes listed \n",
    "        plt.xlim(-1.5, 1.5)\n",
    "        plt.grid(False)\n",
    "        plt.yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Save the plot as a SVG file\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        # Make the file name more descriptive based on the user input, if single unit or multi unit or if stim responsive is provided in the file name \n",
    "        if is_single_unit == 1.0:\n",
    "            file_name = f'{group_name}_single_units'\n",
    "        elif is_single_unit == 0.0:\n",
    "            file_name = f'{group_name}_multi_units'\n",
    "        else:\n",
    "            file_name = f'{group_name}_all_units'\n",
    "        \n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def label_modulation_index(self, df_name):\n",
    "        \"\"\"\n",
    "        Label the ModulationIndex as 'positive', 'negative', or 'none'.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to label.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with a new column 'ModulationIndex'.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "        if 'ModulationIndex' not in df.columns:\n",
    "            df['ModulationIndex'] = df['ModulationIndex'].apply(lambda x: 'positive' if x > 0.3 else ('negative' if x < -0.3 else 'none'))\n",
    "        return df\n",
    "    \n",
    "    def label_modulation_index2(self, df):\n",
    "        \"\"\"\n",
    "        Label the ModulationIndex as 'positive', 'negative', or 'none'.\n",
    "\n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame to label.\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The DataFrame with a new column 'ModulationIndex'.\n",
    "        \"\"\"\n",
    "        df['ModulationIndex'] = df['ModulationIndex'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'none'))\n",
    "        return df\n",
    "\n",
    "    def plot_modulation_index_density(self, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, modulation_type=None):\n",
    "        \"\"\"\n",
    "        Compare the density of cells across electrode numbers between two groups.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                           If None, no filtering by StimResponsivity.\n",
    "        modulation_type (str or None): Filter for 'positive', 'negative', or 'none'. If None, no filtering by modulation type.\n",
    "        \"\"\"\n",
    "        # Prepare the data for both groups\n",
    "        df_ctz = self.prepare_plotting_data(df_name, group_name=self.eed.group_names[0], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity, modulation_type=modulation_type)\n",
    "        df_no_ctz = self.prepare_plotting_data(df_name, group_name=self.eed.group_names[1], is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity, modulation_type=modulation_type)\n",
    "        \n",
    "        if df_ctz is None or df_ctz.empty or df_no_ctz is None or df_no_ctz.empty:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the bins are consistent for both groups\n",
    "        bins = np.linspace(df_ctz['ElectrodeOrder'].min(), df_ctz['ElectrodeOrder'].max(), len(df_ctz['ElectrodeOrder'].unique()))\n",
    "\n",
    "        # Plot the density of cells across electrode numbers comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        sns.histplot(df_ctz, y='ElectrodeOrder', bins=bins, color='blue', alpha=0.5, label='CTZ', kde=True, common_norm=False)\n",
    "        sns.histplot(df_no_ctz, y='ElectrodeOrder', bins=bins, color='red', alpha=0.5, label='No CTZ', kde=False, common_norm=False)\n",
    "\n",
    "        plt.xlabel('Density')\n",
    "        plt.ylabel('Electrode Order')\n",
    "        plt.title(f'{cell_type} multi-units: CTZ vs No CTZ')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def add_modulation_label_column(self, df_name):\n",
    "        \"\"\"\n",
    "        Add the ModulationIndex column to the specified DataFrame in self.dataframes.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to which the column should be added.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "        df = self.label_modulation_index2(df)\n",
    "        self.dataframes[df_name] = df\n",
    "        print(f\"Added ModulationIndex column to '{df_name}' DataFrame.\")\n",
    "        return df\n",
    "     \n",
    "    def filter_dataframe(self, df_name, modulation_type):\n",
    "        \"\"\"\n",
    "        Filter the DataFrame to retain only specific entries based on ModulationIndex.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        modulation_type (str): The type of modulation to retain ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "        pandas.DataFrame: The filtered DataFrame.\n",
    "        \"\"\"\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No DataFrame found with the name '{df_name}'.\")\n",
    "            return None\n",
    "\n",
    "        df = self.dataframes[df_name]\n",
    "\n",
    "        if 'ModulationIndex' not in df.columns:\n",
    "            print(\"ModulationIndex column not found in the DataFrame.\")\n",
    "            return None\n",
    "\n",
    "        # Filter the DataFrame based on ModulationIndex\n",
    "        filtered_df = df[df['ModulationIndex'] == modulation_type]\n",
    "        \n",
    "        return filtered_df\n",
    " \n",
    "    def save_filtered_dataframe(self, df_name, modulation_type):\n",
    "        \"\"\"\n",
    "        Save the filtered DataFrame to self.dataframes and reset the index.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and save.\n",
    "        modulation_type (str): The type of modulation to retain ('positive', 'negative', 'none').\n",
    "        \"\"\"\n",
    "        filtered_df = self.filter_dataframe(df_name, modulation_type)\n",
    "        if filtered_df is not None:\n",
    "            filtered_df = filtered_df.reset_index(drop=True)  # Reset the index\n",
    "            self.dataframes[df_name] = filtered_df\n",
    "            print(f\"Filtered DataFrame with {modulation_type} modulation saved to '{df_name}'.\")\n",
    "        else:\n",
    "            print(\"Filtered DataFrame was not saved.\")\n",
    "\n",
    "    def create_psth_dataframe_2(self):\n",
    "        \"\"\"\n",
    "        Creates and stores a DataFrame for each stimulation type using the 'SpikeTrains_for_PSTHs' and 'PSTHs_conv' columns from the base PSTH DataFrame. \n",
    "        Each DataFrame is stored as an attribute of the DataFrameManager under a name that corresponds to the stimulation type.\n",
    "\n",
    "        \"\"\"\n",
    "        # Create the base dataframe for PSTH analysis with modulation filtering\n",
    "        self.create_dataframe(['Cell_Type', \n",
    "            'LaminarLabel', \n",
    "            'IsSingleUnit', \n",
    "            'StimResponsivity', \n",
    "            'SpikeTrains_for_PSTHs', \n",
    "            'PSTHs_conv', \n",
    "            'ModulationIndex',\n",
    "            'FirstSpikeLatency', \n",
    "            'FirstSpikeLatency_Reliability', \n",
    "            'TroughToPeak_duration', \n",
    "            'SpikeHalfWidth', \n",
    "            'PeakToPeak_ratio', \n",
    "            'peak1_normalized_amplitude', \n",
    "            'Peak1ToTrough_ratio', \n",
    "            'Peak2ToTrough_ratio', \n",
    "            'Template_Channel', \n",
    "            'MeanFR_baseline', \n",
    "            'Normalized_Template_Waveform', \n",
    "            'UnNormalized_Template_Waveform',\n",
    "            'SpikeTimes_all', \n",
    "            'ISI_baseline_CV', \n",
    "            'ISI_baseline_vec', \n",
    "            'ISI_pdf_peak_xy', \n",
    "            'ISI_pdf_x', \n",
    "            'ISI_pdf_y',\n",
    "            'StimProb', \n",
    "            'MeanFR_stim', \n",
    "            'PeakEvokedFR', \n",
    "            'PeakEvokedFR_Latency', \n",
    "            'FanoFactor_baseline', \n",
    "            'FanoFactor_stim', \n",
    "            'MeanFR_inst_stim', \n",
    "            'ISI_violations_percent', \n",
    "            'Recording_Duration', \n",
    "            'Sampling_Frequency'], 'psth_dataframe')\n",
    "        # Extracting trial tags\n",
    "        stim_labels = self.eed.trialTagsLabels['trialTagsLabels']  # ['Zero', 'Low', 'Mid', 'Max']\n",
    "\n",
    "        # Process each label\n",
    "        for index, label in enumerate(stim_labels):\n",
    "            # Retrieve spike trains and PSTHs for each label and create a new DataFrame\n",
    "            df_name = f'psth_dataframe_{label}'\n",
    "            self.dataframes[df_name] = self.dataframes['psth_dataframe'].copy()\n",
    "            self.dataframes[df_name]['SpikeTrains_for_PSTHs'] = self.dataframes['psth_dataframe']['SpikeTrains_for_PSTHs'].apply(lambda x: x[index] if isinstance(x, (list, np.ndarray)) else None)\n",
    "            self.dataframes[df_name]['PSTHs_conv'] = self.dataframes['psth_dataframe']['PSTHs_conv'].apply(lambda x: x[index] if isinstance(x, (list, np.ndarray)) else None)\n",
    "        \n",
    "\n",
    "    def plot_group_comparison(self, stim_labels, cell_type=None, is_single_unit=None, stim_responsivity=None, groupnames=None, modulation_label=None, time_range=None, smoothing_window=10, ylim=None):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the shaded area\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        plot_idx = 0\n",
    "        for groupname in groupnames:\n",
    "            for stim_label in stim_labels:\n",
    "                df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label)\n",
    "                \n",
    "                # Print the number of units that match the filter\n",
    "                print(f\"Number of units that match the filter for group {groupname} and stimulation {stim_label}: {df.shape[0]}\")\n",
    "\n",
    "                if df.empty:\n",
    "                    print(f\"No data matches the specified filters for group {groupname} and stimulation {stim_label}.\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Get relative time array for x-axis\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                \n",
    "                # Create time mask and adjust time array\n",
    "                time_mask = self.create_time_mask(time_array, time_range)\n",
    "                time_array = time_array[time_mask]\n",
    "\n",
    "                all_psths = []\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    individual_psth = row['PSTHs_conv']\n",
    "                    # Apply time mask to individual PSTH\n",
    "                    individual_psth = np.array(individual_psth)[time_mask]\n",
    "                    \n",
    "                    # Apply a smoothing window\n",
    "                    window = np.ones(smoothing_window) / smoothing_window  # 10ms window of smoothing\n",
    "                    individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "                    \n",
    "                    all_psths.append(individual_psth)\n",
    "                    # Plot the individual PSTH with lighter color\n",
    "                    color = lightened_colors.get(groupname, '#CCCCCC')\n",
    "                    axes[plot_idx].plot(time_array, individual_psth, color=color, linewidth=1, alpha=0.5)\n",
    "\n",
    "                # Calculate the mean PSTH\n",
    "                all_psths = np.array(all_psths)\n",
    "                mean_psth = np.mean(all_psths, axis=0)\n",
    "                # Apply a smoothing window to the mean PSTH\n",
    "                mean_psth = np.convolve(mean_psth, window, mode='same')\n",
    "                # Plot the mean PSTH with a thicker line\n",
    "                color = group_colors.get(groupname, 'black')\n",
    "                axes[plot_idx].plot(time_array, mean_psth, label='Mean PSTH', color=color, linewidth=2)\n",
    "\n",
    "                axes[plot_idx].set_title(f'Group: {groupname}, Stimulation: {stim_label}')\n",
    "                axes[plot_idx].set_xlabel('Time (ms)')\n",
    "                axes[plot_idx].set_ylabel('Spike Rate')\n",
    "                axes[plot_idx].legend().set_visible(False)\n",
    "                \n",
    "                plot_idx += 1\n",
    "                #specify the ylimit for all the subplots\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_group_comparison_meansem(self, stim_labels, cell_type=None, is_single_unit=None, stim_responsivity=None, groupnames=None, modulation_label=None, time_range=None, smoothing_window=10, ylim=None, directory=None, file_name=None):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the shaded area\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        plot_idx = 0\n",
    "        for groupname in groupnames:\n",
    "            for stim_label in stim_labels:\n",
    "                df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label)\n",
    "                \n",
    "                # Print the number of units that match the filter\n",
    "                print(f\"Number of units that match the filter for group {groupname} and stimulation {stim_label}: {df.shape[0]}\")\n",
    "\n",
    "                if df.empty:\n",
    "                    print(f\"No data matches the specified filters for group {groupname} and stimulation {stim_label}.\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Get relative time array for x-axis\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                \n",
    "                # Create time mask and adjust time array\n",
    "                time_mask = self.create_time_mask(time_array, time_range)\n",
    "                time_array = time_array[time_mask]\n",
    "\n",
    "                all_psths = []\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    individual_psth = row['PSTHs_conv']\n",
    "                    # Apply time mask to individual PSTH\n",
    "                    individual_psth = np.array(individual_psth)[time_mask]\n",
    "                    \n",
    "                    # Apply a smoothing window\n",
    "                    window = np.ones(smoothing_window) / smoothing_window  # 10ms window of smoothing\n",
    "                    individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "                    \n",
    "                    \n",
    "                    all_psths.append(individual_psth)\n",
    "\n",
    "                # Calculate the mean and SEM PSTH\n",
    "                all_psths = np.array(all_psths)\n",
    "                mean_psth = np.mean(all_psths, axis=0)\n",
    "                sem_psth = np.std(all_psths, axis=0) / np.sqrt(all_psths.shape[0])\n",
    "\n",
    "                # Plot the mean PSTH with SEM as shaded error bars\n",
    "                color = group_colors.get(groupname, 'black')\n",
    "                shaded_color = lightened_colors.get(groupname, 'gray')\n",
    "                axes[plot_idx].plot(time_array, mean_psth, label='Mean PSTH', color=color, linewidth=2)\n",
    "                axes[plot_idx].fill_between(time_array, mean_psth - sem_psth, mean_psth + sem_psth, color=shaded_color, alpha=0.3)\n",
    "\n",
    "                axes[plot_idx].set_title(f'Group: {groupname}, Stimulation: {stim_label}')\n",
    "                axes[plot_idx].set_xlabel('Time (ms)')\n",
    "                axes[plot_idx].set_ylabel('Spike Rate')\n",
    "                axes[plot_idx].legend().set_visible(False)\n",
    "                \n",
    "                plot_idx += 1\n",
    "        #specify the ylimit for all the subplots\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        fig.savefig(file_path, format='svg', transparent=True)\n",
    "    \n",
    "    def plot_group_comparison_1v1(self, stim_labels, cell_type=None, is_single_unit=None, stim_responsivity=None, groupnames=None, modulation_label=None, time_range=None, smoothing_window=10, ylim=None, recordingnames=None):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10), sharex=True, sharey=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the shaded area\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        plot_idx = 0\n",
    "        for group_idx, groupname in enumerate(groupnames):\n",
    "            recordingname = recordingnames[group_idx]\n",
    "            for stim_label in stim_labels:\n",
    "                df = self.filter_data(stim_label, cell_type, is_single_unit, stim_responsivity, groupname, modulation_label, recordingname)\n",
    "                \n",
    "                # Print the number of units that match the filter\n",
    "                print(f\"Number of units that match the filter for group {groupname}, recording {recordingname}, and stimulation {stim_label}: {df.shape[0]}\")\n",
    "\n",
    "                if df.empty:\n",
    "                    print(f\"No data matches the specified filters for group {groupname}, recording {recordingname}, and stimulation {stim_label}.\")\n",
    "                    plot_idx += 1\n",
    "                    continue\n",
    "\n",
    "                # Get relative time array for x-axis\n",
    "                time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "                \n",
    "                # Create time mask and adjust time array\n",
    "                time_mask = self.create_time_mask(time_array, time_range)\n",
    "                time_array = time_array[time_mask]\n",
    "\n",
    "                all_psths = []\n",
    "\n",
    "                for index, row in df.iterrows():\n",
    "                    individual_psth = row['PSTHs_conv']\n",
    "                    # Apply time mask to individual PSTH\n",
    "                    individual_psth = np.array(individual_psth)[time_mask]\n",
    "                    \n",
    "                    # Apply a smoothing window\n",
    "                    window = np.ones(smoothing_window) / smoothing_window  # 10ms window of smoothing\n",
    "                    individual_psth = np.convolve(individual_psth, window, mode='same')\n",
    "                    \n",
    "                    all_psths.append(individual_psth)\n",
    "                    # Plot the individual PSTH with lighter color\n",
    "                    color = lightened_colors.get(groupname, '#CCCCCC')\n",
    "                    axes[plot_idx].plot(time_array, individual_psth, color=color, linewidth=1, alpha=0.5)\n",
    "\n",
    "                # Calculate the mean PSTH\n",
    "                all_psths = np.array(all_psths)\n",
    "                mean_psth = np.mean(all_psths, axis=0)\n",
    "                # Apply a smoothing window to the mean PSTH\n",
    "                mean_psth = np.convolve(mean_psth, window, mode='same')\n",
    "                # Plot the mean PSTH with a thicker line\n",
    "                color = group_colors.get(groupname, 'black')\n",
    "                axes[plot_idx].plot(time_array, mean_psth, label='Mean PSTH', color=color, linewidth=2)\n",
    "\n",
    "                axes[plot_idx].set_title(f'Group: {groupname}, Recording: {recordingname}, Stimulation: {stim_label}')\n",
    "                axes[plot_idx].set_xlabel('Time (ms)')\n",
    "                axes[plot_idx].set_ylabel('Spike Rate')\n",
    "                axes[plot_idx].legend().set_visible(False)\n",
    "                \n",
    "                plot_idx += 1\n",
    "        \n",
    "        # Set y-limit for all subplots\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def process_waveforms(self, cell_type, groupname=None, stim_responsivity=None, modulation_label=None, is_single_unit=None):\n",
    "        \"\"\"\n",
    "        Processes waveforms for the specified cell type and optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        cell_type (str): 'FS' or 'RS'.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (waveforms, mean_waveform, sem_waveform)\n",
    "        \"\"\"\n",
    "        df = self.get_filtered_data(\n",
    "            df_name='basic_metrics', \n",
    "            is_single_unit=is_single_unit,\n",
    "            cell_type=cell_type, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            groupname=groupname, \n",
    "            modulation_label=modulation_label, \n",
    "            spike_cutoff=spike_cutoff)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No data available for cell type: {cell_type}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        waveforms = df['UnNormalized_Template_Waveform'].values\n",
    "        mean_waveform = np.mean(np.stack(waveforms), axis=0)\n",
    "        sem_waveform = np.std(np.stack(waveforms), axis=0) / np.sqrt(len(waveforms))\n",
    "        \n",
    "        #convet data to microvolts conversion_factor = 0.25  # V per unit\n",
    "        waveforms = waveforms * 0.25\n",
    "        mean_waveform = mean_waveform * 0.25\n",
    "        sem_waveform = sem_waveform * 0.25\n",
    "        \n",
    "        \n",
    "        return waveforms, mean_waveform, sem_waveform\n",
    "    \n",
    "    def align_waveforms_on_negative_peak(self, waveforms):\n",
    "        \"\"\"\n",
    "        Aligns waveforms based on the most negative deflection (peak). \n",
    "\n",
    "        Parameters:\n",
    "        waveforms (list of numpy.ndarray): A list of 1D numpy arrays where each array is a waveform.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: A 2D array of aligned waveforms with NaNs added to handle alignment.\n",
    "        \"\"\"\n",
    "        # Find the index of the most negative value in each waveform\n",
    "        peak_indices = [np.argmin(waveform) for waveform in waveforms]\n",
    "\n",
    "        # Determine the maximum shift needed to align the peaks\n",
    "        max_shift = max(peak_indices)\n",
    "\n",
    "        # Determine the length of the waveforms (assuming they are all the same length)\n",
    "        waveform_length = waveforms[0].shape[0]\n",
    "\n",
    "        # Initialize an array to hold the aligned waveforms with NaNs\n",
    "        aligned_waveforms = np.full((len(waveforms), waveform_length + max_shift), np.nan)\n",
    "\n",
    "        # Align each waveform\n",
    "        for i, waveform in enumerate(waveforms):\n",
    "            shift = max_shift - peak_indices[i]\n",
    "            aligned_waveforms[i, shift:shift + waveform_length] = waveform\n",
    "\n",
    "        return aligned_waveforms\n",
    "    \n",
    "    def process_waveforms(self, cell_type, groupname=None, stim_responsivity=None, modulation_label=None,  is_single_unit= None, align_on_negative_peak=True):\n",
    "        \"\"\"\n",
    "        Processes waveforms for the specified cell type and optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        cell_type (str): 'FS' or 'RS'.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        align_on_negative_peak (bool): If True, align waveforms on the most negative deflection before returning.\n",
    "\n",
    "        Returns:\n",
    "        tuple: (waveforms, mean_waveform, sem_waveform)\n",
    "        \"\"\"\n",
    "        df = self.get_filtered_data(\n",
    "            df_name= 'basic_metrics', \n",
    "            is_single_unit=is_single_unit, \n",
    "            cell_type=cell_type, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            groupname=groupname, \n",
    "            modulation_label=modulation_label\n",
    "        )\n",
    "        \n",
    "        if df.empty:\n",
    "            print(f\"No data available for cell type: {cell_type}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        waveforms = df['UnNormalized_Template_Waveform'].values\n",
    "\n",
    "        if align_on_negative_peak:\n",
    "            waveforms = self.align_waveforms_on_negative_peak(waveforms)\n",
    "        \n",
    "        mean_waveform = np.nanmean(waveforms, axis=0)\n",
    "        sem_waveform = np.nanstd(waveforms, axis=0) / np.sqrt(np.sum(~np.isnan(waveforms), axis=0))\n",
    "        \n",
    "        #convert data to microvolts conversion_factor = 0.25  # V per unit, speficic for blackrock data\n",
    "        waveforms = waveforms * 0.25\n",
    "        mean_waveform = mean_waveform * 0.25\n",
    "        sem_waveform = sem_waveform * 0.25\n",
    "        \n",
    "        return waveforms, mean_waveform, sem_waveform\n",
    "    \n",
    "    \n",
    "    def plot_waveforms(self, cell_type, color, label, groupname=None, stim_responsivity=None, modulation_label=None, is_single_unit=None, ylim=None):\n",
    "        \"\"\"\n",
    "        Plots individual and mean waveforms for the specified cell type with optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        cell_type (str): 'FS' or 'RS'.\n",
    "        color (str): Color for the plot.\n",
    "        label (str): Label for the mean waveform plot.\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        ylim (tuple or None): Set y-axis limits. If None, no limit is set.\n",
    "        is_single_units (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        waveforms, mean_waveform, _ = self.process_waveforms(\n",
    "            cell_type, \n",
    "            groupname=groupname, \n",
    "            stim_responsivity=stim_responsivity, \n",
    "            modulation_label=modulation_label, \n",
    "            is_single_unit=is_single_unit\n",
    "        )\n",
    "        \n",
    "        if waveforms is None:\n",
    "            return\n",
    "        \n",
    "        # Plot individual waveforms\n",
    "        for waveform in waveforms:\n",
    "            plt.plot(waveform, color=color, alpha=0.1)\n",
    "        \n",
    "        # Plot mean waveform\n",
    "        plt.plot(mean_waveform, color=color, label=label, linewidth=2)\n",
    "        \n",
    "        # Add ylim range if provided\n",
    "        if ylim:\n",
    "            plt.ylim(ylim)\n",
    "            \n",
    "        # Save the figure\n",
    "        plt.savefig('/Volumes/MannySSD/figures/waveforms.svg', transparent=True)\n",
    "\n",
    "    def plot_trough_to_peak_histogram(self, df_name='basic_metrics', groupname=None, is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Plots histograms of Trough-to-Peak duration for FS and RS cells, with options to filter data.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): The name of the DataFrame to use (default is 'basic_metrics').\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        None: The function plots and saves the histogram.\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve filtered data for FS and RS cells\n",
    "        fs_df = self.get_filtered_data(df_name, is_single_unit=is_single_unit, cell_type='FS', stim_responsivity=stim_responsivity, groupname=groupname, modulation_label=modulation_label)\n",
    "        rs_df = self.get_filtered_data(df_name, is_single_unit=is_single_unit, cell_type='RS', stim_responsivity=stim_responsivity, groupname=groupname, modulation_label=modulation_label)\n",
    "        \n",
    "        #print the shape of the dataframes \n",
    "        print(f'For FS cells: {fs_df.shape} after filtering')\n",
    "        print(f'For RS cells: {rs_df.shape} after filtering')\n",
    "        \n",
    "        # Extract the Trough-to-Peak durations, dropping NaNs\n",
    "        fs_durations = fs_df['TroughToPeak_duration'].dropna()\n",
    "        rs_durations = rs_df['TroughToPeak_duration'].dropna()\n",
    "        \n",
    "        print(f'For FS cells: {fs_durations.shape} after dropping NaNs')\n",
    "        print(f'For RS cells: {rs_durations.shape} after dropping NaNs')\n",
    "        \n",
    "        # Determine the bins based on the combined range of both FS and RS durations\n",
    "        all_durations = np.concatenate((fs_durations, rs_durations))\n",
    "        bins = np.histogram_bin_edges(all_durations, bins=30)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot histograms\n",
    "        plt.hist(fs_durations, bins=bins, alpha=0.7, color='darkgoldenrod', label='FS')\n",
    "        plt.hist(rs_durations, bins=bins, alpha=0.7, color='sienna', label='RS')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Trough-to-Peak Duration Histogram')\n",
    "        plt.xlabel('Trough-to-Peak Duration (ms)')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.legend()\n",
    "    \n",
    "    def plot_combined_waveforms(self, groupname=None, stim_responsivity=None, modulation_label=None,  is_single_unit=None, ylim=None):\n",
    "        \"\"\"\n",
    "        Plots combined waveforms for FS and RS cells with optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        ylim (tuple or None): Set y-axis limits. If None, no limit is set.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot FS waveforms\n",
    "        self.plot_waveforms('FS', color='darkgoldenrod', label='FS Mean', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label, is_single_unit=is_single_unit, ylim=ylim)\n",
    "        \n",
    "        # Plot RS waveforms\n",
    "        self.plot_waveforms('RS', color='sienna', label='RS Mean', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label,  is_single_unit=is_single_unit, ylim=ylim)\n",
    "        \n",
    "        plt.title('Combined Waveforms of FS and RS Units')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Normalized Amplitude')\n",
    "        plt.legend()\n",
    "        \n",
    "        # CONTROL THE X LIMITS\n",
    "        plt.xlim(30, 120)\n",
    "        plt.ylim(-2800, 1000)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig('/Volumes/MannySSD/figures/combined_waveforms.svg', transparent=True)\n",
    "\n",
    "    def plot_combined_waveforms_meansem(self, groupname=None, stim_responsivity=None, modulation_label=None, is_single_unit=None, ylim=None):\n",
    "        \"\"\"\n",
    "        Plots combined mean and SEM waveforms for FS and RS cells with optional filters.\n",
    "\n",
    "        Parameters:\n",
    "        groupname (str or None): Filter by groupname. If None, no filtering by groupname.\n",
    "        stim_responsivity (float or None): Filter by StimResponsivity (1.0, 0.0, or -1.0). If None, no filtering by this criterion.\n",
    "        modulation_label (str or None): Filter by ModulationIndex ('positive', 'negative', 'none'). If None, no filtering by this criterion.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, if None, do not filter by this criterion.\n",
    "        ylim (tuple or None): Set y-axis limits. If None, no limit is set.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        fs_waveforms, fs_mean_waveform, fs_sem_waveform = self.process_waveforms(\n",
    "            'FS', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label, is_single_unit=is_single_unit\n",
    "        )\n",
    "        rs_waveforms, rs_mean_waveform, rs_sem_waveform = self.process_waveforms(\n",
    "            'RS', groupname=groupname, stim_responsivity=stim_responsivity, modulation_label=modulation_label, is_single_unit=is_single_unit\n",
    "        )\n",
    "        \n",
    "        if fs_waveforms is None and rs_waveforms is None:\n",
    "            print(\"No data available for both FS and RS units.\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot FS mean and SEM\n",
    "        if fs_waveforms is not None:\n",
    "            plt.plot(fs_mean_waveform, color='darkgoldenrod', label='FS Mean', linewidth=2)\n",
    "            plt.fill_between(np.arange(len(fs_mean_waveform)), fs_mean_waveform - fs_sem_waveform, fs_mean_waveform + fs_sem_waveform, color='darkgoldenrod', alpha=0.3)\n",
    "        \n",
    "        # Plot RS mean and SEM\n",
    "        if rs_waveforms is not None:\n",
    "            plt.plot(rs_mean_waveform, color='sienna', label='RS Mean', linewidth=2)\n",
    "            plt.fill_between(np.arange(len(rs_mean_waveform)), rs_mean_waveform - rs_sem_waveform, rs_mean_waveform + rs_sem_waveform, color='sienna', alpha=0.3)\n",
    "        \n",
    "        plt.title('Mean and SEM of FS and RS Units')\n",
    "        plt.xlabel('Time (ms)')\n",
    "        plt.ylabel('Normalized Amplitude')\n",
    "        plt.legend()\n",
    "        \n",
    "        # CONTROL THE X LIMITS\n",
    "        plt.xlim(30, 120)\n",
    "        plt.ylim(-2000, 500)\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig('/Volumes/MannySSD/figures/mean_sem_waveforms.svg', transparent=True)\n",
    " \n",
    "        \n",
    "        \n",
    "    def process_and_store_analysis_data(self, is_single_unit=None, cell_type=None, stim_responsivity=None, \n",
    "                                        time_window=None, bin_size=1, smoothing_window=5, analysis_functions=None):\n",
    "        \"\"\"\n",
    "        Iterates over the dataset grouped by 'groupname', 'recordingname', and 'cid', extracts the PSTH data,\n",
    "        performs specified analyses, and stores the results in a structured format.\n",
    "\n",
    "        Args:\n",
    "            is_single_unit (bool, optional): If specified, filters cells based on whether they are considered single units.\n",
    "            cell_type (str, optional): If specified, filters cells based on their type.\n",
    "            stim_responsivity (bool, optional): If specified, filters cells based on their responsiveness to stimuli.\n",
    "            time_window (tuple, optional): The window of time to extract, within the range -500 to 999 ms. \n",
    "                                        Default is None, which uses the full range.\n",
    "            bin_size (int, optional): The size of the bins in milliseconds for the PSTH and raster. Default is 1ms.\n",
    "            smoothing_window (int, optional): The size of the smoothing window for the smoothed PSTH. Default is 5.\n",
    "            analysis_functions (list, optional): List of functions to apply to the extracted data. Each function should\n",
    "                                                 take the extracted data as input and return a dictionary with results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A nested dictionary containing the extracted data and analysis results for each group, recording, and cid.\n",
    "        \"\"\"\n",
    "        stored_data = {}\n",
    "\n",
    "        for (groupname, recordingname, cid), group_df in self.iterate_by_group_recording_cid(\n",
    "                is_single_unit=is_single_unit, cell_type=cell_type, stim_responsivity=stim_responsivity):\n",
    "\n",
    "            print(f\"Processing Group: {groupname}, Recording: {recordingname}, CID: {cid}\")\n",
    "\n",
    "            # Extract the PSTH data\n",
    "            extracted_data = self.extract_psth_and_raster_data(groupname=groupname, recordingname=recordingname, cid=cid, \n",
    "                                                            time_window=time_window, bin_size=bin_size, smoothing_window=smoothing_window)\n",
    "\n",
    "            # Perform additional analyses\n",
    "            if analysis_functions:\n",
    "                for func in analysis_functions:\n",
    "                    analysis_results = func(extracted_data)\n",
    "                    extracted_data.update(analysis_results)\n",
    "\n",
    "            # Add the parameters used to the extracted data\n",
    "            extracted_data['bin_size'] = bin_size\n",
    "            extracted_data['smoothing_window'] = smoothing_window\n",
    "            extracted_data['time_window'] = time_window\n",
    "\n",
    "            # Ensure the nested dictionary structure is created\n",
    "            if groupname not in stored_data:\n",
    "                stored_data[groupname] = {}\n",
    "            if recordingname not in stored_data[groupname]:\n",
    "                stored_data[groupname][recordingname] = {}\n",
    "            \n",
    "            stored_data[groupname][recordingname][cid] = extracted_data\n",
    "\n",
    "        return stored_data\n",
    "\n",
    "### this will be now the new methods for plotting distributuions\n",
    "    def plot_stim_prob_distribution(self, df_name, groupname=None, is_single_unit=None, cell_type=None, xlim_range=None, include_no_change=True, plot_stacked_bar=True):\n",
    "        \"\"\"\n",
    "        Plot the step plots for the StimResponsivity unique labels (-1.0, 0.0, 1.0)\n",
    "        showing the distribution of StimProb AUROC values, and print the total counts for each category.\n",
    "        Also, plot a stacked bar chart visualizing the proportions of the three groups.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        xlim_range (tuple or None): Tuple specifying the lower and upper bounds of the x-axis. \n",
    "                                    If None, the full range of [0, 1] is used.\n",
    "        include_no_change (bool): If True, include the \"No Change\" group in the plot. Default is True.\n",
    "        plot_stacked_bar (bool): If True, plot a stacked bar chart showing the proportions of the three groups. Default is True.\n",
    "        \"\"\"\n",
    "        # Define StimResponsivity labels and their corresponding colors\n",
    "        stim_responsivity_labels = [-1.0, 0.0, 1.0]\n",
    "        colors = ['red', 'grey', 'green']\n",
    "        labels = ['Decrease', 'No Change', 'Increase']\n",
    "        \n",
    "        # Prepare bin edges for the histogram\n",
    "        bin_edges = np.concatenate([np.arange(0, 0.5, 0.01), np.arange(0.501, 1.01, 0.01)])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Dictionary to store total counts\n",
    "        total_counts = {'Decrease': 0, 'No Change': 0, 'Increase': 0}\n",
    "\n",
    "        # Step 1: Get the filtered and categorized DataFrame\n",
    "        filtered_df = self.prepare_StimProb_distribution(df_name, is_single_unit, cell_type, groupname=groupname)\n",
    "        \n",
    "        # Step 2: Iterate over each category and count and plot the AUROC values\n",
    "        for label, stim_label in zip(labels, stim_responsivity_labels):\n",
    "            # Count the number of entries in the current category\n",
    "            total_counts[label] = filtered_df[filtered_df['StimResponsivity'] == stim_label].shape[0]\n",
    "\n",
    "            # Plot only if it's either \"Decrease\" or \"Increase\" or if \"No Change\" is included\n",
    "            if label == 'No Change' and not include_no_change:\n",
    "                continue  # Skip plotting for \"No Change\" if not included\n",
    "\n",
    "            # Extract AUROC values for the current category without additional filtering\n",
    "            auroc_values = filtered_df[filtered_df['StimResponsivity'] == stim_label]['StimProb'].apply(lambda x: x[0]).tolist()\n",
    "\n",
    "            # Calculate the histogram\n",
    "            hist, _ = np.histogram(auroc_values, bins=bin_edges)\n",
    "            \n",
    "            # Create the step plot\n",
    "            plt.step(bin_edges[:-1], hist, where='post', label=f'{label} (StimResponsivity={stim_label})', color=colors[labels.index(label)])\n",
    "            \n",
    "            # Add light fill for \"Decrease\"\n",
    "            if stim_label == -1.0:\n",
    "                plt.fill_between(bin_edges[:-1], hist, step='post', color=colors[labels.index(label)], alpha=0.1)\n",
    "        \n",
    "        # Add legend and title to the plot\n",
    "        plt.legend()\n",
    "        plt.title('StimProb AUROC Distribution by StimResponsivity')\n",
    "        plt.xlabel('AUROC Value')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Print the total counts for each category\n",
    "        print(f\"Total counts - Decrease: {total_counts['Decrease']}, No Change: {total_counts['No Change']}, Increase: {total_counts['Increase']}\")\n",
    "\n",
    "        # Set x-axis limits based on the user input\n",
    "        if xlim_range is not None:\n",
    "            plt.xlim(xlim_range)\n",
    "        else:\n",
    "            plt.xlim([0, 1])  # Default full range\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Step 3: Plot the stacked bar chart if requested\n",
    "        if plot_stacked_bar:\n",
    "            sizes = [total_counts['Decrease'], total_counts['No Change'], total_counts['Increase']]\n",
    "            colors_bar = ['red', 'grey', 'green']\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.bar(['StimResponsivity'], sizes[0], color=colors_bar[0], label='Decrease')\n",
    "            plt.bar(['StimResponsivity'], sizes[1], bottom=sizes[0], color=colors_bar[1], label='No Change')\n",
    "            plt.bar(['StimResponsivity'], sizes[2], bottom=sizes[0] + sizes[1], color=colors_bar[2], label='Increase')\n",
    "            \n",
    "            plt.title('Proportions of StimResponsivity Categories')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    def prepare_StimProb_distribution(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Prepare a distribution of AUROC values from the 'StimProb' column of the filtered DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        DataFrame: Filtered DataFrame with StimProb AUROC values.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Apply the StimResponsivity logic\n",
    "        filtered_df['StimResponsivity'] = filtered_df.apply(determine_stim_responsivity, axis=1)\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_cumulative_probability(self, df_name, group1, group2, is_single_unit=None, cell_type=None):\n",
    "        \"\"\"\n",
    "        Plot the cumulative probability of baseline firing rates between two groups.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter.\n",
    "        group1 (str): Name of the first group.\n",
    "        group2 (str): Name of the second group.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Define groups and colors\n",
    "        groups = [group1, group2]\n",
    "        colors = ['blue', 'orange']\n",
    "        \n",
    "        for group, color in zip(groups, colors):\n",
    "            # Get filtered data for the current group\n",
    "            filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, groupname=group)\n",
    "            \n",
    "            # Extract baseline firing rates\n",
    "            baseline_firing_rates = filtered_df['MeanFR_baseline'].dropna()\n",
    "            \n",
    "            # Calculate cumulative distribution\n",
    "            sorted_fr = np.sort(baseline_firing_rates)\n",
    "            cum_prob = np.arange(1, len(sorted_fr) + 1) / len(sorted_fr)\n",
    "            \n",
    "            # Plot cumulative probability\n",
    "            plt.plot(sorted_fr, cum_prob, label=f'{group} (n={len(sorted_fr)})', color=color)\n",
    "        \n",
    "        # Add legend and title\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Cumulative Probability of Baseline Firing Rates')\n",
    "        plt.xlabel('Mean Baseline Firing Rate')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "\n",
    "    def generate_summary_statistics(self, column_name, df_name='basic_metrics', handle_arrays=True, array_element=0):\n",
    "        \"\"\"\n",
    "        Generates summary statistics for a specified column in the DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        column_name (str): The name of the column to generate statistics for.\n",
    "        df_name (str): The name of the DataFrame to use (default is 'basic_metrics').\n",
    "        handle_arrays (bool): Whether to handle numpy arrays by extracting a specific element.\n",
    "        array_element (int): The index of the element to extract from numpy arrays (default is 0).\n",
    "        \n",
    "        Returns:\n",
    "        summary (dict): A dictionary with counts of unique values in the specified column.\n",
    "        \"\"\"\n",
    "        # Ensure the DataFrame exists\n",
    "        if df_name not in self.dataframes:\n",
    "            raise ValueError(f\"DataFrame '{df_name}' does not exist in self.dataframes.\")\n",
    "        \n",
    "        df = self.dataframes[df_name]\n",
    "        \n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(f\"Column '{column_name}' does not exist in DataFrame '{df_name}'.\")\n",
    "        \n",
    "        # Access the column data\n",
    "        column_data = df[column_name]\n",
    "        \n",
    "        # If the column might contain numpy arrays or lists, and handle_arrays is True,\n",
    "        # process the column to extract a specific element from these arrays/lists.\n",
    "        if handle_arrays:\n",
    "            # Apply the extract_array_element function to each element in the column.\n",
    "            # This replaces numpy arrays or lists with the specific element (defined by array_element)\n",
    "            # or leaves other data types unchanged.\n",
    "            column_data = column_data.apply(extract_array_element, args=(array_element,))\n",
    "        \n",
    "        # Calculate the value counts for the processed column data.\n",
    "        # This will count how often each unique value appears in the column.\n",
    "        summary = column_data.value_counts().to_dict()\n",
    "        \n",
    "        # Return the summary as a dictionary, where the keys are unique values in the column\n",
    "        # and the values are the counts of how often these unique values appear.\n",
    "        return summary\n",
    "    \n",
    "    def inspect_raw_data(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Inspect the raw data to understand the distribution of cells before any transformations.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to inspect.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Prints out the information about the data.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Print out the basic information\n",
    "        print(\"Total number of cells:\", len(filtered_df))\n",
    "        print(filtered_df.head())  # Show the first few rows of the DataFrame\n",
    "\n",
    "        # Display the distribution of AUROC values\n",
    "        print(\"AUROC values distribution:\")\n",
    "        print(filtered_df['StimProb'].apply(lambda x: x[0]).describe())\n",
    "\n",
    "        # Display the initial distribution of ModulationIndex\n",
    "        print(\"Initial ModulationIndex distribution:\")\n",
    "        print(filtered_df['ModulationIndex'].value_counts())\n",
    "        \n",
    "    def inspect_categorized_data(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Categorize the data using determine_stim_responsivity and inspect the resulting distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to inspect.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Prints out the categorized information.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Apply the StimResponsivity logic\n",
    "        filtered_df['StimResponsivity'] = filtered_df.apply(determine_stim_responsivity, axis=1)\n",
    "        \n",
    "        # Print out the basic information after categorization\n",
    "        print(\"Total number of cells after categorization:\", len(filtered_df))\n",
    "        print(filtered_df.head())  # Show the first few rows of the categorized DataFrame\n",
    "\n",
    "        # Display the distribution of StimResponsivity labels\n",
    "        print(\"Distribution of StimResponsivity labels:\")\n",
    "        print(filtered_df['StimResponsivity'].value_counts())\n",
    "\n",
    "    def check_stim_responsivity_distribution_before_after(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Check the distribution of the 'StimResponsivity' column before and after applying the categorization logic.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to inspect.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Prints out the distributions before and after the categorization.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Check initial distribution of StimResponsivity\n",
    "        print(\"Initial StimResponsivity distribution:\")\n",
    "        print(filtered_df['StimResponsivity'].value_counts())\n",
    "\n",
    "        # Check how many cells have an AUROC lower CI > 0.5\n",
    "        auroc_lower_ci = filtered_df['StimProb'].apply(lambda x: x[1] if len(x) > 1 else None)\n",
    "        responsive_cells = auroc_lower_ci > 0.5\n",
    "        print(f\"Number of cells with AUROC lower CI > 0.5: {responsive_cells.sum()} / {len(filtered_df)}\")\n",
    "        \n",
    "        # Apply the StimResponsivity logic\n",
    "        filtered_df['StimResponsivity_New'] = filtered_df.apply(determine_stim_responsivity, axis=1)\n",
    "        \n",
    "        # Check the distribution of StimResponsivity after categorization\n",
    "        print(\"StimResponsivity distribution after categorization:\")\n",
    "        print(filtered_df['StimResponsivity_New'].value_counts())\n",
    "        \n",
    "        \n",
    "        #### plot distrubution without beign redundant\n",
    "\n",
    "    def plot_raw_auroc_distribution(self, df_name, is_single_unit=None, cell_type=None, groupname=None, xlim_range=None):\n",
    "        \"\"\"\n",
    "        Plot the distribution of AUROC values, color-coded based on the 'StimResponsivity' labels.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        xlim_range (tuple or None): Tuple specifying the lower and upper bounds of the x-axis. \n",
    "                                    If None, the full range of [0, 1] is used.\n",
    "        \n",
    "        Returns:\n",
    "        None: Displays the plot.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Define StimResponsivity labels and their corresponding colors\n",
    "        stim_responsivity_labels = [-1.0, 0.0, 1.0]\n",
    "        colors = ['red', 'grey', 'green']\n",
    "        labels = ['Decrease', 'No Change', 'Increase']\n",
    "        \n",
    "        # Prepare bin edges for the histogram\n",
    "        bin_edges = np.linspace(0, 1, 51)  # 50 bins between 0 and 1\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Dictionary to store total counts\n",
    "        total_counts = {'Decrease': 0, 'No Change': 0, 'Increase': 0}\n",
    "\n",
    "        # Iterate over each category and plot the AUROC values\n",
    "        for label, stim_label in zip(labels, stim_responsivity_labels):\n",
    "            # Extract AUROC values for the current category\n",
    "            auroc_values = filtered_df[filtered_df['StimResponsivity'] == stim_label]['StimProb'].apply(lambda x: x[0]).tolist()\n",
    "            total_counts[label] = len(auroc_values)\n",
    "            \n",
    "            # Calculate the histogram\n",
    "            hist, _ = np.histogram(auroc_values, bins=bin_edges)\n",
    "            \n",
    "            # Create the step plot\n",
    "            plt.step(bin_edges[:-1], hist, where='post', label=f'{label} (StimResponsivity={stim_label})', color=colors[labels.index(label)])\n",
    "            \n",
    "            # Add light fill\n",
    "            plt.fill_between(bin_edges[:-1], hist, step='post', color=colors[labels.index(label)], alpha=0.1)\n",
    "        \n",
    "        # Add legend and title to the plot\n",
    "        plt.legend()\n",
    "        plt.title('AUROC Distribution by StimResponsivity')\n",
    "        plt.xlabel('AUROC Value')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Set x-axis limits based on the user input\n",
    "        if xlim_range is not None:\n",
    "            plt.xlim(xlim_range)\n",
    "        else:\n",
    "            plt.xlim([0, 1])  # Default full range\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Print the total counts for each category\n",
    "        print(f\"Total counts - Decrease: {total_counts['Decrease']}, No Change: {total_counts['No Change']}, Increase: {total_counts['Increase']}\")\n",
    "\n",
    "        \n",
    "    def plot_modulation_index_distribution(self, df_name, is_single_unit=None, cell_type=None, groupname=None):\n",
    "        \"\"\"\n",
    "        Plot the distribution of ModulationIndex values directly from the original DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units,\n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        groupname (str or None): Filter by group name. If None, no filtering by group name.\n",
    "        \n",
    "        Returns:\n",
    "        None: Displays the plot.\n",
    "        \"\"\"\n",
    "        # Retrieve the filtered DataFrame\n",
    "        filtered_df = self.get_filtered_data(df_name, is_single_unit, cell_type, None, groupname)\n",
    "        \n",
    "        # Count the occurrences of each ModulationIndex category\n",
    "        modulation_index_counts = filtered_df['ModulationIndex'].value_counts()\n",
    "\n",
    "        # Plot the bar chart of ModulationIndex counts\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        modulation_index_counts.plot(kind='bar', color=['green', 'red', 'grey'])\n",
    "        plt.title('Distribution of ModulationIndex Categories')\n",
    "        plt.xlabel('ModulationIndex')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_modulation_index_with_firing_rate_color_and_size(self, df_name, group_name=None, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location, with firing rates represented by both color and size of circles.\n",
    "        Circles have black outlines and are slightly offset horizontally to reduce overlap. Horizontal lines connect each point to x=0.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        group_name (str or None): Name of the group to filter.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "        \"\"\"\n",
    "        # Prepare the data for plotting\n",
    "        plotting_data = self.prepare_plotting_data(df_name, group_name, is_single_unit, cell_type, stim_responsivity)\n",
    "        if plotting_data is None or plotting_data.empty:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Data preparation (same as before)\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "        plotting_data['MeanFR_baseline'] = plotting_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "        plotting_data['MeanFR_baseline'] = pd.to_numeric(plotting_data['MeanFR_baseline'], errors='coerce')\n",
    "        plotting_data['ModulationIndex_Numeric'] = pd.to_numeric(plotting_data['ModulationIndex_Numeric'], errors='coerce')\n",
    "        plotting_data['ElectrodeOrder'] = pd.to_numeric(plotting_data['ElectrodeOrder'], errors='coerce')\n",
    "        plotting_data = plotting_data.dropna(subset=['MeanFR_baseline', 'ModulationIndex_Numeric', 'ElectrodeOrder'])\n",
    "\n",
    "        # Calculate sizes for scatter points\n",
    "        min_size = 10\n",
    "        max_size = 200\n",
    "        sizes = min_size + (plotting_data['MeanFR_baseline'] / plotting_data['MeanFR_baseline'].max()) * (max_size - min_size)\n",
    "\n",
    "        # Add horizontal jitter\n",
    "        plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "        # Create the plot\n",
    "        fig, ax = plt.subplots(figsize=(4, 8))\n",
    "\n",
    "        # Plot horizontal lines from x=0 to each point\n",
    "        for _, row in plotting_data.iterrows():\n",
    "            ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                    color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "        # Plot the modulation index points\n",
    "        scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                            c=plotting_data['MeanFR_baseline'], cmap='hot', \n",
    "                            s=sizes, alpha=0.8, vmin=0, edgecolors='black', linewidths=0.5)\n",
    "        \n",
    "        # Add colorbar for firing rates\n",
    "        cbar = plt.colorbar(scatter, label='Firing Rate (Hz)')\n",
    "        max_fr = plotting_data['MeanFR_baseline'].max()\n",
    "        cbar.set_ticks([0, max_fr])\n",
    "        cbar.set_ticklabels(['0', f'{max_fr:.2f}'])\n",
    "\n",
    "        # Add a vertical line at x=0\n",
    "        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        ax.set_xlabel('Spontaneous MI')\n",
    "        ax.set_ylabel('Electrode Order')\n",
    "        ax.set_title(f'{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units from {group_name}')\n",
    "        ax.invert_yaxis()  # To match the order from the provided list\n",
    "\n",
    "        ax.set_xlim(-1.5, 1.5)\n",
    "        ax.grid(False)\n",
    "        ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save the plot\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        file_name = f'{group_name}_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_firing_rate_color_size_jittered_hot'\n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity is not None:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    def plot_modulation_index_with_firing_rate_color_and_size_groupcomparison(self, df_name, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_fr_threshold=0.001):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location for CTZ and No_CTZ groups side by side.\n",
    "        Firing rates are represented by both color and size of circles, with customizable size controls.\n",
    "        Circles have black outlines and are slightly offset horizontally to reduce overlap. \n",
    "        Horizontal lines connect each point to x=0.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "        size_multiplier (float): Multiplier for circle sizes. Default is 1.\n",
    "        min_size (float): Minimum size for circles with firing rates below the threshold. Default is 20.\n",
    "        min_fr_threshold (float): Firing rate threshold below which circles will have the minimum size. Default is 0.001.\n",
    "        \"\"\"\n",
    "        groups = ['CTZ', 'No_CTZ']\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(6, 8), sharey=True)\n",
    "\n",
    "        all_data = []\n",
    "        for group in groups:\n",
    "            data = self.prepare_plotting_data(df_name, group, is_single_unit, cell_type, stim_responsivity)\n",
    "            if data is not None and not data.empty:\n",
    "                all_data.append(data)\n",
    "\n",
    "        if not all_data:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Combine all data for global color scaling\n",
    "        combined_data = pd.concat(all_data)\n",
    "        \n",
    "        # Convert MeanFR_baseline to scalar values\n",
    "        combined_data['MeanFR_baseline'] = combined_data['MeanFR_baseline'].apply(lambda x: x.item() if isinstance(x, np.ndarray) else x)\n",
    "        combined_data['MeanFR_baseline'] = pd.to_numeric(combined_data['MeanFR_baseline'], errors='coerce')\n",
    "        \n",
    "        max_fr = combined_data['MeanFR_baseline'].max()\n",
    "\n",
    "        # Define size calculation function\n",
    "        def calculate_size(fr):\n",
    "            if fr <= min_fr_threshold:\n",
    "                return min_size\n",
    "            else:\n",
    "                return min_size + (fr - min_fr_threshold) * size_multiplier\n",
    "\n",
    "        for ax, group in zip(axes, groups):\n",
    "            plotting_data = combined_data[combined_data['groupname'] == group]\n",
    "\n",
    "            if plotting_data.empty:\n",
    "                ax.text(0.5, 0.5, f'No data for {group}', ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "\n",
    "            # Add horizontal jitter\n",
    "            plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "            # Calculate sizes\n",
    "            plotting_data['sizes'] = plotting_data['MeanFR_baseline'].apply(calculate_size)\n",
    "\n",
    "            # Plot horizontal lines from x=0 to each point\n",
    "            for _, row in plotting_data.iterrows():\n",
    "                ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                        color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "            # Plot the modulation index points\n",
    "            scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                                c=plotting_data['MeanFR_baseline'], cmap='hot', \n",
    "                                s=plotting_data['sizes'], alpha=0.8, vmin=0, vmax=max_fr, \n",
    "                                edgecolors='black', linewidths=0.5)\n",
    "\n",
    "            # Add a vertical line at x=0\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "            ax.set_xlabel('Spontaneous MI')\n",
    "            ax.set_title(f'{group}\\n{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units')\n",
    "            ax.invert_yaxis()  # To match the order from the provided list\n",
    "            ax.set_xlim(-1.0, 1.0)\n",
    "            ax.grid(False)\n",
    "            ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Add a common y-label\n",
    "        fig.text(0.04, 0.5, 'Electrode Order', va='center', rotation='vertical')\n",
    "\n",
    "        # Add colorbar for firing rates\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "        cbar.set_label('Firing Rate (Hz)')\n",
    "        cbar.set_ticks([0, max_fr])\n",
    "        cbar.set_ticklabels(['0', f'{max_fr:.2f}'])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.9)  # Make room for the colorbar\n",
    "\n",
    "        # Save the plot\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        file_name = f'CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_firing_rate_color_size_jittered_hot'\n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity is not None:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "\n",
    "    def plot_modulation_index_with_firing_rate_color_and_size_groupcomparison_evoked(self, df_name, stim_index=3, is_single_unit=None, cell_type=None, stim_responsivity=None, jitter=0.1, size_multiplier=1, min_size=20, min_fr_threshold=0.001):\n",
    "        \"\"\"\n",
    "        Plot the modulation index as a function of electrode location for CTZ and No_CTZ groups side by side,\n",
    "        using evoked firing rates from MeanFR_stim.\n",
    "        Firing rates are represented by both color and size of circles, with a minimum size for rates below a threshold.\n",
    "        Circles have black outlines and are slightly offset horizontally to reduce overlap. \n",
    "        Horizontal lines connect each point to x=0.\n",
    "\n",
    "        Parameters:\n",
    "        df_name (str): Name of the DataFrame to filter and plot.\n",
    "        stim_index (int): Index of the stimulus intensity to use (0-3, where 3 is max stimulation). Default is 3.\n",
    "        is_single_unit (float or None): If 1.0, filter for single units, if 0.0, filter for non-single units, \n",
    "                                        if None, do not filter by this criterion.\n",
    "        cell_type (str or None): Filter for 'FS' or 'RS' cell types. If None, no filtering by cell type.\n",
    "        stim_responsivity (float or None): Value to filter by in the StimResponsivity column. Can be 1.0, 0.0, or -1.0.\n",
    "                                        If None, no filtering by StimResponsivity.\n",
    "        jitter (float): Amount of horizontal jitter to apply to points. Default is 0.1.\n",
    "        size_multiplier (float): Multiplier for circle sizes. Default is 1.\n",
    "        min_size (float): Minimum size for circles with firing rates below the threshold. Default is 20.\n",
    "        min_fr_threshold (float): Firing rate threshold below which circles will have the minimum size. Default is 0.001.\n",
    "        \"\"\"\n",
    "        groups = ['CTZ', 'No_CTZ']\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 8), sharey=True)\n",
    "\n",
    "        all_data = []\n",
    "        for group in groups:\n",
    "            data = self.prepare_plotting_data(df_name, group, is_single_unit, cell_type, stim_responsivity)\n",
    "            if data is not None and not data.empty:\n",
    "                all_data.append(data)\n",
    "\n",
    "        if not all_data:\n",
    "            print(\"No data available for plotting.\")\n",
    "            return\n",
    "\n",
    "        # Combine all data for global color scaling\n",
    "        combined_data = pd.concat(all_data)\n",
    "        \n",
    "        # Extract the specified stim_index from MeanFR_stim\n",
    "        combined_data['EvokeFR'] = combined_data['MeanFR_stim'].apply(lambda x: x[stim_index] if isinstance(x, (list, np.ndarray)) and len(x) > stim_index else np.nan)\n",
    "        combined_data['EvokeFR'] = pd.to_numeric(combined_data['EvokeFR'], errors='coerce')\n",
    "        \n",
    "        max_fr = combined_data['EvokeFR'].max()\n",
    "\n",
    "        # Define size calculation function\n",
    "        def calculate_size(fr):\n",
    "            if fr <= min_fr_threshold:\n",
    "                return min_size\n",
    "            else:\n",
    "                return min_size + (fr - min_fr_threshold) * size_multiplier\n",
    "\n",
    "        for ax, group in zip(axes, groups):\n",
    "            plotting_data = combined_data[combined_data['groupname'] == group]\n",
    "\n",
    "            if plotting_data.empty:\n",
    "                ax.text(0.5, 0.5, f'No data for {group}', ha='center', va='center', transform=ax.transAxes)\n",
    "                continue\n",
    "\n",
    "            # Add horizontal jitter\n",
    "            plotting_data['Jittered_ElectrodeOrder'] = plotting_data['ElectrodeOrder'] + np.random.uniform(-jitter, jitter, len(plotting_data))\n",
    "\n",
    "            # Calculate sizes\n",
    "            plotting_data['sizes'] = plotting_data['EvokeFR'].apply(calculate_size)\n",
    "\n",
    "            # Plot horizontal lines from x=0 to each point\n",
    "            for _, row in plotting_data.iterrows():\n",
    "                ax.plot([0, row['ModulationIndex_Numeric']], [row['Jittered_ElectrodeOrder'], row['Jittered_ElectrodeOrder']], \n",
    "                        color='gray', alpha=0.3, linewidth=0.5)\n",
    "\n",
    "            # Plot the modulation index points\n",
    "            scatter = ax.scatter(plotting_data['ModulationIndex_Numeric'], plotting_data['Jittered_ElectrodeOrder'], \n",
    "                                c=plotting_data['EvokeFR'], cmap='hot', \n",
    "                                s=plotting_data['sizes'], alpha=0.8, vmin=0, vmax=max_fr, \n",
    "                                edgecolors='black', linewidths=0.5)\n",
    "\n",
    "            # Add a vertical line at x=0\n",
    "            ax.axvline(x=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "            ax.set_xlabel('Spontaneous MI')\n",
    "            ax.set_title(f'{group}\\n{cell_type} {\"single\" if is_single_unit == 1.0 else \"multi\"}-units\\nStim Index: {stim_index}')\n",
    "            ax.invert_yaxis()  # To match the order from the provided list\n",
    "            ax.set_xlim(-1.0, 1.0)\n",
    "            ax.grid(False)\n",
    "            ax.set_yticks(np.arange(0, 32, 1))\n",
    "\n",
    "        # Add a common y-label\n",
    "        fig.text(0.04, 0.5, 'Electrode Order', va='center', rotation='vertical')\n",
    "\n",
    "        # Add colorbar for firing rates\n",
    "        cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "        cbar.set_label('Evoked Firing Rate (Hz)')\n",
    "        cbar.set_ticks([0, max_fr])\n",
    "        cbar.set_ticklabels(['0', f'{max_fr:.2f}'])\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.9)  # Make room for the colorbar\n",
    "\n",
    "        # Save the plot\n",
    "        directory = '/Volumes/MannySSD/figures/laminar_plots'\n",
    "        try:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating directory: {e}\")\n",
    "            return\n",
    "\n",
    "        file_name = f'CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_evoked_FR_stim{stim_index}_color_size_jittered_hot'\n",
    "        if cell_type:\n",
    "            file_name += f'_{cell_type}'\n",
    "        if stim_responsivity is not None:\n",
    "            file_name += f'_stim_{stim_responsivity}'\n",
    "        \n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        \n",
    "        try:\n",
    "            plt.savefig(file_path, format='svg', dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {file_path}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving plot: {e}\")\n",
    "    \n",
    "    def plot_optogenetic_psth_comparison(self, group1, group2, opto_freq, \n",
    "                                        ax=None, cell_type=None, is_single_unit=None, \n",
    "                                        stim_responsivity=None, modulation_label=None,\n",
    "                                        time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison for optogenetic experiments on the provided axes object \n",
    "        or creates a new figure if not provided.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_array <= time_range[1])\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Check if we need to create a new figure\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "        # DataFrame to store all traces\n",
    "        all_traces_df = pd.DataFrame()\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group in zip([df1, df2], [group1, group2]):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = opto_freq\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "        # Set plot attributes\n",
    "        ax.set_title(f'PSTH Comparison of {opto_freq} between {group1} and {group2}')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Average Spike Rate')\n",
    "        ax.legend()\n",
    "\n",
    "        # Only show the plot if an axes object was not provided\n",
    "        if ax is None:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "\n",
    "    def compare_optogenetic_groups(self, group1, group2, opto_freq, cell_type=None, is_single_unit=None, stim_responsivity=None, modulation_label=None):\n",
    "        \"\"\"\n",
    "        Compares PSTH data between two specified groups for a given optogenetic stimulation frequency.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): The first group name for comparison.\n",
    "            group2 (str): The second group name for comparison.\n",
    "            opto_freq (str): The optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Specific cell type to filter by.\n",
    "            is_single_unit (float, optional): Filter for single units (1.0) or multi-units (0.0).\n",
    "            stim_responsivity (float, optional): Filter by stimulus responsivity.\n",
    "            modulation_label (str, optional): Filter by modulation label ('positive', 'negative', 'none').\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing two pandas DataFrames representing the filtered data for each group.\n",
    "        \"\"\"\n",
    "        df_name = f'psth_dataframe_{opto_freq}'\n",
    "        if df_name not in self.dataframes:\n",
    "            print(f\"No data available for the optogenetic stimulation: {opto_freq}\")\n",
    "            return None, None\n",
    "\n",
    "        base_df = self.dataframes[df_name]\n",
    "\n",
    "        # Print the shape of the pre-filtered DataFrame\n",
    "        print(f\"Pre-filtered DataFrame shape: {base_df.shape}\")\n",
    "\n",
    "        # Define a helper function to create a mask for a given group\n",
    "        def create_mask(group):\n",
    "            mask = (base_df['groupname'] == group)\n",
    "            if cell_type is not None:\n",
    "                mask &= (base_df['Cell_Type'] == cell_type)\n",
    "            if is_single_unit is not None:\n",
    "                mask &= (base_df['IsSingleUnit'] == is_single_unit)\n",
    "            if stim_responsivity is not None:\n",
    "                mask &= (base_df['StimResponsivity'] == stim_responsivity)\n",
    "            if modulation_label is not None:\n",
    "                if modulation_label not in ['positive', 'negative', 'none']:\n",
    "                    raise ValueError(\"Modulation label must be one of 'positive', 'negative', or 'none'.\")\n",
    "                mask &= (base_df['ModulationIndex'] == modulation_label)\n",
    "            return mask\n",
    "\n",
    "        # Create masks for each group\n",
    "        mask1 = create_mask(group1)\n",
    "        mask2 = create_mask(group2)\n",
    "\n",
    "        # Filter the DataFrame for each group\n",
    "        channel1 = base_df[mask1]\n",
    "        channel2 = base_df[mask2]\n",
    "\n",
    "        # Print the shape of the filtered DataFrames\n",
    "        print(f\"Filtered df1 (group {group1}) shape: {channel1.shape}\")\n",
    "        print(f\"Filtered df2 (group {group2}) shape: {channel2.shape}\")\n",
    "\n",
    "        return channel1, channel2\n",
    "    \n",
    "    def plot_optogenetic_psth_comparison_subplot(self, group1, group2, opto_freq, \n",
    "                                                cell_type=None, is_single_unit=None, \n",
    "                                                stim_responsivity=None, modulation_label=None,\n",
    "                                                time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison for optogenetic experiments in a 1x2 subplot,\n",
    "        with each group plotted separately.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Create a new figure with 1x2 subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5), sharey=True)\n",
    "        axes = [ax1, ax2]\n",
    "\n",
    "        # DataFrame to store all traces\n",
    "        all_traces_df = pd.DataFrame()\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group, ax in zip([df1, df2], [group1, group2], axes):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = opto_freq\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "            # Set plot attributes for each subplot\n",
    "            ax.set_title(f'{group} - {opto_freq}')\n",
    "            ax.set_xlabel('Time (ms)')\n",
    "            ax.set_ylabel('Average Spike Rate')\n",
    "            ax.legend()\n",
    "\n",
    "        # Set overall title\n",
    "        fig.suptitle(f'PSTH Comparison of {opto_freq} between {group1} and {group2}', fontsize=16)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "\n",
    "    def plot_optogenetic_psth_comparison_subplot_with_led(self, group1, group2, opto_freq, \n",
    "                                                        cell_type=None, is_single_unit=None, \n",
    "                                                        stim_responsivity=None, modulation_label=None,\n",
    "                                                        time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a PSTH comparison for optogenetic experiments in a 1x2 subplot,\n",
    "        with each group plotted separately and the LED signal on a shared third axis.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty or df2.empty:\n",
    "            print(\"One of the groups has no data after filtering.\")\n",
    "            return\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq]\n",
    "        led_array = led_array[time_mask]  # Apply the same time mask to LED signal\n",
    "\n",
    "        # Create a new figure with 1x2 subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5), sharey=True)\n",
    "        axes = [ax1, ax2]\n",
    "\n",
    "        # Create a third axis for LED signal, shared between the two subplots\n",
    "        ax_led = ax1.twinx()\n",
    "        ax2.twinx().set_ylabel('')  # Hide y-axis label for the second LED axis\n",
    "\n",
    "        # DataFrame to store all traces\n",
    "        all_traces_df = pd.DataFrame()\n",
    "\n",
    "        # Process and plot data\n",
    "        for df, group, ax in zip([df1, df2], [group1, group2], axes):\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            # Store each trace in the DataFrame with additional metadata\n",
    "            group_traces_df = pd.DataFrame(data.tolist(), columns=time_array)\n",
    "            group_traces_df['Group'] = group\n",
    "            group_traces_df['Stimulation'] = opto_freq\n",
    "            group_traces_df['Cell_Type'] = cell_type\n",
    "            group_traces_df['IsSingleUnit'] = is_single_unit\n",
    "            group_traces_df['StimResponsivity'] = stim_responsivity\n",
    "            all_traces_df = pd.concat([all_traces_df, group_traces_df])\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "            # Set plot attributes for each subplot\n",
    "            ax.set_title(f'{group} - {opto_freq}')\n",
    "            ax.set_xlabel('Time (ms)')\n",
    "            ax.set_ylabel('Average Spike Rate')\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "        # Plot LED signal on both subplots\n",
    "        ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "        ax2.twinx().plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "        # Set y-axis label for LED signal\n",
    "        ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "        ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "        # Add legend for LED signal\n",
    "        ax_led.legend(loc='upper right')\n",
    "\n",
    "        # Set overall title\n",
    "        fig.suptitle(f'PSTH Comparison of {opto_freq} between {group1} and {group2}', fontsize=16)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Return the DataFrame containing all traces\n",
    "        return all_traces_df\n",
    "\n",
    "    def plot_optogenetic_psth_per_recording(self, group1, group2, opto_freq, \n",
    "                                            cell_type=None, is_single_unit=None, \n",
    "                                            stim_responsivity=None, modulation_label=None,\n",
    "                                            time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots PSTHs for optogenetic experiments, filtered at the recording level for each group,\n",
    "        saves the results in a specified directory structure, and ensures comparable y-axes between groups.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name.\n",
    "            group2 (str): Second group name.\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_type (str, optional): Cell type to filter.\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Fetch data\n",
    "        df1, df2 = self.compare_optogenetic_groups(group1, group2, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)\n",
    "        if df1.empty and df2.empty:\n",
    "            print(\"Both groups have no data after filtering.\")\n",
    "            return\n",
    "        \n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq]\n",
    "        led_array = led_array[time_mask]  # Apply the same time mask to LED signal\n",
    "\n",
    "        # Base directory for saving figures\n",
    "        base_dir = \"/Volumes/MannySSD/figures/opto_psths_perrecording\"\n",
    "\n",
    "        # Find global y-axis limits\n",
    "        all_psth_data = pd.concat([df1['PSTHs_conv'], df2['PSTHs_conv']])\n",
    "        all_psth_data = all_psth_data.apply(lambda x: np.array(x)[time_mask])\n",
    "        global_ymin = all_psth_data.apply(np.min).min()\n",
    "        global_ymax = all_psth_data.apply(np.max).max()\n",
    "\n",
    "        # Process each group\n",
    "        for group, df in zip([group1, group2], [df1, df2]):\n",
    "            if df.empty:\n",
    "                print(f\"No data for group {group} after filtering.\")\n",
    "                continue\n",
    "\n",
    "            # Create group directory\n",
    "            group_dir = os.path.join(base_dir, group)\n",
    "            os.makedirs(group_dir, exist_ok=True)\n",
    "\n",
    "            # Process each unique recording\n",
    "            for recording in df['recordingname'].unique():\n",
    "                recording_df = df[df['recordingname'] == recording]\n",
    "                \n",
    "                # Create recording directory\n",
    "                recording_dir = os.path.join(group_dir, recording)\n",
    "                os.makedirs(recording_dir, exist_ok=True)\n",
    "\n",
    "                # Create a new figure for this recording\n",
    "                fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "                # Plot PSTH data\n",
    "                data = recording_df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "                if smoothing_window:\n",
    "                    window = np.ones(smoothing_window) / smoothing_window\n",
    "                    data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "                mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "                if plot_mode == 'sem':\n",
    "                    sem = data.apply(pd.Series).sem(axis=0)\n",
    "                    ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)\n",
    "\n",
    "                elif plot_mode == 'traces':\n",
    "                    for trace in data:\n",
    "                        ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)\n",
    "\n",
    "                ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])\n",
    "\n",
    "                # Plot LED signal\n",
    "                ax_led = ax.twinx()\n",
    "                ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "\n",
    "                # Set plot attributes\n",
    "                ax.set_title(f'{group} - {opto_freq} - Recording: {recording}')\n",
    "                ax.set_xlabel('Time (ms)')\n",
    "                ax.set_ylabel('Average Spike Rate')\n",
    "                ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "                ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "                # Set y-axis limits to be the same for all plots\n",
    "                ax.set_ylim(global_ymin, global_ymax)\n",
    "\n",
    "                # Add legends\n",
    "                ax.legend(loc='upper left')\n",
    "                ax_led.legend(loc='upper right')\n",
    "\n",
    "                # Save the figure\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Create filename with user input parameters\n",
    "                filename = f'PSTH_{opto_freq}'\n",
    "                if cell_type:\n",
    "                    filename += f'_celltype-{cell_type}'\n",
    "                if is_single_unit is not None:\n",
    "                    filename += f'_singleunit-{is_single_unit}'\n",
    "                if stim_responsivity is not None:\n",
    "                    filename += f'_stimresp-{stim_responsivity}'\n",
    "                if time_range:\n",
    "                    filename += f'_timerange-{time_range[0]}to{time_range[1]}'\n",
    "                filename += '.png'\n",
    "                \n",
    "                fig_path = os.path.join(recording_dir, filename)\n",
    "                plt.savefig(fig_path)\n",
    "                plt.close(fig)\n",
    "\n",
    "                print(f\"Saved figure for {group} - {recording} at {fig_path}\")\n",
    "\n",
    "        print(\"Finished processing all recordings.\")\n",
    "\n",
    "    def plot_optogenetic_psth_comparison_grid(self, group1, group2, opto_freq, \n",
    "                                            cell_types=['RS', 'FS'], is_single_unit=None, \n",
    "                                            stim_responsivity=None, modulation_label=None,\n",
    "                                            time_range=None, plot_mode='mean', smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots a 2x2 grid PSTH comparison for optogenetic experiments,\n",
    "        with each subplot showing a specific group and cell type combination.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name (e.g., 'CTZ').\n",
    "            group2 (str): Second group name (e.g., 'No_CTZ').\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_types (list): List of cell types to plot (default: ['RS', 'FS']).\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-100, 200)).\n",
    "            plot_mode (str, optional): Plotting mode ('mean', 'traces', 'sem').\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            group1: '#5a00c2',  # CTZ\n",
    "            group2: '#797979'   # No_CTZ\n",
    "        }\n",
    "\n",
    "        # Create a new figure with 2x2 subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16), sharex=True, sharey=True)\n",
    "\n",
    "        # Define the mapping of group and cell type to subplot position\n",
    "        plot_mapping = {\n",
    "            (group1, 'RS'): (0, 0),  # Top left\n",
    "            (group2, 'RS'): (0, 1),  # Top right\n",
    "            (group1, 'FS'): (1, 0),  # Bottom left\n",
    "            (group2, 'FS'): (1, 1)   # Bottom right\n",
    "        }\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq][time_mask]\n",
    "\n",
    "        # Process and plot data for each combination of group and cell type\n",
    "        for (group, cell_type), (row, col) in plot_mapping.items():\n",
    "            ax = axes[row, col]\n",
    "\n",
    "            # Fetch data for the current group and cell type\n",
    "            df = self.compare_optogenetic_groups(group, group, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)[0]\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data after filtering for {group} - {cell_type}.\")\n",
    "                continue\n",
    "\n",
    "            # Create a third axis for LED signal\n",
    "            ax_led = ax.twinx()\n",
    "\n",
    "            # Process data\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "\n",
    "            if plot_mode == 'sem':\n",
    "                sem = data.apply(pd.Series).sem(axis=0)\n",
    "\n",
    "            # Plot individual traces or mean with SEM\n",
    "            if plot_mode == 'traces':\n",
    "                for trace in data:\n",
    "                    ax.plot(time_array, trace, color=group_colors[group]+'33', alpha=0.2)  # Lighter traces\n",
    "            elif plot_mode == 'sem':\n",
    "                ax.fill_between(time_array, mean_psth - sem, mean_psth + sem, color=group_colors[group], alpha=0.2)  # SEM shading\n",
    "\n",
    "            ax.plot(time_array, mean_psth, label=f'{group}', color=group_colors[group])  # Mean trace\n",
    "\n",
    "            # Plot LED signal\n",
    "            ax_led.plot(time_array, led_array, color='blue', linestyle='-', linewidth=1.5, alpha=0.7, label='LED Signal')\n",
    "\n",
    "            # Set plot attributes\n",
    "            ax.set_title(f'{group} - {cell_type}')\n",
    "            if row == 1:  # Bottom row\n",
    "                ax.set_xlabel('Time (ms)')\n",
    "            if col == 0:  # Left column\n",
    "                ax.set_ylabel('Average Spike Rate')\n",
    "            ax.legend(loc='upper left')\n",
    "\n",
    "            # Set y-axis label for LED signal only for the rightmost subplots\n",
    "            if col == 1:\n",
    "                ax_led.set_ylabel('LED Signal (a.u.)', color='blue')\n",
    "                ax_led.tick_params(axis='y', labelcolor='blue')\n",
    "            else:\n",
    "                ax_led.set_ylabel('')\n",
    "\n",
    "        # Set overall title\n",
    "        fig.suptitle(f'PSTH Comparison of {opto_freq} between {group1} and {group2} for {\" and \".join(cell_types)} cells', fontsize=16)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #save the plot in the normal directory\n",
    "        plt.savefig(f'/Volumes/MannySSD/figures/opto_psths_comparison/CTZ_No_CTZ_comparison_{\"single\" if is_single_unit == 1.0 else \"multi\"}_units_evoked_FR.svg', format='svg', dpi=300, bbox_inches='tight', transparent=True)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def plot_interactive_psth_comparison_grid(self, group1, group2, opto_freq, \n",
    "                                            cell_types=['RS', 'FS'], is_single_unit=None, \n",
    "                                            stim_responsivity=None, modulation_label=None,\n",
    "                                            time_range=None, smoothing_window=None):\n",
    "        \"\"\"\n",
    "        Plots an interactive 2x2 grid PSTH comparison for optogenetic experiments using Plotly,\n",
    "        with each subplot showing a specific group and cell type combination, and LED signal on a separate axis.\n",
    "\n",
    "        Args:\n",
    "            group1 (str): First group name (e.g., 'CTZ').\n",
    "            group2 (str): Second group name (e.g., 'No_CTZ').\n",
    "            opto_freq (str): Optogenetic stimulation frequency (e.g., '8 Hz LED').\n",
    "            cell_types (list): List of cell types to plot (default: ['RS', 'FS']).\n",
    "            is_single_unit (float, optional): Single unit filter.\n",
    "            stim_responsivity (float, optional): Stimulus responsivity filter.\n",
    "            modulation_label (str, optional): Modulation label filter.\n",
    "            time_range (tuple, optional): Tuple specifying the start and end of the time range (e.g., (-200, 800)).\n",
    "            smoothing_window (int, optional): Size of the smoothing window; if None, no smoothing is applied.\n",
    "        \"\"\"\n",
    "        # Define colors\n",
    "        group_colors = {\n",
    "            group1: 'rgba(90, 0, 194, 0.7)',  # CTZ (semi-transparent purple)\n",
    "            group2: 'rgba(121, 121, 121, 0.7)'  # No_CTZ (semi-transparent gray)\n",
    "        }\n",
    "\n",
    "        # Create subplot titles\n",
    "        subplot_titles = [\n",
    "            f'{group1} - RS', f'{group2} - RS',\n",
    "            f'{group1} - FS', f'{group2} - FS'\n",
    "        ]\n",
    "\n",
    "        # Create a 2x2 subplot layout with secondary y-axes\n",
    "        fig = make_subplots(rows=2, cols=2, shared_xaxes=True, shared_yaxes=True,\n",
    "                            subplot_titles=subplot_titles,\n",
    "                            specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n",
    "                                [{\"secondary_y\": True}, {\"secondary_y\": True}]])\n",
    "\n",
    "        # Define the mapping of group and cell type to subplot position\n",
    "        plot_mapping = {\n",
    "            (group1, 'RS'): (1, 1),  # Top left\n",
    "            (group2, 'RS'): (1, 2),  # Top right\n",
    "            (group1, 'FS'): (2, 1),  # Bottom left\n",
    "            (group2, 'FS'): (2, 2)   # Bottom right\n",
    "        }\n",
    "\n",
    "        # Get the time array and adjust for the specified time range\n",
    "        time_array = self.eed.relative_time_ms['relative_time_ms']\n",
    "        if time_range:\n",
    "            time_mask = (time_array >= time_range[0]) & (time_range[1] >= time_array)\n",
    "            time_array = time_array[time_mask]\n",
    "        else:\n",
    "            time_mask = slice(None)\n",
    "\n",
    "        # Extract LED signal\n",
    "        extracted_led_signal = self.extract_stim_signals_opto()\n",
    "        led_array = extracted_led_signal[0][opto_freq][time_mask]\n",
    "\n",
    "        # Process and plot data for each combination of group and cell type\n",
    "        for (group, cell_type), (row, col) in plot_mapping.items():\n",
    "            # Fetch data for the current group and cell type\n",
    "            df = self.compare_optogenetic_groups(group, group, opto_freq, cell_type, is_single_unit, stim_responsivity, modulation_label)[0]\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"No data after filtering for {group} - {cell_type}.\")\n",
    "                continue\n",
    "\n",
    "            # Process data\n",
    "            data = df['PSTHs_conv'].apply(lambda x: np.array(x)[time_mask])\n",
    "            if smoothing_window:\n",
    "                window = np.ones(smoothing_window) / smoothing_window\n",
    "                data = data.apply(lambda x: np.convolve(x, window, mode='same'))\n",
    "\n",
    "            # Plot individual traces\n",
    "            for idx, (trace, groupname, recordingname, cid) in enumerate(zip(data, df['groupname'], df['recordingname'], df['cid'])):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=time_array,\n",
    "                        y=trace,\n",
    "                        mode='lines',\n",
    "                        line=dict(color=group_colors[group]),\n",
    "                        name=f'{group} - {cell_type}',\n",
    "                        legendgroup=f'{group} - {cell_type}',\n",
    "                        showlegend=idx == 0,  # Show legend only for the first trace of each group\n",
    "                        hoverinfo='text',\n",
    "                        hovertext=[f'Group: {groupname}<br>Recording: {recordingname}<br>CID: {cid}<br>Time: {t:.2f} ms<br>Rate: {r:.2f}' for t, r in zip(time_array, trace)],\n",
    "                        customdata=np.column_stack((np.full(len(time_array), groupname), \n",
    "                                                    np.full(len(time_array), recordingname), \n",
    "                                                    np.full(len(time_array), cid)))\n",
    "                    ),\n",
    "                    row=row, col=col, secondary_y=False\n",
    "                )\n",
    "\n",
    "            # Plot mean PSTH\n",
    "            mean_psth = data.apply(pd.Series).mean(axis=0)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=time_array,\n",
    "                    y=mean_psth,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='black', width=2),\n",
    "                    name=f'Mean {group} - {cell_type}',\n",
    "                    legendgroup=f'Mean {group} - {cell_type}',\n",
    "                ),\n",
    "                row=row, col=col, secondary_y=False\n",
    "            )\n",
    "\n",
    "            # Plot LED signal on secondary y-axis\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=time_array,\n",
    "                    y=led_array,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='blue', width=1.5),\n",
    "                    name='LED Signal',\n",
    "                    legendgroup='LED Signal',\n",
    "                    showlegend=row == 1 and col == 1,  # Show legend only once for LED signal\n",
    "                ),\n",
    "                row=row, col=col, secondary_y=True\n",
    "            )\n",
    "\n",
    "            # Update axes\n",
    "            fig.update_xaxes(title_text=\"Time (ms)\", row=row, col=col)\n",
    "            fig.update_yaxes(title_text=\"Average Spike Rate\" if col == 1 else \"\", row=row, col=col, secondary_y=False)\n",
    "            fig.update_yaxes(title_text=\"LED Signal (a.u.)\" if col == 2 else \"\", row=row, col=col, secondary_y=True)\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title=f'PSTH Comparison of {opto_freq} between {group1} and {group2} for {\" and \".join(cell_types)} cells',\n",
    "            height=800,\n",
    "            width=1200,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "\n",
    "        # Add custom JavaScript for trace highlighting\n",
    "        fig.update_layout(\n",
    "            updatemenus=[\n",
    "                dict(\n",
    "                    type=\"buttons\",\n",
    "                    direction=\"left\",\n",
    "                    buttons=[\n",
    "                        dict(\n",
    "                            args=[{\"visible\": [True] * len(fig.data)}],\n",
    "                            label=\"Show All\",\n",
    "                            method=\"restyle\"\n",
    "                        ),\n",
    "                        dict(\n",
    "                            args=[{\"visible\": \"legendonly\"}],\n",
    "                            label=\"Hide All\",\n",
    "                            method=\"restyle\"\n",
    "                        )\n",
    "                    ],\n",
    "                    pad={\"r\": 10, \"t\": 10},\n",
    "                    showactive=False,\n",
    "                    x=0.11,\n",
    "                    xanchor=\"left\",\n",
    "                    y=1.1,\n",
    "                    yanchor=\"top\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "def extract_array_element(value, array_element=0):\n",
    "    \"\"\"\n",
    "    Extracts the scalar value from a numpy array or list.\n",
    "    \n",
    "    Parameters:\n",
    "    value: The value from which to extract the element. This can be of any type, \n",
    "           but we are particularly interested in cases where it's a numpy array or list.\n",
    "    array_element (int): The index of the element to extract from the array or list (default is 0).\n",
    "    \n",
    "    Returns:\n",
    "    The extracted element if the value is a numpy array or list. \n",
    "    If the value is a scalar within a numpy array, it extracts and returns the scalar.\n",
    "    Otherwise, it returns the original value.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the value is a numpy array with a single element, extract that element.\n",
    "    if isinstance(value, np.ndarray) and value.size == 1:\n",
    "        return value.item()\n",
    "    \n",
    "    # If the value is a numpy array or list with multiple elements, extract the specified element.\n",
    "    elif isinstance(value, (np.ndarray, list)) and len(value) > array_element:\n",
    "        return value[array_element]\n",
    "    \n",
    "    # If the value is not a numpy array or list, or if it's not possible to extract a valid element, \n",
    "    # return the value as-is.\n",
    "    return value\n",
    "\n",
    "def determine_stim_responsivity(row):\n",
    "    \"\"\"\n",
    "    Determine the StimResponsivity based on the lower bound of the AUROC confidence interval and ModulationIndex.\n",
    "    \n",
    "    Parameters:\n",
    "    row (Series): A row of the DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    int: StimResponsivity value (1 for increase/positive modulation, -1 for decrease/negative modulation, 0 for no significant change).\n",
    "    \"\"\"\n",
    "    # Access the lower bound of the AUROC confidence interval\n",
    "    auroc_lower_bound = row['StimProb'][1] if len(row['StimProb']) > 1 else None\n",
    "    modulation_index = row['ModulationIndex']\n",
    "    \n",
    "    print(f\"AUROC lower bound: {auroc_lower_bound}, Modulation Index: {modulation_index}\")\n",
    "\n",
    "    # Step 1: Determine if the neuron is sensory responsive\n",
    "    if auroc_lower_bound is not None and auroc_lower_bound > 0.5:\n",
    "        # Step 2: Categorize modulation if the neuron is sensory responsive\n",
    "        if modulation_index == 'positive':\n",
    "            return 1  # Sensory responsive and positively modulated (Increase)\n",
    "        elif modulation_index == 'negative':\n",
    "            return -1  # Sensory responsive and negatively modulated (Decrease)\n",
    "    return 0  # No significant change or non-responsive\n",
    "\n",
    "def example_analysis_function(extracted_data):\n",
    "    \"\"\"\n",
    "    Example analysis function that calculates the mean spike count for each stimulus.\n",
    "\n",
    "    Args:\n",
    "        extracted_data (dict): The extracted data from the PSTH and raster data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the analysis results.\n",
    "    \"\"\"\n",
    "    analysis_results = {'mean_spike_counts': {}}\n",
    "    for stim, data in extracted_data['data'].items():\n",
    "        mean_spike_count = np.mean(data['raw_counts'])\n",
    "        analysis_results['mean_spike_counts'][stim] = mean_spike_count\n",
    "    return analysis_results\n",
    "\n",
    "def analyze_sensory_responsiveness(extracted_data, baseline_window=(-500, 0), response_window=(0, 50), analysis_window=(0, 200)):\n",
    "    \"\"\"\n",
    "    Analyze the sensory responsiveness of neurons by calculating the sensory response magnitude, z-scoring it,\n",
    "    and performing a nonparametric permutation test to determine if a neuron is modulated by the stimulus.\n",
    "\n",
    "    Args:\n",
    "        extracted_data (dict): The extracted data from the PSTH and raster data.\n",
    "        baseline_window (tuple): The window (in ms) to calculate the baseline firing rate. Default is (-500, 0).\n",
    "        response_window (tuple): The window (in ms) to calculate the response magnitude. Default is (0, 50).\n",
    "        analysis_window (tuple): The window (in ms) to analyze modulation. Default is (0, 200).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the sensory response analysis results.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for stim, data in extracted_data['data'].items():\n",
    "        spike_trains = data['spike_trains']\n",
    "        time_array = data['time_array']\n",
    "\n",
    "        # Calculate baseline firing rate\n",
    "        baseline_start_idx = np.searchsorted(time_array, baseline_window[0])\n",
    "        baseline_end_idx = np.searchsorted(time_array, baseline_window[1])\n",
    "        baseline_rates = spike_trains[:, baseline_start_idx:baseline_end_idx].mean(axis=1)\n",
    "\n",
    "        # Calculate response magnitude\n",
    "        response_start_idx = np.searchsorted(time_array, response_window[0])\n",
    "        response_end_idx = np.searchsorted(time_array, response_window[1])\n",
    "        response_rates = spike_trains[:, response_start_idx:response_end_idx].mean(axis=1)\n",
    "        response_magnitude = response_rates.mean() - baseline_rates.mean()\n",
    "\n",
    "        # Calculate z-scored magnitude\n",
    "        baseline_sd = baseline_rates.std()\n",
    "        z_scored_magnitude = response_magnitude / baseline_sd if baseline_sd != 0 else np.nan\n",
    "\n",
    "        # Permutation test for modulation analysis\n",
    "        analysis_start_idx = np.searchsorted(time_array, analysis_window[0])\n",
    "        analysis_end_idx = np.searchsorted(time_array, analysis_window[1])\n",
    "        pre_stimulus_rates = spike_trains[:, baseline_start_idx:baseline_end_idx].mean(axis=1)\n",
    "        post_stimulus_rates = spike_trains[:, analysis_start_idx:analysis_end_idx].mean(axis=1)\n",
    "\n",
    "        def permutation_test_func(x, y):\n",
    "            return x.mean() - y.mean()\n",
    "\n",
    "        permutation_result = permutation_test((pre_stimulus_rates, post_stimulus_rates),\n",
    "                                              statistic=permutation_test_func,\n",
    "                                              n_resamples=10000,\n",
    "                                              alternative='two-sided')\n",
    "\n",
    "        results[stim] = {\n",
    "            'response_magnitude': response_magnitude,\n",
    "            'z_scored_magnitude': z_scored_magnitude,\n",
    "            'permutation_p_value': permutation_result.pvalue\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import mat73\n",
    "class CCGHandler:\n",
    "    def __init__(self, eed_object, ccg_directory):\n",
    "        \"\"\"\n",
    "        Initializes the CCGHandler class with an object of ExtractEphysData and a directory for CCG files.\n",
    "\n",
    "        Parameters:\n",
    "            eed_object (ExtractEphysData): An instance of ExtractEphysData which provides\n",
    "                                           access to electrophysiological data methods and attributes.\n",
    "            \n",
    "            ccg_directory (str): The directory path where the CCG files are stored.\n",
    "        \"\"\"\n",
    "        self.eed = eed_object\n",
    "        self.ccg_directory = ccg_directory\n",
    "        self.ccg = {}\n",
    "        self.load_ccg_files()\n",
    "        self.ccg['MonoConnectionsTable'] = self.unpack_mono_connections()\n",
    "        self.integrate_ccgs() # Integrate CCG data into the MonoConnectionsTable DataFrame and delete the MonoConnCCGs data attribute\n",
    "    \n",
    "    def load_ccg_files(self):\n",
    "        \"\"\"\n",
    "        Loads all required .mat files from the specified CCG directory and sets them as attributes under the `ccg` dictionary.\n",
    "\n",
    "        Required Files:\n",
    "            MonoConnCCGs.mat - Contains CCG data for monosynaptic connections.\n",
    "            \n",
    "            MonoConnectionsTable.mat - Contains a struct MonoConnectionsTable converted into a struct by group,\n",
    "            and recording all connection data; accessed using the key 'nested_struct'.\n",
    "            \n",
    "            PointwiseABs.mat - Contains 95% pointwise acceptacne bands for CCG.\n",
    "            SimultaneousABs.mat - Contains 95% pointwise acceptacne bands for CCG.\n",
    "            \n",
    "            t.mat - Contains the time array for the x axis of the CCG.\n",
    "\n",
    "        The data from each file is accessed by stripping the '.mat' and accessing the corresponding key in the loaded dictionary, except for specific exceptions noted.\n",
    "        \"\"\"\n",
    "        required_files = [\n",
    "            'MonoConnCCGs.mat',\n",
    "            'MonoConnectionsTable.mat',\n",
    "            'PointwiseABs.mat',\n",
    "            'SimultaneousABs.mat',\n",
    "            't.mat'\n",
    "        ]\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(self.ccg_directory):\n",
    "            print(f\"CCG directory not found: {self.ccg_directory}\")\n",
    "            return\n",
    "\n",
    "        # Load each required .mat file from the directory\n",
    "        for file_name in required_files:\n",
    "            file_path = os.path.join(self.ccg_directory, file_name)\n",
    "            print(f\"Loading CCG data from: {file_path}\")\n",
    "            try:\n",
    "                # Load the file and extract the data using the base filename as the key\n",
    "                data = mat73.loadmat(file_path)\n",
    "                base_key = file_name.replace('.mat', '')\n",
    "                # Handle special case for 'MonoConnectionsTable'\n",
    "                if base_key == 'MonoConnectionsTable':\n",
    "                    self.ccg[base_key] = data['nested_struct']\n",
    "                elif base_key in data:\n",
    "                    self.ccg[base_key] = data[base_key]\n",
    "                else:\n",
    "                    print(f\"Expected key '{base_key}' not found in {file_name}\")\n",
    "                    self.ccg[base_key] = None\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                self.ccg[base_key] = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "                self.ccg[base_key] = None\n",
    "    \n",
    "    def unpack_mono_connections(self):\n",
    "        \"\"\"\n",
    "        Unpacks the MonoConnectionsTable and assembles the data into a comprehensive DataFrame.\n",
    "        This method iterates over each group and recording in the MonoConnectionsTable, which\n",
    "        is structured by group and recording name. Each entry represents connectivity data between\n",
    "        neuron pairs, including various metrics and identifiers.\n",
    "\n",
    "        The resulting DataFrame compiles all the connectivity data across different groups and recordings,\n",
    "        with scalar values extracted from lists for each metric, providing a holistic view of the monosynaptic connections analyzed.\n",
    "\n",
    "        Adjustments:\n",
    "        - Converts 'Significance' from [1.0, 0.0] to True/False.\n",
    "        - Maps 'StimResp_A' and 'StimResp_B' from 'nr', '-', '+' to 0, -1, 1 respectively for easier filtering.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame containing all the unpacked connectivity data with scalar values.\n",
    "        \"\"\"\n",
    "        # Initialize an empty list to collect data\n",
    "        all_data = []\n",
    "\n",
    "        # Mapping for stimulus response\n",
    "        stim_resp_mapping = {'nr': 0, '-': -1, '+': 1}\n",
    "\n",
    "        # Navigate through each group and recording\n",
    "        for groupname, recordings in self.ccg['MonoConnectionsTable'].items():\n",
    "            for recording, data in recordings.items():\n",
    "                for item in data:\n",
    "                    # Flatten the dictionary and add groupname and recording info\n",
    "                    item_data = {}\n",
    "                    for key, val in item.items():\n",
    "                        if isinstance(val, list) and len(val) == 1:\n",
    "                            if key == 'Significance':\n",
    "                                # Convert Significance to boolean\n",
    "                                item_data[key] = bool(val[0])\n",
    "                            elif key in ['StimResp_A', 'StimResp_B']:\n",
    "                                # Map StimResp_A and StimResp_B\n",
    "                                item_data[key] = stim_resp_mapping[val[0]]\n",
    "                            else:\n",
    "                                item_data[key] = val[0]\n",
    "                        else:\n",
    "                            item_data[key] = val\n",
    "                    item_data['groupname'] = groupname\n",
    "                    item_data['recording'] = recording\n",
    "                    all_data.append(item_data)\n",
    "        return pd.DataFrame(all_data)\n",
    "      \n",
    "    def integrate_ccgs(self):\n",
    "        \"\"\"\n",
    "        Integrates the CCG data from 'MonoConnCCGs' into the 'MonoConnectionsTable' DataFrame.\n",
    "        Each row in 'MonoConnectionsTable' corresponds to a column in 'MonoConnCCGs', which contains\n",
    "        the time points for the CCGs in 1ms bins. This method adds these CCGs as a new column 'ccgs' in\n",
    "        the DataFrame, ensuring each pair's CCGs are correctly aligned with the DataFrame rows.\n",
    "\n",
    "        Post-process:\n",
    "        - Deletes the original 'MonoConnCCGs' data from the 'ccg' attribute to save memory and avoid redundancy.\n",
    "        \"\"\"\n",
    "        # Retrieve the DataFrame and the CCG data\n",
    "        df = self.ccg['MonoConnectionsTable']\n",
    "        ccgs_data = self.ccg['MonoConnCCGs']\n",
    "\n",
    "        # Ensure that the rows of the DataFrame match the columns in the CCGs data\n",
    "        if df.shape[0] != ccgs_data.shape[1]:\n",
    "            raise ValueError(\"Mismatch between DataFrame rows and CCG data columns.\")\n",
    "\n",
    "        # Add the CCGs as a new column to the DataFrame\n",
    "        df['ccgs'] = [ccgs_data[:, i] for i in range(ccgs_data.shape[1])]\n",
    "\n",
    "        # Update the 'MonoConnectionsTable' with the new DataFrame\n",
    "        self.ccg['MonoConnectionsTable'] = df\n",
    "\n",
    "        # Remove the 'MonoConnCCGs' data\n",
    "        del self.ccg['MonoConnCCGs']\n",
    "        \n",
    "    def access_ccgs(self, groupname=None, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, limit=None):\n",
    "        \"\"\"\n",
    "        Accesses CCGs from 'MonoConnectionsTable' based on specified criteria.\n",
    "        Returns a filtered DataFrame containing CCGs that meet the specified criteria.\n",
    "\n",
    "        Parameters:\n",
    "            groupname (str or None): The group name to filter by; defaults to None, which includes all groups.\n",
    "            significance (bool): Filter connections by significance; defaults to True, meaning only significant connections are included.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs\n",
    "            EorI (list of str or None): Filter connections by a list of excitatory/inhibitory statuses; defaults to None, which includes all statuses (e.g 'E', 'I', 'NS', 'S').\n",
    "            connectiontype (str, list of str, or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS' or ['FS->RS', 'RS->FS']); defaults to None, which includes all types.\n",
    "            response (str or None): Filters connections based on responsiveness; 'responsive' for either StimResp_A being 1 or 'nonresponsive' for both being 0.\n",
    "            layers (list of str or None): Filter connections by layer (e.g., 'IG->L4', 'L4->L4'); defaults to None, which includes all layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include; defaults to 150.\n",
    "            threshold (bool): Filter connections by threshold; defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A filtered DataFrame containing CCGs that meet the specified criteria.\n",
    "        \"\"\"\n",
    "        df = self.ccg['MonoConnectionsTable']\n",
    "\n",
    "        # Filter by group name if specified\n",
    "        if groupname is not None:\n",
    "            df = df[df['groupname'] == groupname]\n",
    "\n",
    "        # Filter by significance\n",
    "        if significance:\n",
    "            df = df[df['Significance'] == True]\n",
    "\n",
    "        # Filter by it being sig based on 7*SD of peak relative to noise \n",
    "        if sd_sig: \n",
    "            df = df[df['sd_sig'] == True]\n",
    "\n",
    "        # Filter by EorI status if specified\n",
    "        if EorI is not None:\n",
    "            df = df[df['EorI'].isin(EorI)]\n",
    "\n",
    "        # Filter by connection type if specified\n",
    "        if connectiontype is not None:\n",
    "            if isinstance(connectiontype, list):\n",
    "                df = df[df['CellTypes'].isin(connectiontype)]\n",
    "            else:\n",
    "                df = df[df['CellTypes'] == connectiontype]\n",
    "\n",
    "        # Filter by response if specified\n",
    "        if response:\n",
    "            if response == 'responsive':\n",
    "                df = df[(df['StimResp_A'] == 1)]\n",
    "            elif response == 'nonresponsive':\n",
    "                df = df[(df['StimResp_A'] == 0)]\n",
    "\n",
    "        # Filter by layers if specified\n",
    "        if layers is not None:\n",
    "            df = df[df['layers'].isin(layers)]\n",
    "\n",
    "        # Filter by threshold if specified\n",
    "        if limit:\n",
    "            df = df[df['threshold'] == limit]\n",
    "\n",
    "        # Filter by minimum number of spike pairs\n",
    "        df = df[df['N_SpikePairs'] >= min_spike_pairs]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def plot_ccgs(self, groupname=None, significance=True, EorI=None, connectiontype=None):\n",
    "        \"\"\"\n",
    "        Assembles the CCGs for connections based on specified criteria and plots\n",
    "        individual CCG traces. Returns the DataFrame containing the filtered connections.\n",
    "        \n",
    "        Parameters:\n",
    "            groupname (str or None): The group name to filter by; defaults to None.\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses (e.g 'E', 'I', 'NS', 'S').\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "        \"\"\"\n",
    "        # Access the filtered data\n",
    "        df = self.access_ccgs(groupname=groupname, significance=significance, EorI=EorI, connectiontype=connectiontype)\n",
    "\n",
    "        # Retrieve the 'ccgs' column which contains the CCG data arrays\n",
    "        ccgs_data = df['ccgs'].tolist() # Convert the column to a list of arrays which represent the CCGs of each connection\n",
    "        #calculate the mean of the ccgs\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "\n",
    "        # Plotting each CCG\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        for ccg in ccgs_data:\n",
    "            ax.plot(time_points, ccg)\n",
    "        ax.set_title('CCGs Plot')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_mean_ccgs(self, groupname=None, significance=True, EorI=None, connectiontype=None):\n",
    "        \"\"\"\n",
    "        Plots mean CCGs for connections based on specified criteria.\n",
    "        \n",
    "        Parameters:\n",
    "            groupname (str or None): The group name to filter by; defaults to None.\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses (e.g 'E', 'I', 'NS', 'S').\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "        \"\"\"\n",
    "        # Access the filtered data\n",
    "        df = self.access_ccgs(groupname=groupname, significance=significance, EorI=EorI, connectiontype=connectiontype)\n",
    "\n",
    "        # Retrieve the 'ccgs' column which contains the CCG data arrays\n",
    "        ccgs_data = df['ccgs'].tolist() # Convert the column to a list of arrays which represent the CCGs of each connection\n",
    "        \n",
    "        #calculate the mean of the ccgs\n",
    "        mean_ccg = np.mean(ccgs_data, axis=0)\n",
    "        #calculte the SEM of the ccgs\n",
    "        sem_ccg = np.std(ccgs_data, axis=0) / np.sqrt(len(ccgs_data))\n",
    "\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "        \n",
    "\n",
    "        # Plotting the mean CCG\n",
    "        fig, ax = plt.subplots(figsize=(10, 6)) #plot the mean CCG and shade the SEM region\n",
    "        ax.plot(time_points, mean_ccg, color='b', label='Mean CCG')\n",
    "        ax.fill_between(time_points, mean_ccg-sem_ccg, mean_ccg+sem_ccg, color='b', alpha=0.2, label='SEM')\n",
    "        ax.set_title('Mean CCGs Plot')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_group_ccgs(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, xlim=None, front_group='CTZ',limit=None, directory=None, file_name=None):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Plots overlaid mean CCGs for the 'No_CTZ' and 'CTZ' groups with their SEM as shaded regions.\n",
    "        The SEM shading uses a lighter version of the group colors.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses.\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "            response (str or None): Filters connections based on responsiveness; 'responsive' for either StimResp_A being 1 or 'nonresponsive' for both being 0.\n",
    "            layers (list of str or None): Filter connections by layer (e.g., 'IG->L4', 'L4->L4'); defaults to None, which includes all layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include; defaults to 150.\n",
    "            xlim (tuple of int or None): The x-axis limits for the plot; defaults to None.\n",
    "            \n",
    "        \"\"\"\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        # Define lighter versions of the colors for SEM shading\n",
    "        sem_colors = {\n",
    "            'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "            'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5)) #figsize is (width, height)\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            ccgs_data = df['ccgs'].tolist()\n",
    "\n",
    "            if len(ccgs_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "            \n",
    "            mean_ccg = np.mean(ccgs_data, axis=0)\n",
    "            sem_ccg = np.std(ccgs_data, axis=0) / np.sqrt(len(ccgs_data))\n",
    "\n",
    "            # Plotting the mean CCG\n",
    "            ax.plot(time_points, mean_ccg, color=group_colors[group], label=f'Mean CCG {group}', linewidth=2)\n",
    "            ax.fill_between(time_points, mean_ccg - sem_ccg, mean_ccg + sem_ccg, color=sem_colors[group], label=f'SEM {group}')\n",
    "            \n",
    "\n",
    "        ax.set_title('Comparison of Mean CCGs by Group')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "        \n",
    "        #change the x-axis limits if specified\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #hide legend\n",
    "        ax.legend().set_visible(False)\n",
    "        \n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def prepare_for_boxplot_ccg(self, significance=True, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, xlim=None):\n",
    "        \"\"\"\n",
    "        Organizes data into a DataFrame suitable for plotting boxplots by unique 'CellTypes' and group.\n",
    "        Uses filtered data from 'access_ccgs' method based on provided parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses.\n",
    "            connectiontype (str or None): Filter connections by type; defaults to None, which includes all types.\n",
    "        \n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with columns for the value to plot, 'CellTypes', and 'Group'.\n",
    "        \"\"\"\n",
    "        boxplot_data = []\n",
    "\n",
    "        # Access filtered data using the 'access_ccgs' method\n",
    "        df = self.access_ccgs(significance=significance, EorI=EorI, connectiontype=connectiontype)\n",
    "\n",
    "        if not df.empty:\n",
    "            unique_cell_types = df['CellTypes'].unique()  # Get unique 'CellTypes'\n",
    "            for cell_type in unique_cell_types:\n",
    "                # Filter DataFrame for each cell type\n",
    "                cell_type_df = df[df['CellTypes'] == cell_type]\n",
    "                \n",
    "                # Collect data for each cell type entry\n",
    "                for index, row in cell_type_df.iterrows():\n",
    "                    boxplot_data.append({\n",
    "                        'Value': row['ccgs'],  # Adjust this key as needed\n",
    "                        'CellTypes': cell_type,\n",
    "                        'Group': row['group']  # Assuming 'group' column exists\n",
    "                    })\n",
    "\n",
    "        # Convert list of data to DataFrame\n",
    "        boxplot_df = pd.DataFrame(boxplot_data)\n",
    "        return boxplot_df\n",
    "\n",
    "    def plot_group_ccgs_traces(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, xlim=None, front_group='CTZ', limit=None):\n",
    "        \"\"\"\n",
    "        Plots overlaid mean CCGs for the 'No_CTZ' and 'CTZ' groups along with individual CCG traces.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance; defaults to True.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs\n",
    "            EorI (str or None): Filter connections by excitatory/inhibitory status; defaults to None, which includes all statuses.\n",
    "            connectiontype (str or None): Filter connections by type (e.g., 'FS->RS', 'RS->FS'); defaults to None, which includes all types.\n",
    "            response (str or None): Filters connections based on responsiveness; 'responsive' for either StimResp_A being 1 or 'nonresponsive' for both being 0.\n",
    "            layers (list of str or None): Filter connections by layer (e.g., 'IG->L4', 'L4->L4'); defaults to None, which includes all layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include; defaults to 150.\n",
    "            xlim (tuple of int or None): The x-axis limits for the plot; defaults to None.\n",
    "        \"\"\"\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "        time_points = self.ccg['t']  # assuming this is the correct key for time points array\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            ccgs_data = df['ccgs'].tolist()\n",
    "\n",
    "            if len(ccgs_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "            \n",
    "            mean_ccg = np.mean(ccgs_data, axis=0)\n",
    "\n",
    "            # Plotting the mean CCG\n",
    "            ax.plot(time_points, mean_ccg, color=group_colors[group], label=f'Mean CCG {group}', linewidth=3)\n",
    "\n",
    "            # Plotting individual traces\n",
    "            for trace in ccgs_data:\n",
    "                ax.plot(time_points, trace, color=group_colors[group], alpha=0.3, linewidth=0.5)  # Lower alpha for individual traces\n",
    "\n",
    "        ax.set_title('Comparison of Mean CCGs by Group with Individual Traces')\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Counts')\n",
    "\n",
    "        # Change the x-axis limits if specified\n",
    "        if xlim:\n",
    "            ax.set_xlim(xlim)\n",
    "\n",
    "        # Hide legend\n",
    "        ax.legend().set_visible(False)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_box_and_strip_ccg(self, groups=None, cell_types=None, show_outliers=True, hue_order=None):\n",
    "        \"\"\"\n",
    "        Plots boxplots and stripplots for specified groups and cell types, with color adjustments made directly in the plotting calls.\n",
    "\n",
    "        Args:\n",
    "            groups (list of str, optional): List of groups to include in the plot.\n",
    "            cell_types (list of str, optional): List of cell types to include in the plot.\n",
    "            show_outliers (bool, optional): Whether to show outliers.\n",
    "            hue_order (list, optional): Order of the hue levels.\n",
    "        \"\"\"\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: mcolors.to_rgba(v, alpha=0.5) for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,\n",
    "            'marker': 'o' if show_outliers else 'd',\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Prepare data for boxplot\n",
    "        boxplot_df = self.prepare_for_boxplot_ccg()\n",
    "\n",
    "        # Filter by specified groups and cell types\n",
    "        if groups:\n",
    "            boxplot_df = boxplot_df[boxplot_df['Group'].isin(groups)]\n",
    "        if cell_types:\n",
    "            boxplot_df = boxplot_df[boxplot_df['CellTypes'].isin(cell_types)]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.boxplot(data=boxplot_df, x='CellTypes', y='Value', hue='Group', **boxplot_kwargs)\n",
    "\n",
    "        # Manually set the facecolor for boxplot\n",
    "        for i, artist in enumerate(ax.artists):\n",
    "            col = lightened_colors[ax.get_legend_handles_labels()[1][i // len(cell_types)]]\n",
    "            artist.set_facecolor(col)\n",
    "\n",
    "        # Add stripplot on top of boxplot for raw data visualization\n",
    "        sns.stripplot(data=boxplot_df, x='CellTypes', y='Value', hue='Group', **stripplot_kwargs)\n",
    "\n",
    "        # Enhance the plot\n",
    "        plt.title('Comparison of Values Across Groups and Cell Types')\n",
    "        plt.ylabel('Value')\n",
    "        plt.xlabel('Cell Type')\n",
    "        ax.legend(title='Group')\n",
    "        plt.show()\n",
    "        \n",
    "    def is_significant_ccg(self, sd_multiplier=7):\n",
    "        # Access the time array\n",
    "        t = self.ccg['t']\n",
    "        \n",
    "        # Create an empty list to store significance results temporarily\n",
    "        significance_results = []\n",
    "\n",
    "        # Iterate over all CCG entries\n",
    "        for ccg_values in self.ccg['MonoConnectionsTable']['ccgs']:\n",
    "            # Define the noise distribution from the flanks of the CCG\n",
    "            noise_mask = (np.abs(t) >= 50) & (np.abs(t) <= 100)\n",
    "            noise_values = ccg_values[noise_mask]\n",
    "\n",
    "            # Calculate the mean and standard deviation of the noise distribution\n",
    "            noise_mean = np.mean(noise_values)\n",
    "            noise_std = np.std(noise_values)\n",
    "\n",
    "            # Define the peak detection region within 10ms of zero\n",
    "            peak_mask = (t >= -10) & (t <= 10)\n",
    "            peak_values = ccg_values[peak_mask]\n",
    "\n",
    "            # Find the peak value in the defined region\n",
    "            peak = np.max(peak_values)\n",
    "\n",
    "            # Check if the peak is more than the specified multiplier of standard deviations above the noise mean\n",
    "            significant = peak > noise_mean + sd_multiplier * noise_std\n",
    "\n",
    "            # Append the significance result\n",
    "            significance_results.append(significant)\n",
    "\n",
    "        # Safely add the 'sd_sig' column to the DataFrame\n",
    "        self.ccg['MonoConnectionsTable']['sd_sig'] = significance_results\n",
    "\n",
    "        return self.ccg['MonoConnectionsTable']['sd_sig']\n",
    "    \n",
    "    def plot_group_excess_synchrony(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150,limit=None, directory=None, file_name=None, front_group='CTZ'):\n",
    "\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        # Define lighter versions of the colors for SEM shading\n",
    "        sem_colors = {\n",
    "            'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "            'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            excess_synchrony_data = df['ExcessSynchrony'].tolist()\n",
    "\n",
    "            if len(excess_synchrony_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "\n",
    "            mean_excess_synchrony = np.mean(excess_synchrony_data)\n",
    "            sem_excess_synchrony = np.std(excess_synchrony_data) / np.sqrt(len(excess_synchrony_data))\n",
    "\n",
    "            # Plotting the mean excess synchrony\n",
    "            ax.bar(group, mean_excess_synchrony, yerr=sem_excess_synchrony, color=group_colors[group], alpha=0.7, label=f'{group}')\n",
    "\n",
    "        ax.set_title('Comparison of Mean Excess Synchrony by Group')\n",
    "        ax.set_ylabel('Excess Synchrony')\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_group_excess_synchrony(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150):\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Summary statistics\n",
    "        no_ctz_summary = no_ctz_excess_synchrony.describe()\n",
    "        ctz_summary = ctz_excess_synchrony.describe()\n",
    "\n",
    "        # Perform Mann-Whitney U test\n",
    "        stat, p_value = mannwhitneyu(no_ctz_excess_synchrony, ctz_excess_synchrony)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        # Print Mann-Whitney U test results\n",
    "        print(\"\\nMann-Whitney U Test Results:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        # Return results for further use if needed\n",
    "        return {\n",
    "            'no_ctz_excess_synchrony': no_ctz_excess_synchrony, #returns the filtered data for No_CTZ group\n",
    "            'ctz_excess_synchrony': ctz_excess_synchrony, #returns the filtered data for CTZ group\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'mannwhitneyu_stat': stat,\n",
    "            'mannwhitneyu_p_value': p_value\n",
    "        }\n",
    "\n",
    "    def compare_group_excess_synchrony_welch(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150):\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        no_ctz_summary = no_ctz_excess_synchrony.describe()\n",
    "        ctz_summary = ctz_excess_synchrony.describe()\n",
    "\n",
    "        stat, p_value = ttest_ind(no_ctz_excess_synchrony, ctz_excess_synchrony, equal_var=False)\n",
    "\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        print(\"\\nWelch's t-test Results:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'ttest_stat': stat,\n",
    "            'ttest_p_value': p_value\n",
    "        }\n",
    "\n",
    "    def compare_group_excess_synchrony_ks(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150):\n",
    "        from scipy.stats import ks_2samp\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        no_ctz_summary = no_ctz_excess_synchrony.describe()\n",
    "        ctz_summary = ctz_excess_synchrony.describe()\n",
    "\n",
    "        stat, p_value = ks_2samp(no_ctz_excess_synchrony, ctz_excess_synchrony)\n",
    "\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        print(\"\\nKolmogorov-Smirnov Test Results:\")\n",
    "        print(f\"Statistic: {stat}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'ks_stat': stat,\n",
    "            'ks_p_value': p_value\n",
    "        }\n",
    "        \n",
    "    def permutation_test(self, data1, data2, num_permutations=10000):\n",
    "        # Calculate the observed difference in means\n",
    "        observed_diff = np.mean(data1) - np.mean(data2)\n",
    "        \n",
    "        # Combine the data\n",
    "        combined_data = np.concatenate([data1, data2])\n",
    "        \n",
    "        # Perform permutations\n",
    "        perm_diffs = np.zeros(num_permutations)\n",
    "        for i in range(num_permutations):\n",
    "            np.random.shuffle(combined_data)\n",
    "            perm_data1 = combined_data[:len(data1)]\n",
    "            perm_data2 = combined_data[len(data1):]\n",
    "            perm_diffs[i] = np.mean(perm_data1) - np.mean(perm_data2)\n",
    "        \n",
    "        # Calculate the p-value\n",
    "        p_value = np.sum(np.abs(perm_diffs) >= np.abs(observed_diff)) / num_permutations\n",
    "        \n",
    "        return observed_diff, p_value, perm_diffs\n",
    "\n",
    "    def compare_group_excess_synchrony_permutation(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, num_permutations=10000, limit=None, save_path=None):\n",
    "        \"\"\"\n",
    "        Compares group excess synchrony between No_CTZ and CTZ groups using a permutation test,\n",
    "        and saves the results as a DataFrame. Handles cases where one group may have no data.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance.\n",
    "            sd_sig (bool): Filter connections based on SD significance.\n",
    "            EorI (list of str): Filter connections by excitatory/inhibitory status.\n",
    "            connectiontype (str): Filter connections by type.\n",
    "            response (str): Filter connections by responsiveness.\n",
    "            layers (list of str): Filter connections by layers.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs.\n",
    "            num_permutations (int): Number of permutations for the permutation test.\n",
    "            limit (bool): Filter connections by threshold.\n",
    "            save_path (str): Path to save the results as a CSV file.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame containing summary statistics and permutation test results.\n",
    "        \"\"\"\n",
    "\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        # Check if either group has no data\n",
    "        if len(no_ctz_excess_synchrony) == 0:\n",
    "            no_ctz_summary = pd.Series([None] * 8, index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "        else:\n",
    "            no_ctz_summary = pd.Series(no_ctz_excess_synchrony).describe()\n",
    "\n",
    "        if len(ctz_excess_synchrony) == 0:\n",
    "            ctz_summary = pd.Series([None] * 8, index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])\n",
    "        else:\n",
    "            ctz_summary = pd.Series(ctz_excess_synchrony).describe()\n",
    "\n",
    "        # Perform the permutation test only if both groups have data\n",
    "        if len(no_ctz_excess_synchrony) > 0 and len(ctz_excess_synchrony) > 0:\n",
    "            observed_diff, p_value, perm_diffs = self.permutation_test(no_ctz_excess_synchrony, ctz_excess_synchrony, num_permutations)\n",
    "        else:\n",
    "            observed_diff = None\n",
    "            p_value = None\n",
    "            perm_diffs = []\n",
    "\n",
    "        # Ensure all lists are the same length or adjust appropriately\n",
    "        stats_labels = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
    "        no_ctz_values = no_ctz_summary.values\n",
    "        ctz_values = ctz_summary.values\n",
    "\n",
    "        # Create a DataFrame to store the summary statistics\n",
    "        results_df = pd.DataFrame({\n",
    "            'Statistic': stats_labels,\n",
    "            'No_CTZ': no_ctz_values,\n",
    "            'CTZ': ctz_values\n",
    "        })\n",
    "\n",
    "        # Add the permutation test results to the DataFrame\n",
    "        perm_results_df = pd.DataFrame({\n",
    "            'Statistic': ['Observed Difference', 'P-Value'],\n",
    "            'No_CTZ': [observed_diff, None],  # Observed difference applies to both groups, but we'll store it under No_CTZ\n",
    "            'CTZ': [None, p_value]  # P-Value is only meaningful for the comparison\n",
    "        })\n",
    "\n",
    "        # Combine the results into a single DataFrame\n",
    "        results_df = pd.concat([results_df, perm_results_df], ignore_index=True)\n",
    "\n",
    "        # Save the DataFrame to a CSV file if a save path is provided\n",
    "        if save_path:\n",
    "            results_df.to_csv(save_path, index=False)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return results_df\n",
    "\n",
    "    def compare_group_excess_synchrony_permutation_quartile(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, num_permutations=10000, quartile='top', limit=None):\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        ctz_excess_synchrony = ctz_df['ExcessSynchrony'].apply(pd.to_numeric, errors='coerce').dropna()\n",
    "\n",
    "        # Calculate quartiles\n",
    "        no_ctz_quartiles = no_ctz_excess_synchrony.quantile([0.25, 0.5, 0.75])\n",
    "        ctz_quartiles = ctz_excess_synchrony.quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "        # Filter by specified quartile\n",
    "        if quartile == 'top':\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony[no_ctz_excess_synchrony >= no_ctz_quartiles[0.75]]\n",
    "            ctz_filtered = ctz_excess_synchrony[ctz_excess_synchrony >= ctz_quartiles[0.75]]\n",
    "        elif quartile == 'bottom':\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony[no_ctz_excess_synchrony <= no_ctz_quartiles[0.25]]\n",
    "            ctz_filtered = ctz_excess_synchrony[ctz_excess_synchrony <= ctz_quartiles[0.25]]\n",
    "        elif quartile == 'middle':\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony[(no_ctz_excess_synchrony > no_ctz_quartiles[0.25]) & (no_ctz_excess_synchrony < no_ctz_quartiles[0.75])]\n",
    "            ctz_filtered = ctz_excess_synchrony[(ctz_excess_synchrony > ctz_quartiles[0.25]) & (ctz_excess_synchrony < ctz_quartiles[0.75])]\n",
    "        else:\n",
    "            no_ctz_filtered = no_ctz_excess_synchrony\n",
    "            ctz_filtered = ctz_excess_synchrony\n",
    "\n",
    "        no_ctz_summary = no_ctz_filtered.describe()\n",
    "        ctz_summary = ctz_filtered.describe()\n",
    "\n",
    "        # Perform the permutation test\n",
    "        observed_diff, p_value, perm_diffs = self.permutation_test(no_ctz_filtered, ctz_filtered, num_permutations)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        # Print Permutation Test results\n",
    "        print(\"\\nPermutation Test Results:\")\n",
    "        print(f\"Observed Difference: {observed_diff}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        # Return results for further use if needed\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'observed_diff': observed_diff,\n",
    "            'perm_diffs': perm_diffs,\n",
    "            'perm_p_value': p_value\n",
    "        }\n",
    "\n",
    "    def run_permutation_test_between_groups(self, band_name, condition, quantile_low=None, quantile_high=None, cell_type=None, cell_type2=None, n_permutations=10000):\n",
    "        \"\"\"\n",
    "        Runs a permutation test to compare the PPC2 values between groups for a specified frequency band.\n",
    "\n",
    "        Parameters:\n",
    "        band_name (str): The name of the frequency band to filter by (e.g., 'alpha', 'beta').\n",
    "        condition (bool): The condition to filter by (True for evoked, False for spontaneous).\n",
    "        quantile_low (float): The lower quantile threshold for filtering.\n",
    "        quantile_high (float): The upper quantile threshold for filtering.\n",
    "        cell_type (str): The cell type to filter by.\n",
    "        cell_type2 (int): The secondary cell type to filter by.\n",
    "        n_permutations (int): The number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        float: The p-value from the permutation test.\n",
    "        \"\"\"\n",
    "        filtered_df = self.filter_by_frequency_band(band_name)\n",
    "\n",
    "        # Filter by condition\n",
    "        filtered_df = filtered_df[filtered_df['Condition'] == condition]\n",
    "\n",
    "        # Filter by cell type if specified\n",
    "        if cell_type:\n",
    "            filtered_df = filtered_df[filtered_df['CellType'] == cell_type]\n",
    "        if cell_type2 is not None:\n",
    "            filtered_df = filtered_df[filtered_df['CellType2'] == cell_type2]\n",
    "\n",
    "        # Apply quantile filtering if specified\n",
    "        if quantile_low is not None and quantile_high is not None:\n",
    "            filtered_df = self.filter_data_by_quantile(filtered_df, quantile_low, quantile_high)\n",
    "\n",
    "        # Aggregate PPC2 values per CellID and frequency band midpoint\n",
    "        aggregated_df = filtered_df.groupby(['CellID', 'Group']).agg({'PPC2': 'mean'}).reset_index()\n",
    "\n",
    "        # Separate the data by groups\n",
    "        df_ctz = aggregated_df[aggregated_df['Group'] == 'CTZ']['PPC2']\n",
    "        df_no_ctz = aggregated_df[aggregated_df['Group'] == 'No_CTZ']['PPC2']\n",
    "\n",
    "        # Calculate observed difference in means\n",
    "        observed_diff = np.mean(df_ctz) - np.mean(df_no_ctz)\n",
    "\n",
    "        # Concatenate the data for permutation\n",
    "        all_data = np.concatenate([df_ctz, df_no_ctz])\n",
    "        n_ctz = len(df_ctz)\n",
    "        n_no_ctz = len(df_no_ctz)\n",
    "\n",
    "        # Perform permutations\n",
    "        perm_diffs = np.zeros(n_permutations)\n",
    "        for i in range(n_permutations):\n",
    "            np.random.shuffle(all_data)\n",
    "            perm_ctz = all_data[:n_ctz]\n",
    "            perm_no_ctz = all_data[n_ctz:]\n",
    "            perm_diffs[i] = np.mean(perm_ctz) - np.mean(perm_no_ctz)\n",
    "\n",
    "        # Calculate p-value\n",
    "        p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n",
    "        return p_value\n",
    "    \n",
    "    def filter_by_ccg_threshold(self, time_range, threshold_range):\n",
    "        \"\"\"\n",
    "        Filters CCGs based on whether their values exceed a given threshold within a specified time range.\n",
    "        Adds a new column 'threshold' indicating True or False based on the condition.\n",
    "\n",
    "        Parameters:\n",
    "            time_range (int or tuple): An integer specifying a single time point, or a tuple specifying the start and end of the time range to check (e.g., (-5, 0) or (0, 1)).\n",
    "            threshold_range (tuple): A tuple specifying the lower and upper bounds of the threshold (e.g., (0, 7)).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with a new 'threshold' column.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure 'MonoConnectionsTable' exists in 'self.ccg'\n",
    "        if 'MonoConnectionsTable' not in self.ccg:\n",
    "            raise KeyError(\"'MonoConnectionsTable' not found in 'self.ccg'\")\n",
    "\n",
    "        # Extract the MonoConnectionsTable DataFrame\n",
    "        df = self.ccg['MonoConnectionsTable'].copy()\n",
    "\n",
    "        # Ensure 'ccgs' and 't' exist in 'self.ccg'\n",
    "        if 'ccgs' not in df or 't' not in self.ccg:\n",
    "            raise KeyError(\"'ccgs' or 't' not found in 'self.ccg'\")\n",
    "\n",
    "        # Extract time points and CCG values\n",
    "        time_points = self.ccg['t']\n",
    "        ccgs = df['ccgs']\n",
    "\n",
    "        # Convert time points to integers for ease of indexing\n",
    "        time_points = time_points.astype(int)\n",
    "\n",
    "        # Determine the time range\n",
    "        if isinstance(time_range, int):\n",
    "            start_time = end_time = time_range\n",
    "        else:\n",
    "            start_time, end_time = time_range\n",
    "\n",
    "        # Find the indices of the time range\n",
    "        start_idx = (time_points >= start_time).argmax()\n",
    "        end_idx = (time_points <= end_time).argmin() + 1\n",
    "\n",
    "        # Ensure the indices are valid\n",
    "        if start_idx > end_idx:\n",
    "            raise ValueError(\"Invalid time range specified\")\n",
    "\n",
    "        # Initialize the 'threshold' column with False\n",
    "        df['threshold'] = False\n",
    "\n",
    "        # Unpack the threshold range\n",
    "        lower_threshold, upper_threshold = threshold_range\n",
    "\n",
    "        # Check CCG values within the specified time and threshold ranges\n",
    "        for index, row in df.iterrows():\n",
    "            ccg_values = ccgs.iloc[index][start_idx:end_idx]\n",
    "            if any((ccg_values > lower_threshold) & (ccg_values < upper_threshold)):\n",
    "                df.at[index, 'threshold'] = True\n",
    "\n",
    "        # Update the MonoConnectionsTable in self.ccg\n",
    "        self.ccg['MonoConnectionsTable'] = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def nullify_specific_rows(self, indices):\n",
    "        \"\"\"\n",
    "        Sets all values of specific rows in 'MonoConnectionsTable' to NaN based on the provided indices.\n",
    "        \n",
    "        Parameters:\n",
    "            indices (list): List of indices of the rows to be nullified.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with the specified rows set to NaN.\n",
    "        \"\"\"\n",
    "        # Ensure 'MonoConnectionsTable' exists in 'self.ccg'\n",
    "        if 'MonoConnectionsTable' not in self.ccg:\n",
    "            raise KeyError(\"'MonoConnectionsTable' not found in 'self.ccg'\")\n",
    "\n",
    "        # Extract the MonoConnectionsTable DataFrame\n",
    "        df = self.ccg['MonoConnectionsTable']\n",
    "\n",
    "        # Check if indices are valid\n",
    "        invalid_indices = [i for i in indices if i not in df.index]\n",
    "        if invalid_indices:\n",
    "            raise KeyError(f\"Invalid indices: {invalid_indices}\")\n",
    "\n",
    "        # Set specified rows to NaN\n",
    "        df.loc[indices] = float('nan')\n",
    "\n",
    "        # Update the MonoConnectionsTable in self.ccg\n",
    "        self.ccg['MonoConnectionsTable'] = df\n",
    "\n",
    "        return df\n",
    "\n",
    "    def plot_group_excess_synchrony_with_dots(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, limit=None, directory=None, file_name=None, front_group='CTZ'):\n",
    "        \"\"\"\n",
    "        Plots the mean excess synchrony by group with SEM as error bars and overlays the underlying data as color-matched dots.\n",
    "\n",
    "        Parameters:\n",
    "            significance (bool): Filter connections by significance.\n",
    "            sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs.\n",
    "            EorI (list of str): Filter connections by excitatory/inhibitory status.\n",
    "            connectiontype (str): Filter connections by type (e.g., 'FS->RS').\n",
    "            response (str): Filter connections based on responsiveness.\n",
    "            layers (list of str): Filter connections by layer.\n",
    "            min_spike_pairs (int): Minimum number of spike pairs to include.\n",
    "            limit (int): Limit the number of connections to include.\n",
    "            directory (str): Directory to save the plot.\n",
    "            file_name (str): File name to save the plot.\n",
    "            front_group (str): Group to be plotted in front.\n",
    "        \"\"\"\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',  # Grey color\n",
    "            'CTZ': '#5a00c2'  # Purple color\n",
    "        }\n",
    "\n",
    "        # Define lighter versions of the colors for SEM shading\n",
    "        sem_colors = {\n",
    "            'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "            'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "        groups = ['No_CTZ', 'CTZ']\n",
    "        # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "        groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "        for group in groups:\n",
    "            df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "            excess_synchrony_data = df['ExcessSynchrony'].tolist()\n",
    "\n",
    "            if len(excess_synchrony_data) == 0:\n",
    "                continue  # Skip if no data to plot\n",
    "\n",
    "            mean_excess_synchrony = np.mean(excess_synchrony_data)\n",
    "            sem_excess_synchrony = np.std(excess_synchrony_data) / np.sqrt(len(excess_synchrony_data))\n",
    "\n",
    "            # Plotting the mean excess synchrony\n",
    "            ax.bar(group, mean_excess_synchrony, yerr=sem_excess_synchrony, color=group_colors[group], alpha=0.3, label=f'{group}')\n",
    "            \n",
    "            # Overlaying the underlying data as dots\n",
    "            x_positions = np.full(len(excess_synchrony_data), group)  # x-positions are the same for each group\n",
    "            ax.scatter(x_positions, excess_synchrony_data, color=group_colors[group], alpha=0.7, edgecolor=None)\n",
    "\n",
    "        ax.set_title('Comparison of Mean Excess Synchrony by Group')\n",
    "        ax.set_ylabel('Excess Synchrony')\n",
    "        ax.legend()\n",
    "\n",
    "        # Prompt user for directory and file name if not provided\n",
    "        if directory is None:\n",
    "            directory = input(\"Please enter the directory to save the plot: \")\n",
    "        if file_name is None:\n",
    "            file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "        # Create directory if it does not exist\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Save the figure as an SVG file in the specified directory\n",
    "        file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "        plt.savefig(file_path, format='svg', transparent=True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_group_DCW_with_dots(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, limit=None, directory=None, file_name=None, front_group='CTZ'):\n",
    "            \"\"\"\n",
    "            Plots the mean excess synchrony by group with SEM as error bars and overlays the underlying data as color-matched dots.\n",
    "\n",
    "            Parameters:\n",
    "                significance (bool): Filter connections by significance.\n",
    "                sd_sig (bool): Filter connection based on 7*SD based on peak signal relative to noise of CCGs.\n",
    "                EorI (list of str): Filter connections by excitatory/inhibitory status.\n",
    "                connectiontype (str): Filter connections by type (e.g., 'FS->RS').\n",
    "                response (str): Filter connections based on responsiveness.\n",
    "                layers (list of str): Filter connections by layer.\n",
    "                min_spike_pairs (int): Minimum number of spike pairs to include.\n",
    "                limit (int): Limit the number of connections to include.\n",
    "                directory (str): Directory to save the plot.\n",
    "                file_name (str): File name to save the plot.\n",
    "                front_group (str): Group to be plotted in front.\n",
    "            \"\"\"\n",
    "            group_colors = {\n",
    "                'No_CTZ': '#797979',  # Grey color\n",
    "                'CTZ': '#5a00c2'  # Purple color\n",
    "            }\n",
    "\n",
    "            # Define lighter versions of the colors for SEM shading\n",
    "            sem_colors = {\n",
    "                'No_CTZ': mcolors.to_rgba(group_colors['No_CTZ'], alpha=0.3),\n",
    "                'CTZ': mcolors.to_rgba(group_colors['CTZ'], alpha=0.3)\n",
    "            }\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(5, 5))  # figsize is (width, height)\n",
    "            groups = ['No_CTZ', 'CTZ']\n",
    "            # Ensure the front group is plotted last by sorting the list based on the front_group parameter\n",
    "            groups = sorted(groups, key=lambda x: x == front_group)\n",
    "\n",
    "            for group in groups:\n",
    "                df = self.access_ccgs(groupname=group, significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "                excess_synchrony_data = df['DCW'].tolist()\n",
    "\n",
    "                if len(excess_synchrony_data) == 0:\n",
    "                    continue  # Skip if no data to plot\n",
    "\n",
    "                mean_excess_synchrony = np.mean(excess_synchrony_data)\n",
    "                sem_excess_synchrony = np.std(excess_synchrony_data) / np.sqrt(len(excess_synchrony_data))\n",
    "\n",
    "                # Plotting the mean excess synchrony\n",
    "                ax.bar(group, mean_excess_synchrony, yerr=sem_excess_synchrony, color=group_colors[group], alpha=0.3, label=f'{group}')\n",
    "                \n",
    "                # Overlaying the underlying data as dots\n",
    "                x_positions = np.full(len(excess_synchrony_data), group)  # x-positions are the same for each group\n",
    "                ax.scatter(x_positions, excess_synchrony_data, color=group_colors[group], alpha=0.7, edgecolor=None)\n",
    "\n",
    "            ax.set_title('Comparison of DCW by Group')\n",
    "            ax.set_ylabel('DCW')\n",
    "            ax.legend()\n",
    "\n",
    "            # Prompt user for directory and file name if not provided\n",
    "            if directory is None:\n",
    "                directory = input(\"Please enter the directory to save the plot: \")\n",
    "            if file_name is None:\n",
    "                file_name = input(\"Please enter the file name to save the plot: \")\n",
    "\n",
    "            # Create directory if it does not exist\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "            # Save the figure as an SVG file in the specified directory\n",
    "            file_path = os.path.join(directory, f'{file_name}.svg')\n",
    "            plt.savefig(file_path, format='svg')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    def compare_group_DCW_permutation(self, significance=True, sd_sig=None, EorI=None, connectiontype=None, response=None, layers=None, min_spike_pairs=150, num_permutations=10000, limit=None):\n",
    "        # Filter data for No_CTZ group\n",
    "        no_ctz_df = self.access_ccgs(groupname='No_CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        no_ctz_excess_synchrony = no_ctz_df['DCW'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        # Filter data for CTZ group\n",
    "        ctz_df = self.access_ccgs(groupname='CTZ', significance=significance, sd_sig=sd_sig, EorI=EorI, connectiontype=connectiontype, response=response, layers=layers, min_spike_pairs=min_spike_pairs, limit=limit)\n",
    "        ctz_excess_synchrony = ctz_df['DCW'].apply(pd.to_numeric, errors='coerce').dropna().values\n",
    "\n",
    "        no_ctz_summary = pd.Series(no_ctz_excess_synchrony).describe()\n",
    "        ctz_summary = pd.Series(ctz_excess_synchrony).describe()\n",
    "\n",
    "        # Perform the permutation test\n",
    "        observed_diff, p_value, perm_diffs = self.permutation_test(no_ctz_excess_synchrony, ctz_excess_synchrony, num_permutations)\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"No_CTZ Summary Statistics:\")\n",
    "        print(no_ctz_summary)\n",
    "        print(\"\\nCTZ Summary Statistics:\")\n",
    "        print(ctz_summary)\n",
    "\n",
    "        # Print Permutation Test results\n",
    "        print(\"\\nPermutation Test Results:\")\n",
    "        print(f\"Observed Difference: {observed_diff}\")\n",
    "        print(f\"P-Value: {p_value}\")\n",
    "\n",
    "        # Return results for further use if needed\n",
    "        return {\n",
    "            'no_ctz_summary': no_ctz_summary,\n",
    "            'ctz_summary': ctz_summary,\n",
    "            'observed_diff': observed_diff,\n",
    "            'perm_diffs': perm_diffs,\n",
    "            'perm_p_value': p_value\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mat73\n",
    "import os\n",
    "\n",
    "class PhaseDataHandler:\n",
    "    \n",
    "    FREQUENCY_BANDS = {\n",
    "        'alpha': (8, 15),\n",
    "        'beta': (15, 30),\n",
    "        'gamma': (30, 55),\n",
    "        'high_gamma': (66, 120)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, eed_object, phase_directory):\n",
    "        self.eed = eed_object\n",
    "        self.phase_directory = phase_directory\n",
    "        self.phase = {}\n",
    "        self.load_phase_files()\n",
    "        self.unpack_phase_data()  # This will now directly update self.phase\n",
    "    \n",
    "    def load_phase_files(self):\n",
    "        \"\"\"\n",
    "        Loads all required .mat files from the specified CCG directory and sets them as attributes under the `ccg` dictionary.\n",
    "\n",
    "        Required Files:\n",
    "            \n",
    "        The data from each file is accessed by stripping the '.mat' and accessing the corresponding key in the loaded dictionary, except for specific exceptions noted.\n",
    "        \"\"\"\n",
    "        required_files = [\n",
    "            'phase_data_out.mat'\n",
    "        ]\n",
    "\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(self.phase_directory):\n",
    "            print(f\"Phase directory not found: {self.phase_directory}\")\n",
    "            return\n",
    "\n",
    "        # Load each required .mat file from the directory\n",
    "        for file_name in required_files:\n",
    "            file_path = os.path.join(self.phase_directory, file_name)\n",
    "            print(f\"Loading Phase data from: {file_path}\")\n",
    "            try:\n",
    "                # Load the file and extract the data using the base filename as the key\n",
    "                data = mat73.loadmat(file_path)\n",
    "                base_key = file_name.replace('.mat', '')\n",
    "                # Handle special case for 'MonoConnectionsTable'\n",
    "                if base_key == 'phase_data_out':\n",
    "                    self.phase[base_key] = data['phase_data_out']\n",
    "                elif base_key in data:\n",
    "                    self.phase[base_key] = data[base_key]\n",
    "                else:\n",
    "                    print(f\"Expected key '{base_key}' not found in {file_name}\")\n",
    "                    self.phase[base_key] = None\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                self.phase[base_key] = None\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "                self.phase[base_key] = None\n",
    "                \n",
    "    def unpack_phase_data(self):\n",
    "        frames = []\n",
    "        for groupname, group_data in self.phase['phase_data_out'].items():\n",
    "            for recording, records in group_data.items():\n",
    "                df = pd.DataFrame(records)\n",
    "                df['Group'] = groupname\n",
    "                df['Recording'] = recording\n",
    "\n",
    "                # Renaming group entries\n",
    "                df['Group'] = df['Group'].replace({'Control': 'No_CTZ', 'ES': 'CTZ'})\n",
    "\n",
    "                # Data cleaning and conversion\n",
    "                df['Condition'] = df['Condition'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "                df['Condition'] = df['Condition'].map({'Stim': True, 'No Stim': False})\n",
    "                df['CellType2'] = df['CellType2'].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "                df['CellType2'] = df['CellType2'].map({'+FS': 1, 'nsFS': 0, '-FS': -1, '+RS': 1, 'nsRS': 0, '-RS': -1})\n",
    "\n",
    "                # Flatten all columns that contain lists to scalar if they contain only one value\n",
    "                for col in df.columns:\n",
    "                    if df[col].apply(lambda x: isinstance(x, list)).any():\n",
    "                        df[col] = df[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "                frames.append(df)\n",
    "\n",
    "        # Replace self.phase with the cleaned DataFrame\n",
    "        self.phase = pd.concat(frames, ignore_index=True)\n",
    "        \n",
    "    def plot_ctz_noctz_ppc2(self, cell_type=None, cell_type2=None, show_individual_points=False, smoothing_window=1, min_spikes_evoked=1, min_spikes_spontaneous=1):\n",
    "        # Prepare the subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle('PPC2 Analysis for CTZ and No_CTZ Groups with SEM')\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the SEM face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Function to apply simple moving average for smoothing\n",
    "        def smooth_data(y, window_size):\n",
    "            if window_size <= 1:\n",
    "                return y\n",
    "            window = np.ones(int(window_size)) / float(window_size)\n",
    "            return np.convolve(y, window, 'same')\n",
    "\n",
    "        # Common plotting function for a given condition\n",
    "        def plot_data(df, group, ax, title):\n",
    "            # Filter by cell type if specified\n",
    "            if cell_type:\n",
    "                df = df[df['CellType'] == cell_type]\n",
    "            if cell_type2:\n",
    "                df = df[df['CellType2'] == cell_type2]\n",
    "\n",
    "            # Group by frequency band midpoint\n",
    "            grouped = df.groupby('fband_midpoint')\n",
    "            \n",
    "            # Plot individual data points if toggled on\n",
    "            if show_individual_points:\n",
    "                for fband_midpoint, group_data in grouped:\n",
    "                    ax.plot([fband_midpoint] * len(group_data), group_data['PPC2'], color=lightened_colors[group], alpha=0.7, markersize=4)\n",
    "            \n",
    "            # Calculate mean and SEM\n",
    "            mean_values = grouped['PPC2'].mean()\n",
    "            sem_values = grouped['PPC2'].sem()\n",
    "\n",
    "            # Smooth the mean and SEM if smoothing_window is greater than 1\n",
    "            x_values = mean_values.index\n",
    "            mean_values_smoothed = smooth_data(mean_values, smoothing_window)\n",
    "            sem_values_smoothed = smooth_data(sem_values, smoothing_window)\n",
    "\n",
    "            # Plot mean line\n",
    "            ax.plot(x_values, mean_values_smoothed, linestyle='-', color=group_colors[group], linewidth=3, label=f'Mean {group}')\n",
    "            # Plot SEM as shaded area\n",
    "            ax.fill_between(x_values, (mean_values_smoothed - sem_values_smoothed), (mean_values_smoothed + sem_values_smoothed), color=lightened_colors[group], alpha=0.1)\n",
    "            \n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Frequency Band Midpoint (Hz)')\n",
    "            ax.set_ylabel('PPC2')\n",
    "            ax.grid(False)  # Turn off grid lines\n",
    "            ax.legend()\n",
    "\n",
    "        # Filter data for both groups\n",
    "        df_ctz = self.phase[(self.phase['Group'] == 'CTZ')]\n",
    "        df_no_ctz = self.phase[(self.phase['Group'] == 'No_CTZ')]\n",
    "\n",
    "        #filter data based on spikes \n",
    "        list_of_df = [df_ctz, df_no_ctz]\n",
    "        for df in list_of_df:\n",
    "            df = self.filter_by_spikes(df, min_spikes_spontaneous=min_spikes_spontaneous, min_spikes_evoked=min_spikes_evoked)\n",
    "            #rename the corected df\n",
    "            if df.equals(df_ctz):\n",
    "                df_ctz = df\n",
    "            else:\n",
    "                df_no_ctz = df\n",
    "\n",
    "        \n",
    "        # Separate the data based on the 'Condition'\n",
    "        conditions = [False, True]  # False for Spontaneous, True for Evoked\n",
    "        titles = ['Spontaneous', 'Evoked']\n",
    "\n",
    "        for i, cond in enumerate(conditions):\n",
    "            # Filter by condition for each group\n",
    "            df_ctz_cond = df_ctz[df_ctz['Condition'] == cond]\n",
    "            df_no_ctz_cond = df_no_ctz[df_no_ctz['Condition'] == cond]\n",
    "\n",
    "            # Plot CTZ group data\n",
    "            plot_data(df_ctz_cond, 'CTZ', axes[i], titles[i])\n",
    "\n",
    "            # Plot No_CTZ group data\n",
    "            plot_data(df_no_ctz_cond, 'No_CTZ', axes[i], titles[i])\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "        plt.show()\n",
    "        \n",
    "    def filter_by_spikes(self, df, min_spikes_spontaneous, min_spikes_evoked):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame based on the 'Condition' column with two different\n",
    "        minimum spike thresholds for Spontaneous and Evoked conditions.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): The DataFrame to filter.\n",
    "        min_spikes_spontaneous (int): Minimum number of spikes for Spontaneous condition.\n",
    "        min_spikes_evoked (int): Minimum number of spikes for Evoked condition.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A new DataFrame filtered according to the specified thresholds.\n",
    "        \"\"\"\n",
    "        # Filter for Spontaneous condition (Condition == False)\n",
    "        spontaneous_df = df[(df['Condition'] == False) & (df['NumSpikes'] >= min_spikes_spontaneous)]\n",
    "        \n",
    "        # Filter for Evoked condition (Condition == True)\n",
    "        evoked_df = df[(df['Condition'] == True) & (df['NumSpikes'] >= min_spikes_evoked)]\n",
    "        \n",
    "        # Concatenate the two filtered DataFrames\n",
    "        filtered_df = pd.concat([spontaneous_df, evoked_df])\n",
    "\n",
    "        return filtered_df\n",
    "    \n",
    "    def filter_data_by_quantile(self, df, quantile_low, quantile_high, column='PPC2'):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame based on the quantile range of a specified column.\n",
    "\n",
    "        Parameters:\n",
    "        df (DataFrame): The DataFrame to filter.\n",
    "        quantile_low (float): The lower quantile threshold (e.g., 0.25 for the 25th percentile).\n",
    "        quantile_high (float): The upper quantile threshold (e.g., 0.75 for the 75th percentile).\n",
    "        column (str): The column to apply the quantile filter on.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A new DataFrame filtered according to the specified quantiles.\n",
    "        \"\"\"\n",
    "        lower_bound = df[column].quantile(quantile_low)\n",
    "        upper_bound = df[column].quantile(quantile_high)\n",
    "        filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "        return filtered_df\n",
    "    \n",
    "    def plot_quantile_filtered_ppc2(self, quantile_low=0.05, quantile_high=0.95, show_individual_points=False, \n",
    "                                    smoothing_window=1, min_spikes_evoked=5, min_spikes_spontaneous=50):\n",
    "        \"\"\"\n",
    "        Plots PPC2 data with quantile filtering, and saves the plots dynamically in subfolders based on cell type and sensory response.\n",
    "\n",
    "        Parameters:\n",
    "            quantile_low (float): The lower quantile threshold for filtering.\n",
    "            quantile_high (float): The upper quantile threshold for filtering.\n",
    "            show_individual_points (bool): Whether to show individual data points in the plot.\n",
    "            smoothing_window (int): Window size for smoothing the data.\n",
    "            min_spikes_evoked (int): Minimum number of spikes for evoked condition filtering.\n",
    "            min_spikes_spontaneous (int): Minimum number of spikes for spontaneous condition filtering.\n",
    "\n",
    "        Returns:\n",
    "            None: Plots are saved in the specified directory with the appropriate subfolder structure.\n",
    "        \"\"\"\n",
    "\n",
    "        # Default directory\n",
    "        base_directory = '/Volumes/MannySSD/figures/thesis_final_figs/ppc2/'\n",
    "\n",
    "        # Assume these attributes are set within the object\n",
    "        cell_types = ['FS', 'RS']  # Example: or dynamically extracted from your data\n",
    "        sensory_responses = [1, 0]  # 1 for Sensory Response, 0 for Non-Sensory Response\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the SEM face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Function to apply simple moving average for smoothing\n",
    "        def smooth_data(y, window_size):\n",
    "            if window_size <= 1:\n",
    "                return y\n",
    "            window = np.ones(int(window_size)) / float(window_size)\n",
    "            return np.convolve(y, window, 'same')\n",
    "\n",
    "        # Common plotting function for a given condition\n",
    "        def plot_data(df, group, ax, title):\n",
    "            # Apply quantile filtering\n",
    "            df = self.filter_data_by_quantile(df, quantile_low, quantile_high)\n",
    "\n",
    "            # Group by frequency band midpoint\n",
    "            grouped = df.groupby('fband_midpoint')\n",
    "\n",
    "            # Plot individual data points if toggled on\n",
    "            if show_individual_points:\n",
    "                for fband_midpoint, group_data in grouped:\n",
    "                    ax.plot([fband_midpoint] * len(group_data), group_data['PPC2'], color=lightened_colors[group], alpha=0.7, markersize=4)\n",
    "\n",
    "            # Calculate mean and SEM\n",
    "            mean_values = grouped['PPC2'].mean()\n",
    "            sem_values = grouped['PPC2'].sem()\n",
    "\n",
    "            # Smooth the mean and SEM if smoothing_window is greater than 1\n",
    "            x_values = mean_values.index\n",
    "            mean_values_smoothed = smooth_data(mean_values, smoothing_window)\n",
    "            sem_values_smoothed = smooth_data(sem_values, smoothing_window)\n",
    "\n",
    "            # Plot mean line\n",
    "            ax.plot(x_values, mean_values_smoothed, linestyle='-', color=group_colors[group], linewidth=3, label=f'Mean {group}')\n",
    "            # Plot SEM as shaded area\n",
    "            ax.fill_between(x_values, (mean_values_smoothed - sem_values_smoothed), (mean_values_smoothed + sem_values_smoothed), color=lightened_colors[group], alpha=0.1)\n",
    "\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Frequency Band Midpoint (Hz)')\n",
    "            ax.set_ylabel('PPC2')\n",
    "            ax.grid(False)  # Turn off grid lines\n",
    "            ax.legend()\n",
    "\n",
    "        # Iterate through all combinations of cell types and sensory responses\n",
    "        for cell_type in cell_types:\n",
    "            for cell_type2 in sensory_responses:\n",
    "                cell_type2_label = 'Sensory Response' if cell_type2 == 1 else 'Non-Sensory Response'\n",
    "\n",
    "                # Filter the DataFrame based on the current cell type and sensory response\n",
    "                df_ctz_filtered = self.phase[(self.phase['Group'] == 'CTZ') & (self.phase['CellType'] == cell_type) & (self.phase['CellType2'] == cell_type2)]\n",
    "                df_no_ctz_filtered = self.phase[(self.phase['Group'] == 'No_CTZ') & (self.phase['CellType'] == cell_type) & (self.phase['CellType2'] == cell_type2)]\n",
    "\n",
    "                # Prepare the subplots (1 row by 2 columns for Spontaneous vs Evoked)\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "                fig.suptitle(f'PPC2 Analysis for {cell_type} - {cell_type2_label}', fontsize=16)\n",
    "\n",
    "                conditions = [False, True]  # False for Spontaneous, True for Evoked\n",
    "                titles = ['Spontaneous', 'Evoked']\n",
    "\n",
    "                for i, cond in enumerate(conditions):\n",
    "                    # Filter by condition for each group\n",
    "                    df_ctz_cond = df_ctz_filtered[df_ctz_filtered['Condition'] == cond]\n",
    "                    df_no_ctz_cond = df_no_ctz_filtered[df_no_ctz_filtered['Condition'] == cond]\n",
    "\n",
    "                    # Plot CTZ group data\n",
    "                    plot_data(df_ctz_cond, 'CTZ', axes[i], titles[i])\n",
    "\n",
    "                    # Plot No_CTZ group data\n",
    "                    plot_data(df_no_ctz_cond, 'No_CTZ', axes[i], titles[i])\n",
    "\n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make room for the title\n",
    "\n",
    "                # Create the directory structure based on cell type and sensory response\n",
    "                subfolder = os.path.join(base_directory, cell_type, cell_type2_label)\n",
    "                os.makedirs(subfolder, exist_ok=True)\n",
    "\n",
    "                # Generate the filename based on cell type and sensory response\n",
    "                file_name = f'{cell_type}_{cell_type2_label}_ppc2'\n",
    "                file_path = os.path.join(subfolder, f'{file_name}.svg')\n",
    "\n",
    "                # Save the figure as an SVG file in the specified subfolder\n",
    "                fig.savefig(file_path, format='svg', transparent=True)\n",
    "                plt.close(fig)  # Close the figure after saving to free up memory\n",
    "\n",
    "    def filter_by_frequency_band(self, band_name):\n",
    "        \"\"\"\n",
    "        Filters the data based on the specified frequency band.\n",
    "\n",
    "        Parameters:\n",
    "        band_name (str): The name of the frequency band to filter by (e.g., 'alpha', 'beta').\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: A DataFrame filtered by the specified frequency band.\n",
    "        \"\"\"\n",
    "        if band_name not in self.FREQUENCY_BANDS:\n",
    "            raise ValueError(f\"Invalid band name: {band_name}. Valid names are: {list(self.FREQUENCY_BANDS.keys())}\")\n",
    "\n",
    "        low, high = self.FREQUENCY_BANDS[band_name]\n",
    "        filtered_df = self.phase[(self.phase['fband_midpoint'] >= low) & (self.phase['fband_midpoint'] < high)]\n",
    "        return filtered_df\n",
    "    \n",
    "    def compare_frequency_bands_between_conditions(self, frequency_bands=None, cell_types1=None, cell_types2=None, quantile_low=None, quantile_high=None, show_outliers=None, hue_order=None):\n",
    "        \"\"\"\n",
    "        Compares the data between groups for all specified frequency bands and conditions, and saves the plots dynamically.\n",
    "\n",
    "        Parameters:\n",
    "            frequency_bands (list of str): List of frequency bands to compare (e.g., ['alpha', 'beta']).\n",
    "            cell_types1 (list of str): List of primary cell types to filter by (e.g., ['RS', 'FS']).\n",
    "            cell_types2 (list of int): List of secondary cell types to filter by (1 for Sensory Response, 0 for Non-Sensory Response).\n",
    "            quantile_low (float): The lower quantile threshold for filtering.\n",
    "            quantile_high (float): The upper quantile threshold for filtering.\n",
    "            show_outliers (bool): Whether to show outliers in the boxplot.\n",
    "            hue_order (list of str): Order of the hue levels in the plot.\n",
    "\n",
    "        Returns:\n",
    "            None: Plots are saved in the specified directory.\n",
    "        \"\"\"\n",
    "\n",
    "        # Base directory\n",
    "        base_directory = '/Volumes/MannySSD/figures/thesis_final_figs/ppc2/'\n",
    "\n",
    "        # Initialize default parameters if not provided\n",
    "        if frequency_bands is None:\n",
    "            frequency_bands = ['alpha', 'beta', 'gamma', 'high_gamma']\n",
    "        if cell_types1 is None:\n",
    "            cell_types1 = ['RS', 'FS']\n",
    "        if cell_types2 is None:\n",
    "            cell_types2 = [1, 0]\n",
    "\n",
    "        # Define color mapping for groups\n",
    "        group_colors = {\n",
    "            'No_CTZ': '#797979',\n",
    "            'CTZ': '#5a00c2'\n",
    "        }\n",
    "\n",
    "        # Generate lighter versions for the box face color\n",
    "        lightened_colors = {k: v + '33' for k, v in group_colors.items()}\n",
    "\n",
    "        # Boxplot customization\n",
    "        boxprops = {'edgecolor': 'k', 'linewidth': 2}\n",
    "        whiskerprops = {'color': 'k', 'linewidth': 2}\n",
    "        boxplot_kwargs = {\n",
    "            'boxprops': boxprops,\n",
    "            'medianprops': whiskerprops,\n",
    "            'whiskerprops': whiskerprops,\n",
    "            'capprops': {'linewidth': 0},  # Hide the caps\n",
    "            'showfliers': show_outliers,\n",
    "            'palette': group_colors,\n",
    "            'hue_order': hue_order,\n",
    "            'width': 0.75\n",
    "        }\n",
    "\n",
    "        # Stripplot customization\n",
    "        stripplot_kwargs = {\n",
    "            'linewidth': 0.6,\n",
    "            'size': 6,\n",
    "            'alpha': 0.7,\n",
    "            'jitter': True,\n",
    "            'dodge': True,  # Removed dodge=True from the sns.stripplot() call\n",
    "            'palette': lightened_colors,\n",
    "            'hue_order': hue_order\n",
    "        }\n",
    "\n",
    "        # Loop over each combination of cell types\n",
    "        for cell1 in cell_types1:\n",
    "            for cell2 in cell_types2:\n",
    "                cell_type2_label = 'Sensory Response' if cell2 == 1 else 'Non-Sensory Response'\n",
    "\n",
    "                # Create subfolder path based on hierarchy\n",
    "                subfolder = os.path.join(base_directory, cell1, cell_type2_label)\n",
    "                os.makedirs(subfolder, exist_ok=True)\n",
    "\n",
    "                # Prepare the 1x2 plot\n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "                fig.suptitle(f'{cell1} - {cell_type2_label} Comparison Across Frequency Bands')\n",
    "\n",
    "                conditions = [False, True]  # False for Spontaneous, True for Evoked\n",
    "                titles = ['Spontaneous', 'Evoked']\n",
    "\n",
    "                for i, cond in enumerate(conditions):\n",
    "                    # Filter data for each condition\n",
    "                    filtered_dfs = []\n",
    "                    for band in frequency_bands:\n",
    "                        filtered_df = self.filter_by_frequency_band(band)\n",
    "                        filtered_df = filtered_df[filtered_df['Condition'] == cond]\n",
    "                        if cell1:\n",
    "                            filtered_df = filtered_df[filtered_df['CellType'] == cell1]\n",
    "                        if cell2 is not None:\n",
    "                            filtered_df = filtered_df[filtered_df['CellType2'] == cell2]\n",
    "\n",
    "                        # Apply quantile filtering if specified\n",
    "                        if quantile_low is not None and quantile_high is not None:\n",
    "                            filtered_df = self.filter_data_by_quantile(filtered_df, quantile_low, quantile_high)\n",
    "\n",
    "                        filtered_df['Frequency Band'] = band  # Add frequency band to DataFrame\n",
    "                        filtered_dfs.append(filtered_df)\n",
    "\n",
    "                    # Concatenate data across frequency bands\n",
    "                    combined_df = pd.concat(filtered_dfs)\n",
    "\n",
    "                    # Aggregate PPC2 values per CellID, Group, and Frequency Band\n",
    "                    aggregated_df = combined_df.groupby(['CellID', 'Group', 'Frequency Band']).agg({'PPC2': 'mean'}).reset_index()\n",
    "\n",
    "                    # Plot the comparison for this condition across all frequency bands\n",
    "                    sns.boxplot(data=aggregated_df, x='Frequency Band', y='PPC2', hue='Group', **boxplot_kwargs, ax=axes[i])\n",
    "                    sns.stripplot(data=aggregated_df, x='Frequency Band', y='PPC2', hue='Group', **stripplot_kwargs, ax=axes[i])\n",
    "\n",
    "                    axes[i].set_title(titles[i])\n",
    "                    axes[i].set_xlabel('Frequency Band')\n",
    "                    if i == 0:\n",
    "                        axes[i].set_ylabel('PPC2')\n",
    "                    axes[i].legend(title='Group')\n",
    "\n",
    "                # Save the figure\n",
    "                file_name = f'{cell1}_{cell_type2_label}_FrequencyBandComparison.svg'\n",
    "                file_path = os.path.join(subfolder, file_name)\n",
    "                fig.savefig(file_path, format='svg', transparent=True)\n",
    "                plt.close(fig)  # Close the figure after saving to free up memory\n",
    "\n",
    "    def run_permutation_test_between_groups(self, band_name, condition, quantile_low=None, quantile_high=None, cell_type=None, cell_type2=None, min_num_spikes=None, n_permutations=10000):\n",
    "        \"\"\"\n",
    "        Runs a permutation test to compare the PPC2 values between groups for a specified frequency band.\n",
    "\n",
    "        Parameters:\n",
    "        band_name (str): The name of the frequency band to filter by (e.g., 'alpha', 'beta').\n",
    "        condition (bool): The condition to filter by (True for evoked, False for spontaneous).\n",
    "        quantile_low (float): The lower quantile threshold for filtering.\n",
    "        quantile_high (float): The upper quantile threshold for filtering.\n",
    "        cell_type (str): The cell type to filter by.\n",
    "        cell_type2 (int): The secondary cell type to filter by.\n",
    "        min_num_spikes (int): The minimum number of spikes to filter by.\n",
    "        n_permutations (int): The number of permutations to perform.\n",
    "\n",
    "        Returns:\n",
    "        float: The p-value from the permutation test.\n",
    "        \"\"\"\n",
    "        filtered_df = self.filter_by_frequency_band(band_name)\n",
    "\n",
    "        # Filter by condition\n",
    "        filtered_df = filtered_df[filtered_df['Condition'] == condition]\n",
    "\n",
    "        # Filter by cell type if specified\n",
    "        if cell_type:\n",
    "            filtered_df = filtered_df[filtered_df['CellType'] == cell_type]\n",
    "        if cell_type2 is not None:\n",
    "            filtered_df = filtered_df[filtered_df['CellType2'] == cell_type2]\n",
    "\n",
    "        # Filter by minimum number of spikes if specified\n",
    "        if min_num_spikes is not None:\n",
    "            filtered_df = filtered_df[filtered_df['NumSpikes'] >= min_num_spikes]\n",
    "\n",
    "        # Apply quantile filtering if specified\n",
    "        if quantile_low is not None and quantile_high is not None:\n",
    "            filtered_df = self.filter_data_by_quantile(filtered_df, quantile_low, quantile_high)\n",
    "\n",
    "        # Aggregate PPC2 values per CellID and frequency band midpoint\n",
    "        aggregated_df = filtered_df.groupby(['CellID', 'Group']).agg({'PPC2': 'mean'}).reset_index()\n",
    "\n",
    "        # Separate the data by groups\n",
    "        df_ctz = aggregated_df[aggregated_df['Group'] == 'CTZ']['PPC2']\n",
    "        df_no_ctz = aggregated_df[aggregated_df['Group'] == 'No_CTZ']['PPC2']\n",
    "\n",
    "        # Calculate observed difference in means\n",
    "        observed_diff = np.mean(df_ctz) - np.mean(df_no_ctz)\n",
    "\n",
    "        # Concatenate the data for permutation\n",
    "        all_data = np.concatenate([df_ctz, df_no_ctz])\n",
    "        n_ctz = len(df_ctz)\n",
    "        n_no_ctz = len(df_no_ctz)\n",
    "\n",
    "        # Perform permutations\n",
    "        perm_diffs = np.zeros(n_permutations)\n",
    "        for i in range(n_permutations):\n",
    "            np.random.shuffle(all_data)\n",
    "            perm_ctz = all_data[:n_ctz]\n",
    "            perm_no_ctz = all_data[n_ctz:]\n",
    "            perm_diffs[i] = np.mean(perm_ctz) - np.mean(perm_no_ctz)\n",
    "\n",
    "        # Calculate p-value\n",
    "        p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_diff))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CV_Analysis:\n",
    "    def __init__(self, eed_object):\n",
    "        \"\"\"\n",
    "        Initialize the CV_Analysis class.\n",
    "\n",
    "        Parameters:\n",
    "            eed_object (ExtractEphysData): An instance of ExtractEphysData which provides\n",
    "                                           access to electrophysiological data methods and attributes.\n",
    "        \"\"\"\n",
    "        self.eed = eed_object\n",
    "\n",
    "    def get_spike_times_for_cell(self, groupname, recordingname, cid):\n",
    "        \"\"\"\n",
    "        Retrieves the spike times for a specific cell based on groupname, recordingname, and cid.\n",
    "\n",
    "        Parameters:\n",
    "            groupname (str): The group name identifier.\n",
    "            recordingname (str): The recording name identifier.\n",
    "            cid (int): The unique cell identifier.\n",
    "\n",
    "        Returns:\n",
    "            np.array: The spike times in samples for the specified cell.\n",
    "        \"\"\"\n",
    "        df = self.eed.dataframes['basic_metrics']\n",
    "        row = df[(df['groupname'] == groupname) & \n",
    "                 (df['recordingname'] == recordingname) & \n",
    "                 (df['cid'] == cid)]\n",
    "        \n",
    "        if not row.empty:\n",
    "            return row['SpikeTimes_all'].values[0]\n",
    "        else:\n",
    "            raise ValueError(\"No matching cell found.\")\n",
    "\n",
    "    def calculate_moving_window_firing_rate(self, spike_times, window_duration_seconds=180, step_size_seconds=1):\n",
    "        \"\"\"\n",
    "        Calculates the firing rate across a moving window of a specified duration.\n",
    "\n",
    "        Parameters:\n",
    "            spike_times (np.array): The spike times in samples for a particular cell.\n",
    "            window_duration_seconds (int): The duration of the moving window in seconds (default is 180 seconds).\n",
    "            step_size_seconds (int): The step size for the moving window in seconds (default is 1 second).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the window start time (in seconds) as the key and the firing rate as the value.\n",
    "        \"\"\"\n",
    "        sampling_rate = 30000  # 30 kHz\n",
    "        window_size_samples = int(window_duration_seconds * sampling_rate)\n",
    "        step_size_samples = int(step_size_seconds * sampling_rate)\n",
    "\n",
    "        max_time_samples = int(spike_times[-1])\n",
    "        firing_rate_dict = {}\n",
    "\n",
    "        for start_time in range(0, max_time_samples - window_size_samples + 1, step_size_samples):\n",
    "            end_time = start_time + window_size_samples\n",
    "            spikes_in_window = np.sum((spike_times >= start_time) & (spike_times < end_time))\n",
    "            firing_rate = spikes_in_window / window_duration_seconds\n",
    "            firing_rate_dict[start_time / sampling_rate] = firing_rate\n",
    "\n",
    "        return firing_rate_dict\n",
    "\n",
    "    def analyze_single_cell(self, groupname, recordingname, cid):\n",
    "        \"\"\"\n",
    "        Analyzes a single cell, calculating the moving window firing rate.\n",
    "\n",
    "        Parameters:\n",
    "            groupname (str): The group name identifier.\n",
    "            recordingname (str): The recording name identifier.\n",
    "            cid (int): The unique cell identifier.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the moving window firing rate over time.\n",
    "        \"\"\"\n",
    "        spike_times = self.get_spike_times_for_cell(groupname, recordingname, cid)\n",
    "        firing_rate_dict = self.calculate_moving_window_firing_rate(spike_times)\n",
    "        return firing_rate_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import the data for analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class with the path to your .mat file \n",
    "#whisker = ExtractEphysData('/Volumes/MannySSD/', 'all_data_20ms_99CI_FINAL.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organoid = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid/SpikeStuff/', 'all_data_MEA_final.mat')\n",
    "#organoid = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid/SpikeStuff/', 'all_data_MEA_final_05.mat')\n",
    "organoid = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid_bic/SpikeStuff/', 'all_data_MEA_final_05_bic.mat')\n",
    "#organoid = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid_nobic/SpikeStuff/', 'all_data_MEA_final_05_nobic.mat')\n",
    "\n",
    "organoid_df_manager = DataFrameManagerAxionMEA(organoid)\n",
    "organoid_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'Amplitude',\n",
    "                                      'SpikeTimes_all', 'Template_Channel', \n",
    "                                    'Normalized_Template_Waveform', 'TroughToPeak_duration', \n",
    "                                     'SpikeHalfWidth', 'UnNormalized_Template_Waveform', 'ISI_violations_percent', \n",
    "                                     'Recording_Duration', 'Sampling_Frequency'], 'basic_metrics')\n",
    "organoid_database = organoid_df_manager.dataframes['basic_metrics']\n",
    "organoid_database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organoid_df_manager = DataFrameManagerAxionMEA(organoid)\n",
    "organoid_df_manager.create_dataframe(['Cell_Type', 'IsSingleUnit', 'Amplitude',\n",
    "                                      'SpikeTimes_all', 'Template_Channel', \n",
    "                                    'Normalized_Template_Waveform', 'TroughToPeak_duration', \n",
    "                                     'SpikeHalfWidth', 'UnNormalized_Template_Waveform', 'ISI_violations_percent', \n",
    "                                     'Recording_Duration', 'Sampling_Frequency'], 'basic_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organoid_database = organoid_df_manager.dataframes['basic_metrics']\n",
    "organoid_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the dataframe to a csv file \n",
    "organoid_database.to_csv('/Volumes/Manny2TB/axion_mea_data_organoid/SpikeStuff/organoid_database.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the name of all the columns in the dataframe\n",
    "organoid_database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide the name of all the columns in the dataframe\n",
    "organoid_database.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique types in each column\n",
    "for column in organoid_database.columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(organoid_database[column].apply(type).value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_document_dataframe(df):\n",
    "    \"\"\"\n",
    "    Validates that the provided DataFrame contains the required columns and enforces consistent data types.\n",
    "    \n",
    "    Columns and their expected properties:\n",
    "    - groupname (str): Represents different groups. At least one group must be present.\n",
    "    - recordingname (str): Unique recordings within a group.\n",
    "    - cid (str): Cell ID, which may be repeated across recordings but is unique within a recording.\n",
    "    - Cell_Type (str): Type of the cell, either 'FS' (Fast-Spiking) or 'RS' (Regular-Spiking).\n",
    "    - IsSingleUnit (bool): Indicates if the unit is a single unit. Must be `True` or `False`, converted from `0.0` or `1.0` \n",
    "      regardless of whether the original value is `float` or `np.ndarray`.\n",
    "    - SpikeTimes_all (numpy.ndarray): Array containing all spike times.\n",
    "    - Template_Channel (numpy.ndarray): Array indicating the template channel data.\n",
    "    - Normalized_Template_Waveform (numpy.ndarray): Normalized waveform template data.\n",
    "    - TroughToPeak_duration (numpy.ndarray): Duration from trough to peak of the waveform.\n",
    "    - SpikeHalfWidth (numpy.ndarray): Width of the spike at half amplitude.\n",
    "    - UnNormalized_Template_Waveform (numpy.ndarray): Unnormalized waveform template data.\n",
    "    - ISI_violations_percent (float or numpy.ndarray): ISI violations percentage. Requires further exploration.\n",
    "    - Recording_Duration (numpy.ndarray): Duration of the recording.\n",
    "    - Sampling_Frequency (numpy.ndarray): Sampling frequency of the recording.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to validate.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The validated DataFrame with corrected `IsSingleUnit` column.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If required columns are missing or if specific constraints are violated.\n",
    "        TypeError: If column data types do not match expectations.\n",
    "    \"\"\"\n",
    "    # Define required columns and their expected data types\n",
    "    required_columns = {\n",
    "        \"groupname\": str,\n",
    "        \"recordingname\": str,\n",
    "        \"cid\": str,\n",
    "        \"Cell_Type\": str,\n",
    "        \"IsSingleUnit\": (np.ndarray, float),\n",
    "        \"SpikeTimes_all\": np.ndarray,\n",
    "        \"Template_Channel\": np.ndarray,\n",
    "        \"Normalized_Template_Waveform\": np.ndarray,\n",
    "        \"TroughToPeak_duration\": np.ndarray,\n",
    "        \"SpikeHalfWidth\": np.ndarray,\n",
    "        \"UnNormalized_Template_Waveform\": np.ndarray,\n",
    "        \"ISI_violations_percent\": (np.ndarray, float),\n",
    "        \"Recording_Duration\": np.ndarray,\n",
    "        \"Sampling_Frequency\": np.ndarray,\n",
    "    }\n",
    "    \n",
    "    # Validate column presence\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following required columns are missing: {missing_columns}\")\n",
    "    \n",
    "    # Validate column data types\n",
    "    for column, expected_type in required_columns.items():\n",
    "        if not df[column].apply(lambda x: isinstance(x, expected_type)).all():\n",
    "            raise TypeError(f\"Column '{column}' does not match the expected type {expected_type}.\")\n",
    "    \n",
    "\n",
    "    def convert_is_single_unit(value):\n",
    "        \"\"\"\n",
    "        Converts raw values indicating single unit status into boolean values (True/False).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value : {float, numpy.ndarray}\n",
    "            The input value to process. Must be either a numeric scalar (float) or a numpy array\n",
    "            (0-dimensional or 1-dimensional with a single element) containing `0.0` or `1.0`.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            The converted boolean value:\n",
    "            - `True` for inputs equivalent to `1.0`.\n",
    "            - `False` for inputs equivalent to `0.0`.\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the input does not conform to the expected format or values.\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        >>> convert_is_single_unit(1.0)\n",
    "        True\n",
    "        >>> convert_is_single_unit(0.0)\n",
    "        False\n",
    "        >>> convert_is_single_unit(np.array(1.0))\n",
    "        True\n",
    "        >>> convert_is_single_unit(np.array([0.0]))\n",
    "        False\n",
    "        >>> convert_is_single_unit(np.array([0.0, 1.0]))\n",
    "        Traceback (most recent call last):\n",
    "            ...\n",
    "        ValueError: Invalid array size in 'IsSingleUnit': [0. 1.]\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"Processing value: {value}\")  # Print the raw input value\n",
    "        \n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"Detected np.ndarray with shape {value.shape} and size {value.size}\")\n",
    "            # Extract scalar from 0-dimensional ndarray\n",
    "            if value.ndim == 0:  # Handle 0-dimensional arrays\n",
    "                value = value.item()\n",
    "                print(f\"Converted 0-dimensional array to scalar: {value}\")\n",
    "            # Check if its a single-element array\n",
    "            elif value.size == 1:\n",
    "                value = value[0]\n",
    "                print(f\"Extracted scalar from single-element array: {value}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid array size in 'IsSingleUnit': {value}\")\n",
    "        \n",
    "        # Ensure the value is converted to a boolean if it matches 0.0 or 1.0\n",
    "        if value in [0.0, 1.0]:\n",
    "            converted_value = bool(value)\n",
    "            print(f\"Converted value {value} to boolean: {converted_value}\")\n",
    "            return converted_value\n",
    "        \n",
    "        # Raise an error if the value is invalid\n",
    "        raise ValueError(f\"Invalid value in 'IsSingleUnit': {value}\")\n",
    "        \n",
    "    \n",
    "    df[\"IsSingleUnit\"] = df[\"IsSingleUnit\"].apply(convert_is_single_unit)\n",
    "    \n",
    "    # Check specific constraints\n",
    "    if df[\"groupname\"].nunique() < 1:\n",
    "        raise ValueError(\"The 'groupname' column must have at least one unique group.\")\n",
    "    \n",
    "    if not df[\"Cell_Type\"].isin([\"FS\", \"RS\"]).all():\n",
    "        raise ValueError(\"The 'Cell_Type' column must only contain 'FS' or 'RS'.\")\n",
    "    \n",
    "    # Return the validated DataFrame\n",
    "    print(\"DataFrame validation passed. All required columns are present and correctly formatted.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "organoid_database = validate_and_document_dataframe(organoid_database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameManagerAxionMEA_Plotter:\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initializes the DataFrameManagerAxionMEA_Plotter class.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): A cleaned DataFrame containing the neural data.\n",
    "        \"\"\"\n",
    "        self.df = dataframe\n",
    "\n",
    "    def plot_mean_waveforms(self):\n",
    "        \"\"\"\n",
    "        Plots the mean normalized waveforms for Fast-Spiking (FS) and Regular-Spiking (RS) neurons.\n",
    "\n",
    "        This method groups the data by the 'Cell_Type' column, computes the mean waveform for each type \n",
    "        (using the 'Normalized_Template_Waveform' column), and plots them together for comparison.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Check if required columns are present\n",
    "        if 'Cell_Type' not in self.df.columns or 'Normalized_Template_Waveform' not in self.df.columns:\n",
    "            raise ValueError(\"The DataFrame must contain 'Cell_Type' and 'Normalized_Template_Waveform' columns.\")\n",
    "\n",
    "        # Filter FS and RS neurons\n",
    "        fs_data = self.df[self.df['Cell_Type'] == 'FS']['Normalized_Template_Waveform']\n",
    "        rs_data = self.df[self.df['Cell_Type'] == 'RS']['Normalized_Template_Waveform']\n",
    "\n",
    "        # Check for empty data\n",
    "        if fs_data.empty or rs_data.empty:\n",
    "            raise ValueError(\"Data for FS or RS neurons is missing. Please ensure the DataFrame is complete.\")\n",
    "\n",
    "        # Convert to numpy arrays and calculate the mean waveform\n",
    "        fs_waveforms = np.vstack(fs_data.to_numpy())\n",
    "        rs_waveforms = np.vstack(rs_data.to_numpy())\n",
    "\n",
    "        fs_mean_waveform = np.mean(fs_waveforms, axis=0)\n",
    "        rs_mean_waveform = np.mean(rs_waveforms, axis=0)\n",
    "\n",
    "        # Plot the waveforms\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fs_mean_waveform, label='FS (Fast-Spiking)', color='blue', linewidth=2)\n",
    "        plt.plot(rs_mean_waveform, label='RS (Regular-Spiking)', color='red', linewidth=2)\n",
    "        plt.xlabel('Time (samples)')\n",
    "        plt.ylabel('Amplitude (normalized)')\n",
    "        plt.title('Mean Waveforms of FS vs RS Neurons')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_mean_waveforms_with_individual_traces(self):\n",
    "        \"\"\"\n",
    "        Plots the mean normalized waveforms for FS and RS neurons with individual traces overlaid.\n",
    "\n",
    "        Each individual trace is plotted with increased transparency, and the mean waveform is overlaid \n",
    "        with higher opacity for better visibility.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Check if required columns are present\n",
    "        if 'Cell_Type' not in self.df.columns or 'Normalized_Template_Waveform' not in self.df.columns:\n",
    "            raise ValueError(\"The DataFrame must contain 'Cell_Type' and 'Normalized_Template_Waveform' columns.\")\n",
    "\n",
    "        # Filter FS and RS neurons\n",
    "        fs_data = self.df[self.df['Cell_Type'] == 'FS']['Normalized_Template_Waveform']\n",
    "        rs_data = self.df[self.df['Cell_Type'] == 'RS']['Normalized_Template_Waveform']\n",
    "\n",
    "        # Check for empty data\n",
    "        if fs_data.empty or rs_data.empty:\n",
    "            raise ValueError(\"Data for FS or RS neurons is missing. Please ensure the DataFrame is complete.\")\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        fs_waveforms = np.vstack(fs_data.to_numpy())\n",
    "        rs_waveforms = np.vstack(rs_data.to_numpy())\n",
    "\n",
    "        # Calculate mean waveforms\n",
    "        fs_mean_waveform = np.mean(fs_waveforms, axis=0)\n",
    "        rs_mean_waveform = np.mean(rs_waveforms, axis=0)\n",
    "\n",
    "        # Plot the waveforms\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot individual FS traces\n",
    "        for waveform in fs_waveforms:\n",
    "            plt.plot(waveform, color='blue', alpha=0.1)  # High transparency (low opacity)\n",
    "\n",
    "        # Plot FS mean waveform\n",
    "        plt.plot(fs_mean_waveform, label='FS Mean (Fast-Spiking)', color='blue', linewidth=2)\n",
    "\n",
    "        # Plot individual RS traces\n",
    "        for waveform in rs_waveforms:\n",
    "            plt.plot(waveform, color='red', alpha=0.1)  # High transparency (low opacity)\n",
    "\n",
    "        # Plot RS mean waveform\n",
    "        plt.plot(rs_mean_waveform, label='RS Mean (Regular-Spiking)', color='red', linewidth=2)\n",
    "\n",
    "        # Add labels, legend, and grid\n",
    "        plt.xlabel('Time (samples)')\n",
    "        plt.ylabel('Amplitude (normalized)')\n",
    "        plt.title('Mean Waveforms with Individual Traces for FS vs RS Neurons')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_distribution_stairs(self, metric, bins=None, plot_type=\"stair\", debug=False):\n",
    "        \"\"\"\n",
    "        Plots the distribution of a specified metric using either stair-step or bar plots.\n",
    "\n",
    "        Args:\n",
    "            metric (str): Metric to plot, e.g., 'TroughToPeak_duration'.\n",
    "            bins (array): Bin edges (e.g., np.arange(0.32, 0.73, 0.01)).\n",
    "            plot_type (str): Type of plot ('stair' or 'bar'). Default is 'stair'.\n",
    "            debug (bool): Whether to print debug information about the binning. Default is False.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        if metric not in self.df.columns:\n",
    "            raise ValueError(f\"The DataFrame does not contain the column '{metric}'.\")\n",
    "\n",
    "        # Filter FS and RS data\n",
    "        fs_data = self.df[self.df['Cell_Type'] == 'FS'][metric].dropna()\n",
    "        rs_data = self.df[self.df['Cell_Type'] == 'RS'][metric].dropna()\n",
    "\n",
    "        if fs_data.empty or rs_data.empty:\n",
    "            raise ValueError(f\"No data found for FS or RS neurons for the metric '{metric}'.\")\n",
    "\n",
    "        # Sort data for debugging\n",
    "        sorted_fs_data = fs_data.sort_values()\n",
    "        sorted_rs_data = rs_data.sort_values()\n",
    "\n",
    "        # Set default bins if not provided\n",
    "        if bins is None:\n",
    "            bins = np.arange(0.32, max(fs_data.max(), rs_data.max()) + 0.01, 0.01)  # 0.01 increments\n",
    "\n",
    "        # Calculate histogram counts\n",
    "        fs_hist, _ = np.histogram(fs_data, bins=bins)\n",
    "        rs_hist, _ = np.histogram(rs_data, bins=bins)\n",
    "\n",
    "        # Debug: Print binning and sorted data information\n",
    "        if debug:\n",
    "            print(f\"Sorted FS Data: {sorted_fs_data.tolist()}\")\n",
    "            print(f\"Sorted RS Data: {sorted_rs_data.tolist()}\")\n",
    "            print(f\"New Bin Edges: {bins}\")\n",
    "            print(f\"FS Counts: {fs_hist}\")\n",
    "            print(f\"RS Counts: {rs_hist}\")\n",
    "\n",
    "        # Plot the distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        if plot_type == \"stair\":\n",
    "            # Stair-step plot\n",
    "            plt.step(bins[:-1], fs_hist, label='FS', color='blue', where='post')\n",
    "            plt.step(bins[:-1], rs_hist, label='RS', color='red', where='post')\n",
    "        elif plot_type == \"bar\":\n",
    "            # Bar plot\n",
    "            width = bins[1] - bins[0]\n",
    "            plt.bar(bins[:-1], fs_hist, width=width, alpha=0.5, label='FS', color='blue', edgecolor='black')\n",
    "            plt.bar(bins[:-1], rs_hist, width=width, alpha=0.5, label='RS', color='red', edgecolor='black')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid plot_type. Use 'stair' or 'bar'.\")\n",
    "\n",
    "        # Add labels, title, and legend\n",
    "        plt.xlabel(metric.replace('_', ' '))\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Distribution of {metric.replace(\"_\", \" \")} by Cell Type')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_cell_type_counts(self):\n",
    "        \"\"\"\n",
    "        Creates a bar graph and a pie chart showing the number of FS (Fast-Spiking) \n",
    "        and RS (Regular-Spiking) neurons.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Check if required column is present\n",
    "        if 'Cell_Type' not in self.df.columns:\n",
    "            raise ValueError(\"The DataFrame must contain the 'Cell_Type' column.\")\n",
    "\n",
    "        # Count FS and RS neurons\n",
    "        cell_type_counts = self.df['Cell_Type'].value_counts()\n",
    "\n",
    "        if cell_type_counts.empty:\n",
    "            raise ValueError(\"The 'Cell_Type' column contains no data.\")\n",
    "\n",
    "        # Extract FS and RS counts\n",
    "        fs_count = cell_type_counts.get('FS', 0)\n",
    "        rs_count = cell_type_counts.get('RS', 0)\n",
    "\n",
    "        # Create labels and values\n",
    "        labels = ['FS (Fast-Spiking)', 'RS (Regular-Spiking)']\n",
    "        values = [fs_count, rs_count]\n",
    "\n",
    "        # Bar Graph\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(labels, values, color=['blue', 'red'], alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Cell Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Number of FS vs RS Neurons')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Pie Chart\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie(\n",
    "            values,\n",
    "            labels=labels,\n",
    "            colors=['blue', 'red'],\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            wedgeprops={'edgecolor': 'black'}\n",
    "        )\n",
    "        plt.title('Proportion of FS vs RS Neurons')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "               \n",
    "    def plot_composite_per_recording(self):\n",
    "        \"\"\"\n",
    "        Creates a composite figure for each recording, including:\n",
    "        1. Mean waveforms with individual traces.\n",
    "        2. Bar graph of FS vs RS counts.\n",
    "        3. Pie chart of FS vs RS proportions.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required_columns = ['recordingname', 'Cell_Type', 'Normalized_Template_Waveform']\n",
    "        for column in required_columns:\n",
    "            if column not in self.df.columns:\n",
    "                raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "        # Group by recordings\n",
    "        grouped = self.df.groupby('recordingname')\n",
    "\n",
    "        for recording, group in grouped:\n",
    "            # Create a figure with 3 subplots\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "            fig.suptitle(f'Recording: {recording}', fontsize=16)\n",
    "\n",
    "            # Plot mean waveforms with individual traces (Top-left subplot)\n",
    "            ax1 = axes[0, 0]\n",
    "            fs_data = group[group['Cell_Type'] == 'FS']['Normalized_Template_Waveform']\n",
    "            rs_data = group[group['Cell_Type'] == 'RS']['Normalized_Template_Waveform']\n",
    "\n",
    "            if not fs_data.empty:\n",
    "                fs_waveforms = np.vstack(fs_data.to_numpy())\n",
    "                fs_mean_waveform = np.mean(fs_waveforms, axis=0)\n",
    "                for waveform in fs_waveforms:\n",
    "                    ax1.plot(waveform, color='blue', alpha=0.1)\n",
    "                ax1.plot(fs_mean_waveform, label='FS Mean', color='blue', linewidth=2)\n",
    "\n",
    "            if not rs_data.empty:\n",
    "                rs_waveforms = np.vstack(rs_data.to_numpy())\n",
    "                rs_mean_waveform = np.mean(rs_waveforms, axis=0)\n",
    "                for waveform in rs_waveforms:\n",
    "                    ax1.plot(waveform, color='red', alpha=0.1)\n",
    "                ax1.plot(rs_mean_waveform, label='RS Mean', color='red', linewidth=2)\n",
    "\n",
    "            ax1.set_title('Mean Waveforms with Individual Traces')\n",
    "            ax1.set_xlabel('Time (samples)')\n",
    "            ax1.set_ylabel('Amplitude (normalized)')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True)\n",
    "\n",
    "            # Plot bar graph of FS vs RS counts (Top-right subplot)\n",
    "            ax2 = axes[0, 1]\n",
    "            cell_type_counts = group['Cell_Type'].value_counts()\n",
    "            fs_count = cell_type_counts.get('FS', 0)\n",
    "            rs_count = cell_type_counts.get('RS', 0)\n",
    "            ax2.bar(['FS', 'RS'], [fs_count, rs_count], color=['blue', 'red'], alpha=0.7, edgecolor='black')\n",
    "            ax2.set_title('Counts of FS vs RS Neurons')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "            # Plot pie chart of FS vs RS proportions (Bottom subplot)\n",
    "            ax3 = axes[1, 0]\n",
    "            values = [fs_count, rs_count]\n",
    "            labels = ['FS', 'RS']\n",
    "            ax3.pie(\n",
    "                values,\n",
    "                labels=labels,\n",
    "                colors=['blue', 'red'],\n",
    "                autopct='%1.1f%%',\n",
    "                startangle=90,\n",
    "                wedgeprops={'edgecolor': 'black'}\n",
    "            )\n",
    "            ax3.set_title('Proportion of FS vs RS Neurons')\n",
    "\n",
    "            # Hide unused subplot (Bottom-right)\n",
    "            axes[1, 1].axis('off')\n",
    "\n",
    "            # Adjust layout\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_composite_per_recording_with_sua_mua(self):\n",
    "        \"\"\"\n",
    "        Creates a composite figure for each recording, including:\n",
    "        1. Mean waveforms with individual traces for all units, SUA (True), and MUA (False).\n",
    "        2. Bar graph of FS vs RS counts for all units, SUA, and MUA.\n",
    "        3. Pie chart of FS vs RS proportions for all units, SUA, and MUA.\n",
    "\n",
    "        If data is missing for any condition, a message will be displayed on the plot.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required_columns = ['recordingname', 'Cell_Type', 'Normalized_Template_Waveform', 'IsSingleUnit']\n",
    "        for column in required_columns:\n",
    "            if column not in self.df.columns:\n",
    "                raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "        # Group by recordings\n",
    "        grouped = self.df.groupby('recordingname')\n",
    "\n",
    "        for recording, group in grouped:\n",
    "            # Split data into SUA and MUA\n",
    "            sua_group = group[group['IsSingleUnit'] == True]\n",
    "            mua_group = group[group['IsSingleUnit'] == False]\n",
    "\n",
    "            # Create a figure with 3 rows of subplots for Combined, SUA, and MUA\n",
    "            fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "            fig.suptitle(f'Recording: {recording}', fontsize=16)\n",
    "\n",
    "            # Define conditions for plotting: combined, SUA, and MUA\n",
    "            conditions = {\n",
    "                'Combined': group,\n",
    "                'SUA': sua_group,\n",
    "                'MUA': mua_group\n",
    "            }\n",
    "\n",
    "            for i, (label, subset) in enumerate(conditions.items()):\n",
    "                # Mean waveforms with individual traces\n",
    "                ax1 = axes[i, 0]\n",
    "                if subset.empty:\n",
    "                    ax1.text(0.5, 0.5, f'Not enough data for {label}', fontsize=12, ha='center', va='center')\n",
    "                    ax1.set_title(f'{label}: Mean Waveforms')\n",
    "                    ax1.axis('off')\n",
    "                else:\n",
    "                    fs_data = subset[subset['Cell_Type'] == 'FS']['Normalized_Template_Waveform']\n",
    "                    rs_data = subset[subset['Cell_Type'] == 'RS']['Normalized_Template_Waveform']\n",
    "\n",
    "                    if not fs_data.empty:\n",
    "                        fs_waveforms = np.vstack(fs_data.dropna().to_numpy())\n",
    "                        fs_mean_waveform = np.mean(fs_waveforms, axis=0)\n",
    "                        for waveform in fs_waveforms:\n",
    "                            ax1.plot(waveform, color='blue', alpha=0.1)\n",
    "                        ax1.plot(fs_mean_waveform, label='FS Mean', color='blue', linewidth=2)\n",
    "\n",
    "                    if not rs_data.empty:\n",
    "                        rs_waveforms = np.vstack(rs_data.dropna().to_numpy())\n",
    "                        rs_mean_waveform = np.mean(rs_waveforms, axis=0)\n",
    "                        for waveform in rs_waveforms:\n",
    "                            ax1.plot(waveform, color='red', alpha=0.1)\n",
    "                        ax1.plot(rs_mean_waveform, label='RS Mean', color='red', linewidth=2)\n",
    "\n",
    "                    ax1.set_title(f'{label}: Mean Waveforms with Individual Traces')\n",
    "                    ax1.set_xlabel('Time (samples)')\n",
    "                    ax1.set_ylabel('Amplitude (normalized)')\n",
    "                    ax1.legend()\n",
    "                    ax1.grid(True)\n",
    "\n",
    "                # Bar graph of FS vs RS counts\n",
    "                ax2 = axes[i, 1]\n",
    "                if subset.empty:\n",
    "                    ax2.text(0.5, 0.5, f'Not enough data for {label}', fontsize=12, ha='center', va='center')\n",
    "                    ax2.set_title(f'{label}: Counts of FS vs RS Neurons')\n",
    "                    ax2.axis('off')\n",
    "                else:\n",
    "                    cell_type_counts = subset['Cell_Type'].value_counts()\n",
    "                    fs_count = cell_type_counts.get('FS', 0)\n",
    "                    rs_count = cell_type_counts.get('RS', 0)\n",
    "                    ax2.bar(['FS', 'RS'], [fs_count, rs_count], color=['blue', 'red'], alpha=0.7, edgecolor='black')\n",
    "                    ax2.set_title(f'{label}: Counts of FS vs RS Neurons')\n",
    "                    ax2.set_ylabel('Count')\n",
    "                    ax2.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "                # Pie chart of FS vs RS proportions\n",
    "                ax3 = axes[i, 2]\n",
    "                if subset.empty:\n",
    "                    ax3.text(0.5, 0.5, f'Not enough data for {label}', fontsize=12, ha='center', va='center')\n",
    "                    ax3.set_title(f'{label}: Proportion of FS vs RS Neurons')\n",
    "                    ax3.axis('off')\n",
    "                else:\n",
    "                    values = [\n",
    "                        subset[subset['Cell_Type'] == 'FS'].shape[0],\n",
    "                        subset[subset['Cell_Type'] == 'RS'].shape[0]\n",
    "                    ]\n",
    "                    labels = ['FS', 'RS']\n",
    "                    ax3.pie(\n",
    "                        values,\n",
    "                        labels=labels,\n",
    "                        colors=['blue', 'red'],\n",
    "                        autopct='%1.1f%%',\n",
    "                        startangle=90,\n",
    "                        wedgeprops={'edgecolor': 'black'}\n",
    "                    )\n",
    "                    ax3.set_title(f'{label}: Proportion of FS vs RS Neurons')\n",
    "\n",
    "            # Adjust layout\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "            # Show plot\n",
    "            plt.show()\n",
    "            \n",
    "    def calculate_firing_rate(self, groupname=None, recordingname=None, cid=None):\n",
    "        \"\"\"\n",
    "        Calculates the firing rate for a specific unit identified by groupname, recordingname, and cid.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The unique cell identifier.\n",
    "\n",
    "        Returns:\n",
    "            float: Firing rate in spikes per second (Hz).\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required_columns = ['groupname', 'recordingname', 'cid', 'SpikeTimes_all', 'Recording_Duration', 'Sampling_Frequency']\n",
    "        for column in required_columns:\n",
    "            if column not in self.df.columns:\n",
    "                raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "        # Filter the DataFrame to find the target unit\n",
    "        unit_data = self.df[\n",
    "            (self.df['groupname'] == groupname) &\n",
    "            (self.df['recordingname'] == recordingname) &\n",
    "            (self.df['cid'] == cid)\n",
    "        ]\n",
    "\n",
    "        if unit_data.empty:\n",
    "            raise ValueError(f\"No data found for group '{groupname}', recording '{recordingname}', cid '{cid}'.\")\n",
    "\n",
    "        # Extract spike times and recording info\n",
    "        spike_times = unit_data['SpikeTimes_all'].iloc[0]\n",
    "        recording_duration_samples = unit_data['Recording_Duration'].iloc[0]\n",
    "        sampling_frequency_khz = unit_data['Sampling_Frequency'].iloc[0]\n",
    "\n",
    "        if not isinstance(spike_times, np.ndarray):\n",
    "            raise ValueError(\"Invalid spike times format. Expected a numpy array.\")\n",
    "\n",
    "        # Convert sampling frequency from kHz to Hz\n",
    "        sampling_frequency_hz = sampling_frequency_khz * 1000\n",
    "\n",
    "        # Calculate recording duration in seconds\n",
    "        recording_duration_seconds = recording_duration_samples / sampling_frequency_hz\n",
    "\n",
    "        # Calculate total number of spikes\n",
    "        total_spikes = len(spike_times)\n",
    "\n",
    "        # Calculate firing rate in spikes per second (Hz)\n",
    "        firing_rate_hz = total_spikes / recording_duration_seconds\n",
    "\n",
    "        return firing_rate_hz\n",
    "\n",
    "    def plot_raw_psth(self, groupname=None, recordingname=None, cid=None, bin_size=1.0):\n",
    "        \"\"\"\n",
    "        Plots the raw PSTH for a specific unit identified by groupname, recordingname, and cid.\n",
    "\n",
    "        Args:\n",
    "            groupname (str): The name of the group.\n",
    "            recordingname (str): The name of the recording.\n",
    "            cid (str): The unique cell identifier.\n",
    "            bin_size (float): The size of each bin in seconds. Default is 1.0 second.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Validate required columns\n",
    "        required_columns = ['groupname', 'recordingname', 'cid', 'SpikeTimes_all', 'Recording_Duration', 'Sampling_Frequency']\n",
    "        for column in required_columns:\n",
    "            if column not in self.df.columns:\n",
    "                raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "        # Filter the DataFrame to find the target unit\n",
    "        unit_data = self.df[\n",
    "            (self.df['groupname'] == groupname) &\n",
    "            (self.df['recordingname'] == recordingname) &\n",
    "            (self.df['cid'] == cid)\n",
    "        ]\n",
    "\n",
    "        if unit_data.empty:\n",
    "            raise ValueError(f\"No data found for group '{groupname}', recording '{recordingname}', cid '{cid}'.\")\n",
    "\n",
    "        # Extract spike times and recording info\n",
    "        spike_times = unit_data['SpikeTimes_all'].iloc[0]\n",
    "        recording_duration_seconds = unit_data['Recording_Duration'].iloc[0]\n",
    "        sampling_frequency_khz = unit_data['Sampling_Frequency'].iloc[0]\n",
    "\n",
    "        if not isinstance(spike_times, np.ndarray):\n",
    "            raise ValueError(\"Invalid spike times format. Expected a numpy array.\")\n",
    "\n",
    "        # Convert sampling frequency from kHz to Hz\n",
    "        sampling_frequency_hz = sampling_frequency_khz * 1000\n",
    "\n",
    "        # Convert spike times from samples to seconds\n",
    "        spike_times_seconds = spike_times / sampling_frequency_hz\n",
    "\n",
    "        # Generate PSTH\n",
    "        bin_edges = np.arange(0, recording_duration_seconds + bin_size, bin_size)\n",
    "        psth, _ = np.histogram(spike_times_seconds, bins=bin_edges)\n",
    "\n",
    "        # Plot PSTH\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(bin_edges[:-1], psth, width=bin_size, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel(f'Spike Count per {bin_size}s')\n",
    "        plt.title(f'Raw PSTH\\nGroup: {groupname}, Recording: {recordingname}, CID: {cid}')\n",
    "        plt.xlim(0, recording_duration_seconds)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming organoid_database is the cleaned DataFrame\n",
    "plotter = DataFrameManagerAxionMEA_Plotter(organoid_database)\n",
    "\n",
    "# Plot the mean waveforms for FS and RS neurons\n",
    "plotter.plot_mean_waveforms()\n",
    "plotter.plot_mean_waveforms_with_individual_traces()\n",
    "plotter.plot_cell_type_counts()\n",
    "plotter.plot_composite_per_recording()\n",
    "plotter.plot_composite_per_recording_with_sua_mua()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter.calculate_firing_rate(groupname='Organoids', recordingname='A1', cid='cid33') not used\n",
    "\n",
    "# plot with debug info\n",
    "plotter.plot_distribution_stairs(\"TroughToPeak_duration\", bins=None, plot_type=\"bar\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "\n",
    "def plot_psth_per_cid_with_metadata(df, bin_size_seconds=1.0, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plots PSTHs for each CID separately, including metadata and color-coded by cell type,\n",
    "    and saves them as a multipage PDF.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing spike data and metadata.\n",
    "        bin_size_seconds (float): Size of each bin in seconds. Default is 1.0 second.\n",
    "        output_dir (str): Directory to save the PDF. If None, raises an error.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate required columns\n",
    "    required_columns = ['groupname', 'recordingname', 'cid', 'Cell_Type', 'IsSingleUnit',\n",
    "                        'SpikeTimes_all', 'Recording_Duration', 'Sampling_Frequency']\n",
    "    for column in required_columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "    if not output_dir:\n",
    "        raise ValueError(\"You must specify an output directory for saving PDFs.\")\n",
    "\n",
    "    # Group by groupname and recordingname\n",
    "    grouped = df.groupby(['groupname', 'recordingname'])\n",
    "\n",
    "    for (groupname, recordingname), group_data in grouped:\n",
    "        # Create output directory\n",
    "        save_dir = os.path.join(output_dir, groupname)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f'{recordingname}_PSTHs.pdf')\n",
    "\n",
    "        # Use PdfPages to create a multipage PDF\n",
    "        with PdfPages(save_path) as pdf:\n",
    "            for _, row in group_data.iterrows():\n",
    "                # Extract metadata\n",
    "                cid = row['cid']\n",
    "                cell_type = row['Cell_Type']\n",
    "                is_single_unit = row['IsSingleUnit']\n",
    "                spike_times_samples = row['SpikeTimes_all']\n",
    "                recording_duration_seconds = row['Recording_Duration']\n",
    "                sampling_frequency_hz = row['Sampling_Frequency']\n",
    "\n",
    "                # Handle missing or invalid data\n",
    "                if not isinstance(spike_times_samples, np.ndarray):\n",
    "                    print(f\"Skipping CID {cid} due to missing spike data.\")\n",
    "                    continue\n",
    "\n",
    "                # Convert spike times to seconds\n",
    "                spike_times_seconds = spike_times_samples / sampling_frequency_hz\n",
    "\n",
    "                # Compute PSTH\n",
    "                bin_edges = np.arange(0, recording_duration_seconds + bin_size_seconds, bin_size_seconds)\n",
    "                psth, _ = np.histogram(spike_times_seconds, bins=bin_edges)\n",
    "\n",
    "                # Determine color based on Cell_Type\n",
    "                color = 'blue' if cell_type == 'FS' else 'red'\n",
    "\n",
    "                # Convert single-unit status to string\n",
    "                single_unit_str = 'SUA' if is_single_unit else 'MUA'\n",
    "\n",
    "                # Create a figure for this CID\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                ax.bar(bin_edges[:-1], psth, width=bin_size_seconds, edgecolor='black', alpha=0.7, color=color)\n",
    "                ax.set_title(f'CID: {cid} | Cell Type: {cell_type} | {single_unit_str}\\n'\n",
    "                             f'Recording: {recordingname}, Group: {groupname}', fontsize=10)\n",
    "                ax.set_xlabel('Time (s)', fontsize=8)\n",
    "                ax.set_ylabel('Spike Count', fontsize=8)\n",
    "                ax.set_xlim(0, recording_duration_seconds)\n",
    "                ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "                # Add the figure to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved PSTHs for recording '{recordingname}' in group '{groupname}' to {save_path}.\")\n",
    "\n",
    "# Example usage\n",
    "plot_psth_per_cid_with_metadata(\n",
    "    df=plotter.df, \n",
    "    bin_size_seconds=1.0, \n",
    "    output_dir='/Volumes/Manny2TB/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_psth_per_cid_with_metadata(df, bin_size_seconds=1.0, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plots PSTHs for each CID separately, including metadata and color-coded by cell type,\n",
    "    and saves them as a multipage PDF with mean firing rate, SD, and max spike count.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing spike data and metadata.\n",
    "        bin_size_seconds (float): Size of each bin in seconds. Default is 1.0 second.\n",
    "        output_dir (str): Directory to save the PDF. If None, raises an error.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate required columns\n",
    "    required_columns = ['groupname', 'recordingname', 'cid', 'Cell_Type', 'IsSingleUnit',\n",
    "                        'SpikeTimes_all', 'Recording_Duration', 'Sampling_Frequency']\n",
    "    for column in required_columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "    if not output_dir:\n",
    "        raise ValueError(\"You must specify an output directory for saving PDFs.\")\n",
    "\n",
    "    # Group by groupname and recordingname\n",
    "    grouped = df.groupby(['groupname', 'recordingname'])\n",
    "\n",
    "    for (groupname, recordingname), group_data in grouped:\n",
    "        # Create output directory\n",
    "        save_dir = os.path.join(output_dir, groupname)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f'{recordingname}_PSTHs_with_stats.pdf')\n",
    "\n",
    "        # Use PdfPages to create a multipage PDF\n",
    "        with PdfPages(save_path) as pdf:\n",
    "            for _, row in group_data.iterrows():\n",
    "                # Extract metadata\n",
    "                cid = row['cid']\n",
    "                cell_type = row['Cell_Type']\n",
    "                is_single_unit = row['IsSingleUnit']\n",
    "                spike_times_samples = row['SpikeTimes_all']\n",
    "                recording_duration_seconds = row['Recording_Duration']\n",
    "                sampling_frequency_hz = row['Sampling_Frequency']\n",
    "\n",
    "                # Handle missing or invalid data\n",
    "                if not isinstance(spike_times_samples, np.ndarray):\n",
    "                    print(f\"Skipping CID {cid} due to missing spike data.\")\n",
    "                    continue\n",
    "\n",
    "                # Convert spike times to seconds\n",
    "                spike_times_seconds = spike_times_samples / sampling_frequency_hz\n",
    "\n",
    "                # Compute PSTH\n",
    "                bin_edges = np.arange(0, recording_duration_seconds + bin_size_seconds, bin_size_seconds)\n",
    "                psth, _ = np.histogram(spike_times_seconds, bins=bin_edges)\n",
    "\n",
    "                # Compute statistics\n",
    "                mean_firing_rate = np.mean(psth) / bin_size_seconds  # Mean firing rate in Hz\n",
    "                sd_firing_rate = np.std(psth) / bin_size_seconds  # SD of firing rate in Hz\n",
    "                max_spike_count = np.max(psth)  # Max spike count across bins\n",
    "\n",
    "                # Determine color based on Cell_Type\n",
    "                color = 'blue' if cell_type == 'FS' else 'red'\n",
    "\n",
    "                # Convert single-unit status to string\n",
    "                single_unit_str = 'SUA' if is_single_unit else 'MUA'\n",
    "\n",
    "                # Create a figure for this CID\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                ax.bar(bin_edges[:-1], psth, width=bin_size_seconds, edgecolor='black', alpha=0.7, color=color, label='PSTH')\n",
    "                ax.axhline(y=mean_firing_rate * bin_size_seconds, color='green', linestyle='-', label=f'Mean ({mean_firing_rate:.2f} Hz)')\n",
    "                # Add a vertical line for the maximum spike count\n",
    "                max_bin_time = bin_edges[np.argmax(psth)]\n",
    "                ax.axvline(x=max_bin_time, color='purple', linestyle='--', label=f'Max Spike Count ({max_spike_count})')\n",
    "\n",
    "                # Set plot details\n",
    "                ax.set_title(f'CID: {cid} | Cell Type: {cell_type} | {single_unit_str}\\n'\n",
    "                             f'Recording: {recordingname}, Group: {groupname}\\n'\n",
    "                             f'Mean Firing Rate: {mean_firing_rate:.2f} Hz | SD: {sd_firing_rate:.2f} Hz | Max Spike Count: {max_spike_count}',\n",
    "                             fontsize=10)\n",
    "                ax.set_xlabel('Time (s)', fontsize=8)\n",
    "                ax.set_ylabel('Spike Count', fontsize=8)\n",
    "                ax.set_xlim(0, recording_duration_seconds)\n",
    "                ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "                # Add a legend\n",
    "                ax.legend(fontsize=8)\n",
    "\n",
    "                # Add the figure to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved PSTHs with statistics for recording '{recordingname}' in group '{groupname}' to {save_path}.\")\n",
    "\n",
    "            \n",
    "plot_psth_per_cid_with_metadata(\n",
    "    df=plotter.df, \n",
    "    bin_size_seconds=1.0, \n",
    "    output_dir='/Volumes/Manny2TB/'\n",
    ")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_line_psth_per_cid_with_metadata(df, bin_size_seconds=1.0, output_dir=None):\n",
    "    \"\"\"\n",
    "    Plots line plots of PSTHs for each CID separately, including metadata and color-coded by cell type,\n",
    "    and saves them as a multipage PDF with mean firing rate, SD, and max firing rate.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing spike data and metadata.\n",
    "        bin_size_seconds (float): Size of each bin in seconds. Default is 1.0 second.\n",
    "        output_dir (str): Directory to save the PDF. If None, raises an error.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Validate required columns\n",
    "    required_columns = ['groupname', 'recordingname', 'cid', 'Cell_Type', 'IsSingleUnit',\n",
    "                        'SpikeTimes_all', 'Recording_Duration', 'Sampling_Frequency']\n",
    "    for column in required_columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "    if not output_dir:\n",
    "        raise ValueError(\"You must specify an output directory for saving PDFs.\")\n",
    "\n",
    "    # Group by groupname and recordingname\n",
    "    grouped = df.groupby(['groupname', 'recordingname'])\n",
    "\n",
    "    for (groupname, recordingname), group_data in grouped:\n",
    "        # Create output directory\n",
    "        save_dir = os.path.join(output_dir, groupname)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f'{recordingname}_LinePlots_with_stats.pdf')\n",
    "\n",
    "        # Use PdfPages to create a multipage PDF\n",
    "        with PdfPages(save_path) as pdf:\n",
    "            for _, row in group_data.iterrows():\n",
    "                # Extract metadata\n",
    "                cid = row['cid']\n",
    "                cell_type = row['Cell_Type']\n",
    "                is_single_unit = row['IsSingleUnit']\n",
    "                spike_times_samples = row['SpikeTimes_all']\n",
    "                recording_duration_seconds = row['Recording_Duration']\n",
    "                sampling_frequency_hz = row['Sampling_Frequency']\n",
    "\n",
    "                # Handle missing or invalid data\n",
    "                if not isinstance(spike_times_samples, np.ndarray):\n",
    "                    print(f\"Skipping CID {cid} due to missing spike data.\")\n",
    "                    continue\n",
    "\n",
    "                # Convert spike times to seconds\n",
    "                spike_times_seconds = spike_times_samples / sampling_frequency_hz\n",
    "\n",
    "                # Compute PSTH\n",
    "                bin_edges = np.arange(0, recording_duration_seconds + bin_size_seconds, bin_size_seconds)\n",
    "                psth, _ = np.histogram(spike_times_seconds, bins=bin_edges)\n",
    "                time_bins = bin_edges[:-1] + bin_size_seconds / 2  # Midpoints of bins\n",
    "\n",
    "                # Compute statistics\n",
    "                mean_firing_rate = np.mean(psth) / bin_size_seconds  # Mean firing rate in Hz\n",
    "                sd_firing_rate = np.std(psth) / bin_size_seconds  # SD of firing rate in Hz\n",
    "                max_firing_rate = np.max(psth) / bin_size_seconds  # Max firing rate in Hz\n",
    "                max_firing_rate_time = time_bins[np.argmax(psth)]  # Time of max firing rate\n",
    "\n",
    "                # Determine color based on Cell_Type\n",
    "                color = 'blue' if cell_type == 'FS' else 'red'\n",
    "\n",
    "                # Convert single-unit status to string\n",
    "                single_unit_str = 'SUA' if is_single_unit else 'MUA'\n",
    "\n",
    "                # Create a figure for this CID\n",
    "                fig, ax = plt.subplots(figsize=(8, 6))\n",
    "                ax.plot(time_bins, psth / bin_size_seconds, color=color, label='Firing Rate (Hz)', linewidth=1.5)\n",
    "                ax.axhline(y=mean_firing_rate, color='green', linestyle='-', label=f'Mean ({mean_firing_rate:.2f} Hz)')\n",
    "                # Add a vertical line for the maximum firing rate\n",
    "                ax.axvline(x=max_firing_rate_time, color='purple', linestyle='--', label=f'Max Rate ({max_firing_rate:.2f} Hz)')\n",
    "\n",
    "                # Set plot details\n",
    "                ax.set_title(f'CID: {cid} | Cell Type: {cell_type} | {single_unit_str}\\n'\n",
    "                             f'Recording: {recordingname}, Group: {groupname}\\n'\n",
    "                             f'Mean Firing Rate: {mean_firing_rate:.2f} Hz | SD: {sd_firing_rate:.2f} Hz | Max Rate: {max_firing_rate:.2f} Hz',\n",
    "                             fontsize=10)\n",
    "                ax.set_xlabel('Time (s)', fontsize=8)\n",
    "                ax.set_ylabel('Firing Rate (Hz)', fontsize=8)\n",
    "                ax.set_xlim(0, recording_duration_seconds)\n",
    "                ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "                # Add a legend\n",
    "                ax.legend(fontsize=8)\n",
    "\n",
    "                # Add the figure to the PDF\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "\n",
    "            print(f\"Saved line plots with statistics for recording '{recordingname}' in group '{groupname}' to {save_path}.\")\n",
    "\n",
    "          \n",
    "plot_line_psth_per_cid_with_metadata(\n",
    "    df=plotter.df, \n",
    "    bin_size_seconds=1.0, \n",
    "    output_dir='/Volumes/Manny2TB/'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_psth_trace_to_df(df, bin_size_seconds=1.0):\n",
    "    \"\"\"\n",
    "    Computes the PSTH trace for each row in the DataFrame and adds it as a new column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing spike data and metadata.\n",
    "        bin_size_seconds (float): Size of each bin in seconds. Default is 1.0 second.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with a new column 'PSTH_Trace' containing the PSTH arrays.\n",
    "    \"\"\"\n",
    "    # Validate required columns\n",
    "    required_columns = ['SpikeTimes_all', 'Recording_Duration', 'Sampling_Frequency']\n",
    "    for column in required_columns:\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"The DataFrame must contain the '{column}' column.\")\n",
    "\n",
    "    # Function to compute the PSTH trace\n",
    "    def compute_psth_trace(row):\n",
    "        spike_times_samples = row['SpikeTimes_all']\n",
    "        recording_duration_seconds = row['Recording_Duration']\n",
    "        sampling_frequency_hz = row['Sampling_Frequency']\n",
    "\n",
    "        # Handle missing or invalid data\n",
    "        if not isinstance(spike_times_samples, np.ndarray):\n",
    "            return np.array([])  # Return an empty array if data is missing\n",
    "\n",
    "        # Convert spike times to seconds\n",
    "        spike_times_seconds = spike_times_samples / sampling_frequency_hz\n",
    "\n",
    "        # Compute PSTH\n",
    "        bin_edges = np.arange(0, recording_duration_seconds + bin_size_seconds, bin_size_seconds)\n",
    "        psth, _ = np.histogram(spike_times_seconds, bins=bin_edges)\n",
    "\n",
    "        # Normalize to firing rate (Hz)\n",
    "        return psth / bin_size_seconds\n",
    "\n",
    "    # Apply the function to compute PSTH traces\n",
    "    df['PSTH_Trace'] = df.apply(compute_psth_trace, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "add_psth_trace_to_df(df=plotter.df, bin_size_seconds=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_firing_rate_vs_amplitude(df, firing_rate_metric='mean', output_dir=None):\n",
    "    \"\"\"\n",
    "    Plots the specified firing rate metric (mean or max) against the amplitude of neurons,\n",
    "    grouped by FS or RS units, and saves as a multipage PDF.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the PSTH and amplitude data.\n",
    "        firing_rate_metric (str): Metric to plot, either 'mean' or 'max'. Default is 'mean'.\n",
    "        output_dir (str): Directory to save the PDF. Must be specified.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    import numpy as np\n",
    "\n",
    "    # Validate inputs\n",
    "    if firing_rate_metric not in ['mean', 'max']:\n",
    "        raise ValueError(\"firing_rate_metric must be either 'mean' or 'max'.\")\n",
    "\n",
    "    if not output_dir:\n",
    "        raise ValueError(\"You must specify an output directory to save the PDF.\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create the file name dynamically\n",
    "    file_name = f\"{firing_rate_metric}_firing_rate_vs_amplitude.pdf\"\n",
    "    output_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    # Colors for FS and RS units\n",
    "    colors = {'FS': 'blue', 'RS': 'red'}\n",
    "\n",
    "    # Create a multipage PDF\n",
    "    with PdfPages(output_path) as pdf:\n",
    "        for cell_type in ['FS', 'RS']:\n",
    "            # Filter the dataframe for the specific cell type\n",
    "            cell_type_df = df[df['Cell_Type'] == cell_type].copy()  # Use `.copy()` to avoid warning\n",
    "\n",
    "            if cell_type_df.empty:\n",
    "                print(f\"No data available for cell type {cell_type}. Skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Compute the desired firing rate metric\n",
    "            if firing_rate_metric == 'mean':\n",
    "                cell_type_df.loc[:, 'Firing_Rate_Metric'] = cell_type_df['PSTH_Trace'].apply(lambda x: np.mean(x))\n",
    "            else:  # firing_rate_metric == 'max'\n",
    "                cell_type_df.loc[:, 'Firing_Rate_Metric'] = cell_type_df['PSTH_Trace'].apply(lambda x: np.max(x))\n",
    "\n",
    "            # Create the plot\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            ax.scatter(cell_type_df['Amplitude'], cell_type_df['Firing_Rate_Metric'], \n",
    "                       color=colors[cell_type], alpha=0.7, label=cell_type)\n",
    "            ax.set_title(f\"{firing_rate_metric.capitalize()} Firing Rate vs Amplitude ({cell_type})\", fontsize=12)\n",
    "            ax.set_xlabel('Amplitude (uV), Absoulte Value Trough to Peak', fontsize=10)\n",
    "            ax.set_ylabel(f'{firing_rate_metric.capitalize()} Firing Rate (Hz)', fontsize=10)\n",
    "            ax.grid(True, linestyle='--', alpha=0.6)\n",
    "            ax.legend(fontsize=10)\n",
    "\n",
    "            # Save the plot to the PDF\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(f\"Saved plots to {output_path}.\")\n",
    "    \n",
    "plot_firing_rate_vs_amplitude(df=plotter.df, firing_rate_metric='mean', output_dir='/Volumes/Manny2TB/')\n",
    "plot_firing_rate_vs_amplitude(df=plotter.df, firing_rate_metric='max', output_dir='/Volumes/Manny2TB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique types in each column\n",
    "for column in plotter.df.columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    print(organoid_database[column].apply(type).value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combing MEAs Data section to compare BIC vs No BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_organoid_database(organoid, columns=None, dataframe_name='basic_metrics'):\n",
    "    \"\"\"\n",
    "    Creates and returns a basic metrics DataFrame for organoid electrophysiology data.\n",
    "    \n",
    "    Parameters:\n",
    "        organoid: The organoid data object (e.g., returned from ExtractEphysData)\n",
    "        columns (list of str, optional): List of column names to include in the DataFrame.\n",
    "            If not provided, a default list of columns will be used.\n",
    "        dataframe_name (str, optional): The key name for the created DataFrame in the manager.\n",
    "            Defaults to 'basic_metrics'.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame containing the specified electrophysiology data.\n",
    "    \"\"\"\n",
    "    # Default columns if none are provided\n",
    "    if columns is None:\n",
    "        columns = [\n",
    "            'Cell_Type', 'IsSingleUnit', 'Amplitude',\n",
    "            'SpikeTimes_all', 'Template_Channel', \n",
    "            'Normalized_Template_Waveform', 'TroughToPeak_duration', \n",
    "            'SpikeHalfWidth', 'UnNormalized_Template_Waveform', 'ISI_violations_percent', \n",
    "            'Recording_Duration', 'Sampling_Frequency'\n",
    "        ]\n",
    "    \n",
    "    # Create the DataFrameManager for the organoid object\n",
    "    manager = DataFrameManagerAxionMEA(organoid)\n",
    "    # Create the DataFrame using the provided columns and name\n",
    "    manager.create_dataframe(columns, dataframe_name)\n",
    "    \n",
    "    # Return the created DataFrame\n",
    "    return manager.dataframes[dataframe_name]\n",
    "\n",
    "\n",
    "def combine_organoid_databases(organoid1, organoid2, groupname1, groupname2, columns=None, dataframe_name='basic_metrics'):\n",
    "    \"\"\"\n",
    "    Combines two organoid data instances into a single DataFrame with a 'groupname' column.\n",
    "    \n",
    "    Parameters:\n",
    "        organoid1: The first organoid data instance (e.g., returned from ExtractEphysData).\n",
    "        organoid2: The second organoid data instance.\n",
    "        groupname1: The group name to assign to the first instance (e.g., 'Organoids_NoBic').\n",
    "        groupname2: The group name to assign to the second instance (e.g., 'Organoids_BIc').\n",
    "        columns (list of str, optional): List of columns to include in the DataFrame.\n",
    "        dataframe_name (str, optional): The key/name for the DataFrame in the DataFrame manager.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A combined DataFrame containing data from both organoid instances,\n",
    "                          with a 'groupname' column indicating the source group.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the first organoid instance\n",
    "    df1 = create_organoid_database(organoid1, columns, dataframe_name)\n",
    "    # Create a DataFrame from the second organoid instance\n",
    "    df2 = create_organoid_database(organoid2, columns, dataframe_name)\n",
    "    \n",
    "    # Assign the group names to each DataFrame\n",
    "    df1['groupname'] = groupname1\n",
    "    df2['groupname'] = groupname2\n",
    "    \n",
    "    # Combine the two DataFrames into one\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Extract two organoid data instances from different files\n",
    "organoid1 = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid_nobic/SpikeStuff/', 'all_data_MEA_final_05_nobic.mat')\n",
    "organoid2 = ExtractEphysData('/Volumes/Manny2TB/axion_mea_data_organoid_bic/SpikeStuff/', 'all_data_MEA_final_05_bic.mat')\n",
    "\n",
    "# Combine the two databases, assigning appropriate group names\n",
    "combined_organoid_database = combine_organoid_databases(\n",
    "    organoid1, \n",
    "    organoid2, \n",
    "    groupname1='Organoids_NoBic', \n",
    "    groupname2='Organoids_Bic'\n",
    ")\n",
    "\n",
    "# Optionally, export the combined database to a CSV file\n",
    "combined_organoid_database.to_csv('/Volumes/Manny2TB/axion_mea_data_organoid/SpikeStuff/organoid_database_merged_bicvsnobic.csv', index=False)\n",
    "\n",
    "# Now, you can use combined_organoid_database for further group-level comparisons\n",
    "combined_organoid_database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiketurnpike_postanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
