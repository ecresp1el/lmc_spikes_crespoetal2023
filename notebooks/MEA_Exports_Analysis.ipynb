{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEA Exports Analysis — Spikes + Waveforms\n",
    "\n",
    "Analyze and aggregate the per-pair exports produced by the Pair Viewer or the batch exporter.\n",
    "\n",
    "- Exports location: `<output_root>/exports/spikes_waveforms/...`\n",
    "- Format details: see `docs/exports_spikes_waveforms.md` in this repo (variable types, dataset names, and attributes).\n",
    "- This notebook auto-discovers pairs, loads HDF5/CSV per pair, and can aggregate per-plate.\n",
    "\n",
    "Tip: If your external drive is not mounted, set `OUTPUT_ROOT` to `_mcs_mea_outputs_local`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-Based Analyzer (Baseline FR)\n",
    "Define a reusable analyzer class to discover exported pairs, compute pre/post firing rates, and plot baseline (pre-chem) FR distributions per pair and combined. Waveforms are intentionally ignored here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "try:\n",
    "    from scipy import signal\n",
    "except Exception as e:\n",
    "    raise RuntimeError('This section requires scipy installed.') from e\n",
    "\n",
    "def _resolve_exports_dir() -> Path:\n",
    "    try:\n",
    "        from mcs_mea_analysis.config import CONFIG\n",
    "        REPO_ROOT = next((p for p in [Path.cwd(), *Path.cwd().parents] if (p/'mcs_mea_analysis').exists()), Path.cwd())\n",
    "        output_root = CONFIG.output_root if CONFIG.output_root.exists() else (REPO_ROOT / '_mcs_mea_outputs_local')\n",
    "        return output_root / 'exports' / 'spikes_waveforms'\n",
    "    except Exception:\n",
    "        return Path('/Volumes/Manny2TB/mcs_mea_outputs/exports/spikes_waveforms')\n",
    "\n",
    "def _parse_bounds_attr(attr_val) -> Tuple[float, float]:\n",
    "    try:\n",
    "        if isinstance(attr_val, (bytes, bytearray)):\n",
    "            attr_val = attr_val.decode()\n",
    "        d = json.loads(attr_val) if isinstance(attr_val, str) else attr_val\n",
    "        return float(d.get('t0', 0.0)), float(d.get('t1', 0.0))\n",
    "    except Exception:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "def _noise_level(y: np.ndarray, method: str, pctl: float) -> float:\n",
    "    if y.size == 0:\n",
    "        return np.nan\n",
    "    if method == 'mad':\n",
    "        med = np.median(y)\n",
    "        return 1.4826 * np.median(np.abs(y - med))\n",
    "    if method == 'rms':\n",
    "        return float(np.sqrt(np.mean(np.square(y))))\n",
    "    if method == 'pctl':\n",
    "        med = np.median(y)\n",
    "        return float(np.percentile(np.abs(y - med), pctl))\n",
    "    return np.nan\n",
    "\n",
    "def _detect_pre_post(t: np.ndarray, y: np.ndarray, sr_hz: float,\n",
    "                     t0b: float, t1b: float, t0a: float, t1a: float, dcfg: dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    # Masks\n",
    "    mb = (t >= t0b) & (t <= t1b)\n",
    "    ma = (t >= t0a) & (t <= t1a)\n",
    "    yb = y[mb]\n",
    "    noise = _noise_level(yb, str(dcfg.get('noise','mad')), float(dcfg.get('noise_percentile',68.0)))\n",
    "    if not np.isfinite(noise) or noise <= 0:\n",
    "        return np.empty(0), np.empty(0), np.empty(0), np.empty(0)\n",
    "    thr = float(dcfg.get('K', 5.0)) * noise\n",
    "    dist = max(1, int(round(float(dcfg.get('refractory_ms',1.0)) * 1e-3 * sr_hz)))\n",
    "    minw = max(1, int(round(float(dcfg.get('min_width_ms',0.3)) * 1e-3 * sr_hz)))\n",
    "    pol = str(dcfg.get('polarity','neg'))\n",
    "    # Inner detect\n",
    "    def _detect(mask):\n",
    "        ta = t[mask]; ya = y[mask]\n",
    "        if ta.size == 0:\n",
    "            return np.empty(0), np.empty(0)\n",
    "        arr = -ya if pol in ('neg','both') else ya\n",
    "        peaks, _ = signal.find_peaks(arr, height=thr, distance=dist, width=minw)\n",
    "        widths, _, _, _ = signal.peak_widths(arr, peaks, rel_height=0.5) if peaks.size else (np.empty(0),)*4\n",
    "        st = ta[peaks] if peaks.size else np.empty(0)\n",
    "        w_ms = (widths / float(sr_hz)) * 1000.0 if widths.size else np.empty(0)\n",
    "        return st, w_ms\n",
    "    st_pre, w_pre = _detect(mb)\n",
    "    st_post, w_post = _detect(ma)\n",
    "    return st_pre, w_pre, st_post, w_post\n",
    "\n",
    "@dataclass\n",
    "class PairExportsAnalyzer:\n",
    "    exports_dir: Path = _resolve_exports_dir()\n",
    "\n",
    "    def discover_pairs(self) -> pd.DataFrame:\n",
    "        pairs = []\n",
    "        for h5 in sorted(self.exports_dir.rglob('*.h5')):\n",
    "            if h5.name.endswith('_summary.h5'):\n",
    "                continue\n",
    "            try:\n",
    "                round_name = h5.parents[1].name if len(h5.parents) > 1 else None\n",
    "                plate = None\n",
    "                try:\n",
    "                    ps = h5.parent.name\n",
    "                    plate = int(ps.replace('plate_', '')) if ps.startswith('plate_') else None\n",
    "                except Exception:\n",
    "                    pass\n",
    "                pairs.append({'pair_id': h5.stem, 'h5_path': str(h5), 'round': round_name, 'plate': plate})\n",
    "            except Exception:\n",
    "                continue\n",
    "        return pd.DataFrame(pairs).sort_values(['plate','round','pair_id']).reset_index(drop=True)\n",
    "\n",
    "    def fr_prepost_for_pair(self, h5_path: Path) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        with h5py.File(h5_path.as_posix(),'r') as f:\n",
    "            dcfg = {}\n",
    "            if 'detect_config_json' in f:\n",
    "                try:\n",
    "                    dcfg = json.loads(f['detect_config_json'][()].decode())\n",
    "                except Exception:\n",
    "                    dcfg = {}\n",
    "            for side in ('CTZ','VEH'):\n",
    "                if side not in f: continue\n",
    "                g = f[side]\n",
    "                sr = float(g.attrs.get('sr_hz', 0.0)) or 10000.0\n",
    "                t0b, t1b = _parse_bounds_attr(g.attrs.get('baseline_bounds', '{\"t0\":0,\"t1\":0}'))\n",
    "                t0a, t1a = _parse_bounds_attr(g.attrs.get('analysis_bounds', '{\"t0\":0,\"t1\":0}'))\n",
    "                chs = sorted(int(k[2:4]) for k in g.keys() if k.startswith('ch') and k.endswith('_time'))\n",
    "                for ch in chs:\n",
    "                    t = g[f'ch{ch:02d}_time'][:] if f'ch{ch:02d}_time' in g else np.empty(0)\n",
    "                    yf = g[f'ch{ch:02d}_filtered'][:] if f'ch{ch:02d}_filtered' in g else np.empty(0)\n",
    "                    if t.size == 0 or yf.size == 0: continue\n",
    "                    st_pre, _, st_post, _ = _detect_pre_post(t, yf, sr, t0b, t1b, t0a, t1a, dcfg)\n",
    "                    dur_pre = max(1e-9, (t1b - t0b)); dur_post = max(1e-9, (t1a - t0a))\n",
    "                    fr_pre = (st_pre.size / dur_pre) if dur_pre > 0 else np.nan\n",
    "                    fr_post = (st_post.size / dur_post) if dur_post > 0 else np.nan\n",
    "                    rows.append({'side': side, 'channel': ch, 'period': 'pre', 'fr_hz': fr_pre})\n",
    "                    rows.append({'side': side, 'channel': ch, 'period': 'post', 'fr_hz': fr_post})\n",
    "        df = pd.DataFrame(rows)\n",
    "        return df\n",
    "\n",
    "    def compute_all(self, pairs_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        frames = []\n",
    "        for _, r in pairs_df.iterrows():\n",
    "            h5p = Path(r['h5_path'])\n",
    "            d = self.fr_prepost_for_pair(h5p)\n",
    "            if not d.empty:\n",
    "                d['pair_id'] = r['pair_id']; d['plate'] = r['plate']; d['round'] = r['round']\n",
    "                frames.append(d)\n",
    "        return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=['pair_id','plate','round','side','channel','period','fr_hz'])\n",
    "\n",
    "    def plot_baseline_pair(self, df: pd.DataFrame, pair_id: str) -> None:\n",
    "        sub = df[(df['pair_id']==pair_id) & (df['period']=='pre')].copy()\n",
    "        if sub.empty:\n",
    "            print('No baseline data for', pair_id); return\n",
    "        plt.figure(figsize=(6,4))\n",
    "        if sns is not None:\n",
    "            sns.boxplot(data=sub, x='side', y='fr_hz')\n",
    "            sns.stripplot(data=sub, x='side', y='fr_hz', color='k', size=2, alpha=0.3)\n",
    "        else:\n",
    "            groups = [sub[sub['side']=='CTZ']['fr_hz'].dropna(), sub[sub['side']=='VEH']['fr_hz'].dropna()]\n",
    "            plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "        plt.title(f'Baseline FR (Hz) — {pair_id}')\n",
    "        plt.ylabel('FR (Hz)'); plt.xlabel('')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_baseline_all(self, df: pd.DataFrame) -> None:\n",
    "        sub = df[df['period']=='pre'].copy()\n",
    "        if sub.empty:\n",
    "            print('No baseline data.'); return\n",
    "        plt.figure(figsize=(6,4))\n",
    "        if sns is not None:\n",
    "            sns.boxplot(data=sub, x='side', y='fr_hz')\n",
    "            sns.stripplot(data=sub, x='side', y='fr_hz', color='k', size=2, alpha=0.3)\n",
    "        else:\n",
    "            groups = [sub[sub['side']=='CTZ']['fr_hz'].dropna(), sub[sub['side']=='VEH']['fr_hz'].dropna()]\n",
    "            plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "        plt.title('Baseline FR (Hz) — Combined Across Pairs')\n",
    "        plt.ylabel('FR (Hz)'); plt.xlabel('')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_baseline_pair_means(self, df: pd.DataFrame) -> None:\n",
    "        sub = df[df['period']=='pre'].copy()\n",
    "        if sub.empty:\n",
    "            print('No baseline data.'); return\n",
    "        means = (sub.groupby(['pair_id','side'], as_index=False)['fr_hz'].mean()\n",
    "                     .rename(columns={'fr_hz':'mean_fr_hz'}))\n",
    "        plt.figure(figsize=(6,4))\n",
    "        if sns is not None:\n",
    "            sns.boxplot(data=means, x='side', y='mean_fr_hz')\n",
    "            sns.stripplot(data=means, x='side', y='mean_fr_hz', color='k', size=3, alpha=0.5)\n",
    "        else:\n",
    "            groups = [means[means['side']=='CTZ']['mean_fr_hz'].dropna(), means[means['side']=='VEH']['mean_fr_hz'].dropna()]\n",
    "            plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "        plt.title('Baseline FR (Hz) — Per-Pair Means')\n",
    "        plt.ylabel('Mean FR (Hz)'); plt.xlabel('')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example — computes baseline and post FR, then plots baseline per pair and combined\n",
    "an = PairExportsAnalyzer()\n",
    "pairs_df = an.discover_pairs()\n",
    "print('Discovered pairs:', len(pairs_df))\n",
    "display(pairs_df.head())\n",
    "# Limit to first 3 pairs if desired (or use all)\n",
    "sel = pairs_df.iloc[:3] if len(pairs_df) > 3 else pairs_df\n",
    "fr_df = an.compute_all(sel)\n",
    "print('Computed rows:', len(fr_df))\n",
    "# Plot combined baseline across selected pairs\n",
    "an.plot_baseline_all(fr_df)\n",
    "# Plot baseline per pair\n",
    "for pid in sel['pair_id']:\n",
    "    an.plot_baseline_pair(fr_df, pid)\n",
    "# Plot per-pair mean baseline FR distributions\n",
    "an.plot_baseline_pair_means(fr_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre/Post FR and Spike Durations\n",
    "Compute baseline (pre-chem) and analysis (post-chem) firing rates and spike durations per channel for CTZ and VEH across all exported pairs.\n",
    "This section re-detects spikes from the exported filtered traces to obtain pre and post metrics in a consistent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained pre/post analysis (no dependency on df_pairs)\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "# Resolve exports directory (reuse if defined, else infer)\n",
    "try:\n",
    "    EXPORTS_DIR\n",
    "except NameError:\n",
    "    try:\n",
    "        from mcs_mea_analysis.config import CONFIG\n",
    "        REPO_ROOT = next((p for p in [Path.cwd(), *Path.cwd().parents] if (p/'mcs_mea_analysis').exists()), Path.cwd())\n",
    "        OUTPUT_ROOT = CONFIG.output_root if CONFIG.output_root.exists() else (REPO_ROOT / '_mcs_mea_outputs_local')\n",
    "        EXPORTS_DIR = OUTPUT_ROOT / 'exports' / 'spikes_waveforms'\n",
    "    except Exception:\n",
    "        EXPORTS_DIR = Path('/Volumes/Manny2TB/mcs_mea_outputs/exports/spikes_waveforms')\n",
    "print('Exports dir ->', EXPORTS_DIR)\n",
    "\n",
    "def _noise_level(y: np.ndarray, method: str, pctl: float) -> float:\n",
    "    if y.size == 0:\n",
    "        return np.nan\n",
    "    if method == 'mad':\n",
    "        med = np.median(y)\n",
    "        return 1.4826 * np.median(np.abs(y - med))\n",
    "    if method == 'rms':\n",
    "        return float(np.sqrt(np.mean(np.square(y))))\n",
    "    if method == 'pctl':\n",
    "        med = np.median(y)\n",
    "        return float(np.percentile(np.abs(y - med), pctl))\n",
    "    return np.nan\n",
    "\n",
    "def detect_with_widths(t: np.ndarray, y: np.ndarray, sr_hz: float, t0b: float, t1b: float, t0a: float, t1a: float, dcfg: dict) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    from scipy import signal\n",
    "    # masks\n",
    "    mb = (t >= t0b) & (t <= t1b)\n",
    "    ma = (t >= t0a) & (t <= t1a)\n",
    "    yb = y[mb]\n",
    "    noise = _noise_level(yb, str(dcfg.get('noise','mad')), float(dcfg.get('noise_percentile',68.0)))\n",
    "    if not np.isfinite(noise) or noise <= 0:\n",
    "        return np.empty(0), np.empty(0), np.empty(0)\n",
    "    thr = float(dcfg.get('K', 5.0)) * noise\n",
    "    dist = max(1, int(round(float(dcfg.get('refractory_ms',1.0)) * 1e-3 * sr_hz)))\n",
    "    minw = max(1, int(round(float(dcfg.get('min_width_ms',0.3)) * 1e-3 * sr_hz)))\n",
    "    pol = str(dcfg.get('polarity','neg'))\n",
    "    # analysis slice\n",
    "    ta = t[ma]; ya = y[ma]\n",
    "    if ta.size == 0:\n",
    "        return np.empty(0), np.empty(0), np.empty(0)\n",
    "    # peak finding on analysis segment\n",
    "    if pol in ('neg','both'):\n",
    "        arr = -ya\n",
    "    else:\n",
    "        arr = ya\n",
    "    peaks, props = signal.find_peaks(arr, height=thr, distance=dist, width=minw)\n",
    "    # full-width at half prominence in samples\n",
    "    if peaks.size:\n",
    "        widths, _, _, _ = signal.peak_widths(arr, peaks, rel_height=0.5)\n",
    "    else:\n",
    "        widths = np.empty(0)\n",
    "    # convert peak indices back to absolute time via ta\n",
    "    spike_times = ta[peaks] if peaks.size else np.empty(0)\n",
    "    width_ms = (widths / float(sr_hz)) * 1000.0 if widths.size else np.empty(0)\n",
    "    return spike_times, width_ms, np.asarray([thr], dtype=float)\n",
    "\n",
    "# Iterate all H5 exports and compute pre/post metrics\n",
    "rows_fr = []  # per-channel FR rows\n",
    "rows_dur = [] # per-spike duration rows\n",
    "for h5 in EXPORTS_DIR.rglob('*.h5'):\n",
    "    if h5.name.endswith('_summary.h5'):\n",
    "        continue\n",
    "    try:\n",
    "        round_name = h5.parents[1].name if len(h5.parents) > 1 else None\n",
    "        plate = None\n",
    "        try:\n",
    "            ps = h5.parent.name; plate = int(ps.replace('plate_', '')) if ps.startswith('plate_') else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        base = h5.stem\n",
    "        pair_id = base\n",
    "        with h5py.File(h5.as_posix(),'r') as f:\n",
    "            # parse detect config\n",
    "            dcfg = {}\n",
    "            if 'detect_config_json' in f:\n",
    "                try:\n",
    "                    dcfg = json.loads(f['detect_config_json'][()].decode())\n",
    "                except Exception:\n",
    "                    dcfg = {}\n",
    "            for side in ('CTZ','VEH'):\n",
    "                if side not in f: continue\n",
    "                g = f[side]\n",
    "                sr = float(g.attrs.get('sr_hz',0.0)) or 10000.0\n",
    "                # window bounds\n",
    "                def _parse_bounds(x):\n",
    "                    try:\n",
    "                        if isinstance(x, (bytes, bytearray)):\n",
    "                            x = x.decode()\n",
    "                        d = json.loads(x) if isinstance(x, str) else x\n",
    "                        return float(d.get('t0',0.0)), float(d.get('t1',0.0))\n",
    "                    except Exception:\n",
    "                        return 0.0, 0.0\n",
    "                t0b, t1b = _parse_bounds(g.attrs.get('baseline_bounds','{"t0":0,"t1":0}'))\n",
    "                t0a, t1a = _parse_bounds(g.attrs.get('analysis_bounds','{"t0":0,"t1":0}'))\n",
    "                # channel list by detecting *_time datasets\n",
    "                chs = sorted(int(k[2:4]) for k in g.keys() if k.startswith('ch') and k.endswith('_time'))\n",
    "                for ch in chs:\n",
    "                    t = g[f'ch{ch:02d}_time'][:] if f'ch{ch:02d}_time' in g else np.empty(0)\n",
    "                    yf = g[f'ch{ch:02d}_filtered'][:] if f'ch{ch:02d}_filtered' in g else np.empty(0)\n",
    "                    if t.size == 0 or yf.size == 0:\n",
    "                        continue\n",
    "                    # detect pre and post\n",
    "                    st_pre, w_pre_ms, _ = detect_with_widths(t, yf, sr, t0b, t1b, t0b, t1b, dcfg)\n",
    "                    st_post, w_post_ms, _ = detect_with_widths(t, yf, sr, t0b, t1b, t0a, t1a, dcfg)\n",
    "                    # FRs\n",
    "                    dur_pre = max(1e-9, (t1b - t0b))\n",
    "                    dur_post = max(1e-9, (t1a - t0a))\n",
    "                    fr_pre = (st_pre.size / dur_pre) if dur_pre > 0 else np.nan\n",
    "                    fr_post = (st_post.size / dur_post) if dur_post > 0 else np.nan\n",
    "                    rows_fr.append({'pair_id': pair_id, 'plate': plate, 'round': round_name, 'side': side, 'channel': ch, 'period': 'pre', 'fr_hz': fr_pre})\n",
    "                    rows_fr.append({'pair_id': pair_id, 'plate': plate, 'round': round_name, 'side': side, 'channel': ch, 'period': 'post', 'fr_hz': fr_post})\n",
    "                    # Durations per spike (event-level)\n",
    "                    for dms in w_pre_ms:\n",
    "                        rows_dur.append({'pair_id': pair_id, 'plate': plate, 'round': round_name, 'side': side, 'channel': ch, 'period': 'pre', 'width_ms': float(dms)})\n",
    "                    for dms in w_post_ms:\n",
    "                        rows_dur.append({'pair_id': pair_id, 'plate': plate, 'round': round_name, 'side': side, 'channel': ch, 'period': 'post', 'width_ms': float(dms)})\n",
    "    except Exception as e:\n",
    "        print('Skip H5', h5.name, ':', e)\n",
    "\n",
    "fr_data = pd.DataFrame(rows_fr) if rows_fr else pd.DataFrame(columns=['pair_id','plate','round','side','channel','period','fr_hz'])\n",
    "dur_data = pd.DataFrame(rows_dur) if rows_dur else pd.DataFrame(columns=['pair_id','plate','round','side','channel','period','width_ms'])\n",
    "print('FR rows:', len(fr_data), '| Duration rows:', len(dur_data))\n",
    "\n",
    "# Nested box plots: FR pre vs post by side\n",
    "if not fr_data.empty:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    if sns is not None:\n",
    "        sns.boxplot(data=fr_data, x='side', y='fr_hz', hue='period')\n",
    "        sns.stripplot(data=fr_data, x='side', y='fr_hz', hue='period', dodge=True, color='k', size=2, alpha=0.3)\n",
    "        plt.legend_.remove() if hasattr(plt, 'legend_') else None\n",
    "    else:\n",
    "        for i, side in enumerate(['CTZ','VEH']):\n",
    "            grp = [fr_data[(fr_data.side==side)&(fr_data.period=='pre')]['fr_hz'].dropna(),\n",
    "                   fr_data[(fr_data.side==side)&(fr_data.period=='post')]['fr_hz'].dropna()]\n",
    "            plt.boxplot(grp, positions=[i*3+1, i*3+2], labels=[f'{side}-pre', f'{side}-post'])\n",
    "    plt.title('FR (Hz) — Pre vs Post by Treatment')\n",
    "    plt.ylabel('FR (Hz)')\n",
    "    plt.show()\n",
    "\n",
    "# Nested box plots: Spike duration (ms) pre vs post by side\n",
    "if not dur_data.empty:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    if sns is not None:\n",
    "        sns.boxplot(data=dur_data, x='side', y='width_ms', hue='period')\n",
    "        sns.stripplot(data=dur_data, x='side', y='width_ms', hue='period', dodge=True, color='k', size=2, alpha=0.3)\n",
    "        plt.legend_.remove() if hasattr(plt, 'legend_') else None\n",
    "    else:\n",
    "        for i, side in enumerate(['CTZ','VEH']):\n",
    "            grp = [dur_data[(dur_data.side==side)&(dur_data.period=='pre')]['width_ms'].dropna(),\n",
    "                   dur_data[(dur_data.side==side)&(dur_data.period=='post')]['width_ms'].dropna()]\n",
    "            plt.boxplot(grp, positions=[i*3+1, i*3+2], labels=[f'{side}-pre', f'{side}-post'])\n",
    "    plt.title('Spike Duration (ms, FWHM) — Pre vs Post by Treatment')\n",
    "    plt.ylabel('Width (ms)')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FR Box Plots\n",
    "Channel-level and per-pair mean firing rate distributions comparing CTZ vs VEH across all exported pairs.\n",
    "This cell discovers exports on disk and does not depend on prior variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained FR box plots (no dependency on df_pairs)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "# Resolve exports directory\n",
    "try:\n",
    "    EXPORTS_DIR\n",
    "except NameError:\n",
    "    try:\n",
    "        from mcs_mea_analysis.config import CONFIG\n",
    "        REPO_ROOT = next((p for p in [Path.cwd(), *Path.cwd().parents] if (p/'mcs_mea_analysis').exists()), Path.cwd())\n",
    "        OUTPUT_ROOT = CONFIG.output_root if CONFIG.output_root.exists() else (REPO_ROOT / '_mcs_mea_outputs_local')\n",
    "        EXPORTS_DIR = OUTPUT_ROOT / 'exports' / 'spikes_waveforms'\n",
    "    except Exception:\n",
    "        EXPORTS_DIR = Path('/Volumes/Manny2TB/mcs_mea_outputs/exports/spikes_waveforms')\n",
    "print('Exports dir ->', EXPORTS_DIR)\n",
    "\n",
    "# Load all per-pair summary CSVs\n",
    "rows = []\n",
    "for csvp in EXPORTS_DIR.rglob('*_summary.csv'):\n",
    "    try:\n",
    "        # Extract plate/round from path\n",
    "        round_name = csvp.parents[1].name if len(csvp.parents) > 1 else None\n",
    "        plate = None\n",
    "        try:\n",
    "            ps = csvp.parent.name\n",
    "            plate = int(ps.replace('plate_', '')) if ps.startswith('plate_') else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        base = csvp.stem.replace('_summary','')\n",
    "        pair_id = base\n",
    "        df = pd.read_csv(csvp)\n",
    "        # Types\n",
    "        df['fr_hz'] = pd.to_numeric(df['fr_hz'], errors='coerce')\n",
    "        df['channel'] = pd.to_numeric(df['channel'], errors='coerce').astype('Int64')\n",
    "        df['side'] = df['side'].astype(str)\n",
    "        df['pair_id'] = pair_id\n",
    "        df['plate'] = plate\n",
    "        df['round'] = round_name\n",
    "        rows.append(df)\n",
    "    except Exception as e:\n",
    "        print('Skip', csvp.name, ':', e)\n",
    "\n",
    "data = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['channel','side','n_spikes','fr_hz','pair_id','plate','round'])\n",
    "print('Rows loaded:', len(data), 'from', len(rows), 'pairs')\n",
    "\n",
    "if data.empty:\n",
    "    print('No summary CSVs found under', EXPORTS_DIR)\n",
    "else:\n",
    "    # Channel-level box plot (CTZ vs VEH)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if sns is not None:\n",
    "        sns.boxplot(data=data, x='side', y='fr_hz')\n",
    "        sns.stripplot(data=data, x='side', y='fr_hz', color='k', size=2, alpha=0.3)\n",
    "    else:\n",
    "        # Fallback to matplotlib only\n",
    "        groups = [data[data['side']=='CTZ']['fr_hz'].dropna(), data[data['side']=='VEH']['fr_hz'].dropna()]\n",
    "        plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "    plt.title('Firing Rate (Hz) by Treatment — Channel Level')\n",
    "    plt.ylabel('FR (Hz)'); plt.xlabel('')\n",
    "    plt.show()\n",
    "\n",
    "    # Per-pair mean FR box plot\n",
    "    mean_per_pair = (\n",
    "        data.groupby(['pair_id','side'], as_index=False)['fr_hz'].mean()\n",
    "        .rename(columns={'fr_hz':'mean_fr_hz'})\n",
    "    )\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if sns is not None:\n",
    "        sns.boxplot(data=mean_per_pair, x='side', y='mean_fr_hz')\n",
    "        sns.stripplot(data=mean_per_pair, x='side', y='mean_fr_hz', color='k', size=3, alpha=0.5)\n",
    "    else:\n",
    "        groups = [mean_per_pair[mean_per_pair['side']=='CTZ']['mean_fr_hz'].dropna(),\n",
    "                  mean_per_pair[mean_per_pair['side']=='VEH']['mean_fr_hz'].dropna()]\n",
    "        plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "    plt.title('Mean Firing Rate (Hz) by Treatment — Per-Pair Means')\n",
    "    plt.ylabel('Mean FR (Hz)'); plt.xlabel('')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair-Level Summary\n",
    "Roll up aggregated channel rows into one row per exported pair (CTZ vs VEH),\n",
    "with mean FRs and mean ΔFR across the pair's channels (optionally filtered to accepted channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'aggregated_all' in globals() and not aggregated_all.empty:\n",
    "    group_cols = ['plate','round','ctz_stem','veh_stem']\n",
    "    pair_summary = (\n",
    "        aggregated_all\n",
    "        .groupby(group_cols)\n",
    "        .agg(n_channels=('channel','count'), ctz_mean_hz=('CTZ','mean'), veh_mean_hz=('VEH','mean'), delta_fr_mean_hz=('delta_fr_hz','mean'))\n",
    "        .reset_index()\n",
    "        .sort_values(group_cols)\n",
    "    )\n",
    "    print('Pairs summarized:', len(pair_summary))\n",
    "    display(pair_summary)\n",
    "else:\n",
    "    print('Aggregate all plates first to build `aggregated_all`.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate All Pairs\n",
    "Combine every exported pair across all plates into one DataFrame; optionally save a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all plates found in df_pairs\n",
    "if df_pairs.empty:\n",
    "    print('No pairs found. Run exports first.')\n",
    "    aggregated_all = pd.DataFrame(columns=['plate','round','ctz_stem','veh_stem','channel','CTZ','VEH','delta_fr_hz'])\n",
    "else:\n",
    "    plates = sorted(df_pairs['plate'].dropna().astype(int).unique().tolist())\n",
    "    frames = []\n",
    "    for p in plates:\n",
    "        g = aggregate_plate(int(p), use_selections=True)\n",
    "        if not g.empty:\n",
    "            frames.append(g)\n",
    "    aggregated_all = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=['plate','round','ctz_stem','veh_stem','channel','CTZ','VEH','delta_fr_hz'])\n",
    "    print('Aggregated rows (all plates):', len(aggregated_all))\n",
    "    display(aggregated_all.head(20))\n",
    "\n",
    "# Quick rollups\n",
    "if not aggregated_all.empty:\n",
    "    print('\nDelta FR mean per plate:')\n",
    "    display(aggregated_all.groupby('plate')['delta_fr_hz'].mean().to_frame('delta_fr_mean_hz'))\n",
    "    print('\nCounts per plate:')\n",
    "    display(aggregated_all.groupby('plate').size().to_frame('n_rows'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a single combined CSV (all plates)\n",
    "if 'aggregated_all' in globals() and not aggregated_all.empty:\n",
    "    out_csv_all = EXPORTS_DIR / 'all_plates_aggregate_summary.csv'\n",
    "    aggregated_all.to_csv(out_csv_all, index=False)\n",
    "    print('Wrote ->', out_csv_all)\n",
    "else:\n",
    "    print('Nothing to save (no aggregated data).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs Overview\n",
    "Quick ways to see how many pairs were discovered and which recordings they are.\n",
    "\n",
    "- `df_pairs` is the discovered pairs index (one row per exported pair).\n",
    "- Use the cells below to summarize counts and list pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary counts and listing of pairs\n",
    "if df_pairs.empty:\n",
    "    print('No pairs found. Run GUI export or batch exporter, then re-run discovery.')\n",
    "else:\n",
    "    print('Total pairs:', len(df_pairs))\n",
    "    print('\nPairs per plate:')\n",
    "    display(df_pairs.groupby('plate').size().to_frame('n_pairs').reset_index().sort_values('plate'))\n",
    "    print('\nPairs per plate and round:')\n",
    "    display(df_pairs.groupby(['plate','round']).size().to_frame('n_pairs').reset_index().sort_values(['plate','round']))\n",
    "    print('\nPair list (plate, round, stems):')\n",
    "    cols = ['plate','round','ctz_stem','veh_stem']\n",
    "    display(df_pairs[cols].sort_values(cols).reset_index(drop=True))\n",
    "    # Example filter: set a plate number you care about\n",
    "    # plate_focus = 5\n",
    "    # display(df_pairs[df_pairs['plate']==plate_focus][cols].sort_values(cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%gui qt5  # harmless outside Qt\n",
    "import sys, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Ensure repo root on path\n",
    "def _ensure_repo_on_path():\n",
    "    here = Path.cwd()\n",
    "    for cand in [here, *here.parents]:\n",
    "        if (cand / 'mcs_mea_analysis').exists():\n",
    "            if str(cand) not in sys.path:\n",
    "                sys.path.insert(0, str(cand))\n",
    "            return cand\n",
    "    return here\n",
    "REPO_ROOT = _ensure_repo_on_path()\n",
    "\n",
    "from mcs_mea_analysis.config import CONFIG\n",
    "\n",
    "# Pick output root: external drive if present, else local mirror\n",
    "OUTPUT_ROOT = CONFIG.output_root if CONFIG.output_root.exists() else (REPO_ROOT / '_mcs_mea_outputs_local')\n",
    "EXPORTS_DIR = OUTPUT_ROOT / 'exports' / 'spikes_waveforms'\n",
    "print('Using OUTPUT_ROOT ->', OUTPUT_ROOT)\n",
    "print('Exports dir ->', EXPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover exported pairs (HDF5 + CSV summary)\n",
    "pairs = []\n",
    "for h5 in EXPORTS_DIR.rglob('*.h5'):\n",
    "    if h5.name.endswith('_summary.h5'):\n",
    "        continue\n",
    "    # Path structure: .../<round>/plate_<N>/<CTZ>__VS__<VEH>.h5\n",
    "    try:\n",
    "        round_name = h5.parents[1].name  # immediate parent is plate_*, next is round\n",
    "        plate_str = h5.parent.name\n",
    "        plate = int(plate_str.replace('plate_', '')) if plate_str.startswith('plate_') else None\n",
    "    except Exception:\n",
    "        round_name, plate = None, None\n",
    "    base = h5.stem\n",
    "    if '__VS__' in base:\n",
    "        ctz_stem, veh_stem = base.split('__VS__', 1)\n",
    "    else:\n",
    "        ctz_stem, veh_stem = base, ''\n",
    "    csv_sum = h5.with_name(h5.stem + '_summary.csv')\n",
    "    pairs.append({\n",
    "        'round': round_name, 'plate': plate,\n",
    "        'ctz_stem': ctz_stem, 'veh_stem': veh_stem,\n",
    "        'h5_path': str(h5), 'csv_summary': str(csv_sum) if csv_sum.exists() else ''\n",
    "    })\n",
    "\n",
    "df_pairs = pd.DataFrame(pairs).sort_values(['plate','round','ctz_stem']).reset_index(drop=True)\n",
    "print('Found pairs:', len(df_pairs))\n",
    "df_pairs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to read exported HDF5\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "def read_pair_attrs(h5_path: Path) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    with h5py.File(h5_path.as_posix(), 'r') as f:\n",
    "        # Root attrs\n",
    "        for k, v in f.attrs.items():\n",
    "            out[k] = v\n",
    "        # Config JSON datasets\n",
    "        def _json_of(name: str):\n",
    "            if name in f:\n",
    "                try:\n",
    "                    return json.loads(f[name][()].decode())\n",
    "                except Exception:\n",
    "                    return None\n",
    "            return None\n",
    "        out['filter_config'] = _json_of('filter_config_json')\n",
    "        out['detect_config'] = _json_of('detect_config_json')\n",
    "        # Group-level attrs\n",
    "        for side in ('CTZ','VEH'):\n",
    "            if side in f:\n",
    "                g = f[side]\n",
    "                out[f'{side.lower()}_sr_hz'] = float(g.attrs.get('sr_hz', 0.0))\n",
    "                out[f'{side.lower()}_baseline_bounds'] = g.attrs.get('baseline_bounds', '')\n",
    "                out[f'{side.lower()}_analysis_bounds'] = g.attrs.get('analysis_bounds', '')\n",
    "    return out\n",
    "\n",
    "def load_channel(h5_path: Path, side: str, ch: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (time, raw, filtered, timestamps, waveforms) for one channel.\n",
    "    side in {'CTZ','VEH'}.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path.as_posix(), 'r') as f:\n",
    "        g = f[side]\n",
    "        t = g[f'ch{ch:02d}_time'][:] if f'ch{ch:02d}_time' in g else np.empty(0)\n",
    "        raw = g[f'ch{ch:02d}_raw'][:] if f'ch{ch:02d}_raw' in g else np.empty(0)\n",
    "        fil = g[f'ch{ch:02d}_filtered'][:] if f'ch{ch:02d}_filtered' in g else np.empty(0)\n",
    "        ts = g[f'ch{ch:02d}_timestamps'][:] if f'ch{ch:02d}_timestamps' in g else np.empty(0)\n",
    "        wf = g[f'ch{ch:02d}_waveforms'][:] if f'ch{ch:02d}_waveforms' in g else np.empty((0,0))\n",
    "    return t, raw, fil, ts, wf\n",
    "\n",
    "def n_channels(h5_path: Path, side: str='CTZ') -> int:\n",
    "    with h5py.File(h5_path.as_posix(), 'r') as f:\n",
    "        g = f[side]\n",
    "        # count by presence of ch00_time datasets\n",
    "        k = [k for k in g.keys() if k.startswith('ch') and k.endswith('_time')]\n",
    "        return len(k)\n",
    "\n",
    "def read_summary_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame(columns=['channel','side','n_spikes','fr_hz'])\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['channel'] = df['channel'].astype(int)\n",
    "    df['side'] = df['side'].astype(str)\n",
    "    df['n_spikes'] = df['n_spikes'].astype(int)\n",
    "    df['fr_hz'] = df['fr_hz'].astype(float)\n",
    "    return df\n",
    "\n",
    "def load_selections_if_any(output_root: Path, plate: Optional[int], ctz_stem: str, veh_stem: str) -> Dict[int, str]:\n",
    "    sel_dir = output_root / 'selections'\n",
    "    sel_name = f\"plate_{plate or 'NA'}__{ctz_stem}_ifr_per_channel_1ms__{veh_stem}_ifr_per_channel_1ms.json\"\n",
    "    # Fallback: scan selections dir for matching stems if pattern differs\n",
    "    sel = {}\n",
    "    if not sel_dir.exists():\n",
    "        return sel\n",
    "    for p in sel_dir.glob('*.json'):\n",
    "        try:\n",
    "            data = json.loads(p.read_text())\n",
    "            if (ctz_stem in str(data.get('ctz_npz',''))) and (veh_stem in str(data.get('veh_npz',''))):\n",
    "                d = data.get('selections') or {}\n",
    "                sel = {int(k): str(v) for k, v in d.items()}\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    return sel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one pair\n",
    "if len(df_pairs):\n",
    "    i = 0  # change index to pick a pair\n",
    "    r = df_pairs.iloc[i]\n",
    "    h5p = Path(r['h5_path'])\n",
    "    attrs = read_pair_attrs(h5p)\n",
    "    print('Pair:', r['plate'], r['round'], r['ctz_stem'], 'VS', r['veh_stem'])\n",
    "    print('Attrs keys:', sorted(attrs.keys()))\n",
    "    print('CTZ sr_hz:', attrs.get('ctz_sr_hz'), '| VEH sr_hz:', attrs.get('veh_sr_hz'))\n",
    "    # Load channel 0 arrays (if present)\n",
    "    t, raw, fil, ts, wf = load_channel(h5p, 'CTZ', 0)\n",
    "    print('CTZ ch00 shapes: t/raw/fil:', t.shape, raw.shape, fil.shape, '| spikes:', ts.shape, '| wf:', wf.shape)\n",
    "else:\n",
    "    print('No exports found. Run GUI export or batch exporter first.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-plate aggregate summary (joins CTZ/VEH FR per channel; optional selections)\n",
    "def aggregate_plate(plate: int, use_selections: bool = True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    dpf = df_pairs[df_pairs['plate'] == plate]\n",
    "    for _, r in dpf.iterrows():\n",
    "        h5p = Path(r['h5_path'])\n",
    "        csvp = Path(r['csv_summary'])\n",
    "        attrs = read_pair_attrs(h5p)\n",
    "        df = read_summary_csv(csvp)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # Optional: apply selections\n",
    "        accepted = None\n",
    "        if use_selections:\n",
    "            sel = load_selections_if_any(OUTPUT_ROOT, plate, r['ctz_stem'], r['veh_stem'])\n",
    "            accepted = {ch for ch, v in sel.items() if str(v).lower() == 'accept'} if sel else None\n",
    "        # pivot to have CTZ and VEH FR columns per channel\n",
    "        piv = df.pivot_table(index='channel', columns='side', values='fr_hz', aggfunc='first').reset_index()\n",
    "        piv['plate'] = plate\n",
    "        piv['round'] = r['round']\n",
    "        piv['ctz_stem'] = r['ctz_stem']\n",
    "        piv['veh_stem'] = r['veh_stem']\n",
    "        if 'CTZ' not in piv.columns: piv['CTZ'] = np.nan\n",
    "        if 'VEH' not in piv.columns: piv['VEH'] = np.nan\n",
    "        piv['delta_fr_hz'] = piv['CTZ'] - piv['VEH']\n",
    "        if accepted is not None:\n",
    "            piv['accepted'] = piv['channel'].isin(accepted)\n",
    "            piv = piv[piv['accepted']].copy()\n",
    "        rows.append(piv)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['plate','round','ctz_stem','veh_stem','channel','CTZ','VEH','delta_fr_hz'])\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "# Example: aggregate one plate (set your plate number)\n",
    "if len(df_pairs) and df_pairs['plate'].notna().any():\n",
    "    plate_example = int(df_pairs['plate'].dropna().iloc[0])\n",
    "    agg = aggregate_plate(plate_example, use_selections=True)\n",
    "    print('Aggregated rows:', len(agg))\n",
    "    display(agg.head(10))\n",
    "else:\n",
    "    print('No plate numbers found in exports.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-plate aggregates to CSV for downstream analysis\n",
    "def write_plate_aggregate_csv(plate: int, use_selections: bool = True) -> Path:\n",
    "    agg = aggregate_plate(plate, use_selections=use_selections)\n",
    "    out_dir = EXPORTS_DIR / f'plate_{plate}'\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = out_dir / f'plate_{plate}_aggregate_summary.csv'\n",
    "    agg.to_csv(out_csv, index=False)\n",
    "    print('Wrote ->', out_csv)\n",
    "    return out_csv\n",
    "\n",
    "# Example (uncomment to run):\n",
    "# write_plate_aggregate_csv(plate_example, use_selections=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- HDF5 data types are float64; CSV fields are typed on load as indicated.\n",
    "- The analysis window for FR is `post_s` (stored in HDF5 root attrs).\n",
    "- To inspect waveforms programmatically, use `load_channel(...)[-1]` to get an `n_spikes × n_snippet` array.\n",
    "- Use selections JSON (if available) to focus aggregation on accepted channels.\n",
    "- You can extend aggregation to compute pre/post baselines, averages of waveforms, etc., using the same loaders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
