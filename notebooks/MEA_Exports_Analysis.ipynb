{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEA Exports Analysis — Spikes + Waveforms\n",
    "\n",
    "Analyze and aggregate the per-pair exports produced by the Pair Viewer or the batch exporter.\n",
    "\n",
    "- Exports location: `<output_root>/exports/spikes_waveforms/...`\n",
    "- Format details: see `docs/exports_spikes_waveforms.md` in this repo (variable types, dataset names, and attributes).\n",
    "- This notebook auto-discovers pairs, loads HDF5/CSV per pair, and can aggregate per-plate.\n",
    "\n",
    "Tip: If your external drive is not mounted, set `OUTPUT_ROOT` to `_mcs_mea_outputs_local`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FR Box Plots\n",
    "Channel-level and per-pair mean firing rate distributions comparing CTZ vs VEH across all exported pairs.\n",
    "This cell discovers exports on disk and does not depend on prior variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-contained FR box plots (no dependency on df_pairs)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except Exception:\n",
    "    sns = None\n",
    "\n",
    "# Resolve exports directory\n",
    "try:\n",
    "    EXPORTS_DIR\n",
    "except NameError:\n",
    "    try:\n",
    "        from mcs_mea_analysis.config import CONFIG\n",
    "        REPO_ROOT = next((p for p in [Path.cwd(), *Path.cwd().parents] if (p/'mcs_mea_analysis').exists()), Path.cwd())\n",
    "        OUTPUT_ROOT = CONFIG.output_root if CONFIG.output_root.exists() else (REPO_ROOT / '_mcs_mea_outputs_local')\n",
    "        EXPORTS_DIR = OUTPUT_ROOT / 'exports' / 'spikes_waveforms'\n",
    "    except Exception:\n",
    "        EXPORTS_DIR = Path('/Volumes/Manny2TB/mcs_mea_outputs/exports/spikes_waveforms')\n",
    "print('Exports dir ->', EXPORTS_DIR)\n",
    "\n",
    "# Load all per-pair summary CSVs\n",
    "rows = []\n",
    "for csvp in EXPORTS_DIR.rglob('*_summary.csv'):\n",
    "    try:\n",
    "        # Extract plate/round from path\n",
    "        round_name = csvp.parents[1].name if len(csvp.parents) > 1 else None\n",
    "        plate = None\n",
    "        try:\n",
    "            ps = csvp.parent.name\n",
    "            plate = int(ps.replace('plate_', '')) if ps.startswith('plate_') else None\n",
    "        except Exception:\n",
    "            pass\n",
    "        base = csvp.stem.replace('_summary','')\n",
    "        pair_id = base\n",
    "        df = pd.read_csv(csvp)\n",
    "        # Types\n",
    "        df['fr_hz'] = pd.to_numeric(df['fr_hz'], errors='coerce')\n",
    "        df['channel'] = pd.to_numeric(df['channel'], errors='coerce').astype('Int64')\n",
    "        df['side'] = df['side'].astype(str)\n",
    "        df['pair_id'] = pair_id\n",
    "        df['plate'] = plate\n",
    "        df['round'] = round_name\n",
    "        rows.append(df)\n",
    "    except Exception as e:\n",
    "        print('Skip', csvp.name, ':', e)\n",
    "\n",
    "data = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['channel','side','n_spikes','fr_hz','pair_id','plate','round'])\n",
    "print('Rows loaded:', len(data), 'from', len(rows), 'pairs')\n",
    "\n",
    "if data.empty:\n",
    "    print('No summary CSVs found under', EXPORTS_DIR)\n",
    "else:\n",
    "    # Channel-level box plot (CTZ vs VEH)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if sns is not None:\n",
    "        sns.boxplot(data=data, x='side', y='fr_hz')\n",
    "        sns.stripplot(data=data, x='side', y='fr_hz', color='k', size=2, alpha=0.3)\n",
    "    else:\n",
    "        # Fallback to matplotlib only\n",
    "        groups = [data[data['side']=='CTZ']['fr_hz'].dropna(), data[data['side']=='VEH']['fr_hz'].dropna()]\n",
    "        plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "    plt.title('Firing Rate (Hz) by Treatment — Channel Level')\n",
    "    plt.ylabel('FR (Hz)'); plt.xlabel('')\n",
    "    plt.show()\n",
    "\n",
    "    # Per-pair mean FR box plot\n",
    "    mean_per_pair = (\n",
    "        data.groupby(['pair_id','side'], as_index=False)['fr_hz'].mean()\n",
    "        .rename(columns={'fr_hz':'mean_fr_hz'})\n",
    "    )\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if sns is not None:\n",
    "        sns.boxplot(data=mean_per_pair, x='side', y='mean_fr_hz')\n",
    "        sns.stripplot(data=mean_per_pair, x='side', y='mean_fr_hz', color='k', size=3, alpha=0.5)\n",
    "    else:\n",
    "        groups = [mean_per_pair[mean_per_pair['side']=='CTZ']['mean_fr_hz'].dropna(),\n",
    "                  mean_per_pair[mean_per_pair['side']=='VEH']['mean_fr_hz'].dropna()]\n",
    "        plt.boxplot(groups, labels=['CTZ','VEH'])\n",
    "    plt.title('Mean Firing Rate (Hz) by Treatment — Per-Pair Means')\n",
    "    plt.ylabel('Mean FR (Hz)'); plt.xlabel('')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair-Level Summary\n",
    "Roll up aggregated channel rows into one row per exported pair (CTZ vs VEH),\n",
    "with mean FRs and mean ΔFR across the pair's channels (optionally filtered to accepted channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'aggregated_all' in globals() and not aggregated_all.empty:\n",
    "    group_cols = ['plate','round','ctz_stem','veh_stem']\n",
    "    pair_summary = (\n",
    "        aggregated_all\n",
    "        .groupby(group_cols)\n",
    "        .agg(n_channels=('channel','count'), ctz_mean_hz=('CTZ','mean'), veh_mean_hz=('VEH','mean'), delta_fr_mean_hz=('delta_fr_hz','mean'))\n",
    "        .reset_index()\n",
    "        .sort_values(group_cols)\n",
    "    )\n",
    "    print('Pairs summarized:', len(pair_summary))\n",
    "    display(pair_summary)\n",
    "else:\n",
    "    print('Aggregate all plates first to build `aggregated_all`.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate All Pairs\n",
    "Combine every exported pair across all plates into one DataFrame; optionally save a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all plates found in df_pairs\n",
    "if df_pairs.empty:\n",
    "    print('No pairs found. Run exports first.')\n",
    "    aggregated_all = pd.DataFrame(columns=['plate','round','ctz_stem','veh_stem','channel','CTZ','VEH','delta_fr_hz'])\n",
    "else:\n",
    "    plates = sorted(df_pairs['plate'].dropna().astype(int).unique().tolist())\n",
    "    frames = []\n",
    "    for p in plates:\n",
    "        g = aggregate_plate(int(p), use_selections=True)\n",
    "        if not g.empty:\n",
    "            frames.append(g)\n",
    "    aggregated_all = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=['plate','round','ctz_stem','veh_stem','channel','CTZ','VEH','delta_fr_hz'])\n",
    "    print('Aggregated rows (all plates):', len(aggregated_all))\n",
    "    display(aggregated_all.head(20))\n",
    "\n",
    "# Quick rollups\n",
    "if not aggregated_all.empty:\n",
    "    print('\nDelta FR mean per plate:')\n",
    "    display(aggregated_all.groupby('plate')['delta_fr_hz'].mean().to_frame('delta_fr_mean_hz'))\n",
    "    print('\nCounts per plate:')\n",
    "    display(aggregated_all.groupby('plate').size().to_frame('n_rows'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a single combined CSV (all plates)\n",
    "if 'aggregated_all' in globals() and not aggregated_all.empty:\n",
    "    out_csv_all = EXPORTS_DIR / 'all_plates_aggregate_summary.csv'\n",
    "    aggregated_all.to_csv(out_csv_all, index=False)\n",
    "    print('Wrote ->', out_csv_all)\n",
    "else:\n",
    "    print('Nothing to save (no aggregated data).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs Overview\n",
    "Quick ways to see how many pairs were discovered and which recordings they are.\n",
    "\n",
    "- `df_pairs` is the discovered pairs index (one row per exported pair).\n",
    "- Use the cells below to summarize counts and list pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary counts and listing of pairs\n",
    "if df_pairs.empty:\n",
    "    print('No pairs found. Run GUI export or batch exporter, then re-run discovery.')\n",
    "else:\n",
    "    print('Total pairs:', len(df_pairs))\n",
    "    print('\nPairs per plate:')\n",
    "    display(df_pairs.groupby('plate').size().to_frame('n_pairs').reset_index().sort_values('plate'))\n",
    "    print('\nPairs per plate and round:')\n",
    "    display(df_pairs.groupby(['plate','round']).size().to_frame('n_pairs').reset_index().sort_values(['plate','round']))\n",
    "    print('\nPair list (plate, round, stems):')\n",
    "    cols = ['plate','round','ctz_stem','veh_stem']\n",
    "    display(df_pairs[cols].sort_values(cols).reset_index(drop=True))\n",
    "    # Example filter: set a plate number you care about\n",
    "    # plate_focus = 5\n",
    "    # display(df_pairs[df_pairs['plate']==plate_focus][cols].sort_values(cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%gui qt5  # harmless outside Qt\n",
    "import sys, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Ensure repo root on path\n",
    "def _ensure_repo_on_path():\n",
    "    here = Path.cwd()\n",
    "    for cand in [here, *here.parents]:\n",
    "        if (cand / 'mcs_mea_analysis').exists():\n",
    "            if str(cand) not in sys.path:\n",
    "                sys.path.insert(0, str(cand))\n",
    "            return cand\n",
    "    return here\n",
    "REPO_ROOT = _ensure_repo_on_path()\n",
    "\n",
    "from mcs_mea_analysis.config import CONFIG\n",
    "\n",
    "# Pick output root: external drive if present, else local mirror\n",
    "OUTPUT_ROOT = CONFIG.output_root if CONFIG.output_root.exists() else (REPO_ROOT / '_mcs_mea_outputs_local')\n",
    "EXPORTS_DIR = OUTPUT_ROOT / 'exports' / 'spikes_waveforms'\n",
    "print('Using OUTPUT_ROOT ->', OUTPUT_ROOT)\n",
    "print('Exports dir ->', EXPORTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover exported pairs (HDF5 + CSV summary)\n",
    "pairs = []\n",
    "for h5 in EXPORTS_DIR.rglob('*.h5'):\n",
    "    if h5.name.endswith('_summary.h5'):\n",
    "        continue\n",
    "    # Path structure: .../<round>/plate_<N>/<CTZ>__VS__<VEH>.h5\n",
    "    try:\n",
    "        round_name = h5.parents[1].name  # immediate parent is plate_*, next is round\n",
    "        plate_str = h5.parent.name\n",
    "        plate = int(plate_str.replace('plate_', '')) if plate_str.startswith('plate_') else None\n",
    "    except Exception:\n",
    "        round_name, plate = None, None\n",
    "    base = h5.stem\n",
    "    if '__VS__' in base:\n",
    "        ctz_stem, veh_stem = base.split('__VS__', 1)\n",
    "    else:\n",
    "        ctz_stem, veh_stem = base, ''\n",
    "    csv_sum = h5.with_name(h5.stem + '_summary.csv')\n",
    "    pairs.append({\n",
    "        'round': round_name, 'plate': plate,\n",
    "        'ctz_stem': ctz_stem, 'veh_stem': veh_stem,\n",
    "        'h5_path': str(h5), 'csv_summary': str(csv_sum) if csv_sum.exists() else ''\n",
    "    })\n",
    "\n",
    "df_pairs = pd.DataFrame(pairs).sort_values(['plate','round','ctz_stem']).reset_index(drop=True)\n",
    "print('Found pairs:', len(df_pairs))\n",
    "df_pairs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to read exported HDF5\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "def read_pair_attrs(h5_path: Path) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    with h5py.File(h5_path.as_posix(), 'r') as f:\n",
    "        # Root attrs\n",
    "        for k, v in f.attrs.items():\n",
    "            out[k] = v\n",
    "        # Config JSON datasets\n",
    "        def _json_of(name: str):\n",
    "            if name in f:\n",
    "                try:\n",
    "                    return json.loads(f[name][()].decode())\n",
    "                except Exception:\n",
    "                    return None\n",
    "            return None\n",
    "        out['filter_config'] = _json_of('filter_config_json')\n",
    "        out['detect_config'] = _json_of('detect_config_json')\n",
    "        # Group-level attrs\n",
    "        for side in ('CTZ','VEH'):\n",
    "            if side in f:\n",
    "                g = f[side]\n",
    "                out[f'{side.lower()}_sr_hz'] = float(g.attrs.get('sr_hz', 0.0))\n",
    "                out[f'{side.lower()}_baseline_bounds'] = g.attrs.get('baseline_bounds', '')\n",
    "                out[f'{side.lower()}_analysis_bounds'] = g.attrs.get('analysis_bounds', '')\n",
    "    return out\n",
    "\n",
    "def load_channel(h5_path: Path, side: str, ch: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return (time, raw, filtered, timestamps, waveforms) for one channel.\n",
    "    side in {'CTZ','VEH'}.\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path.as_posix(), 'r') as f:\n",
    "        g = f[side]\n",
    "        t = g[f'ch{ch:02d}_time'][:] if f'ch{ch:02d}_time' in g else np.empty(0)\n",
    "        raw = g[f'ch{ch:02d}_raw'][:] if f'ch{ch:02d}_raw' in g else np.empty(0)\n",
    "        fil = g[f'ch{ch:02d}_filtered'][:] if f'ch{ch:02d}_filtered' in g else np.empty(0)\n",
    "        ts = g[f'ch{ch:02d}_timestamps'][:] if f'ch{ch:02d}_timestamps' in g else np.empty(0)\n",
    "        wf = g[f'ch{ch:02d}_waveforms'][:] if f'ch{ch:02d}_waveforms' in g else np.empty((0,0))\n",
    "    return t, raw, fil, ts, wf\n",
    "\n",
    "def n_channels(h5_path: Path, side: str='CTZ') -> int:\n",
    "    with h5py.File(h5_path.as_posix(), 'r') as f:\n",
    "        g = f[side]\n",
    "        # count by presence of ch00_time datasets\n",
    "        k = [k for k in g.keys() if k.startswith('ch') and k.endswith('_time')]\n",
    "        return len(k)\n",
    "\n",
    "def read_summary_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    if not csv_path.exists():\n",
    "        return pd.DataFrame(columns=['channel','side','n_spikes','fr_hz'])\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['channel'] = df['channel'].astype(int)\n",
    "    df['side'] = df['side'].astype(str)\n",
    "    df['n_spikes'] = df['n_spikes'].astype(int)\n",
    "    df['fr_hz'] = df['fr_hz'].astype(float)\n",
    "    return df\n",
    "\n",
    "def load_selections_if_any(output_root: Path, plate: Optional[int], ctz_stem: str, veh_stem: str) -> Dict[int, str]:\n",
    "    sel_dir = output_root / 'selections'\n",
    "    sel_name = f\"plate_{plate or 'NA'}__{ctz_stem}_ifr_per_channel_1ms__{veh_stem}_ifr_per_channel_1ms.json\"\n",
    "    # Fallback: scan selections dir for matching stems if pattern differs\n",
    "    sel = {}\n",
    "    if not sel_dir.exists():\n",
    "        return sel\n",
    "    for p in sel_dir.glob('*.json'):\n",
    "        try:\n",
    "            data = json.loads(p.read_text())\n",
    "            if (ctz_stem in str(data.get('ctz_npz',''))) and (veh_stem in str(data.get('veh_npz',''))):\n",
    "                d = data.get('selections') or {}\n",
    "                sel = {int(k): str(v) for k, v in d.items()}\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    return sel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one pair\n",
    "if len(df_pairs):\n",
    "    i = 0  # change index to pick a pair\n",
    "    r = df_pairs.iloc[i]\n",
    "    h5p = Path(r['h5_path'])\n",
    "    attrs = read_pair_attrs(h5p)\n",
    "    print('Pair:', r['plate'], r['round'], r['ctz_stem'], 'VS', r['veh_stem'])\n",
    "    print('Attrs keys:', sorted(attrs.keys()))\n",
    "    print('CTZ sr_hz:', attrs.get('ctz_sr_hz'), '| VEH sr_hz:', attrs.get('veh_sr_hz'))\n",
    "    # Load channel 0 arrays (if present)\n",
    "    t, raw, fil, ts, wf = load_channel(h5p, 'CTZ', 0)\n",
    "    print('CTZ ch00 shapes: t/raw/fil:', t.shape, raw.shape, fil.shape, '| spikes:', ts.shape, '| wf:', wf.shape)\n",
    "else:\n",
    "    print('No exports found. Run GUI export or batch exporter first.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build per-plate aggregate summary (joins CTZ/VEH FR per channel; optional selections)\n",
    "def aggregate_plate(plate: int, use_selections: bool = True) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    dpf = df_pairs[df_pairs['plate'] == plate]\n",
    "    for _, r in dpf.iterrows():\n",
    "        h5p = Path(r['h5_path'])\n",
    "        csvp = Path(r['csv_summary'])\n",
    "        attrs = read_pair_attrs(h5p)\n",
    "        df = read_summary_csv(csvp)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        # Optional: apply selections\n",
    "        accepted = None\n",
    "        if use_selections:\n",
    "            sel = load_selections_if_any(OUTPUT_ROOT, plate, r['ctz_stem'], r['veh_stem'])\n",
    "            accepted = {ch for ch, v in sel.items() if str(v).lower() == 'accept'} if sel else None\n",
    "        # pivot to have CTZ and VEH FR columns per channel\n",
    "        piv = df.pivot_table(index='channel', columns='side', values='fr_hz', aggfunc='first').reset_index()\n",
    "        piv['plate'] = plate\n",
    "        piv['round'] = r['round']\n",
    "        piv['ctz_stem'] = r['ctz_stem']\n",
    "        piv['veh_stem'] = r['veh_stem']\n",
    "        if 'CTZ' not in piv.columns: piv['CTZ'] = np.nan\n",
    "        if 'VEH' not in piv.columns: piv['VEH'] = np.nan\n",
    "        piv['delta_fr_hz'] = piv['CTZ'] - piv['VEH']\n",
    "        if accepted is not None:\n",
    "            piv['accepted'] = piv['channel'].isin(accepted)\n",
    "            piv = piv[piv['accepted']].copy()\n",
    "        rows.append(piv)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['plate','round','ctz_stem','veh_stem','channel','CTZ','VEH','delta_fr_hz'])\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "# Example: aggregate one plate (set your plate number)\n",
    "if len(df_pairs) and df_pairs['plate'].notna().any():\n",
    "    plate_example = int(df_pairs['plate'].dropna().iloc[0])\n",
    "    agg = aggregate_plate(plate_example, use_selections=True)\n",
    "    print('Aggregated rows:', len(agg))\n",
    "    display(agg.head(10))\n",
    "else:\n",
    "    print('No plate numbers found in exports.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-plate aggregates to CSV for downstream analysis\n",
    "def write_plate_aggregate_csv(plate: int, use_selections: bool = True) -> Path:\n",
    "    agg = aggregate_plate(plate, use_selections=use_selections)\n",
    "    out_dir = EXPORTS_DIR / f'plate_{plate}'\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_csv = out_dir / f'plate_{plate}_aggregate_summary.csv'\n",
    "    agg.to_csv(out_csv, index=False)\n",
    "    print('Wrote ->', out_csv)\n",
    "    return out_csv\n",
    "\n",
    "# Example (uncomment to run):\n",
    "# write_plate_aggregate_csv(plate_example, use_selections=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- HDF5 data types are float64; CSV fields are typed on load as indicated.\n",
    "- The analysis window for FR is `post_s` (stored in HDF5 root attrs).\n",
    "- To inspect waveforms programmatically, use `load_channel(...)[-1]` to get an `n_spikes × n_snippet` array.\n",
    "- Use selections JSON (if available) to focus aggregation on accepted channels.\n",
    "- You can extend aggregation to compute pre/post baselines, averages of waveforms, etc., using the same loaders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
